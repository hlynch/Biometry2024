<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>25 Week 13 Lab | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="25 Week 13 Lab | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="25 Week 13 Lab | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="25 Week 13 Lab | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-13-lecture.html"/>
<link rel="next" href="week-14-lecture.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#t-test"><i class="fa fa-check"></i><b>10.1</b> t-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.2</b> F-test</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.4" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.4</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-13-lab" class="section level1 hasAnchor" number="25">
<h1><span class="header-section-number">25</span> Week 13 Lab<a href="week-13-lab.html#week-13-lab" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In lab we’ll go through</p>
<ol style="list-style-type: decimal">
<li><p>Model selection / model comparison</p></li>
<li><p>Model criticism</p></li>
</ol>
<p><strong>We will need a series of packages for today’s lab, some of which we have not used before: MASS, lmtest, MuMIn, car, and gvlma.</strong></p>
<div id="part-1-model-selection-model-comparison" class="section level2 hasAnchor" number="25.1">
<h2><span class="header-section-number">25.1</span> Part 1: Model selection / model comparison<a href="week-13-lab.html#part-1-model-selection-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is a frog dataset on the distribution of the Southern Corroboree frog that we are going to attach. More information on the definition of the covariates in this dataset can be found <a href="https://www.key2stats.com/data-set/view/235">here</a>.</p>
<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb852-1"><a href="week-13-lab.html#cb852-1" tabindex="-1"></a>frogs<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;_data/frogs.csv&quot;</span>,<span class="at">header=</span>T)</span>
<span id="cb852-2"><a href="week-13-lab.html#cb852-2" tabindex="-1"></a><span class="fu">head</span>(frogs)</span></code></pre></div>
<pre><code>##   X pres.abs northing easting altitude distance NoOfPools NoOfSites   avrain
## 1 2        1      115    1047     1500      500       232         3 155.0000
## 2 3        1      110    1042     1520      250        66         5 157.6667
## 3 4        1      112    1040     1540      250        32         5 159.6667
## 4 5        1      109    1033     1590      250         9         5 165.0000
## 5 6        1      109    1032     1590      250        67         5 165.0000
## 6 7        1      106    1018     1600      500        12         4 167.3333
##    meanmin  meanmax
## 1 3.566667 14.00000
## 2 3.466667 13.80000
## 3 3.400000 13.60000
## 4 3.200000 13.16667
## 5 3.200000 13.16667
## 6 3.133333 13.06667</code></pre>
<div class="sourceCode" id="cb854"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb854-1"><a href="week-13-lab.html#cb854-1" tabindex="-1"></a><span class="fu">attach</span>(frogs)</span>
<span id="cb854-2"><a href="week-13-lab.html#cb854-2" tabindex="-1"></a><span class="fu">plot</span>(northing <span class="sc">~</span> easting, <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">16</span>)[frogs<span class="sc">$</span>pres.abs<span class="sc">+</span><span class="dv">1</span>],<span class="at">xlab=</span><span class="st">&quot;Meters east&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Meters north&quot;</span>)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Make sure you understand how the pch command is working in the above plot command.</p>
<div class="sourceCode" id="cb855"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb855-1"><a href="week-13-lab.html#cb855-1" tabindex="-1"></a><span class="fu">pairs</span>(<span class="fu">cbind</span>(altitude,distance,NoOfPools,NoOfSites,avrain,meanmin,meanmax))</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Looking at the data in this way, are there any covariates that might benefit from transformation?</p>
<p><strong>Question: Why bother transforming a covariate?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
As you can see from the scatterplots, some covariates have points with large leverage, and these point may (but don’t always) have high influence on model fit. Often we want all the points to have roughly equal influence on the model fit so we will apply a transformation like the log() to spread out small values all bunched together and to pull in large values that are much larger than the others. These transformations (e.hg., log, square root) are often referred to as “variance stabilizing” transformations.
</span>
</details>
<p>
 
</p>
<p>Let’s try log-transforming “distance” and “Number of Pools”.</p>
<div class="sourceCode" id="cb856"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb856-1"><a href="week-13-lab.html#cb856-1" tabindex="-1"></a><span class="fu">pairs</span>(<span class="fu">cbind</span>(altitude,<span class="fu">log</span>(distance),<span class="fu">log</span>(NoOfPools),NoOfSites,avrain,meanmin,meanmax))</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Let’s fit a GLM with all the variables.</p>
<p><strong><span style="color: green;">Checkpoint #1: Why a GLM?</span></strong> <span style="color: white;">Because the response variable is binary 0/1.</span></p>
<div class="sourceCode" id="cb857"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb857-1"><a href="week-13-lab.html#cb857-1" tabindex="-1"></a>frogs.glm0<span class="ot">&lt;-</span><span class="fu">glm</span>(pres.abs<span class="sc">~</span>altitude<span class="sc">+</span><span class="fu">log</span>(distance)<span class="sc">+</span><span class="fu">log</span>(NoOfPools)<span class="sc">+</span>NoOfSites<span class="sc">+</span>avrain<span class="sc">+</span>meanmin<span class="sc">+</span>meanmax,<span class="at">family=</span>binomial,<span class="at">data=</span>frogs,<span class="at">na.action=</span>na.fail)</span>
<span id="cb857-2"><a href="week-13-lab.html#cb857-2" tabindex="-1"></a><span class="fu">summary</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + 
##     NoOfSites + avrain + meanmin + meanmax, family = binomial, 
##     data = frogs, na.action = na.fail)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9795  -0.7193  -0.2785   0.7964   2.5658  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     4.090e+01  1.327e+02   0.308 0.757845    
## altitude       -6.648e-03  3.866e-02  -0.172 0.863466    
## log(distance)  -7.593e-01  2.554e-01  -2.973 0.002945 ** 
## log(NoOfPools)  5.727e-01  2.162e-01   2.649 0.008083 ** 
## NoOfSites      -8.979e-04  1.074e-01  -0.008 0.993330    
## avrain         -6.793e-03  5.999e-02  -0.113 0.909848    
## meanmin         5.305e+00  1.543e+00   3.439 0.000584 ***
## meanmax        -3.173e+00  4.839e+00  -0.656 0.512048    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 279.99  on 211  degrees of freedom
## Residual deviance: 197.62  on 204  degrees of freedom
## AIC: 213.62
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>(The na.action flag is so MuMIn won’t complain further down.)</p>
<p>Anything look funny? Well, for one, meanmin is highly significant but meanmax and altitude are not - but we would expect these three variables to be highly correlated.</p>
<p>Use the vif() function to explore this further</p>
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb859-1"><a href="week-13-lab.html#cb859-1" tabindex="-1"></a><span class="fu">vif</span>(frogs.glm0)</span></code></pre></div>
<pre><code>##       altitude  log(distance) log(NoOfPools)      NoOfSites         avrain 
##     850.879518       1.396014       1.306058       1.401175      16.664582 
##        meanmin        meanmax 
##      29.319704     996.949239</code></pre>
<p>It appears that the variances for altitude and meanmax are inflated. <strong><span style="color: green;">Checkpoint #2: Why?</span></strong></p>
<p>Plot the data:</p>
<div class="sourceCode" id="cb861"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb861-1"><a href="week-13-lab.html#cb861-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb861-2"><a href="week-13-lab.html#cb861-2" tabindex="-1"></a><span class="fu">plot</span>(altitude,meanmax)</span>
<span id="cb861-3"><a href="week-13-lab.html#cb861-3" tabindex="-1"></a><span class="fu">plot</span>(altitude,meanmin)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb862"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb862-1"><a href="week-13-lab.html#cb862-1" tabindex="-1"></a><span class="fu">cor</span>(altitude,meanmax)</span></code></pre></div>
<pre><code>## [1] -0.996557</code></pre>
<div class="sourceCode" id="cb864"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb864-1"><a href="week-13-lab.html#cb864-1" tabindex="-1"></a><span class="fu">cor</span>(altitude,meanmin)</span></code></pre></div>
<pre><code>## [1] -0.953661</code></pre>
<p>Question: So…what do we do?</p>
<p>Answer: Let’s try removing the least biologically significant variable first. I suggest we remove altitude.</p>
<div class="sourceCode" id="cb866"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb866-1"><a href="week-13-lab.html#cb866-1" tabindex="-1"></a>frogs.glm1<span class="ot">&lt;-</span><span class="fu">glm</span>(pres.abs<span class="sc">~</span><span class="fu">log</span>(distance)<span class="sc">+</span><span class="fu">log</span>(NoOfPools)<span class="sc">+</span>NoOfSites<span class="sc">+</span>avrain<span class="sc">+</span>meanmin<span class="sc">+</span>meanmax,<span class="at">family=</span>binomial,<span class="at">data=</span>frogs)</span>
<span id="cb866-2"><a href="week-13-lab.html#cb866-2" tabindex="-1"></a><span class="fu">summary</span>(frogs.glm1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax, family = binomial, data = frogs)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9763  -0.7189  -0.2786   0.7970   2.5745  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    18.2689000 16.1381912   1.132 0.257622    
## log(distance)  -0.7583198  0.2558117  -2.964 0.003033 ** 
## log(NoOfPools)  0.5708953  0.2153335   2.651 0.008020 ** 
## NoOfSites      -0.0036201  0.1061469  -0.034 0.972794    
## avrain          0.0007003  0.0411710   0.017 0.986429    
## meanmin         5.3540724  1.5254665   3.510 0.000448 ***
## meanmax        -2.3624614  1.0678821  -2.212 0.026947 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 279.99  on 211  degrees of freedom
## Residual deviance: 197.65  on 205  degrees of freedom
## AIC: 211.65
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Better, but we still have a lot of multicollinearity between meanmin and meanmax.</p>
<p>We could choose one or the other but at this point, let’s leave them both in and try and find the best model we can.</p>
<p>First, let’s do a little review of the three comparison criteria we discussed on Tuesday:</p>
<ol style="list-style-type: decimal">
<li><p>Likelihood (specifically, likelihood ratio)</p></li>
<li><p>Akaike’s Information Criteria (AIC)</p></li>
<li><p>Bayesian Information Criterion (BIC)</p></li>
</ol>
<p>We get the log-likelihood of a model in R using the logLik command</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb868-1"><a href="week-13-lab.html#cb868-1" tabindex="-1"></a><span class="fu">logLik</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -98.81244 (df=8)</code></pre>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb870-1"><a href="week-13-lab.html#cb870-1" tabindex="-1"></a><span class="fu">logLik</span>(frogs.glm1)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -98.82714 (df=7)</code></pre>
<p>How do we do the likelihood ratio test?</p>
<p>Remember that</p>
<p><span class="math display">\[
-2*(LL(smaller)-LL(bigger))
\sim \chi^{2}_{\mbox{difference in parameters}}
\]</span></p>
<p>In R we can do this with</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb872-1"><a href="week-13-lab.html#cb872-1" tabindex="-1"></a>test.stat<span class="ot">&lt;-</span><span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>(<span class="fu">logLik</span>(frogs.glm1)<span class="sc">-</span><span class="fu">logLik</span>(frogs.glm0))</span>
<span id="cb872-2"><a href="week-13-lab.html#cb872-2" tabindex="-1"></a><span class="fu">as.numeric</span>(test.stat) <span class="co"># as.numeric is just to suppress the labels which can be misleading</span></span></code></pre></div>
<pre><code>## [1] 0.02941079</code></pre>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb874-1"><a href="week-13-lab.html#cb874-1" tabindex="-1"></a><span class="fu">as.numeric</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pchisq</span>(test.stat,<span class="at">df=</span><span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.863834</code></pre>
<p><strong><span style="color: green;">Checkpoint #3: How do we interpret that p-value?</span></strong></p>
<p><span style="color: white;">Answer: This p-value is the probability that the larger model fits the data better only by the amount expected by its additional degree of freedom. In this case, we can not reject the null hypothesis that the two models are equivalent, and so we would prefer the smaller model on the basis of parsimony.</span></p>
<p>We can actually get R to do the LRT automatically using the ‘lrtest’ function in the ‘lmtest’ package.</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb876-1"><a href="week-13-lab.html#cb876-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;lmtest&#39;</span>)</span>
<span id="cb876-2"><a href="week-13-lab.html#cb876-2" tabindex="-1"></a><span class="fu">lrtest</span>(frogs.glm0,frogs.glm1)</span></code></pre></div>
<pre><code>## Likelihood ratio test
## 
## Model 1: pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax
## Model 2: pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + avrain + 
##     meanmin + meanmax
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)
## 1   8 -98.812                     
## 2   7 -98.827 -1 0.0294     0.8638</code></pre>
<p>This gives us exactly the same result we got before.</p>
<p>We could also have also compared these models by looking at the ANOVA table for comparison</p>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb878-1"><a href="week-13-lab.html#cb878-1" tabindex="-1"></a><span class="fu">anova</span>(frogs.glm0,frogs.glm1)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax
## Model 2: pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + avrain + 
##     meanmin + meanmax
##   Resid. Df Resid. Dev Df  Deviance
## 1       204     197.62             
## 2       205     197.65 -1 -0.029411</code></pre>
<p>How do we calculate AIC for these two models?</p>
<div class="sourceCode" id="cb880"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb880-1"><a href="week-13-lab.html#cb880-1" tabindex="-1"></a><span class="fu">logLik</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -98.81244 (df=8)</code></pre>
<div class="sourceCode" id="cb882"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb882-1"><a href="week-13-lab.html#cb882-1" tabindex="-1"></a>k<span class="ot">&lt;-</span><span class="dv">8</span></span>
<span id="cb882-2"><a href="week-13-lab.html#cb882-2" tabindex="-1"></a>AIC.glm0<span class="ot">&lt;-</span><span class="fu">as.numeric</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">logLik</span>(frogs.glm0)<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>k)</span>
<span id="cb882-3"><a href="week-13-lab.html#cb882-3" tabindex="-1"></a>AIC.glm0</span></code></pre></div>
<pre><code>## [1] 213.6249</code></pre>
<div class="sourceCode" id="cb884"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb884-1"><a href="week-13-lab.html#cb884-1" tabindex="-1"></a><span class="fu">AIC</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## [1] 213.6249</code></pre>
<div class="sourceCode" id="cb886"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb886-1"><a href="week-13-lab.html#cb886-1" tabindex="-1"></a><span class="fu">AIC</span>(frogs.glm1)</span></code></pre></div>
<pre><code>## [1] 211.6543</code></pre>
<p>Question: Are these significantly different?</p>
<p>Answer: This is a trick question. In an Information Theoretic context, we avoid the term “significant” because it is implied that we mean “statistically significant” in the context of hypothesis testing, and no hypothesis test is being performed. AIC simply gives us information on the weight of evidence for one model over another. There are no theoretically justified guidelines (although Burnham and Anderson suggest some as we discussed in lecture). It is better (in my opinion) to convert AICs into model weights and, if it makes sense in the context, to do model averaging.</p>
<p>How about BIC? USe the AIC function with a different flag for the penalty.</p>
<div class="sourceCode" id="cb888"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb888-1"><a href="week-13-lab.html#cb888-1" tabindex="-1"></a><span class="fu">AIC</span>(frogs.glm0,<span class="at">k=</span><span class="fu">log</span>(<span class="fu">nrow</span>(frogs)))</span></code></pre></div>
<pre><code>## [1] 240.4776</code></pre>
<div class="sourceCode" id="cb890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb890-1"><a href="week-13-lab.html#cb890-1" tabindex="-1"></a><span class="fu">AIC</span>(frogs.glm1,<span class="at">k=</span><span class="fu">log</span>(<span class="fu">nrow</span>(frogs)))</span></code></pre></div>
<pre><code>## [1] 235.1504</code></pre>
<p>We could also do this with the BIC() function</p>
<div class="sourceCode" id="cb892"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb892-1"><a href="week-13-lab.html#cb892-1" tabindex="-1"></a><span class="fu">BIC</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## [1] 240.4776</code></pre>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb894-1"><a href="week-13-lab.html#cb894-1" tabindex="-1"></a><span class="fu">BIC</span>(frogs.glm1)</span></code></pre></div>
<pre><code>## [1] 235.1504</code></pre>
<p>As a group, find a small set (3-5) of candidate models, calculate the AIC for each of these models, and calculate model weights.</p>
<p><strong><span style="color: green;">Checkpoint #4: What covariates were in the best performing model (among the ones you tried as a group) and what was its model weight?</span></strong></p>
</div>
<div id="model-selection-via-step-wise-regression" class="section level2 hasAnchor" number="25.2">
<h2><span class="header-section-number">25.2</span> Model selection via step-wise regression<a href="week-13-lab.html#model-selection-via-step-wise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ideally, we can narrow down the set of candidate covariates based on biology alone. Another approach that is common in the literature, but which has been criticized, is stepwise regression. The default of the step() function is to use both forward and backward steps. If the function step is given the full model, it will begin with the full model, no matter which direction it is working.</p>
<div class="sourceCode" id="cb896"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb896-1"><a href="week-13-lab.html#cb896-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm0) <span class="co">#same as step(frogs.glm0,direction=&quot;both&quot;)</span></span></code></pre></div>
<pre><code>## Start:  AIC=213.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax
## 
##                  Df Deviance    AIC
## - NoOfSites       1   197.62 211.62
## - avrain          1   197.64 211.64
## - altitude        1   197.65 211.65
## - meanmax         1   198.05 212.05
## &lt;none&gt;                197.62 213.62
## - log(NoOfPools)  1   205.31 219.31
## - log(distance)   1   207.22 221.22
## - meanmin         1   211.27 225.27
## 
## Step:  AIC=211.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + 
##     meanmin + meanmax
## 
##                  Df Deviance    AIC
## - avrain          1   197.64 209.64
## - altitude        1   197.66 209.66
## - meanmax         1   198.06 210.06
## &lt;none&gt;                197.62 211.62
## - log(NoOfPools)  1   205.31 217.31
## - log(distance)   1   209.80 221.80
## - meanmin         1   211.31 223.31
## 
## Step:  AIC=209.64
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + 
##     meanmax
## 
##                  Df Deviance    AIC
## - altitude        1   197.66 207.66
## - meanmax         1   198.74 208.74
## &lt;none&gt;                197.64 209.64
## - log(NoOfPools)  1   205.31 215.31
## - log(distance)   1   209.88 219.88
## - meanmin         1   213.32 223.32
## 
## Step:  AIC=207.66
## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax
## 
##                  Df Deviance    AIC
## &lt;none&gt;                197.66 207.66
## - log(NoOfPools)  1   205.34 213.34
## - log(distance)   1   209.91 217.91
## - meanmax         1   214.18 222.18
## - meanmin         1   222.40 230.40</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + 
##     meanmax, family = binomial, data = frogs, na.action = na.fail)
## 
## Coefficients:
##    (Intercept)   log(distance)  log(NoOfPools)         meanmin         meanmax  
##        18.5268         -0.7547          0.5707          5.3791         -2.3821  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  207 Residual
## Null Deviance:       280 
## Residual Deviance: 197.7     AIC: 207.7</code></pre>
<p>Now compare this to what you would get from</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="week-13-lab.html#cb899-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm0,<span class="at">direction=</span><span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=213.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax
## 
##                  Df Deviance    AIC
## - NoOfSites       1   197.62 211.62
## - avrain          1   197.64 211.64
## - altitude        1   197.65 211.65
## - meanmax         1   198.05 212.05
## &lt;none&gt;                197.62 213.62
## - log(NoOfPools)  1   205.31 219.31
## - log(distance)   1   207.22 221.22
## - meanmin         1   211.27 225.27
## 
## Step:  AIC=211.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + 
##     meanmin + meanmax
## 
##                  Df Deviance    AIC
## - avrain          1   197.64 209.64
## - altitude        1   197.66 209.66
## - meanmax         1   198.06 210.06
## &lt;none&gt;                197.62 211.62
## - log(NoOfPools)  1   205.31 217.31
## - log(distance)   1   209.80 221.80
## - meanmin         1   211.31 223.31
## 
## Step:  AIC=209.64
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + 
##     meanmax
## 
##                  Df Deviance    AIC
## - altitude        1   197.66 207.66
## - meanmax         1   198.74 208.74
## &lt;none&gt;                197.64 209.64
## - log(NoOfPools)  1   205.31 215.31
## - log(distance)   1   209.88 219.88
## - meanmin         1   213.32 223.32
## 
## Step:  AIC=207.66
## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax
## 
##                  Df Deviance    AIC
## &lt;none&gt;                197.66 207.66
## - log(NoOfPools)  1   205.34 213.34
## - log(distance)   1   209.91 217.91
## - meanmax         1   214.18 222.18
## - meanmin         1   222.40 230.40</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + 
##     meanmax, family = binomial, data = frogs, na.action = na.fail)
## 
## Coefficients:
##    (Intercept)   log(distance)  log(NoOfPools)         meanmin         meanmax  
##        18.5268         -0.7547          0.5707          5.3791         -2.3821  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  207 Residual
## Null Deviance:       280 
## Residual Deviance: 197.7     AIC: 207.7</code></pre>
<p>The output can be interpreted as follows: the full model has an AIC of 213.62, but if NoOfSites were removed the subsequent model would have an AIC of 211.62, if avrain were removed 211.64, etc, including <span class="math inline">\(&lt;none&gt;\)</span>, which represents removing no variables. The covariates are listed in order of lowest AIC if they were to be removed, to highest AIC. Since removing NoOfSites would result in the lowest AIC, including being lower than the current model, <span class="math inline">\(&lt;none&gt;\)</span>, NoOfSites is removed for the next step and the process is repeated over again. This continues until <span class="math inline">\(&lt;none&gt;\)</span> has the lowest AIC, meaning no variables can be removed to decrease the AIC from the current model. In this case, this occurred when log(NoOfPools), log(distance), meanmax, and meanmin were in the model. At this point, the process ends and the model from that step is reported as the output.</p>
<p><strong>However, if we attempt to use forward stepwise selection starting with the full model, the process does not work.</strong> The function begins with the full model, sees that there are no variables it could add to decrease the AIC because we haven’t given it any more variables, then reports the full model as the outcome.</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb902-1"><a href="week-13-lab.html#cb902-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm0,<span class="at">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=213.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + 
##     NoOfSites + avrain + meanmin + meanmax, family = binomial, 
##     data = frogs, na.action = na.fail)
## 
## Coefficients:
##    (Intercept)        altitude   log(distance)  log(NoOfPools)       NoOfSites  
##     40.8988354      -0.0066477      -0.7593044       0.5727269      -0.0008979  
##         avrain         meanmin         meanmax  
##     -0.0067930       5.3047879      -3.1729589  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  204 Residual
## Null Deviance:       280 
## Residual Deviance: 197.6     AIC: 213.6</code></pre>
<p>This is clearly not the result that we want, because there are many insignificant variables in the model.</p>
<div class="sourceCode" id="cb905"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb905-1"><a href="week-13-lab.html#cb905-1" tabindex="-1"></a><span class="fu">summary</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + 
##     NoOfSites + avrain + meanmin + meanmax, family = binomial, 
##     data = frogs, na.action = na.fail)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9795  -0.7193  -0.2785   0.7964   2.5658  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     4.090e+01  1.327e+02   0.308 0.757845    
## altitude       -6.648e-03  3.866e-02  -0.172 0.863466    
## log(distance)  -7.593e-01  2.554e-01  -2.973 0.002945 ** 
## log(NoOfPools)  5.727e-01  2.162e-01   2.649 0.008083 ** 
## NoOfSites      -8.979e-04  1.074e-01  -0.008 0.993330    
## avrain         -6.793e-03  5.999e-02  -0.113 0.909848    
## meanmin         5.305e+00  1.543e+00   3.439 0.000584 ***
## meanmax        -3.173e+00  4.839e+00  -0.656 0.512048    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 279.99  on 211  degrees of freedom
## Residual deviance: 197.62  on 204  degrees of freedom
## AIC: 213.62
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p><strong>We need to give the function the model that we want it to start with, in this case, the “empty model.”</strong> The empty model predicts the number of species at a site using no covariates, only an intercept, and is denoted as pres.abs ~ 1 in R.</p>
<div class="sourceCode" id="cb907"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb907-1"><a href="week-13-lab.html#cb907-1" tabindex="-1"></a>frogs.glm.empty <span class="ot">&lt;-</span> <span class="fu">glm</span>(pres.abs<span class="sc">~</span><span class="dv">1</span>, frogs, <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb907-2"><a href="week-13-lab.html#cb907-2" tabindex="-1"></a><span class="fu">summary</span>(frogs.glm.empty)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = pres.abs ~ 1, family = &quot;binomial&quot;, data = frogs)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9657  -0.9657  -0.9657   1.4051   1.4051  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -0.5209     0.1420  -3.667 0.000245 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 279.99  on 211  degrees of freedom
## Residual deviance: 279.99  on 211  degrees of freedom
## AIC: 281.99
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We also need to give the step function all of the possible covariates that it can add to find the best model. This is done using the scope= ~ command in step.</p>
<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb909-1"><a href="week-13-lab.html#cb909-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm.empty, <span class="at">scope=</span> <span class="sc">~</span><span class="fu">log</span>(distance)<span class="sc">+</span><span class="fu">log</span>(NoOfPools)<span class="sc">+</span>NoOfSites<span class="sc">+</span>avrain<span class="sc">+</span>meanmin<span class="sc">+</span>meanmax, <span class="at">data=</span>frogs, <span class="at">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=281.99
## pres.abs ~ 1
## 
##                  Df Deviance    AIC
## + log(distance)   1   229.15 233.15
## + meanmin         1   254.40 258.40
## + meanmax         1   269.52 273.52
## + log(NoOfPools)  1   273.59 277.59
## + NoOfSites       1   274.51 278.51
## &lt;none&gt;                279.99 281.99
## + avrain          1   279.95 283.95
## 
## Step:  AIC=233.15
## pres.abs ~ log(distance)
## 
##                  Df Deviance    AIC
## + meanmin         1   220.90 226.90
## + meanmax         1   226.55 232.55
## &lt;none&gt;                229.15 233.15
## + log(NoOfPools)  1   227.44 233.44
## + NoOfSites       1   228.28 234.28
## + avrain          1   229.07 235.07
## 
## Step:  AIC=226.9
## pres.abs ~ log(distance) + meanmin
## 
##                  Df Deviance    AIC
## + meanmax         1   205.34 213.34
## + avrain          1   210.12 218.12
## + log(NoOfPools)  1   214.18 222.18
## &lt;none&gt;                220.90 226.90
## + NoOfSites       1   220.26 228.26
## 
## Step:  AIC=213.34
## pres.abs ~ log(distance) + meanmin + meanmax
## 
##                  Df Deviance    AIC
## + log(NoOfPools)  1   197.66 207.66
## &lt;none&gt;                205.34 213.34
## + avrain          1   205.33 215.33
## + NoOfSites       1   205.34 215.34
## 
## Step:  AIC=207.66
## pres.abs ~ log(distance) + meanmin + meanmax + log(NoOfPools)
## 
##             Df Deviance    AIC
## &lt;none&gt;           197.66 207.66
## + NoOfSites  1   197.66 209.66
## + avrain     1   197.66 209.66</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ log(distance) + meanmin + meanmax + 
##     log(NoOfPools), family = &quot;binomial&quot;, data = frogs)
## 
## Coefficients:
##    (Intercept)   log(distance)         meanmin         meanmax  log(NoOfPools)  
##        18.5268         -0.7547          5.3791         -2.3821          0.5707  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  207 Residual
## Null Deviance:       280 
## Residual Deviance: 197.7     AIC: 207.7</code></pre>
<p>The function now does what we want it to do! It begins with the empty model and reports an AIC of 281.99, and lists the AIC for each variable if it were to be added to the empty model (log(distance) = 229.15, meanmin = 258.40, etc.). Since adding log(distance) results in the lowest AIC, it adds log(distance), then repeats the process. It continues this process until <span class="math inline">\(&lt;none&gt;\)</span> is at the top of the list, meaning there are no variables that can be added that will decrease the AIC. It then reports that model, in this case pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax, as the best model.</p>
<p>Starting with the full model works fine for direction = “both”. It takes the full model, takes away the least useful covariate, then has the option to add that covariate back in, or take away another. It repeats this until neither adding nor subtracting variables decreases the AIC of the model.</p>
<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb912-1"><a href="week-13-lab.html#cb912-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm0, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=213.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + 
##     avrain + meanmin + meanmax
## 
##                  Df Deviance    AIC
## - NoOfSites       1   197.62 211.62
## - avrain          1   197.64 211.64
## - altitude        1   197.65 211.65
## - meanmax         1   198.05 212.05
## &lt;none&gt;                197.62 213.62
## - log(NoOfPools)  1   205.31 219.31
## - log(distance)   1   207.22 221.22
## - meanmin         1   211.27 225.27
## 
## Step:  AIC=211.62
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + 
##     meanmin + meanmax
## 
##                  Df Deviance    AIC
## - avrain          1   197.64 209.64
## - altitude        1   197.66 209.66
## - meanmax         1   198.06 210.06
## &lt;none&gt;                197.62 211.62
## + NoOfSites       1   197.62 213.62
## - log(NoOfPools)  1   205.31 217.31
## - log(distance)   1   209.80 221.80
## - meanmin         1   211.31 223.31
## 
## Step:  AIC=209.64
## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + 
##     meanmax
## 
##                  Df Deviance    AIC
## - altitude        1   197.66 207.66
## - meanmax         1   198.74 208.74
## &lt;none&gt;                197.64 209.64
## + avrain          1   197.62 211.62
## + NoOfSites       1   197.64 211.64
## - log(NoOfPools)  1   205.31 215.31
## - log(distance)   1   209.88 219.88
## - meanmin         1   213.32 223.32
## 
## Step:  AIC=207.66
## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax
## 
##                  Df Deviance    AIC
## &lt;none&gt;                197.66 207.66
## + altitude        1   197.64 209.64
## + NoOfSites       1   197.66 209.66
## + avrain          1   197.66 209.66
## - log(NoOfPools)  1   205.34 213.34
## - log(distance)   1   209.91 217.91
## - meanmax         1   214.18 222.18
## - meanmin         1   222.40 230.40</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + 
##     meanmax, family = binomial, data = frogs, na.action = na.fail)
## 
## Coefficients:
##    (Intercept)   log(distance)  log(NoOfPools)         meanmin         meanmax  
##        18.5268         -0.7547          0.5707          5.3791         -2.3821  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  207 Residual
## Null Deviance:       280 
## Residual Deviance: 197.7     AIC: 207.7</code></pre>
<p>However, it can also be done starting with the empty model, where its first step is to add a covariate to the empty model, then proceed from there.</p>
<div class="sourceCode" id="cb915"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb915-1"><a href="week-13-lab.html#cb915-1" tabindex="-1"></a><span class="fu">step</span>(frogs.glm.empty, <span class="at">scope=</span> <span class="sc">~</span><span class="fu">log</span>(distance)<span class="sc">+</span><span class="fu">log</span>(NoOfPools)<span class="sc">+</span>NoOfSites<span class="sc">+</span>avrain<span class="sc">+</span>meanmin<span class="sc">+</span>meanmax, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=281.99
## pres.abs ~ 1
## 
##                  Df Deviance    AIC
## + log(distance)   1   229.15 233.15
## + meanmin         1   254.40 258.40
## + meanmax         1   269.52 273.52
## + log(NoOfPools)  1   273.59 277.59
## + NoOfSites       1   274.51 278.51
## &lt;none&gt;                279.99 281.99
## + avrain          1   279.95 283.95
## 
## Step:  AIC=233.15
## pres.abs ~ log(distance)
## 
##                  Df Deviance    AIC
## + meanmin         1   220.90 226.90
## + meanmax         1   226.55 232.55
## &lt;none&gt;                229.15 233.15
## + log(NoOfPools)  1   227.44 233.44
## + NoOfSites       1   228.28 234.28
## + avrain          1   229.07 235.07
## - log(distance)   1   279.99 281.99
## 
## Step:  AIC=226.9
## pres.abs ~ log(distance) + meanmin
## 
##                  Df Deviance    AIC
## + meanmax         1   205.34 213.34
## + avrain          1   210.12 218.12
## + log(NoOfPools)  1   214.18 222.18
## &lt;none&gt;                220.90 226.90
## + NoOfSites       1   220.26 228.26
## - meanmin         1   229.15 233.15
## - log(distance)   1   254.40 258.40
## 
## Step:  AIC=213.34
## pres.abs ~ log(distance) + meanmin + meanmax
## 
##                  Df Deviance    AIC
## + log(NoOfPools)  1   197.66 207.66
## &lt;none&gt;                205.34 213.34
## + avrain          1   205.33 215.33
## + NoOfSites       1   205.34 215.34
## - meanmax         1   220.90 226.90
## - log(distance)   1   225.85 231.85
## - meanmin         1   226.55 232.55
## 
## Step:  AIC=207.66
## pres.abs ~ log(distance) + meanmin + meanmax + log(NoOfPools)
## 
##                  Df Deviance    AIC
## &lt;none&gt;                197.66 207.66
## + NoOfSites       1   197.66 209.66
## + avrain          1   197.66 209.66
## - log(NoOfPools)  1   205.34 213.34
## - log(distance)   1   209.91 217.91
## - meanmax         1   214.18 222.18
## - meanmin         1   222.40 230.40</code></pre>
<pre><code>## 
## Call:  glm(formula = pres.abs ~ log(distance) + meanmin + meanmax + 
##     log(NoOfPools), family = &quot;binomial&quot;, data = frogs)
## 
## Coefficients:
##    (Intercept)   log(distance)         meanmin         meanmax  log(NoOfPools)  
##        18.5268         -0.7547          5.3791         -2.3821          0.5707  
## 
## Degrees of Freedom: 211 Total (i.e. Null);  207 Residual
## Null Deviance:       280 
## Residual Deviance: 197.7     AIC: 207.7</code></pre>
<p>The real magic comes when we use a package like ‘MuMIn’ (Multimodel Inference)</p>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="week-13-lab.html#cb918-1" tabindex="-1"></a><span class="fu">dredge</span>(frogs.glm0)</span></code></pre></div>
<pre><code>## Fixed term is &quot;(Intercept)&quot;</code></pre>
<pre><code>## Global model call: glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + 
##     NoOfSites + avrain + meanmin + meanmax, family = binomial, 
##     data = frogs, na.action = na.fail)
## ---
## Model selection table 
##          (Int)        alt        avr log(dst) log(NOP)     mnmx   mnmn
## 61    18.53000                        -0.7547   0.5707 -2.38200 5.3790
## 46   -51.58000  0.0231400             -0.7619   0.5598          5.9520
## 62    28.93000 -0.0034840             -0.7553   0.5716 -2.72100 5.2570
## 125   18.53000                        -0.7590   0.5710 -2.37700 5.3700
## 63    18.16000             0.0009902  -0.7543   0.5707 -2.36100 5.3560
## 48   -45.93000  0.0181700  0.0266100  -0.7437   0.5631          5.3080
## 110  -50.64000  0.0228600             -0.7953   0.5623          5.8910
## 64    41.04000 -0.0066960 -0.0067780  -0.7583   0.5727 -3.17800 5.3050
## 126   28.84000 -0.0034520             -0.7558   0.5716 -2.71700 5.2570
## 127   18.27000             0.0007003  -0.7583   0.5709 -2.36200 5.3540
## 112  -45.80000  0.0182300  0.0257700  -0.7551   0.5638          5.3080
## 47   -14.29000             0.0721300  -0.7924   0.5455          2.1740
## 53    21.47000                        -0.9221          -2.25600 4.7810
## 128   40.90000 -0.0066480 -0.0067930  -0.7593   0.5727 -3.17300 5.3050
## 38   -44.64000  0.0217700             -0.9299                   5.2960
## 111  -14.33000             0.0722500  -0.7903   0.5454          2.1760
## 54    33.40000 -0.0039860             -0.9222          -2.64600 4.6470
## 55    20.22000             0.0034080  -0.9202          -2.18500 4.7050
## 117   21.47000                        -0.9193          -2.25900 4.7870
## 40   -39.23000  0.0168400  0.0272300  -0.9080                   4.6710
## 102  -43.92000  0.0215600             -0.9548                   5.2490
## 121   15.48000                                  0.7078 -2.86800 6.5640
## 118   34.67000 -0.0044110             -0.9147          -2.69700 4.6490
## 56    38.63000 -0.0053740 -0.0029350  -0.9239          -2.84300 4.6660
## 119   20.08000             0.0037900  -0.9150          -2.18300 4.7070
## 104  -39.22000  0.0168500  0.0271500  -0.9091                   4.6700
## 57    14.78000                                  0.7515 -2.76600 6.4440
## 39   -10.29000             0.0704800  -0.9389                   1.7990
## 42   -67.27000  0.0271400                       0.7407          7.1610
## 123   10.13000             0.0145800            0.7037 -2.55800 6.2240
## 106  -68.17000  0.0275000                       0.7057          7.1910
## 122   36.11000 -0.0068950                       0.7084 -3.54600 6.3380
## 120   39.59000 -0.0057160 -0.0027840  -0.9164          -2.88200 4.6670
## 108  -59.48000  0.0198800  0.0415200            0.6935          6.1980
## 44   -60.67000  0.0214900  0.0300900            0.7388          6.4230
## 58     4.26100  0.0035230                       0.7501 -2.42300 6.5640
## 59    14.15000             0.0016700            0.7514 -2.72900 6.4030
## 103  -10.54000             0.0711800  -0.9270                   1.8100
## 16     7.78400 -0.0095890  0.0772300  -0.9211   0.4622                
## 124    7.02900  0.0009015  0.0156800            0.7033 -2.44600 6.2280
## 31   -25.39000             0.0889800  -0.9126   0.4583  1.21000       
## 60   -25.77000  0.0115700  0.0161800            0.7449 -1.28600 6.4470
## 45     0.68640                        -0.9050   0.5027          1.1150
## 30   171.20000 -0.0549900             -0.9926   0.4544 -5.93600       
## 109    1.52600                        -0.9867   0.5102          1.1030
## 80     8.03300 -0.0094940  0.0759000  -0.9417   0.4640                
## 32    28.01000 -0.0153900  0.0696800  -0.9275   0.4632 -0.73800       
## 95   -24.73000             0.0873300  -0.9365   0.4605  1.19700       
## 8      7.89600 -0.0079910  0.0738300  -1.0310                         
## 23   -19.78000             0.0836700  -1.0230           1.01000       
## 107  -24.84000             0.0878800            0.6831          2.7640
## 94   167.30000 -0.0536800             -1.0180   0.4564 -5.78400       
## 96    26.31000 -0.0147500  0.0691700  -0.9460   0.4648 -0.66760       
## 22   163.80000 -0.0513600             -1.0970          -5.66500       
## 37     4.04100                        -1.0450                   0.8095
## 24    27.44000 -0.0135900  0.0665100  -1.0370          -0.71300       
## 72     8.03500 -0.0079340  0.0730600  -1.0420                         
## 43   -22.87000             0.0784300            0.7370          2.7040
## 87   -19.37000             0.0826300  -1.0380           1.00100       
## 101    4.82900                        -1.1200                   0.7954
## 86   161.10000 -0.0504200             -1.1140          -5.55700       
## 113   18.49000                                         -2.84900 6.1290
## 88    26.58000 -0.0132600  0.0662000  -1.0470          -0.67720       
## 14    10.82000 -0.0035650             -1.0220   0.4011                
## 78    11.71000 -0.0035710             -1.1130   0.4129                
## 29     0.38330                        -1.0400   0.3807  0.37270       
## 115    9.95400             0.0233400                   -2.36100 5.6050
## 100  -54.64000  0.0185100  0.0480700                            5.6130
## 114   46.40000 -0.0093210                              -3.77000 5.8350
## 93     1.22500                        -1.1330   0.3946  0.37800       
## 98   -63.97000  0.0270400                                       6.6860
## 6     10.90000 -0.0024360             -1.1230                         
## 49    17.74000                                         -2.70600 5.9250
## 21     3.78600                        -1.1340           0.25010       
## 116  -14.39000  0.0070520  0.0321900                   -1.48000 5.6320
## 34   -62.19000  0.0264000                                       6.5890
## 70    11.71000 -0.0024230             -1.2060                         
## 5      7.68400                        -1.2040                         
## 13     6.80900                        -1.1680   0.2237                
## 36   -55.32000  0.0203300  0.0332800                            5.8200
## 85     4.61900                        -1.2190           0.25220       
## 51    15.12000             0.0070460                   -2.55200 5.7580
## 50     7.34400  0.0034820                              -2.36600 6.0410
## 69     8.51000                        -1.2860                         
## 77     7.65500                        -1.2540   0.2315                
## 7      7.11800             0.0040160  -1.2080                         
## 15     6.95000            -0.0011020  -1.1670   0.2273                
## 52   -57.05000  0.0208200  0.0339800                    0.06385 5.8190
## 99   -22.42000             0.0909300                            2.4310
## 71     8.25500             0.0016800  -1.2860                         
## 79     8.23400            -0.0041500  -1.2530   0.2459                
## 35   -19.64000             0.0789800                            2.3260
## 91   -41.16000             0.1113000            0.6039  1.59300       
## 76     2.41000 -0.0125800  0.0953200            0.6084                
## 41    -7.61300                                  0.7316          1.5830
## 105   -7.72800                                  0.7033          1.5490
## 92   -68.98000  0.0080870  0.1209000            0.5997  2.61100       
## 27   -38.57000             0.1006000            0.6573  1.54900       
## 12     3.82200 -0.0121600  0.0843300            0.6619                
## 28  -108.30000  0.0202200  0.1254000            0.6439  4.09600       
## 90   174.80000 -0.0596700                       0.6255 -6.25300       
## 83   -37.08000             0.1114000                    1.41100       
## 68     1.51100 -0.0111000  0.0969900                                  
## 84   -83.69000  0.0135300  0.1277000                    3.11500       
## 26   142.50000 -0.0493600                       0.6721 -5.02400       
## 19   -33.37000             0.0978600                    1.33700       
## 4      3.21500 -0.0104300  0.0832300                                  
## 20  -130.00000  0.0279800  0.1324000                    4.86500       
## 97    -4.69200                                                  1.1780
## 33    -4.34600                                                  1.2070
## 10     6.32200 -0.0056080                       0.6344                
## 74     5.74400 -0.0053960                       0.6019                
## 82   170.70000 -0.0570500                              -6.13000       
## 25   -10.59000                                  0.6152  0.60690       
## 89   -10.44000                                  0.5822  0.57870       
## 18   132.00000 -0.0445500                              -4.65700       
## 66     4.88500 -0.0038050                                             
## 81    -6.45900                                          0.40020       
## 2      5.63300 -0.0039940                                             
## 17    -6.38200                                          0.42720       
## 73    -1.97500                                  0.3479                
## 75    -1.28400            -0.0048460            0.3651                
## 9     -1.58400                                  0.3765                
## 65    -1.04300                                                        
## 11    -0.02344            -0.0112600            0.4140                
## 67    -1.64300             0.0039450                                  
## 1     -0.52090                                                        
## 3     -0.17310            -0.0023490                                  
##            NOS df   logLik  AICc delta weight
## 61              5  -98.828 207.9  0.00  0.250
## 46              5  -99.368 209.0  1.08  0.146
## 62              6  -98.819 210.0  2.10  0.088
## 125 -0.0039920  6  -98.827 210.1  2.12  0.087
## 63              6  -98.828 210.1  2.12  0.087
## 48              6  -99.030 210.5  2.52  0.071
## 110 -0.0313600  6  -99.320 211.1  3.10  0.053
## 64              7  -98.812 212.2  4.23  0.030
## 126 -0.0005225  7  -98.819 212.2  4.24  0.030
## 127 -0.0036200  7  -98.827 212.2  4.26  0.030
## 112 -0.0100800  7  -99.025 212.6  4.65  0.024
## 47              5 -101.473 213.2  5.29  0.018
## 53              4 -102.670 213.5  5.59  0.015
## 128 -0.0008979  8  -98.812 214.3  6.39  0.010
## 38              4 -103.185 214.6  6.62  0.009
## 111  0.0019890  6 -101.473 215.4  7.41  0.006
## 54              5 -102.658 215.6  7.66  0.005
## 55              5 -102.666 215.6  7.68  0.005
## 117  0.0025550  5 -102.670 215.6  7.68  0.005
## 40              5 -102.831 216.0  8.01  0.005
## 102 -0.0229600  5 -103.158 216.6  8.66  0.003
## 121  0.1417000  5 -103.690 217.7  9.72  0.002
## 118  0.0069150  6 -102.655 217.7  9.77  0.002
## 56              6 -102.656 217.7  9.78  0.002
## 119  0.0045760  6 -102.665 217.7  9.79  0.002
## 104 -0.0009286  6 -102.831 218.1 10.12  0.002
## 57              4 -104.954 218.1 10.15  0.002
## 39              4 -105.058 218.3 10.36  0.001
## 42              4 -105.494 219.2 11.23  0.001
## 123  0.1495000  6 -103.612 219.6 11.69  0.001
## 106  0.1109000  5 -104.704 219.7 11.75  0.001
## 122  0.1494000  6 -103.648 219.7 11.76  0.001
## 120  0.0067250  7 -102.654 219.9 11.91  0.001
## 108  0.1414000  6 -103.743 219.9 11.95  0.001
## 44              5 -104.940 220.2 12.22  0.001
## 58              5 -104.941 220.2 12.23  0.001
## 59              5 -104.953 220.2 12.25  0.001
## 103  0.0109500  5 -105.052 220.4 12.45  0.000
## 16              5 -105.670 221.6 13.68  0.000
## 124  0.1491000  7 -103.611 221.8 13.82  0.000
## 31              5 -105.761 221.8 13.87  0.000
## 60              6 -104.902 222.2 14.27  0.000
## 45              4 -107.089 222.4 14.42  0.000
## 30              5 -106.658 223.6 15.66  0.000
## 109 -0.0868600  5 -106.703 223.7 15.75  0.000
## 80  -0.0208000  6 -105.649 223.7 15.76  0.000
## 32              6 -105.654 223.7 15.77  0.000
## 95  -0.0240100  6 -105.733 223.9 15.93  0.000
## 8               4 -108.393 225.0 17.03  0.000
## 23              4 -108.460 225.1 17.17  0.000
## 107  0.1784000  5 -107.491 225.3 17.33  0.000
## 94  -0.0277700  6 -106.622 225.7 17.71  0.000
## 96  -0.0193800  7 -105.637 225.8 17.88  0.000
## 22              4 -109.274 226.7 18.79  0.000
## 37              3 -110.450 227.0 19.07  0.000
## 24              5 -108.379 227.0 19.10  0.000
## 72  -0.0112700  5 -108.387 227.1 19.12  0.000
## 43              4 -109.478 227.1 19.20  0.000
## 87  -0.0139100  5 -108.451 227.2 19.25  0.000
## 101 -0.0774900  4 -110.128 228.4 20.50  0.000
## 86  -0.0188800  5 -109.256 228.8 20.86  0.000
## 113  0.1874000  4 -110.480 229.2 21.21  0.000
## 88  -0.0098920  6 -108.374 229.2 21.21  0.000
## 14              4 -110.658 229.5 21.56  0.000
## 78  -0.0998600  5 -110.119 230.5 22.58  0.000
## 29              4 -111.202 230.6 22.65  0.000
## 115  0.1996000  5 -110.252 230.8 22.85  0.000
## 100  0.1917000  5 -110.283 230.9 22.91  0.000
## 114  0.1977000  5 -110.391 231.1 23.13  0.000
## 93  -0.1042000  5 -110.610 231.5 23.56  0.000
## 98   0.1580000  4 -111.733 231.7 23.71  0.000
## 6               3 -112.923 232.0 24.01  0.000
## 49              3 -112.924 232.0 24.02  0.000
## 21              3 -113.273 232.7 24.71  0.000
## 116  0.1964000  6 -110.234 232.9 24.93  0.000
## 34              3 -113.514 233.1 25.20  0.000
## 70  -0.0886600  4 -112.483 233.2 25.21  0.000
## 5               2 -114.577 233.2 25.26  0.000
## 13              3 -113.719 233.6 25.61  0.000
## 36              4 -112.729 233.7 25.70  0.000
## 85  -0.0917800  4 -112.799 233.8 25.84  0.000
## 51              4 -112.900 234.0 26.05  0.000
## 50              4 -112.910 234.0 26.07  0.000
## 69  -0.0861500  3 -114.142 234.4 26.45  0.000
## 77  -0.0911900  4 -113.233 234.7 26.71  0.000
## 7               3 -114.537 235.2 27.24  0.000
## 15              4 -113.716 235.6 27.68  0.000
## 52              5 -112.729 235.7 27.80  0.000
## 99   0.2238000  4 -113.959 236.1 28.16  0.000
## 71  -0.0841400  4 -114.135 236.5 28.52  0.000
## 79  -0.0966200  5 -113.194 236.7 28.73  0.000
## 35              3 -117.391 240.9 32.95  0.000
## 91   0.1876000  5 -115.662 241.6 33.67  0.000
## 76   0.1952000  5 -115.875 242.0 34.09  0.000
## 41              3 -118.231 242.6 34.63  0.000
## 105  0.0995700  4 -117.532 243.3 35.31  0.000
## 92   0.1825000  6 -115.625 243.7 35.71  0.000
## 27              4 -118.048 244.3 36.34  0.000
## 12              4 -118.452 245.1 37.15  0.000
## 28              5 -117.797 245.9 37.94  0.000
## 90   0.1989000  5 -119.943 250.2 42.23  0.000
## 83   0.2269000  4 -121.222 250.6 42.69  0.000
## 68   0.2335000  4 -121.485 251.2 43.22  0.000
## 84   0.2183000  5 -121.111 252.5 44.57  0.000
## 26              4 -122.592 253.4 45.43  0.000
## 19              3 -124.993 256.1 48.15  0.000
## 4               3 -125.468 257.1 49.10  0.000
## 20              4 -124.476 257.1 49.20  0.000
## 97   0.1438000  3 -125.583 257.3 49.33  0.000
## 33              2 -127.199 258.5 50.51  0.000
## 10              3 -126.706 259.5 51.58  0.000
## 74   0.1141000  4 -125.693 259.6 51.63  0.000
## 82   0.2354000  4 -126.085 260.4 52.42  0.000
## 25              3 -127.803 261.7 53.77  0.000
## 89   0.1093000  4 -126.867 261.9 53.98  0.000
## 18              3 -130.106 266.3 58.38  0.000
## 66   0.1516000  3 -132.034 270.2 62.24  0.000
## 81   0.1481000  3 -132.892 271.9 63.95  0.000
## 2               2 -133.978 272.0 64.07  0.000
## 17              2 -134.759 273.6 65.63  0.000
## 73   0.1564000  3 -134.617 275.4 67.40  0.000
## 75   0.1491000  4 -134.550 277.3 69.35  0.000
## 9               2 -136.793 277.6 69.70  0.000
## 65   0.1733000  2 -137.255 278.6 70.62  0.000
## 11              3 -136.394 278.9 70.96  0.000
## 67   0.1786000  3 -137.204 280.5 72.58  0.000
## 1               1 -139.993 282.0 74.06  0.000
## 3               2 -139.974 284.0 76.06  0.000
## Models ranked by AICc(x)</code></pre>
<p>We can make a table to compare a hand-selected set of models.</p>
<div class="sourceCode" id="cb921"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb921-1"><a href="week-13-lab.html#cb921-1" tabindex="-1"></a><span class="fu">model.sel</span>(frogs.glm0,frogs.glm1)</span></code></pre></div>
<pre><code>## Model selection table 
##            (Int)       alt        avr log(dst) log(NOP)   mnmx  mnmn        NOS
## frogs.glm1 18.27            0.0007003  -0.7583   0.5709 -2.362 5.354 -0.0036200
## frogs.glm0 40.90 -0.006648 -0.0067930  -0.7593   0.5727 -3.173 5.305 -0.0008979
##                     family na.action df  logLik  AICc delta weight
## frogs.glm1 binomial(logit)            7 -98.827 212.2  0.00  0.744
## frogs.glm0 binomial(logit)   na.fail  8 -98.812 214.3  2.13  0.256
## Models ranked by AICc(x)</code></pre>
<p>We can do model averaging</p>
<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb923-1"><a href="week-13-lab.html#cb923-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">model.avg</span>(frogs.glm0,frogs.glm1))</span></code></pre></div>
<pre><code>## 
## Call:
## model.avg(object = frogs.glm0, frogs.glm1)
## 
## Component model call: 
## glm(formula = &lt;2 unique values&gt;, family = binomial, data = frogs, 
##      na.action = &lt;2 unique values&gt;)
## 
## Component models: 
##         df logLik   AICc delta weight
## 234567   7 -98.83 212.20  0.00   0.74
## 1234567  8 -98.81 214.33  2.13   0.26
## 
## Term codes: 
##       altitude         avrain  log(distance) log(NoOfPools)        meanmax 
##              1              2              3              4              5 
##        meanmin      NoOfSites 
##              6              7 
## 
## Model-averaged coefficients:  
## (full average) 
##                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    
## (Intercept)    24.068195  69.288185   69.693226   0.345  0.72984    
## log(distance)  -0.758572   0.255698    0.257218   2.949  0.00319 ** 
## log(NoOfPools)  0.571365   0.215568    0.216850   2.635  0.00842 ** 
## NoOfSites      -0.002923   0.106481    0.107115   0.027  0.97823    
## avrain         -0.001220   0.046837    0.047114   0.026  0.97934    
## meanmin         5.341442   1.530021    1.539117   3.470  0.00052 ***
## meanmax        -2.570165   2.641027    2.656497   0.968  0.33329    
## altitude       -0.001704   0.019784    0.019899   0.086  0.93178    
##  
## (conditional average) 
##                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    
## (Intercept)    24.068195  69.288185   69.693226   0.345  0.72984    
## log(distance)  -0.758572   0.255698    0.257218   2.949  0.00319 ** 
## log(NoOfPools)  0.571365   0.215568    0.216850   2.635  0.00842 ** 
## NoOfSites      -0.002923   0.106481    0.107115   0.027  0.97823    
## avrain         -0.001220   0.046837    0.047114   0.026  0.97934    
## meanmin         5.341442   1.530021    1.539117   3.470  0.00052 ***
## meanmax        -2.570165   2.641027    2.656497   0.968  0.33329    
## altitude       -0.006648   0.038658    0.038888   0.171  0.86427    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>What does the output mean “with shrinkage”? These estimates include a zero value when a parameter does not actually appear in a model. In this case ‘altitude’ appears in only one of the two models and so the estimate “with shrinkage” is signicantly smaller than the parameter value estimated only from a weighting of the models including the covariate ‘altitude’.</p>
</div>
<div id="part-2-model-criticism" class="section level2 hasAnchor" number="25.3">
<h2><span class="header-section-number">25.3</span> Part 2: Model criticism<a href="week-13-lab.html#part-2-model-criticism" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are going to use a dataset that comes from the journal Ecology, which is available <a href="https://github.com/hlynch/Biometry2021/tree/master/_data/Ernest2003.pdf">here</a>.</p>
<p>STOP: Let’s read the abstract so we know what we are modelling.</p>
<p>The goal is to create the best possible, most parsimonious model for maximum longevity in non-volant mammals. The covariates we have in this dataset to consider are:</p>
<p>Order<br />
Family<br />
Genus<br />
Species<br />
Mass (g)<br />
Gestation (mo)<br />
Newborn weight (g)<br />
Weaning age (mo)<br />
Weaning mass (g)<br />
Age of first reproduction (mo)<br />
Litter size<br />
Litters/year</p>
<p>First, we will load the data and the ‘car’ package, and use ‘names’ to see what the columns are named</p>
<div class="sourceCode" id="cb925"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb925-1"><a href="week-13-lab.html#cb925-1" tabindex="-1"></a>data<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;_data/MammalLifeHistory.csv&quot;</span>)</span>
<span id="cb925-2"><a href="week-13-lab.html#cb925-2" tabindex="-1"></a><span class="fu">attach</span>(data)</span>
<span id="cb925-3"><a href="week-13-lab.html#cb925-3" tabindex="-1"></a><span class="fu">names</span>(data)</span></code></pre></div>
<pre><code>##  [1] &quot;Order&quot;          &quot;Family&quot;         &quot;Genus&quot;          &quot;Species&quot;       
##  [5] &quot;Mass&quot;           &quot;Gestation&quot;      &quot;Newborn&quot;        &quot;Weaning&quot;       
##  [9] &quot;WeanMass&quot;       &quot;AFR&quot;            &quot;MaxLifespan&quot;    &quot;LitterSize&quot;    
## [13] &quot;LittersPerYear&quot;</code></pre>
<p>Before fitting any models, let’s just look at each potential covariate vs. maximum longevity to get a sense for which variables need to be transformed. I list get the first few here…</p>
<div class="sourceCode" id="cb927"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb927-1"><a href="week-13-lab.html#cb927-1" tabindex="-1"></a><span class="fu">boxplot</span>(MaxLifespan<span class="sc">~</span>Order)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb928-1"><a href="week-13-lab.html#cb928-1" tabindex="-1"></a><span class="fu">boxplot</span>(MaxLifespan<span class="sc">~</span>Family)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<div class="sourceCode" id="cb929"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb929-1"><a href="week-13-lab.html#cb929-1" tabindex="-1"></a><span class="fu">boxplot</span>(MaxLifespan<span class="sc">~</span>Genus)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-3.png" width="672" /></p>
<div class="sourceCode" id="cb930"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb930-1"><a href="week-13-lab.html#cb930-1" tabindex="-1"></a><span class="fu">boxplot</span>(MaxLifespan<span class="sc">~</span>Species)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-4.png" width="672" /></p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="week-13-lab.html#cb931-1" tabindex="-1"></a><span class="fu">plot</span>(MaxLifespan<span class="sc">~</span>Mass)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-5.png" width="672" /></p>
<div class="sourceCode" id="cb932"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb932-1"><a href="week-13-lab.html#cb932-1" tabindex="-1"></a><span class="fu">plot</span>(MaxLifespan<span class="sc">~</span>Gestation)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-28-6.png" width="672" /></p>
<p>Note that most of the covariates need to be transformed to linearize the relationship. The easiest transformation to try is the log() - the covariates that should probably be transformed relate to mass and time periods: Mass, Gestation, Newborn, Weaning, WeanMass, AFR, LittersPerYear (possible, not clear what the best transmation is for this). In other words, look at:</p>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb933-1"><a href="week-13-lab.html#cb933-1" tabindex="-1"></a><span class="fu">plot</span>(MaxLifespan<span class="sc">~</span><span class="fu">log10</span>(Mass))</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb934-1"><a href="week-13-lab.html#cb934-1" tabindex="-1"></a><span class="fu">plot</span>(MaxLifespan<span class="sc">~</span><span class="fu">log10</span>(Gestation))</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<p>etc.</p>
<p>For now, let’s ignore the taxonomic covariates and focus on just three covariates: Mass, AFR, and LitterSize.</p>
<p><em>Fitting the model using ‘lm’</em></p>
<p>Let’s fit the model with Mass,AFR, and LitterSize. We will consider this the full model.</p>
<div class="sourceCode" id="cb935"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb935-1"><a href="week-13-lab.html#cb935-1" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">lm</span>(MaxLifespan<span class="sc">~</span><span class="fu">log10</span>(Mass)<span class="sc">+</span><span class="fu">log10</span>(AFR)<span class="sc">+</span>LitterSize)</span>
<span id="cb935-2"><a href="week-13-lab.html#cb935-2" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MaxLifespan ~ log10(Mass) + log10(AFR) + LitterSize)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -263.66  -78.11  -19.31   53.52  763.75 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -168.4019    26.8904  -6.263 8.35e-10 ***
## log10(Mass)   60.7061     5.6008  10.839  &lt; 2e-16 ***
## log10(AFR)   152.5190    17.5461   8.692  &lt; 2e-16 ***
## LitterSize    -0.1925     3.8823  -0.050     0.96    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 118.7 on 485 degrees of freedom
##   (951 observations deleted due to missingness)
## Multiple R-squared:  0.6282, Adjusted R-squared:  0.6259 
## F-statistic: 273.1 on 3 and 485 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><em>Model diagnostics</em></p>
<p>Let’s look at the residuals as a function of the fitted values:</p>
<div class="sourceCode" id="cb937"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb937-1"><a href="week-13-lab.html#cb937-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(fit),<span class="fu">residuals</span>(fit))</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Let’s look at the residuals using the ‘car’ package function ‘residualPlots’. This command produces scatterplots of the residuals versus each of the predictors and versus the final fitted value. Note that what we did manually above is reproduced as the final panel here.</p>
<div class="sourceCode" id="cb938"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb938-1"><a href="week-13-lab.html#cb938-1" tabindex="-1"></a><span class="fu">residualPlots</span>(fit)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre><code>##             Test stat Pr(&gt;|Test stat|)    
## log10(Mass)   12.8413           &lt;2e-16 ***
## log10(AFR)    11.6289           &lt;2e-16 ***
## LitterSize     0.2717            0.786    
## Tukey test    15.0728           &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can assess the normality of the residuals by histogramming the studentized residuals. We can pull these out from the fitted model using the ‘studres’ function from the ‘MASS’ package.</p>
<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb940-1"><a href="week-13-lab.html#cb940-1" tabindex="-1"></a>sresid<span class="ot">&lt;-</span><span class="fu">studres</span>(fit)</span>
<span id="cb940-2"><a href="week-13-lab.html#cb940-2" tabindex="-1"></a><span class="fu">hist</span>(sresid,<span class="at">freq=</span><span class="cn">FALSE</span>)</span>
<span id="cb940-3"><a href="week-13-lab.html#cb940-3" tabindex="-1"></a>xfit<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="fu">min</span>(sresid),<span class="fu">max</span>(sresid),<span class="at">length=</span><span class="dv">40</span>)</span>
<span id="cb940-4"><a href="week-13-lab.html#cb940-4" tabindex="-1"></a>yfit<span class="ot">&lt;-</span><span class="fu">dnorm</span>(xfit)</span>
<span id="cb940-5"><a href="week-13-lab.html#cb940-5" tabindex="-1"></a><span class="fu">lines</span>(xfit,yfit)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>A variation on the basic residual plot is the marginal model plot.</p>
<div class="sourceCode" id="cb941"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb941-1"><a href="week-13-lab.html#cb941-1" tabindex="-1"></a><span class="fu">marginalModelPlots</span>(fit)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Note that loess smoothers have been added showing the non-parametric regression between the actual data (solid line) and the model prediction (dashed line) against each of the predictor variables. If these two lines are close together, that is an indication of good model fit.</p>
<p>Note that the marginal plots display the relationship between the response and each covariates IGNORING the other covariates. We can also look at the relationship between the response and each covariates CONTROLLING for the other covariates. We do this through “added variable plots”.</p>
<p>Let’s say we have the following model</p>
<p><span class="math display">\[
Y \sim X_{1} + X_{2} + X_{3}
\]</span></p>
<p>There are two steps in building the added-variable plot for <span class="math inline">\(X_{1}\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Regress</li>
</ol>
<p><span class="math display">\[
Y \sim X_{2} + X_{3}
\]</span></p>
<p>The residuals from this plot reflect all the variation that is not otherwise explained in the model (i.e. by all the covariates except <span class="math inline">\(X_{1}\)</span>).</p>
<ol start="2" style="list-style-type: decimal">
<li>Regress</li>
</ol>
<p><span class="math display">\[
X_{1} \sim X_{2} + X_{3}
\]</span></p>
<p>The residuals from this plot represent the part of <span class="math inline">\(X_{1}\)</span> not explained by the other covariates.</p>
<p>The added variable plot is simply a plot of the residuals from #1 on the y-axis and the residuals from #2 on the x-axis.</p>
<p>We can do this in R using the function ‘avPlots’ from the ‘car’ package</p>
<div class="sourceCode" id="cb942"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb942-1"><a href="week-13-lab.html#cb942-1" tabindex="-1"></a><span class="fu">avPlots</span>(fit,<span class="at">id.n=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<pre><code>## Warning in box(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in title(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<pre><code>## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<pre><code>## Warning in box(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in title(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<pre><code>## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<pre><code>## Warning in box(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in title(...): &quot;id.n&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a
## graphical parameter</code></pre>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>The id.n option will cause the plot to identify the two points that are furthest from the mean on the x axis and the two with the largest absolute residuals.</p>
<p>The added variable plot allows us to visualize the effect of each covariate after adjusting for all the other covariates in the model.</p>
<p>We can also look at leverage by using the command ‘leveragePlots’</p>
<div class="sourceCode" id="cb961"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb961-1"><a href="week-13-lab.html#cb961-1" tabindex="-1"></a><span class="fu">leveragePlots</span>(fit)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>For covariates with only a single degree of freedom (i.e. not different levels of a factor), this will simply be a rescaled version of the added-variable plots.</p>
<p><em>Outliers</em></p>
<p>Let’s look for outliers using the ‘car’ function ‘outlierTest’ which reports the Bonferroni p-values for Studentized residuals:</p>
<div class="sourceCode" id="cb962"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb962-1"><a href="week-13-lab.html#cb962-1" tabindex="-1"></a><span class="fu">outlierTest</span>(fit)</span></code></pre></div>
<pre><code>##      rstudent unadjusted p-value Bonferroni p
## 1440 6.797206         3.1481e-11   1.5394e-08
## 1439 6.284883         7.3267e-10   3.5828e-07
## 1438 4.650225         4.2834e-06   2.0946e-03
## 1437 4.488601         8.9697e-06   4.3862e-03</code></pre>
<p>Plot the qqplot for the studentized residuals using the ‘car’ package function qqPlot. WARNING: This is not the same as qqplot (lower case p); this is a specialized function in the ‘car’ package that will plot the appropriate qqplot for the studentized residuals given a fitted model as input.</p>
<div class="sourceCode" id="cb964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb964-1"><a href="week-13-lab.html#cb964-1" tabindex="-1"></a><span class="fu">qqPlot</span>(fit)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<pre><code>## [1] 1439 1440</code></pre>
<p>To identify points with high leverage, we want to calculate the hat values for each of the points. We can do this using the ‘hatvalues” command in the ’car’ package.</p>
<div class="sourceCode" id="cb966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb966-1"><a href="week-13-lab.html#cb966-1" tabindex="-1"></a><span class="fu">hatvalues</span>(fit)</span></code></pre></div>
<pre><code>##         842         846         847         848         849         850 
## 0.015068582 0.017116554 0.013839481 0.021939881 0.017150953 0.018853062 
##         853         854         856         857         858         859 
## 0.009549529 0.005599577 0.014340191 0.019328979 0.012823175 0.013772585 
##         861         862         863         865         866         867 
## 0.009440959 0.016442917 0.017600641 0.007572179 0.013613866 0.007801192 
##         870         873         874         875         877         879 
## 0.011338383 0.008197299 0.014927487 0.006055714 0.011443251 0.016563954 
##         880         882         884         885         886         887 
## 0.013955050 0.010053036 0.015267217 0.010993510 0.012341340 0.014343983 
##         888         889         891         893         894         895 
## 0.008865008 0.005970050 0.011063709 0.029705488 0.011271224 0.003770331 
##         896         897         899         900         901         902 
## 0.004983555 0.009589510 0.011642918 0.010190113 0.008382325 0.007342826 
##         905         906         907         908         909         910 
## 0.014709335 0.015055631 0.006571569 0.006224857 0.015789476 0.012402864 
##         911         912         913         914         915         916 
## 0.006796631 0.015641460 0.009947711 0.011609740 0.013271976 0.008664947 
##         919         920         921         922         923         924 
## 0.011320700 0.009584098 0.008836642 0.023302778 0.013285879 0.013900936 
##         925         926         928         929         931         932 
## 0.006221926 0.009090168 0.004859531 0.004634291 0.010114932 0.003656926 
##         933         937         938         939         940         941 
## 0.011388887 0.010593856 0.007535007 0.007456337 0.007670776 0.030754607 
##         942         944         945         946         947         949 
## 0.011129173 0.008955209 0.010397163 0.014035279 0.011948458 0.012339818 
##         950         951         952         954         955         956 
## 0.006449339 0.011225454 0.008363365 0.008531905 0.016430642 0.010567908 
##         959         960         961         962         963         966 
## 0.009671992 0.015610676 0.007195425 0.006295996 0.007158921 0.013672739 
##         968         969         970         971         972         973 
## 0.034809065 0.005230397 0.005791008 0.017812912 0.005496217 0.008931287 
##         974         975         976         977         978         979 
## 0.010714088 0.007259116 0.013047745 0.012098943 0.003733398 0.003745609 
##         980         981         983         985         986         987 
## 0.007505282 0.122249610 0.011234272 0.003202089 0.003567088 0.013845063 
##         988         989         990         992         993         996 
## 0.005235163 0.009494211 0.008892926 0.012990091 0.008085905 0.005684921 
##         998        1001        1002        1003        1004        1005 
## 0.003768405 0.007223905 0.005180949 0.004814076 0.007704883 0.004911695 
##        1006        1008        1009        1011        1012        1013 
## 0.008829438 0.008971676 0.008165306 0.007724797 0.006735167 0.011439088 
##        1016        1017        1018        1020        1021        1024 
## 0.011792716 0.003381829 0.009339714 0.003349616 0.004859820 0.005352438 
##        1025        1027        1028        1029        1030        1032 
## 0.006487226 0.006408716 0.005061128 0.003532664 0.008939130 0.004258913 
##        1033        1034        1035        1036        1037        1038 
## 0.002395187 0.004871201 0.005370386 0.007518471 0.010234137 0.003800835 
##        1039        1040        1041        1043        1046        1047 
## 0.013633921 0.002976951 0.005133274 0.007015061 0.003943074 0.051406682 
##        1049        1051        1052        1053        1054        1056 
## 0.004092141 0.006286774 0.002611721 0.013539610 0.008068323 0.004325113 
##        1057        1059        1060        1061        1062        1063 
## 0.005555973 0.004945531 0.002924891 0.004345880 0.005991305 0.007685537 
##        1064        1065        1066        1067        1068        1069 
## 0.004908637 0.004400779 0.003253593 0.011575449 0.007305610 0.005860831 
##        1070        1071        1072        1074        1075        1076 
## 0.002402003 0.002792913 0.005557776 0.007675980 0.010483247 0.006799054 
##        1077        1082        1083        1084        1085        1086 
## 0.006660530 0.006995133 0.006078065 0.002901274 0.007114720 0.004327091 
##        1087        1089        1090        1091        1092        1094 
## 0.005282506 0.003614222 0.006269742 0.006495829 0.008953724 0.004307162 
##        1095        1096        1097        1099        1100        1101 
## 0.007351841 0.005469209 0.003710382 0.010650969 0.002978093 0.003473076 
##        1102        1103        1105        1106        1108        1110 
## 0.006171337 0.007069918 0.005347415 0.003187388 0.024443728 0.011503286 
##        1111        1112        1113        1114        1115        1116 
## 0.011335444 0.006680094 0.007107539 0.007312845 0.002519741 0.007070413 
##        1117        1118        1119        1120        1121        1122 
## 0.002830881 0.004408249 0.003807998 0.003297593 0.003592997 0.002743881 
##        1123        1124        1126        1127        1128        1129 
## 0.002194139 0.003298492 0.009266549 0.006794262 0.007208532 0.008079244 
##        1130        1131        1132        1133        1134        1135 
## 0.006286228 0.008345838 0.003273337 0.003828792 0.005575621 0.006207749 
##        1136        1137        1138        1139        1142        1143 
## 0.007097858 0.013811813 0.006330278 0.011706480 0.003789064 0.002575829 
##        1144        1145        1146        1147        1149        1150 
## 0.006992516 0.003514526 0.005379992 0.034096666 0.005852205 0.033293641 
##        1151        1152        1153        1154        1155        1156 
## 0.003453114 0.009386505 0.007006645 0.030158122 0.004003816 0.005932827 
##        1157        1158        1159        1160        1161        1163 
## 0.006238935 0.007099396 0.003294258 0.004460579 0.002382034 0.004959417 
##        1164        1165        1166        1167        1168        1169 
## 0.005815032 0.005636536 0.007810206 0.011412521 0.016350024 0.003145305 
##        1172        1173        1174        1175        1176        1177 
## 0.006659253 0.004164675 0.010321781 0.008759469 0.008968261 0.005112558 
##        1178        1179        1180        1181        1182        1183 
## 0.007413931 0.004092970 0.004144759 0.005136702 0.004079677 0.007101366 
##        1184        1185        1187        1188        1189        1191 
## 0.007859365 0.004565706 0.007752359 0.004891404 0.004984572 0.010947371 
##        1192        1193        1194        1195        1196        1197 
## 0.005974901 0.003571493 0.006637753 0.005020810 0.002784743 0.008507969 
##        1198        1199        1200        1202        1203        1204 
## 0.005564695 0.004808824 0.005100021 0.011504907 0.002532735 0.005352594 
##        1205        1206        1207        1208        1209        1210 
## 0.005481630 0.004135045 0.008106898 0.010955028 0.005124791 0.003077065 
##        1211        1212        1213        1215        1216        1217 
## 0.003610983 0.004251044 0.004395735 0.003557946 0.006615751 0.003897994 
##        1218        1222        1223        1226        1227        1228 
## 0.007260722 0.004722739 0.008573003 0.006186477 0.007481516 0.006778917 
##        1230        1231        1232        1233        1234        1235 
## 0.004511308 0.007878819 0.002950337 0.004743704 0.007035720 0.005283641 
##        1236        1237        1238        1240        1241        1242 
## 0.011306445 0.022893409 0.003614331 0.003258847 0.003970306 0.004500544 
##        1243        1244        1245        1246        1247        1248 
## 0.008833069 0.011832830 0.004962626 0.006739456 0.002961121 0.004095768 
##        1249        1250        1251        1252        1253        1254 
## 0.002697826 0.007713930 0.008888300 0.006103942 0.004462294 0.004126934 
##        1255        1256        1257        1258        1259        1260 
## 0.003575584 0.012908842 0.007919903 0.003450691 0.005238098 0.002709156 
##        1261        1262        1263        1264        1265        1266 
## 0.004487147 0.004141025 0.005462472 0.005616879 0.007023565 0.004157081 
##        1267        1269        1270        1271        1272        1274 
## 0.005405134 0.004237417 0.008758783 0.003659095 0.003641221 0.006924737 
##        1275        1277        1278        1279        1280        1282 
## 0.006556800 0.006777154 0.004657738 0.007400513 0.006358517 0.006840930 
##        1283        1284        1285        1286        1287        1288 
## 0.003604012 0.004650281 0.002931686 0.004993995 0.004907787 0.004930544 
##        1289        1290        1291        1292        1293        1294 
## 0.005290486 0.008136328 0.006958753 0.006647525 0.003779911 0.007934545 
##        1295        1296        1297        1298        1300        1301 
## 0.004946197 0.004302767 0.006799393 0.006732886 0.004519527 0.005707493 
##        1302        1303        1304        1305        1306        1307 
## 0.005550477 0.002595298 0.007779227 0.009609406 0.004636675 0.011250687 
##        1308        1309        1310        1311        1312        1313 
## 0.004781732 0.005060416 0.003886592 0.003321002 0.011590056 0.004771912 
##        1314        1315        1316        1317        1318        1319 
## 0.004385500 0.005503135 0.010437770 0.006328062 0.009054948 0.005575354 
##        1320        1321        1323        1325        1326        1327 
## 0.006215209 0.005548372 0.008165573 0.002922401 0.006738098 0.006856105 
##        1328        1329        1331        1332        1334        1335 
## 0.004278941 0.005994052 0.008439600 0.005687190 0.012655364 0.004428018 
##        1336        1337        1338        1339        1340        1341 
## 0.002716818 0.006707334 0.006023086 0.007232496 0.003974231 0.005413279 
##        1342        1343        1344        1346        1347        1348 
## 0.010988882 0.003898428 0.005552927 0.005835286 0.005067843 0.006460036 
##        1349        1350        1351        1353        1355        1356 
## 0.003628967 0.005276712 0.005131207 0.006341430 0.005387209 0.005671682 
##        1357        1358        1359        1360        1361        1362 
## 0.007056530 0.005369556 0.005524772 0.007859248 0.015081516 0.005429746 
##        1364        1365        1366        1367        1368        1369 
## 0.004934686 0.002370536 0.008380701 0.007978252 0.006482352 0.005136907 
##        1370        1371        1372        1373        1374        1375 
## 0.002962098 0.004225617 0.006654086 0.010193078 0.004704468 0.003955831 
##        1376        1377        1378        1379        1380        1382 
## 0.005892978 0.006761250 0.009081694 0.004618119 0.003413752 0.005023688 
##        1384        1385        1386        1388        1389        1390 
## 0.006697765 0.011954991 0.007447664 0.004846690 0.007081667 0.007402309 
##        1391        1392        1394        1395        1396        1397 
## 0.005589098 0.006310048 0.005901080 0.008585187 0.016351942 0.003439244 
##        1398        1399        1401        1402        1403        1404 
## 0.006193983 0.004744828 0.005438690 0.005680335 0.010608833 0.006236326 
##        1405        1406        1407        1408        1409        1411 
## 0.006949090 0.004709298 0.010467387 0.008132087 0.014792229 0.006401322 
##        1412        1413        1414        1415        1416        1417 
## 0.008215980 0.010237253 0.007768586 0.005145764 0.007202106 0.008845023 
##        1418        1419        1420        1421        1422        1423 
## 0.008912327 0.004916235 0.010635773 0.008120296 0.007774509 0.012684103 
##        1424        1426        1427        1428        1429        1430 
## 0.008368418 0.008586505 0.016052450 0.016559061 0.012372053 0.015524328 
##        1431        1432        1433        1434        1435        1437 
## 0.008787016 0.014546150 0.017984508 0.014618941 0.011304205 0.012288729 
##        1438        1439        1440 
## 0.022172504 0.025746100 0.020671173</code></pre>
<p>Note that the ‘stats’ package has a function called ‘lm.influence’ that provides much the same information.</p>
<p>We can make a plot that highlights highly influential values. We will use John Fox’s suggested cut-off of 4/(n-p-1) where n=number of data points and p=number of estimated parameters.</p>
<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb968-1"><a href="week-13-lab.html#cb968-1" tabindex="-1"></a>cutoff<span class="ot">&lt;-</span><span class="dv">4</span><span class="sc">/</span>((<span class="fu">nrow</span>(data)<span class="sc">-</span><span class="fu">length</span>(fit<span class="sc">$</span>coefficients)<span class="sc">-</span><span class="dv">2</span>))</span>
<span id="cb968-2"><a href="week-13-lab.html#cb968-2" tabindex="-1"></a><span class="fu">plot</span>(fit,<span class="at">which=</span><span class="dv">4</span>,<span class="at">cook.levels=</span>cutoff)</span></code></pre></div>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>Another useful plot is created by ‘influencePlot’ which creates a “bubble” plot of studentized residuals by hat values, with the areas of the circles representing the observations proportional to Cook’s distances. Vertical reference lines are drawn at twice and three times the average hat value, horizontal reference lines at -2,0,2 on the studentized-residual scale.</p>
<div class="sourceCode" id="cb969"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb969-1"><a href="week-13-lab.html#cb969-1" tabindex="-1"></a><span class="fu">influencePlot</span>(fit,<span class="at">id.method=</span><span class="st">&quot;identify&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in plot.window(...): &quot;id.method&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy, type, ...): &quot;id.method&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.method&quot; is not
## a graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.method&quot; is not
## a graphical parameter</code></pre>
<pre><code>## Warning in box(...): &quot;id.method&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in title(...): &quot;id.method&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.method&quot; is not a
## graphical parameter</code></pre>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre><code>##         StudRes        Hat       CookD
## 981  -0.4853275 0.12224961 0.008214314
## 1047 -0.7584248 0.05140668 0.007799829
## 1439  6.2848828 0.02574610 0.241768060
## 1440  6.7972059 0.02067117 0.223016923</code></pre>
<p><em>Heteroskedacity</em></p>
<p>Although we can often pick up heteroskedacity graphically, we can test for it formally by using the ‘ncvTest’ function in the ‘car’ package</p>
<div class="sourceCode" id="cb977"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb977-1"><a href="week-13-lab.html#cb977-1" tabindex="-1"></a><span class="fu">ncvTest</span>(fit)</span></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 297.9263, Df = 1, p = &lt; 2.22e-16</code></pre>
<p>We can also use the function ‘spreadLevelPlot’ from the ‘car’ package to create plots for examining potential heteroskedacity in the data.</p>
<div class="sourceCode" id="cb979"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb979-1"><a href="week-13-lab.html#cb979-1" tabindex="-1"></a><span class="fu">spreadLevelPlot</span>(fit)</span></code></pre></div>
<pre><code>## Warning in spreadLevelPlot.lm(fit): 
## 47 negative fitted values removed</code></pre>
<p><img src="Week-13-lab_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<pre><code>## 
## Suggested power transformation:  0.4508167</code></pre>
<p><em>Multicollinearity</em></p>
<p>We can calculate the variance inflation factors using the function ‘vif’</p>
<div class="sourceCode" id="cb982"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb982-1"><a href="week-13-lab.html#cb982-1" tabindex="-1"></a><span class="fu">vif</span>(fit)</span></code></pre></div>
<pre><code>## log10(Mass)  log10(AFR)  LitterSize 
##    2.466580    2.434861    1.759614</code></pre>
<p><em>Serially correlated residuals</em></p>
<p>Remember, linear modelling assumes that the residuals independent and identically distributed. You can test whether the residuals are serially autocorrelated using the Durban-Watson statistic</p>
<p><span class="math display">\[
d=\frac{\sum^{T}_{t=2}(e_{t}-e_{t-1})^{2}}{\sum^{T}_{t=1}e^{2}_{t}}
\]</span></p>
<p>where T is the number of data points.</p>
<p>We can calculate the Durban-Watson test as follows:</p>
<div class="sourceCode" id="cb984"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb984-1"><a href="week-13-lab.html#cb984-1" tabindex="-1"></a><span class="fu">durbinWatsonTest</span>(fit)</span></code></pre></div>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1       0.6286888     0.6560464       0
##  Alternative hypothesis: rho != 0</code></pre>
<p><em>More help with model diagnostics</em></p>
<p>There is another package that can help with model diagnostics called the ‘gvlma’ or “Global Validation of Linear Model Assumptions”.</p>
<p>Install ‘gvlma’</p>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="week-13-lab.html#cb986-1" tabindex="-1"></a><span class="fu">library</span>(gvlma)</span>
<span id="cb986-2"><a href="week-13-lab.html#cb986-2" tabindex="-1"></a>gvmodel<span class="ot">&lt;-</span><span class="fu">gvlma</span>(fit)</span>
<span id="cb986-3"><a href="week-13-lab.html#cb986-3" tabindex="-1"></a><span class="fu">summary</span>(gvmodel)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MaxLifespan ~ log10(Mass) + log10(AFR) + LitterSize)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -263.66  -78.11  -19.31   53.52  763.75 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -168.4019    26.8904  -6.263 8.35e-10 ***
## log10(Mass)   60.7061     5.6008  10.839  &lt; 2e-16 ***
## log10(AFR)   152.5190    17.5461   8.692  &lt; 2e-16 ***
## LitterSize    -0.1925     3.8823  -0.050     0.96    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 118.7 on 485 degrees of freedom
##   (951 observations deleted due to missingness)
## Multiple R-squared:  0.6282, Adjusted R-squared:  0.6259 
## F-statistic: 273.1 on 3 and 485 DF,  p-value: &lt; 2.2e-16
## 
## 
## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
## Level of Significance =  0.05 
## 
## Call:
##  gvlma(x = fit) 
## 
##                     Value p-value                   Decision
## Global Stat        1752.4       0 Assumptions NOT satisfied!
## Skewness            285.2       0 Assumptions NOT satisfied!
## Kurtosis           1121.3       0 Assumptions NOT satisfied!
## Link Function       156.2       0 Assumptions NOT satisfied!
## Heteroscedasticity  189.6       0 Assumptions NOT satisfied!</code></pre>
<p>Yet another package we won’t discuss but may prove helpful is the ‘lmtest’ package.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-13-lecture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-14-lecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
