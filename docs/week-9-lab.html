<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>17 Week 9 Lab | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="17 Week 9 Lab | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="17 Week 9 Lab | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="17 Week 9 Lab | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-9-lecture.html"/>
<link rel="next" href="week-10-lecture.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-9-lab" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">17</span> Week 9 Lab<a href="week-9-lab.html#week-9-lab" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We will need 4 packages for this week’s lab, so we might as well load them all in now.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="week-9-lab.html#cb514-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb514-2"><a href="week-9-lab.html#cb514-2" tabindex="-1"></a><span class="fu">library</span>(car)</span></code></pre></div>
<pre><code>## Loading required package: carData</code></pre>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="week-9-lab.html#cb516-1" tabindex="-1"></a><span class="fu">library</span>(boot)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;boot&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:car&#39;:
## 
##     logit</code></pre>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="week-9-lab.html#cb519-1" tabindex="-1"></a><span class="fu">library</span>(smatr)</span></code></pre></div>
<div id="correlation-1" class="section level2 hasAnchor" number="17.1">
<h2><span class="header-section-number">17.1</span> Correlation<a href="week-9-lab.html#correlation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will first go over how to test for correlation between two variables. We will use a dataset of July mean temperatures at an Alaskan weather station (Prudhoe Bay) over a period of 12 years.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="week-9-lab.html#cb520-1" tabindex="-1"></a>Temperature<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">5.1</span>,<span class="fl">5.6</span>,<span class="fl">5.7</span>,<span class="fl">6.6</span>,<span class="fl">6.7</span>)</span>
<span id="cb520-2"><a href="week-9-lab.html#cb520-2" tabindex="-1"></a>Year<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">1979</span>,<span class="dv">1982</span>,<span class="dv">1985</span>,<span class="dv">1988</span>,<span class="dv">1991</span>)</span></code></pre></div>
<p>First we will plot the data to get a sense for whether the correlation coefficient is likely to be positive or negative.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="week-9-lab.html#cb521-1" tabindex="-1"></a><span class="fu">plot</span>(Year,Temperature,<span class="at">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We can test the correlation between Temperature and Year using the R function ‘cor.test’</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="week-9-lab.html#cb522-1" tabindex="-1"></a>ans<span class="ot">&lt;-</span><span class="fu">cor.test</span>(Temperature,Year)</span>
<span id="cb522-2"><a href="week-9-lab.html#cb522-2" tabindex="-1"></a>ans</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  Temperature and Year
## t = 6.4299, df = 3, p-value = 0.007626
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.5625553 0.9978118
## sample estimates:
##      cor 
## 0.965581</code></pre>
<p>‘cor’ is the correlation coefficient - we see there is a strongly positive (and statistically significant) correlation between year and temperature.</p>
<p>Let’s make sure we understand every part of this output.</p>
<p>Part 1: This is just spitting back what two variables are being correlated.</p>
<p>Part 2: t=6.4299
How do we get this? Remember:</p>
<p><span class="math display">\[
t_{s} = r\sqrt{\frac{n-2}{1-r^{2}}}
\]</span></p>
<p>We can calculate this by pulling out various elements of the variable ‘ans’.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="week-9-lab.html#cb524-1" tabindex="-1"></a><span class="fu">names</span>(ans)</span></code></pre></div>
<pre><code>## [1] &quot;statistic&quot;   &quot;parameter&quot;   &quot;p.value&quot;     &quot;estimate&quot;    &quot;null.value&quot; 
## [6] &quot;alternative&quot; &quot;method&quot;      &quot;data.name&quot;   &quot;conf.int&quot;</code></pre>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="week-9-lab.html#cb526-1" tabindex="-1"></a>r<span class="ot">&lt;-</span><span class="fu">as.numeric</span>(ans<span class="sc">$</span>estimate) <span class="co">#as.numeric supresses labels</span></span>
<span id="cb526-2"><a href="week-9-lab.html#cb526-2" tabindex="-1"></a>df<span class="ot">&lt;-</span><span class="fu">as.numeric</span>(ans<span class="sc">$</span>parameter)</span>
<span id="cb526-3"><a href="week-9-lab.html#cb526-3" tabindex="-1"></a>r<span class="sc">*</span><span class="fu">sqrt</span>(df<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>(r<span class="sc">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] 6.429911</code></pre>
<p>This gives us the same as</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="week-9-lab.html#cb528-1" tabindex="-1"></a>ans<span class="sc">$</span>statistic</span></code></pre></div>
<pre><code>##        t 
## 6.429911</code></pre>
<p>Part 3:
Why is df=3? For a correlation coefficient, you have n-2 degrees of freedom - you lose one degree of freedom for the mean of each variable.</p>
<p>Part 4:
How do we get p-value = 0.007626? First, lets start with a plot of the t-distribution with three d.o.f.</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="week-9-lab.html#cb530-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>,<span class="fl">0.01</span>),<span class="fu">dt</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>,<span class="fl">0.01</span>),<span class="at">df=</span><span class="dv">3</span>),<span class="at">typ=</span><span class="st">&quot;l&quot;</span>,<span class="at">col=</span><span class="st">&quot;purple&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb530-2"><a href="week-9-lab.html#cb530-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fl">6.4299</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can ask P(t&gt;6.4299)=1-P(t&lt;6.4299) by</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="week-9-lab.html#cb531-1" tabindex="-1"></a><span class="fu">pt</span>(<span class="fl">6.4299</span>,<span class="at">df=</span><span class="dv">3</span>,<span class="at">lower.tail=</span>F)<span class="sc">*</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.007625665</code></pre>
<p>Why multiple by 2? Because we want a two-tailed test, so we want to consider correlations larger in magnitude that are either positive or negative.</p>
<p>Part 5: 95 percent confidence interval (0.5625553,0.9978118).</p>
<p><span class="math display">\[
z=\frac{1}{2}ln(\frac{1+r}{1-r})=arctanh(r)
\]</span></p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="week-9-lab.html#cb533-1" tabindex="-1"></a>z<span class="ot">&lt;-</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">+</span>r)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>r))</span>
<span id="cb533-2"><a href="week-9-lab.html#cb533-2" tabindex="-1"></a>z</span></code></pre></div>
<pre><code>## [1] 2.022468</code></pre>
<p><span class="math display">\[
P(z-\frac{t_{[1-\alpha/2](\infty)}}{\sqrt{n-3}}\leq arctanh(\rho) \leq z+\frac{t_{[1-\alpha/2](\infty)}}{\sqrt{n-3}})=1-\alpha
\]</span></p>
<p><span class="math display">\[
P(tanh(z-\frac{t_{[1-\alpha/2](\infty)}}{\sqrt{n-3}})\leq \rho \leq tanh(z+\frac{t_{[1-\alpha/2](\infty)}}{\sqrt{n-3}}))=1-\alpha
\]</span></p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="week-9-lab.html#cb535-1" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">5</span></span>
<span id="cb535-2"><a href="week-9-lab.html#cb535-2" tabindex="-1"></a>LL.z<span class="ot">&lt;-</span>z<span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(n<span class="dv">-3</span>))<span class="sc">*</span><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb535-3"><a href="week-9-lab.html#cb535-3" tabindex="-1"></a>LL.r<span class="ot">&lt;-</span><span class="fu">tanh</span>(LL.z)</span>
<span id="cb535-4"><a href="week-9-lab.html#cb535-4" tabindex="-1"></a>LL.r</span></code></pre></div>
<pre><code>## [1] 0.5625553</code></pre>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="week-9-lab.html#cb537-1" tabindex="-1"></a>UL.z<span class="ot">&lt;-</span>z<span class="sc">+</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(n<span class="dv">-3</span>))<span class="sc">*</span><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb537-2"><a href="week-9-lab.html#cb537-2" tabindex="-1"></a>UL.r<span class="ot">&lt;-</span><span class="fu">tanh</span>(UL.z)</span>
<span id="cb537-3"><a href="week-9-lab.html#cb537-3" tabindex="-1"></a>UL.r</span></code></pre></div>
<pre><code>## [1] 0.9978118</code></pre>
<p>Notice that I can extract the LL and the UL by taking the tanh of the limits for the transformed variable.</p>
<p>In class we discussed several different ways of testing for a correlation. We can see these by querying the help page for ‘cor’ and ‘cor.test’:</p>
<pre><code>?cor.test</code></pre>
<p>Notice that under the method option there are three options. The “pearson” is the first (default). We can change the default by trying</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="week-9-lab.html#cb540-1" tabindex="-1"></a><span class="fu">cor.test</span>(Temperature,Year, <span class="at">method=</span><span class="st">&quot;kendall&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  Temperature and Year
## T = 10, p-value = 0.01667
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
## tau 
##   1</code></pre>
<p>Does it make sense why Kendall’s tau=1.0?</p>
<p>Now that we have some practice with correlations, let play a game! Each member of your group should visit <a href="https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html">this site</a> and play a few rounds. Click the “Track Performance” check box to track your performance over time. <strong><span style="color: green;">Checkpoint #1: Who in your group is doing the best?</span></strong></p>
</div>
<div id="linear-modelling" class="section level2 hasAnchor" number="17.2">
<h2><span class="header-section-number">17.2</span> Linear modelling<a href="week-9-lab.html#linear-modelling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear modeling in R occurs primarily through two functions ‘lm’ and ‘glm’. The first is reserved for linear regression in the form we have been discussing this week. The second function is for generalized linear models; we will discuss these in the next few weeks.</p>
<p><strong>Fitting simulated data</strong></p>
<p>Before working with real data, let’s play around with a simulated dataset, so you can see how the values used to simulate the data are reflected in the parameter estimates themselves.</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="week-9-lab.html#cb542-1" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">30</span></span>
<span id="cb542-2"><a href="week-9-lab.html#cb542-2" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">1</span>,n) <span class="co">#a stand in for some covariate</span></span>
<span id="cb542-3"><a href="week-9-lab.html#cb542-3" tabindex="-1"></a>value<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb542-4"><a href="week-9-lab.html#cb542-4" tabindex="-1"></a>intercept<span class="ot">&lt;-</span><span class="fl">0.15</span></span>
<span id="cb542-5"><a href="week-9-lab.html#cb542-5" tabindex="-1"></a>slope<span class="ot">&lt;-</span><span class="sc">-</span><span class="fl">2.2</span></span>
<span id="cb542-6"><a href="week-9-lab.html#cb542-6" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">10</span></span>
<span id="cb542-7"><a href="week-9-lab.html#cb542-7" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(X))</span>
<span id="cb542-8"><a href="week-9-lab.html#cb542-8" tabindex="-1"></a>{</span>
<span id="cb542-9"><a href="week-9-lab.html#cb542-9" tabindex="-1"></a>  value<span class="ot">&lt;-</span><span class="fu">c</span>(value,<span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="at">mean=</span>intercept<span class="sc">+</span>slope<span class="sc">*</span>X[i],<span class="at">sd=</span>sigma))</span>
<span id="cb542-10"><a href="week-9-lab.html#cb542-10" tabindex="-1"></a>}</span>
<span id="cb542-11"><a href="week-9-lab.html#cb542-11" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">lm</span>(value<span class="sc">~</span>X)</span>
<span id="cb542-12"><a href="week-9-lab.html#cb542-12" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = value ~ X)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -20.4686  -7.3213  -0.4324   7.2259  26.9337 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.7662     4.4898   1.062    0.298    
## X            -2.4079     0.2529  -9.521 2.81e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.99 on 28 degrees of freedom
## Multiple R-squared:  0.764,  Adjusted R-squared:  0.7556 
## F-statistic: 90.65 on 1 and 28 DF,  p-value: 2.812e-10</code></pre>
<p>Copy this script into R and r-run it several times. Notice how the estimates for slope and intercept bounce around, but they should be correct <em>on average</em> and also the scale of variation from one run to the next should make sense given the estimate of the standard error. (Their standard deviation should be the standard error.) Notice also that as you increase sigma, the R2 goes down because now you are increasing the variation that is <em>not</em> explained by the covariate. Try changing the number of samples drawn, either by extending the vector of the covariates or by drawing multiple times for each value (you will have to modify the code to make this latter change work). Notice how the standard errors on the intercept and slope coefficients gets smaller as the data set gets larger but the estimate for sigma (which is listed as the residual standard error near the bottom) does not. The parameter sigma is a property of the underlying population, not a property of the sample drawn, so it does not get smaller as you increase the number of samples in the dataset. (If this does not make sense, ask me!)</p>
<p>We can use this simple model to define some common (and often confusing) terms used in regression (and later, ANOVA), which will require using the function “residuals” to pull out the difference between each point and the best-fit line.</p>
<p>The <em>mean squared error</em> is usually defined as</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="week-9-lab.html#cb544-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 134.1711</code></pre>
<p>which is the same as</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="week-9-lab.html#cb546-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>n</span></code></pre></div>
<pre><code>## [1] 134.1711</code></pre>
<p>while the <em>root mean squared error</em> is</p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="week-9-lab.html#cb548-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 11.58323</code></pre>
<p>which is just the square-root of the mean squared error above. Note that some authors (like Aho) will divide by the degrees of freedom to get an unbiased estimate of the population variance <span class="math inline">\(\sigma^{2}_{\epsilon}\)</span> and call that the mean squared error (or MSE). Just be careful with these terms to know whether the calculation is a description of the residuals observed (in which case the denominator is <span class="math inline">\(n\)</span>) or whether the calculation is being used as an unbiased estimate of the larger population of residuals (in which case the denominator should be the degrees of freedom).</p>
<p>The <em>residual sum of squares</em> is actually better thought of as the “sum of residuals squared”, i.e.</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="week-9-lab.html#cb550-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 4025.134</code></pre>
<p>and the <em>residual standard error</em> is</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="week-9-lab.html#cb552-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="dv">-2</span>))</span></code></pre></div>
<pre><code>## [1] 11.98978</code></pre>
<p>Note that this last term uses the degrees of freedom in the numerator. The residual standard error is taking the data you have as a sample from the larger population and trying to estimate the standard error from the larger population. So it takes the residual sum of squares, divides that by the degrees of freedom (we have 30 data points, we lost 2 degrees of freedom, so we are left with 28 degrees of freedom for the estimation of the residual standard error) and then takes the square root. We can think of the mean squared error as being the population variance (i.e. the variance of the residuals if the dataset represented the entire population, see <a href="week-1-lab.html#pop_vs_sample_var">our discussion in Week 1</a>) but this underestimates the variance if all we have is a sample from the larger population, so in this case we want to calculate the <em>sample variance</em> (i.e. our estimate of the population variance if all we have is a sample)</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="week-9-lab.html#cb554-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">residuals</span>(fit)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span>(n<span class="sc">/</span>(n<span class="dv">-2</span>))</span></code></pre></div>
<pre><code>## [1] 143.7548</code></pre>
<p>This should come close to what we used to generate the data (<span class="math inline">\(\sigma=10\)</span> so <span class="math inline">\(\sigma^{2}=100\)</span>). (If you go back and change the code to use a larger number of data points, the estimate will be closer.)</p>
<p>There is a function called sigma() is the R package ‘stats’ that may be loaded by default in your workspace. sigma(fit) will grab that residual standard error value, which is helpful if you need to extract an estimate of <span class="math inline">\(\sigma\)</span> (a.k.a. <span class="math inline">\(\hat{\sigma}\)</span>, which is really estimated as <span class="math inline">\(\sqrt{\widehat{\sigma^2}}\)</span> ) from the model fit.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="week-9-lab.html#cb556-1" tabindex="-1"></a><span class="fu">sigma</span>(fit)</span></code></pre></div>
<pre><code>## [1] 11.98978</code></pre>
<p>Notice that this is note quite the same (and is always slightly larger) than this alternative estimate of <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="week-9-lab.html#cb558-1" tabindex="-1"></a><span class="fu">fitdistr</span>(<span class="fu">residuals</span>(fit),<span class="st">&quot;normal&quot;</span>)<span class="sc">$</span>estimate[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>##       sd 
## 11.58323</code></pre>
<p>This takes your residuals, uses fitdistr to fit a Normal distribution to them, and then reports the standard deviation of the residuals. This should give you a measure of the spread of the residuals, which should be the same as the residual standard error extracted from sigma(fit).</p>
<p><strong>Question: Why does this method always underestimate <span class="math inline">\(\sigma\)</span> (which you know here because you used a known value to simulate the data)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">Because fitdistr() uses MLE to estimate the parameters of the Normal fit to the residuals, and the MLE for <span class="math inline">\(\sigma^2\)</span> are <strong>biased</strong> estimates. Keep in mind that MLE estimates are not guaranteed to be unbiased.
</span>
</details>
<p><strong>Fitting real data</strong></p>
<p>Now that we’ve gained some intuition, we can dive into fitting a real dataset. The syntax of ‘lm’ is straightforward. We will run through some examples using a dataset on Old Faithful eruptions. The dataset is built into the MASS library, so we just have to load it.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="week-9-lab.html#cb560-1" tabindex="-1"></a><span class="fu">attach</span>(faithful) <span class="co">#A rare exception to the rule of avoiding &#39;attach&#39;</span></span>
<span id="cb560-2"><a href="week-9-lab.html#cb560-2" tabindex="-1"></a><span class="fu">head</span>(faithful)</span></code></pre></div>
<pre><code>##   eruptions waiting
## 1     3.600      79
## 2     1.800      54
## 3     3.333      74
## 4     2.283      62
## 5     4.533      85
## 6     2.883      55</code></pre>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="week-9-lab.html#cb562-1" tabindex="-1"></a><span class="fu">plot</span>(waiting, eruptions,<span class="at">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>This dataset lists the times of an Old Faithful eruption as a function of the waiting time prior to the eruption.</p>
<p>We can see that as the waiting time increases, so does the length of the eruption. We can fit a linear regression model to this relationship using the R function ‘lm’.</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="week-9-lab.html#cb563-1" tabindex="-1"></a>eruption.lm<span class="ot">&lt;-</span><span class="fu">lm</span>(eruptions<span class="sc">~</span>waiting)</span></code></pre></div>
<p>Model: y~x1<br />
Meaning: y is explained by x1 only (intercept implicit)</p>
<p>Model: y~x1-1<br />
Meaning: y is explained by x1 only (no intercept)</p>
<p>Model: y~x1+x2<br />
Meaning: y is explained x1 and x2</p>
<p>Model: x1+x2+x1:x2<br />
Meaning: y is explained by x1,x2 and also by the interaction between them</p>
<p>Model: y~x1*x2<br />
Meaning: y is explained by x1,x2 and also by the interaction between them (this is an alternative way of writing the above)</p>
<p>We print out a summary of the linear regression results as follows:</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="week-9-lab.html#cb564-1" tabindex="-1"></a><span class="fu">summary</span>(eruption.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = eruptions ~ waiting)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.29917 -0.37689  0.03508  0.34909  1.19329 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.874016   0.160143  -11.70   &lt;2e-16 ***
## waiting      0.075628   0.002219   34.09   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4965 on 270 degrees of freedom
## Multiple R-squared:  0.8115, Adjusted R-squared:  0.8108 
## F-statistic:  1162 on 1 and 270 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><em>Question: In words, how would we interpret the coefficients for this model?</em></p>
<p>Make sure you understand all of this output!</p>
<p>We can check the output on the residuals using either the R function ‘residuals’ which takes in the lm object and spits out the residuals of the fit</p>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="week-9-lab.html#cb566-1" tabindex="-1"></a><span class="fu">quantile</span>(<span class="fu">residuals</span>(eruption.lm),<span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.0</span>,<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="fl">0.75</span>,<span class="fl">1.0</span>))</span></code></pre></div>
<pre><code>##          0%         25%         50%         75%        100% 
## -1.29917268 -0.37689320  0.03508321  0.34909412  1.19329194</code></pre>
<p>or by using the R function ‘predict’ (which calculates the predicted y value for each x value) and calculating the residuals ourselves:</p>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="week-9-lab.html#cb568-1" tabindex="-1"></a>residuals<span class="ot">&lt;-</span>eruptions<span class="sc">-</span><span class="fu">predict</span>(eruption.lm, <span class="fu">data.frame</span>(waiting))</span>
<span id="cb568-2"><a href="week-9-lab.html#cb568-2" tabindex="-1"></a><span class="co"># Note that predict wants a dataframe of values</span></span>
<span id="cb568-3"><a href="week-9-lab.html#cb568-3" tabindex="-1"></a><span class="fu">quantile</span>(residuals,<span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.0</span>,<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="fl">0.75</span>,<span class="fl">1.0</span>))</span></code></pre></div>
<pre><code>##          0%         25%         50%         75%        100% 
## -1.29917268 -0.37689320  0.03508321  0.34909412  1.19329194</code></pre>
<p>Now we can calculate the slope and its standard error:</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="week-9-lab.html#cb570-1" tabindex="-1"></a>x<span class="ot">&lt;-</span>waiting</span>
<span id="cb570-2"><a href="week-9-lab.html#cb570-2" tabindex="-1"></a>y<span class="ot">&lt;-</span>eruptions</span>
<span id="cb570-3"><a href="week-9-lab.html#cb570-3" tabindex="-1"></a>SSXY<span class="ot">&lt;-</span><span class="fu">sum</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">*</span>(y<span class="sc">-</span><span class="fu">mean</span>(y)))</span>
<span id="cb570-4"><a href="week-9-lab.html#cb570-4" tabindex="-1"></a>SSX<span class="ot">&lt;-</span><span class="fu">sum</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">*</span>(x<span class="sc">-</span><span class="fu">mean</span>(x)))</span>
<span id="cb570-5"><a href="week-9-lab.html#cb570-5" tabindex="-1"></a>slope.est<span class="ot">&lt;-</span>SSXY<span class="sc">/</span>SSX</span>
<span id="cb570-6"><a href="week-9-lab.html#cb570-6" tabindex="-1"></a><span class="co">#Also could have used slope.est&lt;-cov(x,y)/var(x)</span></span>
<span id="cb570-7"><a href="week-9-lab.html#cb570-7" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="fu">length</span>(x)</span>
<span id="cb570-8"><a href="week-9-lab.html#cb570-8" tabindex="-1"></a>residuals<span class="ot">&lt;-</span><span class="fu">residuals</span>(eruption.lm)</span>
<span id="cb570-9"><a href="week-9-lab.html#cb570-9" tabindex="-1"></a>var.slope<span class="ot">&lt;-</span>(<span class="dv">1</span><span class="sc">/</span>(n<span class="dv">-2</span>))<span class="sc">*</span><span class="fu">sum</span>((residuals<span class="sc">-</span><span class="fu">mean</span>(residuals))<span class="sc">*</span>(residuals<span class="sc">-</span><span class="fu">mean</span>(residuals)))<span class="sc">/</span>SSX</span>
<span id="cb570-10"><a href="week-9-lab.html#cb570-10" tabindex="-1"></a>s.e.slope<span class="ot">&lt;-</span><span class="fu">sqrt</span>(var.slope)</span></code></pre></div>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="week-9-lab.html#cb571-1" tabindex="-1"></a>slope.est</span></code></pre></div>
<pre><code>## [1] 0.07562795</code></pre>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="week-9-lab.html#cb573-1" tabindex="-1"></a>s.e.slope</span></code></pre></div>
<pre><code>## [1] 0.002218541</code></pre>
<p>We calculate the t-value as:</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="week-9-lab.html#cb575-1" tabindex="-1"></a>t.value<span class="ot">&lt;-</span>slope.est<span class="sc">/</span>s.e.slope</span>
<span id="cb575-2"><a href="week-9-lab.html#cb575-2" tabindex="-1"></a>p.value<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(<span class="fu">abs</span>(t.value),n<span class="dv">-2</span>))</span>
<span id="cb575-3"><a href="week-9-lab.html#cb575-3" tabindex="-1"></a>t.value</span></code></pre></div>
<pre><code>## [1] 34.08904</code></pre>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="week-9-lab.html#cb577-1" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>We can calculate the intercept and its standard error in a similar manner.</p>
<p>The residual standard error is:</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="week-9-lab.html#cb579-1" tabindex="-1"></a>residual.se<span class="ot">&lt;-</span><span class="fu">sqrt</span>((<span class="dv">1</span><span class="sc">/</span>(n<span class="dv">-2</span>))<span class="sc">*</span><span class="fu">sum</span>((residuals<span class="sc">-</span><span class="fu">mean</span>(residuals))<span class="sc">*</span>(residuals<span class="sc">-</span><span class="fu">mean</span>(residuals))))</span>
<span id="cb579-2"><a href="week-9-lab.html#cb579-2" tabindex="-1"></a>residual.se</span></code></pre></div>
<pre><code>## [1] 0.4965129</code></pre>
<p>and the R2 as</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="week-9-lab.html#cb581-1" tabindex="-1"></a>SST<span class="ot">&lt;-</span><span class="fu">sum</span>((y<span class="sc">-</span><span class="fu">mean</span>(y))<span class="sc">*</span>(y<span class="sc">-</span><span class="fu">mean</span>(y)))</span>
<span id="cb581-2"><a href="week-9-lab.html#cb581-2" tabindex="-1"></a>SSR<span class="ot">&lt;-</span>SST<span class="sc">-</span><span class="fu">sum</span>(residuals<span class="sc">*</span>residuals)</span>
<span id="cb581-3"><a href="week-9-lab.html#cb581-3" tabindex="-1"></a>R2<span class="ot">&lt;-</span>SSR<span class="sc">/</span>SST</span>
<span id="cb581-4"><a href="week-9-lab.html#cb581-4" tabindex="-1"></a>R2</span></code></pre></div>
<pre><code>## [1] 0.8114608</code></pre>
<p><strong><span style="color: green;">Checkpoint #2: Were you able to reproduce all of the output from cor.test covered so far?</span></strong> R also produces an “adjusted R2”, which attempts to account for the number of parameters being estimated, and provides one way of comparing goodness of fit between models with different numbers of parameters. It is defined as</p>
<p><span class="math display">\[
R^{2}_{adj} = 1-(1-R^{2})\left(\frac{n-1}{n-p-1}\right)
\]</span></p>
<p>but we won’t get into more details here.</p>
<p>Notice that the percentage of explained variation <span class="math inline">\(R^{2}\)</span> is just the square of the Pearson’s product moment correlation coefficient.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="week-9-lab.html#cb583-1" tabindex="-1"></a>(<span class="fu">cor</span>(x,y))<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.8114608</code></pre>
<p>We will hold off on a discussion of the F statistic until we do ANOVA next week.</p>
<p>In lecture we distinguished between confidence intervals and prediction intervals. The former tells us our uncertainty about the <em>mean</em> of Y at a given X, whereas the latter gives us the interval within which we expect a new value of Y to fall for a given X. We can calculate both of these using the option ‘interval’ in the predict function.</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="week-9-lab.html#cb585-1" tabindex="-1"></a>newdata<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">waiting=</span><span class="fu">seq</span>(<span class="fu">min</span>(waiting),<span class="fu">max</span>(waiting)))</span>
<span id="cb585-2"><a href="week-9-lab.html#cb585-2" tabindex="-1"></a>confidence.bands<span class="ot">&lt;-</span><span class="fu">predict</span>(eruption.lm,newdata,<span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb585-3"><a href="week-9-lab.html#cb585-3" tabindex="-1"></a>prediction.bands<span class="ot">&lt;-</span><span class="fu">predict</span>(eruption.lm,newdata,<span class="at">interval=</span><span class="st">&quot;predict&quot;</span>)</span>
<span id="cb585-4"><a href="week-9-lab.html#cb585-4" tabindex="-1"></a><span class="fu">plot</span>(waiting,eruptions,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">7</span>))</span>
<span id="cb585-5"><a href="week-9-lab.html#cb585-5" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],confidence.bands[,<span class="dv">1</span>],<span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb585-6"><a href="week-9-lab.html#cb585-6" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],confidence.bands[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb585-7"><a href="week-9-lab.html#cb585-7" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],confidence.bands[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb585-8"><a href="week-9-lab.html#cb585-8" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],prediction.bands[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb585-9"><a href="week-9-lab.html#cb585-9" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],prediction.bands[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p><strong><span style="color: green;">Checkpoint #3: Do you understand the difference in interpretation between a confidence interval and a prediction interval? Do you understand why the prediction interval is always wider? </span></strong></p>
<p>What do we do if we want to force the intercept through the origin (i.e., set the intercept to zero)?</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="week-9-lab.html#cb586-1" tabindex="-1"></a>eruption.lm2<span class="ot">&lt;-</span><span class="fu">lm</span>(eruptions<span class="sc">~</span>waiting<span class="dv">-1</span>)</span>
<span id="cb586-2"><a href="week-9-lab.html#cb586-2" tabindex="-1"></a><span class="fu">summary</span>(eruption.lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = eruptions ~ waiting - 1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.54127 -0.57533 -0.00846  0.42257  1.25718 
## 
## Coefficients:
##          Estimate Std. Error t value Pr(&gt;|t|)    
## waiting 0.0501292  0.0005111   98.09   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6084 on 271 degrees of freedom
## Multiple R-squared:  0.9726, Adjusted R-squared:  0.9725 
## F-statistic:  9621 on 1 and 271 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Wait…look closely…what’s strange about the model when we suppress the intercept?</p>
<p>Somehow we have gone from a bigger model (intercept and slope) to a smaller model (slope only) and R is telling us that the fit of the model has actually improved. Not possible! So what is going on?</p>
<p>Remember the definition of R2:</p>
<p><span class="math display">\[
R^{2} = \frac{SSR}{SST} = 1-\frac{SSE}{SST} = 1-\frac{\Sigma{(Y_{i}-\hat{Y_{i}})^{2}}}{\Sigma(Y_{i}-\bar{Y})^{2}}
\]</span></p>
<p>When there is no intercept, R (silently!) uses an alternative definition of R2</p>
<p><span class="math display">\[
R^{2} = 1-\frac{\Sigma{(Y_{i}-\hat{Y_{i}})^{2}}}{\Sigma{Y_{i}^{2}}}
\]</span></p>
<p>Why does R do that? In the first case, you have a slope and an intercept, and R is comparing the model you have against an alternate model which includes only an intercept. When you have an intercept-only model, that intercept is going to be the mean <span class="math inline">\(\bar{Y}\)</span>. (Does it make sense why that is?) However, when you have supressed the intercept, the original alternate model (intercept only) no longer makes sense. So R chooses a new alternate model which is one of just random noise with <span class="math inline">\(\bar{Y}=0\)</span>. If we look at the expression above, the effect of this is to increase the residuals going into SSE and the total sum of squares SST. However, the increase in SST is generally larger than the increase in SSE, which means that the R2 actually increases. The bottom line is that funny things happy when you suppress the intercept and while the output (effect sizes and standard errors) is still perfectly valid, the metrics of model fit become different and the with-intercept and without-intercept models can no longer be compared sensibly.</p>
</div>
<div id="weighted-regression" class="section level2 hasAnchor" number="17.3">
<h2><span class="header-section-number">17.3</span> Weighted regression<a href="week-9-lab.html#weighted-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In lecture, we introduced the idea that ordinary least squares regression involves minimizing the sum-of-squares error</p>
<p><span class="math display">\[
SSE = \sum^{n}_{i=1}(Y_{i}-\hat{Y}_{i})^{2}
\]</span></p>
<p>where squared residuals are summed as a measure of model fit. Sometimes, however, you want to weight some residuals more or less than others. Often this is done to account for increased variability in the responses Y over a certain range of Xs. We can do this through weighted linear regression, i.e. by minimizing the weighted residuals</p>
<p><span class="math display">\[
SSE = \sum^{n}_{i=1}w_{i}(Y_{i}-\hat{Y}_{i})^{2}
\]</span></p>
<p>We can illustrate doing this by using the ‘weights’ option in lm. In this example, we will see what happens when we weight the short eruptions 2 and 10 as much as the long eruptions. The result of this will be that the best fit model will try and fit the short eruptions better because residuals for short eruptions are two or ten times as influential to SSE than residuals for long eruptions.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="week-9-lab.html#cb588-1" tabindex="-1"></a><span class="fu">plot</span>(waiting,eruptions,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">7</span>))</span>
<span id="cb588-2"><a href="week-9-lab.html#cb588-2" tabindex="-1"></a><span class="fu">lines</span>(newdata[,<span class="dv">1</span>],confidence.bands[,<span class="dv">1</span>])</span>
<span id="cb588-3"><a href="week-9-lab.html#cb588-3" tabindex="-1"></a>short<span class="ot">&lt;-</span>(eruptions<span class="sc">&lt;</span><span class="dv">3</span>)</span>
<span id="cb588-4"><a href="week-9-lab.html#cb588-4" tabindex="-1"></a><span class="fu">points</span>(waiting[short],eruptions[short],<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb588-5"><a href="week-9-lab.html#cb588-5" tabindex="-1"></a>eruption.lm<span class="ot">&lt;-</span><span class="fu">lm</span>(eruptions<span class="sc">~</span>waiting,<span class="at">weights=</span><span class="fu">rep</span>(<span class="dv">1</span>,<span class="at">times=</span><span class="dv">272</span>))</span>
<span id="cb588-6"><a href="week-9-lab.html#cb588-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span>eruption.lm<span class="sc">$</span>coef[<span class="dv">1</span>],<span class="at">b=</span>eruption.lm<span class="sc">$</span>coef[<span class="dv">2</span>],<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb588-7"><a href="week-9-lab.html#cb588-7" tabindex="-1"></a>eruption.lm.wt<span class="ot">&lt;-</span><span class="fu">lm</span>(eruptions<span class="sc">~</span>waiting,<span class="at">weights=</span><span class="fu">rep</span>(<span class="dv">1</span>,<span class="at">times=</span><span class="dv">272</span>)<span class="sc">+</span><span class="fu">as.numeric</span>(short))</span>
<span id="cb588-8"><a href="week-9-lab.html#cb588-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span>eruption.lm.wt<span class="sc">$</span>coef[<span class="dv">1</span>],<span class="at">b=</span>eruption.lm.wt<span class="sc">$</span>coef[<span class="dv">2</span>],<span class="at">col=</span><span class="st">&quot;green&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb588-9"><a href="week-9-lab.html#cb588-9" tabindex="-1"></a>eruption.lm.wt<span class="ot">&lt;-</span><span class="fu">lm</span>(eruptions<span class="sc">~</span>waiting,<span class="at">weights=</span><span class="fu">rep</span>(<span class="dv">1</span>,<span class="at">times=</span><span class="dv">272</span>)<span class="sc">+</span><span class="dv">9</span><span class="sc">*</span><span class="fu">as.numeric</span>(short))</span>
<span id="cb588-10"><a href="week-9-lab.html#cb588-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span>eruption.lm.wt<span class="sc">$</span>coef[<span class="dv">1</span>],<span class="at">b=</span>eruption.lm.wt<span class="sc">$</span>coef[<span class="dv">2</span>],<span class="at">col=</span><span class="st">&quot;purple&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
<div id="robust-regression-1" class="section level2 hasAnchor" number="17.4">
<h2><span class="header-section-number">17.4</span> Robust regression<a href="week-9-lab.html#robust-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Weighted linear regression would be one method that could be used for downweighting the influence of certain data points. Robust regression is a another method for making sure that your linear model fit is not unduly influenced by outliers (points with large residuals).</p>
<p>We will use the Duncan occupational dataset we used once before</p>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="week-9-lab.html#cb589-1" tabindex="-1"></a><span class="fu">library</span>(car) <span class="co"># for Duncan data and (later) data.ellipse)</span></span>
<span id="cb589-2"><a href="week-9-lab.html#cb589-2" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb589-3"><a href="week-9-lab.html#cb589-3" tabindex="-1"></a><span class="fu">data</span>(Duncan)</span>
<span id="cb589-4"><a href="week-9-lab.html#cb589-4" tabindex="-1"></a>Duncan</span></code></pre></div>
<pre><code>##                    type income education prestige
## accountant         prof     62        86       82
## pilot              prof     72        76       83
## architect          prof     75        92       90
## author             prof     55        90       76
## chemist            prof     64        86       90
## minister           prof     21        84       87
## professor          prof     64        93       93
## dentist            prof     80       100       90
## reporter             wc     67        87       52
## engineer           prof     72        86       88
## undertaker         prof     42        74       57
## lawyer             prof     76        98       89
## physician          prof     76        97       97
## welfare.worker     prof     41        84       59
## teacher            prof     48        91       73
## conductor            wc     76        34       38
## contractor         prof     53        45       76
## factory.owner      prof     60        56       81
## store.manager      prof     42        44       45
## banker             prof     78        82       92
## bookkeeper           wc     29        72       39
## mail.carrier         wc     48        55       34
## insurance.agent      wc     55        71       41
## store.clerk          wc     29        50       16
## carpenter            bc     21        23       33
## electrician          bc     47        39       53
## RR.engineer          bc     81        28       67
## machinist            bc     36        32       57
## auto.repairman       bc     22        22       26
## plumber              bc     44        25       29
## gas.stn.attendant    bc     15        29       10
## coal.miner           bc      7         7       15
## streetcar.motorman   bc     42        26       19
## taxi.driver          bc      9        19       10
## truck.driver         bc     21        15       13
## machine.operator     bc     21        20       24
## barber               bc     16        26       20
## bartender            bc     16        28        7
## shoe.shiner          bc      9        17        3
## cook                 bc     14        22       16
## soda.clerk           bc     12        30        6
## watchman             bc     17        25       11
## janitor              bc      7        20        8
## policeman            bc     34        47       41
## waiter               bc      8        32       10</code></pre>
<p>Let’s identify any data points we think are outliers</p>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="week-9-lab.html#cb591-1" tabindex="-1"></a><span class="fu">plot</span>(Duncan<span class="sc">$</span>education,Duncan<span class="sc">$</span>income,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">100</span>))</span>
<span id="cb591-2"><a href="week-9-lab.html#cb591-2" tabindex="-1"></a>temp<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">which</span>(<span class="fu">rownames</span>(Duncan)<span class="sc">==</span><span class="st">&quot;RR.engineer&quot;</span>),<span class="fu">which</span>(<span class="fu">rownames</span>(Duncan)<span class="sc">==</span><span class="st">&quot;conductor&quot;</span>))</span>
<span id="cb591-3"><a href="week-9-lab.html#cb591-3" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>Duncan<span class="sc">$</span>education[temp]<span class="sc">-</span><span class="dv">8</span>,<span class="at">y=</span>Duncan<span class="sc">$</span>income[temp],<span class="at">labels=</span><span class="fu">rownames</span>(Duncan)[temp],<span class="at">cex=</span><span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="week-9-lab.html#cb592-1" tabindex="-1"></a><span class="co">#identify(x=Duncan$education, y=Duncan$income, labels=rownames(Duncan))</span></span></code></pre></div>
<p>Visually, we may think that conductors and railroad engineers may have a disproportionate influence on the linear regression of income and education. We will explore this by first doing a regular linear regression using ‘lm’ and then doing a robust linear regression using ‘rlm’.</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="week-9-lab.html#cb593-1" tabindex="-1"></a>Duncan.model.lm<span class="ot">&lt;-</span><span class="fu">lm</span>(income<span class="sc">~</span>education, <span class="at">data=</span>Duncan)</span>
<span id="cb593-2"><a href="week-9-lab.html#cb593-2" tabindex="-1"></a><span class="fu">summary</span>(Duncan.model.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = income ~ education, data = Duncan)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.572 -11.346  -1.501   9.669  53.740 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  10.6035     5.1983   2.040   0.0475 *  
## education     0.5949     0.0863   6.893 1.84e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 17.04 on 43 degrees of freedom
## Multiple R-squared:  0.5249, Adjusted R-squared:  0.5139 
## F-statistic: 47.51 on 1 and 43 DF,  p-value: 1.84e-08</code></pre>
<p>Let’s compare the fit with the one in which we remove the two potential outliers.</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="week-9-lab.html#cb595-1" tabindex="-1"></a>outliers<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">which</span>(<span class="fu">rownames</span>(Duncan)<span class="sc">==</span><span class="st">&quot;RR.engineer&quot;</span>),<span class="fu">which</span>(<span class="fu">rownames</span>(Duncan)<span class="sc">==</span><span class="st">&quot;conductor&quot;</span>))</span>
<span id="cb595-2"><a href="week-9-lab.html#cb595-2" tabindex="-1"></a>Duncan.model2<span class="ot">&lt;-</span><span class="fu">lm</span>(income[<span class="sc">-</span>outliers]<span class="sc">~</span>education[<span class="sc">-</span>outliers],<span class="at">data=</span>Duncan)</span></code></pre></div>
<p>We see that removing these two professions changes the slope and intercept, as expected. <strong><span style="color: green;">Checkpoint #4: Does the change in slope and intercept when you remove those outliers make sense intuitively?</span></strong> Let’s try doing a robust regression now. First, let’s remind ourselves that robust regression minimizes some function of the errors.</p>
<p><span class="math display">\[
\sum^{n}_{i=1}f(Y_{i}-\hat{Y}_{i})
\]</span></p>
<p>Let’s look at the help file for ‘rlm’:</p>
<pre><code>?rlm</code></pre>
<p>The default robust weighting scheme is Huber’s method.</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="week-9-lab.html#cb597-1" tabindex="-1"></a>Duncan.model.rlm<span class="ot">&lt;-</span><span class="fu">rlm</span>(income<span class="sc">~</span>education,<span class="at">data=</span>Duncan)</span>
<span id="cb597-2"><a href="week-9-lab.html#cb597-2" tabindex="-1"></a><span class="fu">summary</span>(Duncan.model.rlm)</span></code></pre></div>
<pre><code>## 
## Call: rlm(formula = income ~ education, data = Duncan)
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -40.8684  -9.8692   0.8085   7.8394  56.1770 
## 
## Coefficients:
##             Value  Std. Error t value
## (Intercept) 6.3002 4.4943     1.4018 
## education   0.6615 0.0746     8.8659 
## 
## Residual standard error: 13.06 on 43 degrees of freedom</code></pre>
<p>The residuals are much more similar to what we got from ‘lm’ when we excluded the outlying datapoints. Robust methods are generally preferred over removing outliers.</p>
</div>
<div id="bootstrapping-standard-errors-for-robust-regression" class="section level2 hasAnchor" number="17.5">
<h2><span class="header-section-number">17.5</span> Bootstrapping standard errors for robust regression<a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard errors reported by ‘rlm’ rely on asymptotic approximations that may not be particularly reliable in this case because our sample size is only 45. We will use bootstrapping to construct more appropriate standard errors.</p>
<p>There are two ways to do bootstrapping for calculating the standard errors of regression model parameters.</p>
<ol style="list-style-type: decimal">
<li>We can sample with replacement (X,Y) pairs from the original dataset.<br />
</li>
<li>We can sample with replacement residuals from the original model and use the same predictor variables, i.e. we use</li>
</ol>
<p><span class="math display">\[
(x_{1},\hat{y_{1}}+\epsilon^{*}_{1})
\]</span>
<span class="math display">\[
(x_{2},\hat{y_{2}}+\epsilon^{*}_{2})
\]</span>
<span class="math display">\[
(x_{3},\hat{y_{3}}+\epsilon^{*}_{3})
\]</span></p>
<p>You might use this latter approach if the predictor variables were fixed by the experimentor (they do not reflect a larger population of fixed values), so they should really remain fixed in calculating the standard errors.</p>
<p>Today I will only go through the mechanics of the first approach, called “random x resampling”. Although writing the bootstrap script yourself is straightforward, we will go through the functions available in the package ‘boot’.</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="week-9-lab.html#cb599-1" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb599-2"><a href="week-9-lab.html#cb599-2" tabindex="-1"></a>boot.huber<span class="ot">&lt;-</span><span class="cf">function</span>(data,indices,maxit)</span>
<span id="cb599-3"><a href="week-9-lab.html#cb599-3" tabindex="-1"></a>  {</span>
<span id="cb599-4"><a href="week-9-lab.html#cb599-4" tabindex="-1"></a>  data<span class="ot">&lt;-</span>data[indices,] <span class="co">#select observations in bootstrap sample</span></span>
<span id="cb599-5"><a href="week-9-lab.html#cb599-5" tabindex="-1"></a>  mod<span class="ot">&lt;-</span><span class="fu">rlm</span>(income<span class="sc">~</span>education,<span class="at">data=</span>data,<span class="at">maxit=</span>maxit)</span>
<span id="cb599-6"><a href="week-9-lab.html#cb599-6" tabindex="-1"></a>  <span class="fu">coefficients</span>(mod) <span class="co">#return the coefficient vector</span></span>
<span id="cb599-7"><a href="week-9-lab.html#cb599-7" tabindex="-1"></a>  }</span></code></pre></div>
<p>Note that we have to pass the function the data and the indices to be sampled. I’ve added an additional option to increase the number of iterations allowed for the rlm estimator to converge.</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="week-9-lab.html#cb600-1" tabindex="-1"></a>duncan.boot<span class="ot">&lt;-</span><span class="fu">boot</span>(Duncan,boot.huber,<span class="dv">1999</span>,<span class="at">maxit=</span><span class="dv">100</span>)</span>
<span id="cb600-2"><a href="week-9-lab.html#cb600-2" tabindex="-1"></a>duncan.boot</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Duncan, statistic = boot.huber, R = 1999, maxit = 100)
## 
## 
## Bootstrap Statistics :
##      original       bias    std. error
## t1* 6.3002197  0.333641380  4.52151690
## t2* 0.6615263 -0.006322549  0.07444823</code></pre>
<p><strong><span style="color: green;">Checkpoint #5: How would we know if the bias is significant (i.e., how would we calculate the standard error of the bias)?</span></strong></p>
</div>
<div id="type-i-vs.-type-ii-regression-the-smatr-package" class="section level2 hasAnchor" number="17.6">
<h2><span class="header-section-number">17.6</span> Type I vs. Type II regression: The ‘smatr’ package<a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The two main functions in the smatr package are ‘sma’ and ‘ma’ regression for doing standardized major axis regression. Look at the help file for sma to see what some of the options are.</p>
<pre><code>?sma</code></pre>
<p>Let’s say we wanted to look at the Duncan dataset again, but instead of asking whether we can use income to predict education, we can ask instead simply whether the two are correlated.</p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="week-9-lab.html#cb603-1" tabindex="-1"></a>Duncan.model.sma<span class="ot">&lt;-</span><span class="fu">sma</span>(income<span class="sc">~</span>education, <span class="at">data=</span>Duncan)</span>
<span id="cb603-2"><a href="week-9-lab.html#cb603-2" tabindex="-1"></a>Duncan.model.sma</span></code></pre></div>
<pre><code>## Call: sma(formula = income ~ education, data = Duncan) 
## 
## Fit using Standardized Major Axis 
## 
## ------------------------------------------------------------
## Coefficients:
##              elevation     slope
## estimate     -1.283968 0.8210480
## lower limit -11.965282 0.6652483
## upper limit   9.397345 1.0133356
## 
## H0 : variables uncorrelated
## R-squared : 0.5249182 
## P-value : 1.8399e-08</code></pre>
<p>This gives us a very different result from what we got from ‘lm’. Let’s plot the data, and the best-fit lines to see why this makes sense.</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="week-9-lab.html#cb605-1" tabindex="-1"></a><span class="fu">plot</span>(Duncan<span class="sc">$</span>education,Duncan<span class="sc">$</span>income)</span>
<span id="cb605-2"><a href="week-9-lab.html#cb605-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="fu">coef</span>(Duncan.model.lm)[<span class="dv">1</span>],<span class="at">b=</span><span class="fu">coef</span>(Duncan.model.lm)[<span class="dv">2</span>])</span>
<span id="cb605-3"><a href="week-9-lab.html#cb605-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="fu">coef</span>(Duncan.model.sma)[<span class="dv">1</span>],<span class="at">b=</span><span class="fu">coef</span>(Duncan.model.sma)[<span class="dv">2</span>],<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Week-9-lab_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>The SMA line is closer to what you would probably draw by eye as going through the ‘cloud’ of points, since our instinct is to draw a line through the principle axis of variation and not through the regression line, which has a smaller slope.</p>
<p><span class="math display">\[
\mbox{SMA slope} = \frac{\mbox{OLS slope}}{|\mbox{correlation r}|}
\]</span></p>
<p>The SMA slope is the OLS slope divided by the absolute value of the Pearson’s product moment correlation coefficient and is always, therefore, steeper than the OLS slope.</p>
<p>So, how do we use the ‘smatr’ package?</p>
<p>sma(y~x) will fit a SMA for y vs. x, and report confidence intervals for the slope and elevation (a.k.a., the intercept).</p>
<p>sma(y~x,robust=T) will fit a robust SMA for y vs. x using Huber’s M estimation, and will report (approximate) confidence intervals for the slope and elevation.</p>
<p>ma(y~x*groups-1) will fit MA lines for y vs. x that are forced through the origin (because we explicitly removed the intercept) with a separate MA line fit to each of several samples as specifed by the argument groups. It will also report results from a test of the hypothesis that the true MA slope is equal across all samples.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-9-lecture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-10-lecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
