<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>20 Week 11 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="20 Week 11 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="20 Week 11 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Week 11 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-10-lab.html"/>
<link rel="next" href="week-11-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#t-test"><i class="fa fa-check"></i><b>10.1</b> t-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.2</b> F-test</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.4" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.4</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-11-lecture" class="section level1 hasAnchor" number="20">
<h1><span class="header-section-number">20</span> Week 11 Lecture<a href="week-11-lecture.html#week-11-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-11-readings" class="section level2 hasAnchor" number="20.1">
<h2><span class="header-section-number">20.1</span> Week 11 Readings<a href="week-11-lecture.html#week-11-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Sections 10.1-10.7, as well as Logan Chapter 10.</p>
</div>
<div id="week-11-outline" class="section level2 hasAnchor" number="20.2">
<h2><span class="header-section-number">20.2</span> Week 11 outline<a href="week-11-lecture.html#week-11-outline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Basic idea behind ANOVA</p></li>
<li><p>Single-factor ANOVA</p></li>
<li><p>Fixed effects (Model I ANOVA) vs. Random effects (Model II ANOVA)</p></li>
<li><p>Follow up analyses to ANOVA</p></li>
</ol>
<p>The model structure of ANOVA is identical to a linear regression of categorical covariate(s). Specifically, ANOVA involves statistics used to estimate group/treatment effects.</p>
<p><span class="math display">\[
Y_{ij} = \mu + A_i + \epsilon_{ij} \text{, where } \epsilon_{ij} \sim \mathrm{N}(0, \sigma^2)
\]</span></p>
<p>The value of each data point, <span class="math inline">\(Y_{ij}\)</span> is modeled as a function of: 1) the overall mean <span class="math inline">\(\mu\)</span>, 2) the modification of the mean due to membership in group <span class="math inline">\(i\)</span>, <span class="math inline">\(A_i\)</span>, and 3) the unexplained variability remaining after the group-level effect is <span class="math inline">\(\epsilon_{ij}\)</span>. Variation <strong>among</strong> groups is represented by <span class="math inline">\(A_i\)</span> and variation <strong>within</strong> groups is represented by <span class="math inline">\(\epsilon_{ij}\)</span>. The statistical question is whether or not “group” has a statistically significant effect on the overall mean <span class="math inline">\(\mu\)</span>. This is identical to the linear models we have already talked about, except that now we are focusing on models in which the predictor variables are categorical factors (or groups).</p>
<p><strong>What is the null hypothesis here?</strong></p>
<p>Under the null hypothesis, this linear model can be written</p>
<p><span class="math display">\[
Y_{ij}=\mu+\epsilon_{ij}
\]</span></p>
<p>So the null hypothesis being tested is <span class="math inline">\(H_{0}\)</span>: The differences <strong>among</strong> groups is no larger than would be expected based on the differences <strong>within</strong> groups. Therefore, there are no significant differences among the groups.</p>
<p>So, do the groups all come from the same population? Based on what we know already know, we might try using pairwise t-tests. But wait! If we use pairwise <em>t</em>-tests to determine which groups were different, we would have <span class="math inline">\(a(a - 1) / 2\)</span> pairwise comparisons with <span class="math inline">\(a\)</span> group means.</p>
<p><strong>Question: Why is this ill advised?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Multiple comparisons will inflate the familywise error rate.
</span>
</details>
<p>
 
</p>
<p>ANOVA is an <strong>omnibus test</strong>, meaning that if we reject the null hypothesis, we only know that <strong>at least one group is different</strong>. We do not know which groups in particular are different.</p>
<p>Omnibus tests are very useful to keep familywise error rates down while testing many groups.</p>
<p>So, our null and alternative hypothesis are:</p>
<p><span class="math inline">\(H_0\)</span>: The groups have the same mean value</p>
<p><span class="math inline">\(H_A\)</span>: At least one of the groups comes from a population with a different mean</p>
<p>Note that often the null and alternative hypotheses of ANOVA are stated in terms of groups coming from the same or different populations. However, I have carefully worded the null and alternative hypotheses so as to emphasize that ANOVA is a test of means. Because ANOVA assumes that the distributions within each group are Gaussian and have the same variance, if they have the same mean, they must come from the same population (remember: the Gaussian has only two parameters, mean and variance). However, if your data do not exactly meet the assumptions, ANOVA may not flag the groups as coming from different populations if the means are the same. In other words, groups that have the same mean but different variances (a violation of the ANOVA assumptions) may not yield a statistically significant result.</p>
<p>An ANOVA with one categorical predictor with two levels is identical to a two sample <em>t</em>-test.</p>
<p>Before we discuss the mathematics behind ANOVA, we will start with a conceptual example adapted from the McKillup textbook (the chapter is posted in “Additional Readings”).</p>
<p>We are studying the effect of experimental drugs on the growth of brain tumors in humans. We have 12 experimental patients with brain tumors. We assign four to a control group (no drug), we treat four with Tumostat, and four with Inhibin 4. We are measuring (our response variable is) tumor diameter.</p>
<p>We want to know whether there is a difference in the mean tumor diamerer among the drug treatments.</p>
<p>Imagine our results look like this:</p>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-1-1.png" width="384" /></p>
<p>How do we test our hypothesis? First let’s relate the figure back to the equation: <span class="math inline">\(Y_{ij}\)</span> are each of the individual data points (e.g., <span class="math inline">\(Y_{21}\)</span> is the first individual in treatment 2, Tumostat). The horizonal line is equal to <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(A_i\)</span> is equal to the difference between <span class="math inline">\(\mu\)</span> and the treatment means (narrow horizontal lines).</p>
<p>Notice that there is variation at two levels: 1) variation among individuals within a given treatment group, and 2) variation among the three treatment group means. <strong>We can partition the variance into these two components.</strong></p>
<div id="variation-within-treatment-group" class="section level3 hasAnchor" number="20.2.1">
<h3><span class="header-section-number">20.2.1</span> Variation within treatment group<a href="week-11-lecture.html#variation-within-treatment-group" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Variation among individuals within a treatment group is residual variation. This variation exists because of any number of factors we weren’t able to measure.</p>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
</div>
<div id="variation-among-treatment-group-means" class="section level3 hasAnchor" number="20.2.2">
<h3><span class="header-section-number">20.2.2</span> Variation among treatment group means<a href="week-11-lecture.html#variation-among-treatment-group-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Variation among treatment means is variation due to the effect of the treatment (if there is an effect) <em>in addition to the individual variation</em>.</p>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
</div>
<div id="comparing-variance-components" class="section level3 hasAnchor" number="20.2.3">
<h3><span class="header-section-number">20.2.3</span> Comparing variance components<a href="week-11-lecture.html#comparing-variance-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can relate these two components of variation to one another, and this is our test statistic. Informally (we will get into the mathematical formulas soon) we can write this as:</p>
<p><span class="math display">\[
\frac{\text{Among group variance (group effect + error)}}{\text{Within group variance (error)}}
\]</span></p>
<p><strong>Question: Approximately what is this ratio equal to under the null hypothesis for ANOVA?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Under the null hypothesis, we assume there is no group effect. So, the ratio is about 1.
</span>
</details>
<p>
 
</p>
<p>Let’s imagine our data looked slightly different:</p>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-4-1.png" width="576" /></p>
<p>How much variation is there among groups relative to within groups? Without knowing the details yet, do you think there is a significant difference among groups?</p>
</div>
</div>
<div id="comparing-variance-components-1" class="section level2 hasAnchor" number="20.3">
<h2><span class="header-section-number">20.3</span> Comparing variance components<a href="week-11-lecture.html#comparing-variance-components-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Question: Given that we are using null hypothesis significance testing methods, what do you think the next step is after we estimate the test statistic?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Compare the value of the test statistic to the distribution of the test statistic under the null hypothesis.
</span>
</details>
<p>
 
</p>
<p><strong>Question: The distribution of the test statistic under the null hypothesis describes the ratios of variances. What distribution do you think it is?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The F distribution (see Week 5 lecture)
</span>
</details>
<p>
 
</p>
<p>Another way we can phrase the statistical question is to ask whether the differences among group means are significantly different from the differences seen within groups. <em>In ANOVA, we compare variances in order to compare means.</em></p>
</div>
<div id="two-ways-to-estimate-variance" class="section level2 hasAnchor" number="20.4">
<h2><span class="header-section-number">20.4</span> Two ways to estimate variance<a href="week-11-lecture.html#two-ways-to-estimate-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Variation within groups</li>
</ol>
<p>We can calculate the within-group variance as an average of the within group variances from each group. In other words:</p>
<p><span class="math display">\[
\sigma^2 = \frac{1}{a} (\text{Within-grp var grp }1 +\text{Within-grp var for grp }2+...+\text{Within-grp var group }a)
\]</span>
This can be re-written as:</p>
<p><span class="math display">\[
\sigma^2 = \frac{1}{a} \sum_{i = 1}^a \text{Within-grp var group }i
\]</span>
There is actually a second way we can calculate the within-group variance:</p>
<ol start="2" style="list-style-type: decimal">
<li>Variation among groups</li>
</ol>
<p>Under <span class="math inline">\(H_0\)</span>, the data in each group are an independent sample from the same underlying population. We can calculate a mean for each of the <span class="math inline">\(a\)</span> groups. Here we are assuming that each group has the same number (<span class="math inline">\(n\)</span>) of data points. (This is called a “balanced design”. More on this in a bit.)</p>
<p>We can use the Central Limit Theorem to estimate the variance. We calculate the variation in means as: <span class="math inline">\(\sigma^2_{\bar{Y}} = \frac{\sigma^2}{n}\)</span>, where <span class="math inline">\(\sigma^2_{\bar{Y}}\)</span> is our estimated variance of the group means and <span class="math inline">\(\frac{\sigma^2}{n}\)</span> is our overall uncertainty divided by <span class="math inline">\(n\)</span> (thus, our ability to estimate <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\bar{Y}\)</span> improves with increasing sample size).</p>
<p>We can rearrange this to get a second formula for the within-group variance:</p>
<p><span class="math display">\[
\sigma^2 = n \sigma^2_{\bar{Y}}
\]</span></p>
<p><strong>Under <span class="math inline">\(H_0\)</span>, both ways of calculating variance (among groups using CLT or within groups as an average of group variance) will give equivalent estimates.</strong></p>
<p><span class="math display">\[
\text{Under H}_o \rightarrow n \sigma^2_{\bar{Y}} = \frac{1}{a} \sum_{i = 1}^a \text{Within-group variance in group }i
\]</span></p>
<p><strong>However, if there are true group differences, the variation among groups will be large compared to the variation within groups.</strong></p>
<p>Now we’ll use our example of the tumor diameter data with <strong>no true treatment differences</strong> and run simulations of the experiment with 100 samples in each treatment in order to estimate and compare variance within groups and variance among groups.</p>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="week-11-lecture.html#cb682-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3467</span>)</span>
<span id="cb682-2"><a href="week-11-lecture.html#cb682-2" tabindex="-1"></a>replicates <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb682-3"><a href="week-11-lecture.html#cb682-3" tabindex="-1"></a>var.among <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb682-4"><a href="week-11-lecture.html#cb682-4" tabindex="-1"></a>var.within <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb682-5"><a href="week-11-lecture.html#cb682-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) {</span>
<span id="cb682-6"><a href="week-11-lecture.html#cb682-6" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(replicates <span class="sc">*</span> <span class="fu">length</span>(treatment.names))</span>
<span id="cb682-7"><a href="week-11-lecture.html#cb682-7" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">rep</span>(treatment.names, <span class="at">each =</span> replicates)</span>
<span id="cb682-8"><a href="week-11-lecture.html#cb682-8" tabindex="-1"></a>  dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Treatment =</span> x, <span class="at">TumorDiameter =</span> y)</span>
<span id="cb682-9"><a href="week-11-lecture.html#cb682-9" tabindex="-1"></a>  dat<span class="sc">$</span>GroupMeans <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="fu">mean</span>(dat<span class="sc">$</span>TumorDiameter[dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Control&quot;</span>]), <span class="fu">mean</span>(dat<span class="sc">$</span>TumorDiameter[dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Tumostat&quot;</span>]), <span class="fu">mean</span>(dat<span class="sc">$</span>TumorDiameter[dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Inhibin 4&quot;</span>])), <span class="at">each =</span> replicates)</span>
<span id="cb682-10"><a href="week-11-lecture.html#cb682-10" tabindex="-1"></a>  dat<span class="sc">$</span>Treatment <span class="ot">&lt;-</span> <span class="fu">factor</span>(dat<span class="sc">$</span>Treatment, <span class="at">levels =</span> treatment.names)</span>
<span id="cb682-11"><a href="week-11-lecture.html#cb682-11" tabindex="-1"></a>  var.among[i] <span class="ot">&lt;-</span> replicates <span class="sc">*</span> <span class="fu">var</span>(<span class="fu">unique</span>(dat<span class="sc">$</span>GroupMeans))</span>
<span id="cb682-12"><a href="week-11-lecture.html#cb682-12" tabindex="-1"></a>  v1 <span class="ot">&lt;-</span> <span class="fu">var</span>(dat<span class="sc">$</span>TumorDiameter[<span class="fu">which</span>(dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Control&quot;</span>)])</span>
<span id="cb682-13"><a href="week-11-lecture.html#cb682-13" tabindex="-1"></a>  v2 <span class="ot">&lt;-</span> <span class="fu">var</span>(dat<span class="sc">$</span>TumorDiameter[<span class="fu">which</span>(dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Tumostat&quot;</span>)])</span>
<span id="cb682-14"><a href="week-11-lecture.html#cb682-14" tabindex="-1"></a>  v3 <span class="ot">&lt;-</span> <span class="fu">var</span>(dat<span class="sc">$</span>TumorDiameter[<span class="fu">which</span>(dat<span class="sc">$</span>Treatment <span class="sc">==</span> <span class="st">&quot;Inhibin 4&quot;</span>)])</span>
<span id="cb682-15"><a href="week-11-lecture.html#cb682-15" tabindex="-1"></a>  var.within[i] <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> <span class="dv">3</span>) <span class="sc">*</span> <span class="fu">sum</span>(<span class="fu">c</span>(v1, v2, v3))</span>
<span id="cb682-16"><a href="week-11-lecture.html#cb682-16" tabindex="-1"></a>}</span>
<span id="cb682-17"><a href="week-11-lecture.html#cb682-17" tabindex="-1"></a><span class="fu">mean</span>(var.among)</span></code></pre></div>
<pre><code>## [1] 0.991662</code></pre>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="week-11-lecture.html#cb684-1" tabindex="-1"></a><span class="fu">mean</span>(var.within)</span></code></pre></div>
<pre><code>## [1] 0.9948506</code></pre>
</div>
<div id="single-factor-anova" class="section level2 hasAnchor" number="20.5">
<h2><span class="header-section-number">20.5</span> Single-factor ANOVA<a href="week-11-lecture.html#single-factor-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>First we will start will the simplest ANOVA type, where we have a single factor (discrete covariate). We will work through an example involving the heights of plants, <span class="math inline">\(Y\)</span>, under three different experimental treatments (<span class="math inline">\(i\)</span>), low nitrogen (N), ambient N, and high N. We have four replicates in each treatment. We index the heights as <span class="math inline">\(Y_{ij}\)</span>, where <span class="math inline">\(i\)</span> is treatment <span class="math inline">\(i = (1, ..., a)\)</span> and <span class="math inline">\(j\)</span> is the individual plant <span class="math inline">\(j = (1, ..., n)\)</span>. There will be a lot of variation in plant heights for a million reasons we can’t measure. But, we want to partition the variance into 1) the variance due to individual fluctuations (“error”), and 2) the variance due to the treatment (nitrogen level).</p>
<p>This is a single-factor (one-way) ANOVA because we have one grouping category, nitrogen treatment, with three mutually exclusive levels (low N, ambient N, high N).</p>
<p>Our data is as follows:</p>
<table>
<thead>
<tr class="header">
<th align="left">Height</th>
<th align="left">Treatment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">10</td>
<td align="left">low N</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="left">low N</td>
</tr>
<tr class="odd">
<td align="left">12</td>
<td align="left">low N</td>
</tr>
<tr class="even">
<td align="left">13</td>
<td align="left">low N</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">ambient N</td>
</tr>
<tr class="even">
<td align="left">11</td>
<td align="left">ambient N</td>
</tr>
<tr class="odd">
<td align="left">11</td>
<td align="left">ambient N</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="left">ambient N</td>
</tr>
<tr class="odd">
<td align="left">12</td>
<td align="left">high N</td>
</tr>
<tr class="even">
<td align="left">13</td>
<td align="left">high N</td>
</tr>
<tr class="odd">
<td align="left">15</td>
<td align="left">high N</td>
</tr>
<tr class="even">
<td align="left">16</td>
<td align="left">high N</td>
</tr>
</tbody>
</table>
<p>We first need the group means:</p>
<p><span class="math display">\[
\bar{Y}_{\text{low N}} = \frac{10 + 12 + 12 + 13}{4} = 11.75
\]</span></p>
<p><span class="math display">\[
\bar{Y}_{\text{ambient N}} = \frac{9 + 11 + 11 + 12}{4} = 10.75
\]</span></p>
<p><span class="math display">\[
\bar{Y}_{\text{high N}} = \frac{12 + 13 + 15 + 16}{4} = 14.00
\]</span></p>
<p>The overall mean is <span class="math inline">\(\bar{Y} = 12.17\)</span>.</p>
<p>Both this week and next week, we will be working through different ANOVA tables for each model we review. ANOVAs can be expressed in a table where each row is a category of variation that we are estimating. As we go from the leftmost column to the rightmost column, we estimate the partitioned variation and then conduct our hypothesis test.</p>
<p>I suggest you try to fill out the ANOVA tables in your notes during lecture as we go through them, and practice filling out ANOVA tables on your own as well.</p>
<table>
<colgroup>
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center"><em>P</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Within groups</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Then, we will calculate the total amount of variation, or the squared difference between each data point (<span class="math inline">\(Y_{ij}\)</span>) and the overall mean (<span class="math inline">\(\bar{Y}\)</span>):</p>
<p><span class="math display">\[
\text{SS}_{\text{total}} = \sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2 = 41.67
\]</span></p>
<p>You can think of a double sum like a nested for loop in R. Note that different sources express the same formula in different ways (I will try to be consistent in lecture, but let me know if you need a clarification on notation).</p>
<p>Next, we will calculate the sum of squares among groups, or how much the mean of group <span class="math inline">\(i\)</span> (<span class="math inline">\(\bar{Y}_i\)</span>) differs from the overall mean (<span class="math inline">\(\bar{Y}\)</span>):</p>
<p><span class="math display">\[
\text{SS}_{\text{among groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2 = 22.17
\]</span></p>
<p>We sum up <span class="math inline">\((\bar{Y}_{i} - \bar{Y})^2\)</span> for every data point <span class="math inline">\(j\)</span> in group <span class="math inline">\(i\)</span>, so in this case we sum up the same number four times for each cell.</p>
<p><strong>Question: Because there are the same number of replicates in each treatment for this case, can you think of another way to express <span class="math inline">\(\text{SS}_{\text{among groups}}\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\text{SS}_{\text{among groups}} = n \sum^a_{i = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span>. Note that this only works if there are the same number of observations in each group. Always explicitly using the double sum is applicable to more situations (but a lot of textbooks express the formula in this way).
</span>
</details>
<p>
 
</p>
<p>Last, we will calculate the sum of squares within groups, or how much each data point in group <span class="math inline">\(i\)</span> (<span class="math inline">\(Y_{ij}\)</span>) differs from the the mean of group <span class="math inline">\(i\)</span> (<span class="math inline">\(\bar{Y}_i\)</span>):</p>
<p><span class="math display">\[
\text{SS}_{\text{within groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2 = 19.50
\]</span></p>
<p>Does this remind you of linear regression?</p>
<p><span class="math display">\[
\text{SS}_{\text{among groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2 ; \quad \text{SSR} = \sum_{i = 1}^n{(\hat{Y}_i - \bar{Y})^2} \\
\text{SS}_{\text{within groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2 ; \quad \text{SSE} = \sum_{i = 1}^n{(Y_i - \hat{Y}_i)^2}
\]</span></p>
<p>The estimated <span class="math inline">\(\hat{Y}_i\)</span> from the regression is comparable to the group means</p>
<pre><code>## Loading required package: tcltk</code></pre>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-7-1.png" width="384" /></p>
<p>Now we have the first column to fill in our ANOVA table:</p>
<table>
<colgroup>
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center"><em>P</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Within groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p><strong>Question: How do we calculate degrees of freedom (in general)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Subtract the number of quantities estimated from the data from the total number of data points.
</span>
</details>
<p>
 
</p>
<p><strong>Question: How many quantities did we estimate in the ANOVA calculation? First, how many quantities were estimated and how many data points are included in the formula for the total sums of squares?</strong></p>
<p><span class="math display">\[
\text{SS}_{\text{total}} = \sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We estimated the overall mean (“uses up” one degree of freedom). We have <span class="math inline">\(a \times n\)</span> data points. So, for <span class="math inline">\(\text{SS}_{\text{total}}\)</span> we have <span class="math inline">\((a \times n) - 1\)</span> degrees of freedom.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Now, how many quantities were estimated and how many data points are included in the formula for the sums of squares among groups?</strong></p>
<p><span class="math display">\[
\text{SS}_{\text{among groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We estimated the overall mean (“uses up” one degree of freedom), and we have <span class="math inline">\(a\)</span> data points (one for each group mean). For <span class="math inline">\(\text{SS}_{\text{among groups}}\)</span> we have <span class="math inline">\(a - 1\)</span> degrees of freedom.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Last, how many quantities were estimated and how many data points are included in the formula for the sums of squares within groups?</strong></p>
<p><span class="math display">\[
\text{SS}_{\text{within groups}} = \sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We estimated the group means (“uses up” <span class="math inline">\(a\)</span> degrees of freedom). We have <span class="math inline">\(a \times n\)</span> data points. So, for <span class="math inline">\(\text{SS}_{\text{within groups}}\)</span> we have <span class="math inline">\((a \times n) - a\)</span> degrees of freedom. Importantly, though, note that if the groups have different numbers of data points, you need to add up the numbers of degrees of freedom within each group. In other words, if you have three groups (a=3) that have different numbers in each group, the degrees of freedom “within groups” is <span class="math inline">\((n_{1}-1)+(n_{2}-1)+(n_{3}-1)\)</span>, which does not simplify to <span class="math inline">\(a*n\)</span> since the n’s are all different. We will discuss such unbalanced designs in more depth next week.
</span>
</details>
<p>
 
</p>
<p>We have the second column to fill in our ANOVA table:</p>
<table>
<colgroup>
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center"><em>P</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(a - 1 = 2\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Within groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2\)</span></td>
<td align="center"><span class="math inline">\((a \times n) - a = 9\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\((a \times n) - 1 = 11\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Mean squares are calculated by dividing the sums of squares by the degrees of freedom for each row of the table. <strong>Mean squares are estimates of variance for components of the model</strong>.</p>
<p>Relating this idea back to our first example of tumor diameters:</p>
<p><span class="math display">\[
\text{MS}_{\text{among groups}} = \text{Among group variance (group + error)}
\]</span></p>
<p><span class="math display">\[
\text{MS}_{\text{within groups}} = \text{Within group variance (error)}
\]</span></p>
<p>Our estimate of the variance among groups,</p>
<p><span class="math display">\[
\sigma^2 = n \sigma^2_{\bar{Y}} = \text{MS}_{\text{among groups}}
\]</span></p>
<p>Our estimate of the variance within groups,</p>
<p><span class="math display">\[
\sigma^2 = \frac{1}{a} \sum_{i = 1}^a \text{Variance in group }i = \text{MS}_{\text{within groups}}
\]</span></p>
<p><strong>Mean squares within groups is also called the residual mean squares or mean squares error.</strong></p>
<p>Remember that under <span class="math inline">\(H_0\)</span>, both ways of estimating variance (among groups using CLT or within groups as an average of group variance) will give equivalent estimates. However, if there are true group differences, the variation among groups will be large compared to the variation within groups.</p>
<p>Out test statistic, <span class="math inline">\(F\)</span>, is the ratio of these estimates of variance components.</p>
<p><span class="math display">\[
\frac{\text{Among group variance (group + error)}}{\text{Within group variance (error)}} = \frac{\text{MS}_{\text{among groups}}}{\text{MS}_{\text{within groups}}} = F
\]</span></p>
<p>And the third and fourth columns of our ANOVA table can be filled in:</p>
<table>
<colgroup>
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center"><em>P</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2 = 22.17\)</span></td>
<td align="center"><span class="math inline">\(a - 1 = 2\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{among groups}}}{\text{DOF}_{\text{among groups}}} = 11.08\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{\text{among groups}}}{\text{MS}_{\text{within groups}}} = 5.11\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Within groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2 = 19.50\)</span></td>
<td align="center"><span class="math inline">\((a \times n) - a = 9\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{within groups}}}{\text{DOF}_{\text{within groups}}} = 2.17\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2 = 41.67\)</span></td>
<td align="center"><span class="math inline">\(a \times n - 1 = 11\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Because doing this in R (especially when we get to multi-way ANOVA next week) can be tricky, I show you here how this can be done in Excel so the underlying logic is clear</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="One-way_ANOVA_example_in_Excel.png" alt="Example of how Excel can be used to work out the sums-of-squares for one way ANOVA" width="100%" />
<p class="caption">
Figure 20.1: Example of how Excel can be used to work out the sums-of-squares for one way ANOVA
</p>
</div>
<p>We now test whether the F ratio (our test statistic) is significant given its distribution under the null hypothesis. The distribution of test statistic <span class="math inline">\(F\)</span> under the null hypothesis is <span class="math inline">\(F | H_0 \sim F_{\text{DOF}_{\text{among}}, \text{DOF}_{\text{within}}}\)</span>.</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="week-11-lecture.html#cb687-1" tabindex="-1"></a>x.values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="fl">0.001</span>)</span>
<span id="cb687-2"><a href="week-11-lecture.html#cb687-2" tabindex="-1"></a>y.values <span class="ot">&lt;-</span> <span class="fu">df</span>(<span class="at">x =</span> x.values, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">9</span>)</span>
<span id="cb687-3"><a href="week-11-lecture.html#cb687-3" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">FStatistic =</span> x.values, <span class="at">Density =</span> y.values)</span>
<span id="cb687-4"><a href="week-11-lecture.html#cb687-4" tabindex="-1"></a><span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> FStatistic, <span class="at">y =</span> Density)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb687-5"><a href="week-11-lecture.html#cb687-5" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">5.11</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue3&quot;</span>, <span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb687-6"><a href="week-11-lecture.html#cb687-6" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue3&quot;</span>, <span class="at">label =</span> <span class="st">&quot;F*&quot;</span>, <span class="at">x =</span> <span class="fl">5.6</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">size =</span> <span class="dv">8</span>) <span class="sc">+</span></span>
<span id="cb687-7"><a href="week-11-lecture.html#cb687-7" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">8</span>)) <span class="sc">+</span></span>
<span id="cb687-8"><a href="week-11-lecture.html#cb687-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Probability&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Quantiles of the F dist.&quot;</span>) <span class="sc">+</span></span>
<span id="cb687-9"><a href="week-11-lecture.html#cb687-9" tabindex="-1"></a>    <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-11-lecture_files/figure-html/unnamed-chunk-9-1.png" width="384" /></p>
<p>Note that the F distribution has a different shape when <code>df1</code> has higher values.</p>
<p>How would you calculate <em>P</em> here?</p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="week-11-lecture.html#cb688-1" tabindex="-1"></a><span class="fu">sum</span>(dat<span class="sc">$</span>Density[dat<span class="sc">$</span>FStatistic <span class="sc">&gt;=</span> <span class="fl">5.11</span>] <span class="sc">*</span> <span class="fl">0.001</span>)  <span class="co"># Riemann sums integration</span></span></code></pre></div>
<pre><code>## [1] 0.03290738</code></pre>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="week-11-lecture.html#cb690-1" tabindex="-1"></a><span class="fu">pf</span>(<span class="fl">5.11</span>, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">9</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.03290039</code></pre>
<table>
<colgroup>
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center"><em>P</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (\bar{Y}_{i} - \bar{Y})^2 = 22.17\)</span></td>
<td align="center"><span class="math inline">\(a - 1 = 2\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{among groups}}}{\text{DOF}_{\text{among groups}}} = 11.08\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{\text{among groups}}}{\text{MS}_{\text{within groups}}} = 5.11\)</span></td>
<td align="center"><span class="math inline">\(0.033\)</span></td>
</tr>
<tr class="even">
<td>Within groups</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y}_i)^2 = 19.50\)</span></td>
<td align="center"><span class="math inline">\((a \times n) - a = 9\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{within groups}}}{\text{DOF}_{\text{within groups}}} = 2.17\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^n_{j = 1} (Y_{ij} - \bar{Y})^2 = 41.67\)</span></td>
<td align="center"><span class="math inline">\((a \times n) - 1 = 11\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>How do we report our findings in a manuscripts/thesis/report/etc.?</p>
<p><strong>We found a significant difference among the three nitrogen treatments given a single-factor ANOVA (<span class="math inline">\(F_{2, 9} = 5.11\)</span>, <span class="math inline">\(P = 0.033\)</span>).</strong></p>
<p><strong>Special note</strong></p>
<p>I strongly prefer to keep all sums explicit, e.g.,</p>
<p><span class="math display">\[
\sum_{i=1}^{a}\sum_{j=1}^{n}(\bar{Y_{i}}-\bar{Y})^{2}
\]</span>
instead of</p>
<p><span class="math display">\[
n\sum_{i=1}^{a}(\bar{Y_{i}}-\bar{Y})^{2}
\]</span>
because the former can be used even if the ANOVA design is not balanced. (A balanced design is one in which the sample size in each cell is the same (n).) Many other texts use the latter notation, so do not let this confuse you.</p>
<p>This figure from Logan (2010) does a nice job of summarizing the steps of one-way ANOVA:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-11"></span>
<img src="One-way-ANOVA%20figure%20from%20Logan.png" alt="Source: Logan (2010) Biostatistical Design and Analysis Using R" width="80%" />
<p class="caption">
Figure 20.2: Source: Logan (2010) Biostatistical Design and Analysis Using R
</p>
</div>
<p>ANOVA comes with a number of assumptions. We assume that the residuals are:</p>
<ol style="list-style-type: decimal">
<li><p>Normally distributed. The error terms, <span class="math inline">\(\epsilon_{ij}\)</span>, should be normaly distributed within each group. Note that ANOVA is robust to non-normality if you have a balanced design and similar variances.</p></li>
<li><p>Independent within and among groups, i.e., each experimental unit is independent of each other experimentl unit.</p></li>
<li><p>Variance is equal (or similar) across all treatments.</p></li>
</ol>
<p><strong>Question: What are the residuals in an ANOVA model?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The difference between the data points <span class="math inline">\(Y_{ij}\)</span> and the group mean <span class="math inline">\(\mu_i\)</span>, <span class="math inline">\(\epsilon_{ij} = Y_{ij} - \mu_i \sim \mathrm{N} (0, \sigma^2)\)</span>.
</span>
</details>
<p>
 
</p>
</div>
<div id="fixed-effects-vs.-random-effects" class="section level2 hasAnchor" number="20.6">
<h2><span class="header-section-number">20.6</span> Fixed effects vs. random effects<a href="week-11-lecture.html#fixed-effects-vs.-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have so far assumed that factors are fixed effects. <strong>Fixed effects</strong> represent all the states in the system you would ever be interested in. Our null hypothesis for a fixed effects factor is <span class="math inline">\(H_0: \text{all treatment effects, } A_i = 0\)</span> (e.g., the three experimental, pre-planned nitrogen treatments as in our plant example). ANOVA models with only fixed effects are sometimes called Model I ANOVAs.</p>
<p><strong>Random effects</strong> represent a random sample of some larger population about which you want to make inference. For example, you are studying literacy in a human population, and in your sample you have English, French, Spanish, and Russian speakers. You are not interested in the specific effect of Russian vs. English, you just want to be able to include in your model variability that may come from the fact that different languages are represented. Random effects are also useful to extrapolate your findings to other languages that were not included in your sample. Our null hypothesis for a random effects factor is <span class="math inline">\(H_0: \mathrm{Var}(\text{All treatment effects, } A_i) = 0\)</span>. ANOVA models with only random effects are sometimes called Model II ANOVAs.</p>
<p>Models with both fixed and random effects are called <strong>mixed models</strong></p>
<p><span class="math display">\[
Y_{ij} = \mu + A_i + \epsilon_{ij}
\]</span></p>
<table>
<colgroup>
<col width="7%" />
<col width="14%" />
<col width="11%" />
<col width="47%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th align="center">Effects</th>
<th align="center">Model</th>
<th align="center">Description</th>
<th align="center"><span class="math inline">\(H_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A_i\)</span></td>
<td align="center">Fixed</td>
<td align="center">I</td>
<td align="center">effect of the <span class="math inline">\(i\)</span>th group</td>
<td align="center"><span class="math inline">\(A_i = 0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_i\)</span></td>
<td align="center">Random</td>
<td align="center">II</td>
<td align="center">effect of the <span class="math inline">\(i\)</span>th random group selected from larger population of groups</td>
<td align="center"><span class="math inline">\(\mathrm{Var}(A_i) = 0\)</span></td>
</tr>
</tbody>
</table>
<p>The models and statistics (<span class="math inline">\(\text{SS, MS, F, } P\)</span>) for single-factor ANOVAs are the same with fixed effects or random effects.</p>
<p>Some researchers, such as Gelman and Hill, argue that all effects are actually random. The distinction is between whether randomness is modeled or not. Randomness that is modeled is what we call a random effect and randomness that is unmodeled is a fixed effect. This distinction is the basis for hierarchical models (often Bayesian). For example, if we model <span class="math inline">\(A_i \sim \text{Distribution(parameters)}\)</span>, the ANOVA model is now hierarchical.</p>
<p><em>PS:</em> Here’s my first plug for Gelman and Hill’s excellent book: Data analysis using regression and multilevel/hierarchical models. If you plan to use regression or ANOVA in your research, particularly if you plan on fitting mixed/hierarchical models, I highly recommend getting a copy of this book.</p>
<p><img src="RandomEffects.png" /></p>
<p>Figure 10.5 from Aho: A random effects model, where the random effects factor is soil fertilizer brand and the response variable is wheat yield <span class="math inline">\(Y_{ij}\)</span>. The top panel describes the underlying distribution of the random factor <span class="math inline">\(\mathrm{N} (0, \sigma_A^2)\)</span>. The bottom two panels describe the distribution of the data for two randomly selected fertilizer brands <span class="math inline">\(Y_{1j} \sim\mathrm{N} (\mu_1, \sigma^2)\)</span> and <span class="math inline">\(Y_{2j} \sim\mathrm{N} (\mu_2, \sigma^2)\)</span>.</p>
<p>Let’s practice a bit on reasoning whether a factor is best modelled as a fixed effect or a random effect.</p>
<ol style="list-style-type: decimal">
<li><p>Survey of student satisfaction at U.S. colleges and universities. (What if the universities all belonged to a category, e.g., Ivy League? Would this change your answer?)</p></li>
<li><p>Survey of public health in Massachusetts (post Romney-care) vs. New Hampshire, with five cities sampled in each state. Is state a fixed or random effect? City?</p></li>
<li><p>Growth rates of plants as a function of nutrient level (What if this was a study in the field, sampling across a gradient of nutrient levels?)</p></li>
</ol>
<p>Again, ANOVA is an omnibus test, so if we get a significant result, we only know that at least one group is different.</p>
<p>What do we do to find out which group(s) is/are different? This brings us to the subject of “post hoc tests”.</p>
</div>
<div id="post-hoc-tests" class="section level2 hasAnchor" number="20.7">
<h2><span class="header-section-number">20.7</span> Post-hoc tests<a href="week-11-lecture.html#post-hoc-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To follow up on a significant ANOVA result, we can conduct <em>post hoc</em> (or <em>a posteriori</em>) tests. These only make sense for fixed effects, because with random effects, the factors belong to the same population, so differences between pairs has no interpretation.</p>
<p>In general, these are similar to pairwise <em>t</em>-tests assuming equal variance while accounting for familywise error rate and using a pooled estimate of standard error. * Remember that we assume equal variance as one of the assumptions of ANOVA*</p>
<p>There are a huge number of <em>post hoc</em> tests: Fishers Least Significant Difference (LSD), Tukey’s Honest Significant Difference (HSD), Dunnett’s test, Bonferroni’s test, Holm’s test, Scheffe’s test (see Day and Quinn 1989 in “Additional readings” for descriptions).</p>
<div id="tukeys-hsd" class="section level3 hasAnchor" number="20.7.1">
<h3><span class="header-section-number">20.7.1</span> Tukey’s HSD<a href="week-11-lecture.html#tukeys-hsd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tukey’s HSD is one of the most common <em>post hoc</em> tests. It is very similar to an unpaired two-sample <em>t</em>-test assuming equal variances.</p>
<p>With the <em>t</em>-test, we use a <strong>null distribution for the difference between two means</strong>, <span class="math inline">\(T | H_0 \sim t_{n_A + n_B - 2}\)</span>.</p>
<p>With Tukey’s HSD, we use the <strong>null distribution for the range of an arbitrary number of means</strong>, or the distribution for the largest pairwise differences to be expected under the null model for a given number of comparisons, <span class="math inline">\(q^* | H_0 \sim q_{n_{\text{groups}}, \text{DOF}}\)</span>.</p>
<p>This is a new distribution called the <strong>studentized range distribution</strong>. We will simulate draws from the studentized range distribution on Thursday. For now, its fine to think of this like any other univariate distribution. The q distribution has two parameters, the first representing the number of groups being compared and the second representing the degrees of freedom left over after calculating all the group means.</p>
<p>We will get into more details on Tukey’s HSD and the q distribution on Thursday. For now, imagine that you have four groups for your ANOVA and the p-value of the ANOVA is sufficiently low that you reject the null hypothesis. We now want to do an additional (post-hoc) analysis to determine which pairs of groups are actually significantly different.</p>
<p>In our hypothestical example of having four groups in our analysis (a=4), we can rank hypothetical group means A through D from largest value to smallest: <span class="math inline">\(B &gt; D &gt; A &gt; C\)</span>. We then calculate our test statistic as</p>
<p><span class="math display">\[
q^* = \frac{\bar{X}_B - \bar{X}_C}{\sqrt{\text{MS}_{\text{within}} \frac{\frac{1}{n_B} + \frac{1}{n_C}}{2}}} \sim q_{a, \text{DOF_within}}
\]</span>
where <span class="math inline">\(MS_{within}\)</span> is the mean squares within groups and is directly analogous to the pooled variance when we were doing two-sample t-tests. (Note that in comparing this equation to <a href="week-5-lecture.html#pooledvar">the expression for a pooled t test</a> there is a factor of 2 difference; this is absorbed into the definition the q distribution.)</p>
<p>Note that <span class="math inline">\(n_{B}\)</span> and <span class="math inline">\(n_{C}\)</span> are the number of data points in each group, and because we are assuming a balanced design, we can simplify this a bit to</p>
<p><span class="math display">\[
q^* = \frac{\bar{X}_B - \bar{X}_C}{\sqrt{\frac{\text{MS}_{\text{within}}}{n}}} \sim q_{a, \text{DOF_within=an-a}}
\]</span></p>
<p>We can re-arrange this to define the Minimum Significant Range (MSR)</p>
<p><span class="math display">\[
\bar{X}_B - \bar{X}_C = MSR = \sqrt{\frac{\text{MS}_{\text{within}}}{n}} q_{(0.05)[a, \text{DOF_within=an-a}]}
\]</span>
where now <span class="math inline">\(q_{(0.05)[a, \text{DOF_within=an-a}]}\)</span> is the quantile of the q-distribution.</p>
<p>After comparing the largest mean to the smallest, you compare the second largest mean to the smallest, and so on, until you find nonsignificant differences.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-10-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-11-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
