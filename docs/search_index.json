[["index.html", "Biometry Lecture and Lab Notes Preface, data sets, and past exams", " Biometry Lecture and Lab Notes Heather Lynch 2024-04-01 Preface, data sets, and past exams This eBook contains all of the lecture notes and lab exercises that we will do this semester in Biometry. While I have made every effort to cite sources where I’ve used “found” material, this eBook reflects my own personal notes drawn up over nearly a decade of work and some material may not properly identify the original sources used in drawing up my initial lecture notes. As I have moved this material into an online eBook, I have tried to better document material inspired by or drawn from other sources. If you find anything in these notes that is not properly cited or sourced, please let me know so it can be amended. Any mistakes are mine and mine alone. Data sets Here are the datasets used in this course. You can download the data set from the course GitHub page and then save it to your local directory. To do that, right-click or control-click on the “raw” button; this will allow you to download the file in its original format. Figure 0.1: The dataset links take you here. The “raw” button allows you to download the file Another way to do it is to use the ‘readr’ package, as I demonstrate here for the clouds.csv dataset: library(readr) clouds &lt;- read_csv(&quot;https://raw.githubusercontent.com/hlynch/Biometry2024/master/_data/clouds.csv&quot;) clouds (csv): link [1] clouds (txt): link [1] Prestige: link [2] skulls: link [3] diabetes: link [4] WaterData: link [5] FoxFurProduction: link [6] fish: link [7] Week-9-Data: link [8] bomregions2012: link [9] Challenger_data: link [10] Brogan_et_al_2013_Fig1Data: link [11] fruit_flies: link [12] medley: link [13] quinn: link [14] TwoWayANOVAdata_balanced: link [15] TwoWayANOVAdata: link [16] TwoWayANOVAdata: link [17] flatworms: link [18] flies: link [19] rats: link [20] tobacco: link [21] crabs: link [22] frogs: link [23] MammalLifeHistory: link [24] Historic-lichen_data: link [25] Lichen-sites-area: link [26] Temperature_and_isolation: link [27] lovett2: link [28] [1] Source: Chambers, Cleveland, Kleiner, and Tukey. (1983). Graphical Methods for Data Analysis. Wadsworth International Group, Belmont, CA, 351. Original Source: Simpson, Alsen, and Eden. (1975). A Bayesian analysis of a multiplicative treatment effect in weather modification. Technometrics 17, 161-166. [2] Fox, J and Weisberg, HS (2011) An R Companion to Applied Regression. 2nd ed. Thousand Oaks, CA: Sage Publications. [3] Thomson, A and Randall-Maciver, R (1905) Ancient Races of the Thebaid, Oxford: Oxford University Press. Also found in: Hand, D.J., et al. (1994) A Handbook of Small Data Sets, New York: Chapman &amp; Hall, pp. 299-301. Manly, B.F.J. (1986) Multivariate Statistical Methods, New York: Chapman &amp; Hall. [4] Willems JP, Saunders JT, DE Hunt, JB Schorling (1997) Prevalence of coronary heart disease risk factors among rural blacks. A community-based study. Southern Medical Journal 90:814-820; and Schorling JB, Roach J, Siegel M, Baturka N, Hunt DE, Guterbock TM, Stewart HL (1997) A trial of church-based smoking cessation interventions for rural African Americans. Preventive Medicine 26:92-101. [9] ‘DAAG’ R package [10] Siddhartha, RD, Fowlkes, EB, and Hoadley, B (1989) Risk analysis of the space shuttle: Pre-Challenger prediction of failure. Journal of the American Statistical Association 84: 945-957. [11] Brogan, WR III, and Relyea, RA (2013) Mitigation of Malathion’s acute toxicity by four submersed macrophyte species. Environmental toxicology and Chemistry 32(7): 1535-1543. [13] Quinn G, and Keough, M. Experimental Design &amp; Data Analysis for Biologists. (2002) Cambridge University Press, UK. [14] Quinn G, and Keough, M. Experimental Design &amp; Data Analysis for Biologists. (2002) Cambridge University Press, UK. [23] Hunter, D (2000) The conservation and demography of the southern corroboree frog (Pseudophryne corroboree). M.Sc. thesis, University of Canberra, Canberra. [24] Morgan Ernest SK (2003) Life history characteristics of placental non-volant mammals. Ecology 84: 3402. [25] Lynch, HJ Personal data. [26] Lynch, HJ Personal data. [27] Lynch, HJ Personal data. [28] Lovett, GM, Weathers, KC, and Sobczak, WV (2000) Nitrogen saturation and retention in forested watersheds of the Catskill Mountains, New York. Ecological Applications 10(1): 73-84. Exams In an effort to provide as much transparency as possible on the scope and style of exams, I have made all previous exams available below. Keep in mind that Biometry has developed and changed over the 10+ years I have been teaching it. There are topics we used to cover that we no longer cover, and there are topics we cover now that were not always included as part of the syllabus. Some of the questions used in previous exams are now being used in problem sets. Exams are largely written anew each year but some questions get re-used and some of these questions may appear in future exams. Note that I was on sabbatical in 2019 and there are no exams provided for that year. Midterms Spring 2012 Midterm: link Spring 2013 Midterm: link Spring 2014 Midterm: link Spring 2015 Midterm: link Spring 2016 Midterm: link Spring 2017 Midterm: link Spring 2018 Midterm: link Spring 2020 Midterm: link Spring 2021 Midterm: link Spring 2022 Midterm: link Spring 2023 Midterm: link Finals Spring 2012 Final: link Spring 2013 Final: link Spring 2014 Final: link Spring 2015 Final: link Spring 2016 Final: link Spring 2017 Final: link Spring 2018 Final: link [See note below] Spring 2020 Final: link Spring 2021 Final: link Spring 2022 Final: link [See note below] Spring 2023 Final: link Note that there are some questions that are poorly worded or confusing. Spring 2018 Final 10c and 10d are ill posed and should be ignored. Spring 2022 Final question 5 should have been written as \\[ Y_{i} \\sim N(\\beta_{0}+\\beta_{1}SST_{i}+\\beta_{2}PreyDensity_{i}+ \\\\ \\beta_{3}StartingLength_{i}+\\beta_{4}I[Group_{i}==\\mbox{Adult Female}]+\\beta_{5}I[Group_{i}==\\mbox{Juvenile Male}]+\\\\ \\beta_{6}I[Group_{i}==\\mbox{Juvenile Female}],\\sigma^{2}) \\] Helpful RMarkdown resources Biometry students throughout the years have discovered various helpful tools, which I share here. No endorsement of these tools is implied, and their functionality and existence on the web is subject to change at any time. Making pretty tables* TableConvert How to make beautiful tables in R (blog post) "],["week-1-lecture.html", "1 Week 1 Lecture 1.1 Week 1 Readings 1.2 Basic Outline 1.3 Today’s Agenda 1.4 Basic Probability Theory 1.5 Multiple events 1.6 Conditionals 1.7 A few foundational ideas 1.8 Degrees of freedom 1.9 Quick intro to the Gaussian distribution 1.10 Overview of Univariate Distributions 1.11 What can you ask of a distribution? 1.12 A brief introduction to inference, logic, and reasoning", " 1 Week 1 Lecture 1.1 Week 1 Readings For this week, I suggest Aho Chapter 2, and Sections 4.1, 4.2, 4.3.1, 4.3.2, 4.3.3, 4.3.4, 4.3.7, and 4.5. Logan Chapter 1 will help you get started in R if you are not already familiar with it. I also recommend reviewing the Algebra Rules sheet. 1.2 Basic Outline First half: - R - bootstrap, jackknife, and other randomization techniques - hypothesis testing - probability distributions - the “classic tests” of statistics - graphical analysis of data Second half: regression (incl. ANOVA, ANCOVA) model building model criticism non-linear regression multivariate regression Class Structure Lecture on Tuesday “Lab”” on Thursday Problem sets are posted on Wednesdays (feel free to remind me via Slack if I forget), and are due before lecture the following Tuesday. This deadline is very strict, no exceptions. Turn in what you have before 8:00 am on Tuesday, even if its not complete. Communication Use slack! Come to (both) office hours I have instituted a new participation component to Biometry’s grading, see syllabus. 1.3 Today’s Agenda Basic probability theory An overview of univariate distributions Calculating the expected value of a random variable A brief introduction to the Scientific Method Introduction to statistical inference 1.4 Basic Probability Theory Let’s imagine that we have a bag with a mix of regular and peanut M&amp;Ms. Each M&amp;M has two traits: Color and Type. \\[ \\sum_{all \\: colors} P(color) = 1 \\] \\[ \\sum_{all \\: types} P(types) = ? \\] Note that the probabilities have to sum to 1.0 and that the probabilities also have to be non-negative. It turns out these are the only two requirements for a “legal” probability function. (Here we have discrete categories and so the probabilities add to one through a straightforward summation. The distribution of probabilities across categories is called the probability mass function. If these were continuous probabilities, like the distribution describing the weight of each M&amp;M, the distribution would be called a probability density function and we would have to integrate [the continuous version of a sum] over all possible weights. Either way, the sum [or integral] over all possible values of the variable has to equal 1.0.) The complement (indicated by a superscript C) of a trait represents every object that does not have that trait, so the probability of the complement to Green is the probability of getting an M&amp;M that is anything but green. Figure 0.1: Red shading represents the complement. Source: Wikimedia Commons \\[ P(Green^c) = 1 - P(Green) \\] 1.4.1 Intersection Figure 1.1: Red shading represents the intersection. Source: Wikimedia Commons Now let’s pull one M&amp;M out of the bag. If the color distribution of chocolate M&amp;Ms and peanut M&amp;Ms is the same, then these two traits are independent, and we can write the probability of being both Green and Peanut as \\[ P(Green \\: AND \\: Peanut) = P(Green \\cap Peanut) = P(Green) \\cdot P(Peanut) \\] This is called a Joint Probability and we usually write it as \\(P(Green,Peanut)\\). This only works if these two traits are independent of one another. If color and type were not independent of one another, we would have to calculate the joint probability differently, but in the vast majority of cases we are working with data that we assuming are independent of one another. In these cases, the joint probability is simply the product of all the individual probabilities. Note that \\[ P(Green \\: AND \\: Blue) = P(Green \\cap Blue) = 0\\] because an M&amp;M cannot be Green and Blue at the same time. in this case, Figure 1.2: In the case of two different colors, the intersection is empty. 1.4.2 Union Figure 1.3: Red shading represents the union. Source: Wikimedia Commons \\[ \\begin{align*} P(Green \\: OR \\: Peanut) &amp;= P(Green \\cup Peanut) \\\\ &amp;= P(Green) + P(Peanut) - P(Green \\cap Peanut) \\end{align*} \\] Question: Why do we have to subtract off the intersection? Click for Answer If we do not subtract off the intersection, then the probability of Green AND Peanut will be double counted. 1.5 Multiple events Let’s consider what happens when we pull 2 M&amp;Ms out of the bag \\[ P (Green \\: AND \\: THEN \\: Blue) = P(Green) \\cdot P(Blue) \\] Question: What if we didn’t care about the order? Click for Answer If we do not care about the order, then the combination of one Green M&amp;M and one Blue M&amp;M could have come about because we drew a Blue M&amp;M and then a Green, or a Green and then a Blue. Because there are two ways to get this outcome (and they are mutually exclusive, so we can simply add the two probabilities), the total probability is simply 2 \\(\\times\\) P(Green) \\(\\times\\) P(Blue). 1.6 Conditionals Now we will introduce the ideal of a conditional probability. \\[ P(A \\mid B) = P(A \\: conditional \\: on \\: B) \\] Let’s say we have a bivariate distribution (that just means we have two traits being discussed, similar to M&amp;M color and M&amp;M type above) for discrete quantities such as hair color and eye color (which we will use because they are intuitive but not independent traits), and we survey a number (n=20 in this case) of students. Brown Blond Red Blue eyes 3 4 0 Brown eyes 7 2 0 Green eyes 2 1 1 This table summarizes the joint distribution for hair color and eye color, which we would write as \\[ P(hair,eye) \\] Remember that for any two traits A and B that are independent, \\[ P(A,B) = P(A) \\times P(B) \\] However, in this case, we don’t have any reason to believe that hair and eye color are independent traits. People with blue eyes have a different probability of having brown hair than people with brown eyes. These are called conditional probabilities. For example, the probability of having blue eyes conditional on having blond hair is given by 4/7. We write this as follows \\[ P(eyes=blue|hair=blond) \\] The | symbol represents the “conditional on” statement. We might also be interested in the marginal probabilities, which are those probabilities representing hair color irrespective of eye color, or eye color irrespective of hair color. In the example given, the marginal probability of having blue eyes is 7/20. The marginal probability of having blond hair is also 7/20. These are univariate probabilities, and are written as \\[ P(eye) \\] or \\[ P(hair) \\] The relationship between joint, marginal, and conditional distributions can be seen in the following statement \\[ P(\\mbox{eyes}=\\mbox{blue},\\mbox{hair}=\\mbox{blond})=P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{blond})P(\\mbox{hair}=\\mbox{blond}) \\] We can see that this works out as it should \\[ \\frac{4}{20}=\\frac{4}{7} \\times \\frac{7}{20} \\] If we want to know the probability of having blue eyes (and didn’t care about hair color) than we would want to add up all the possibilities: \\[ P(\\mbox{eyes}=\\mbox{blue})=P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{blond})P(\\mbox{hair}=\\mbox{blond}) \\\\ +P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{brown})P(\\mbox{hair}=\\mbox{brown}) \\\\ +P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{red})P(\\mbox{hair}=\\mbox{red}) \\] We can state this more simply as a sum: \\[ P(\\mbox{eyes}=\\mbox{blue})=\\sum_{\\mbox{all hair colors}}P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\square)P(\\mbox{hair}=\\square) \\] For continuous distributions, the same principles apply. Let’s say we have a continuous bivariate distribution \\(p(A,B)\\). The marginal distribution for A can be calculated by integrating over B (we call this “marginalizing out” or “marginalizing over” B) \\[ p(A=a)=\\int_{b=-\\infty}^{b=\\infty}p(A=a|B=b)p(B=b)db = \\int_{b=-\\infty}^{b=\\infty}p(A=a,B=b)db \\] Notice that, in all cases (discrete or continuous), the relationships among marginal, conditional, and joint distribution can written as \\[ p(A|B)\\times p(B)=p(A,B) \\] or as \\[ p(B|A)\\times p(A)=p(A,B) \\] Therefore, \\[ p(A|B)=\\frac{p(B|A)p(A)}{p(B)} = \\frac{p(A,B)}{p(B)} \\] Ta da! We’ve arrived at Bayes Theorem, using nothing more than some basic definitions of probability. In Bayesian analyses (which we will not get into this semester), we are using this to calculate the probability of certain model parameters conditional on the data you have. But to find out more, you’ll have to take BEE 569. \\[ P(parameters \\mid data) \\cdot P(data) = P(data \\mid parameters) \\cdot P(parameters) \\] \\[ P(parameters \\mid data) = \\frac{P(data \\mid parameters) \\cdot P(parameters)}{P(data)}\\] 1.7 A few foundational ideas There are a few statistics (a statistic is just something calculated from data) that we will need to know right at the beginning. For illustration purposes, lets assume we have the following (sorted) series of data points: (1,3,3,4,7,8,13) There are three statistics relating the “central tendency”: the mean (the average value; 5.57), the mode (the most common value; 3), and the median (the “middle” value; 4). We often denote the mean of a variable with a bar, as in \\(\\bar{x}\\). There are also two statistics relating to how much variation there is in the data. The variance measures the average squared distance between each point and the mean. For reasons that we will discuss in lab, we estimate the variance using the following formula \\[ \\mbox{variance}_{unbiased} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\] rather than the more intuitive \\[ \\mbox{variance}_{biased} = \\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\] Why? It turns out that the unbiased estimator is not the sum of squared deviation per data point but rather the sum of squared deviation per degree of freedom. What’s a degree of freedom? Glad you asked… 1.8 Degrees of freedom What is meant by “degrees of freedom”? Draw five boxes on the board: We want the mean to be 4 so the sum has to be 20. Let’s start filling in boxes… Do we get to choose the last number? No! The last number is constrained by the fact that we already fixed the mean to be 4. So we only have 4 degrees of freedom, the last box is prescribed by the mean. So if we know the mean of a sample of size n, we only have n-1 remaining degrees of freedom in the sample. degrees of freedom (dof)=sample size (n)-number of parameters already estimated from the data (p) If we go back for a second, it turns out that the real definition of the sample variance is \\[ \\mbox{variance} = \\frac{\\mbox{sum of squares}}{\\mbox{degree of freedom}} \\] What is the sum of squares? \\[ SS = \\sum(Y-\\bar{Y})^2 \\] How many degrees of freedom did I start with? (n) How many did I lose in the calculation of the SS? (1, for \\(\\bar{Y}\\)) Therefore, an unbiased estimate of the population variance is: \\[ SS = \\frac{\\sum(Y-\\bar{Y})^{2}}{n-1} \\] ##Back to talking about variance and standard deviation Keep in mind that variance measures a distance squared. So if your data represent heights in m, than the variance will have units \\(m^{2}\\) or square-meters. The standard deviation is simply the square-root of variance, and is often denoted by the symbol \\(\\sigma\\). \\[ \\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}} \\] If you were handed a distribution and you were asked to measure a characteristic “fatness” for the distribution, your estimate would be approximately \\(\\sigma\\). Note that \\(\\sigma\\) has the same units as the original data, so if your data were in meters, \\(\\sigma\\) would also be in meters. 1.9 Quick intro to the Gaussian distribution We won’t get to Normal (a.k.a. Gaussian) distributions properly until Week 3, but we will need one fact about the “Standard Normal Distribution” now. The Standard Normal distribution is a Normal (or Gaussian, bell-shaped) distribution with mean equal to zero and standard deviation equal to 1. 68\\(\\%\\) of the probability is contained within 1 standard deviation of the mean (so from -\\(\\sigma\\) to +\\(\\sigma\\)), and 95\\(\\%\\) of the probability is contained within 2 standard deviations of the mean (so from -2\\(\\sigma\\) to +2\\(\\sigma\\)). (Actually, 95\\(\\%\\) is contained with 1.96 standard deviations, so sometimes we will use the more precise 1.96 and sometimes you will see this rounded to 2.) 1.10 Overview of Univariate Distributions Discrete Distributions Binomial Multinomial Poisson Geometric Continuous Distributions Normal/Gaussian Beta Gamma Student’s t \\(\\chi^2\\) 1.11 What can you ask of a distribution? Probability Density Function: \\(P(x_1&lt;X&lt;x_2)\\) (continuous distributions) Stop: Let’s pause for a second and discuss the probability density function. This is a concept that student’s often struggle with. What is the interpretation of \\(P(x)\\)? What is \\(P(x=3)\\)? Can \\(P(x)\\) ever be negative? [No.] Can \\(P(x)\\) ever be greater than 1? [Yes! Why?] Probability Mass Function: \\(P(X=x_1)\\) (discrete distributions) Cumulative Density Function (CDF): What is \\(P(X \\le X^*)\\)? Quantiles of the distributions: What is \\(X^{*}\\) if \\(P(X \\le X^{*})=0.37\\)? Sample from the distribution: With a large enough sample, the histogram will come very close to the underlying PDF. Note that the CDF is the integration of the PDF, and the PDF is the derivative of the CDF, so if you have one of these you can always get the other. Likewise, you can always get from the quantiles to the CDF (and then to the PDF). These three things are all equally informative about the shape of the distribution. 1.11.1 Expected Value of a Random Variable In probability theory the expected value of a random variable is the weighted average of all possible values that this random variable can take on. The weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of continious random variable. 1.11.2 Discrete Case \\[ X = \\{X_1, X_2,...,X_k\\} \\\\ E[X] = \\sum_{i=1}^n{X_i \\cdot P(X_i)}\\] Example: Draw numbered balls with numbers 1, 2, 3, 4 and 5 with probabilities 0.1, 0.1, 0.1, 0.1, 0.6. \\[ \\begin{align*} E[X] &amp;= (0.1 \\cdot 1) + (0.1 \\cdot 2) + (0.1 \\cdot 3) + (0.1 \\cdot 4) + (0.6 \\cdot 5) \\\\ &amp;=4 \\end{align*}\\] 1.11.3 Continuous Case \\[ E[X] = \\int_{-\\infty}^{\\infty}{X \\cdot f(X)dX}\\] Note that the function \\(f(x)\\) in the above equation is the probability density function. 1.12 A brief introduction to inference, logic, and reasoning INDUCTIVE reasoning: A set of specific observations \\(\\rightarrow\\) A general principle Example: I observe a number of elephants and they were all gray. Therefore, all elephants are gray. DEDUCTIVE reasoning: A general principle \\(\\rightarrow\\) A set of predictions or explanations Example: All elephants are gray. Therefore, I predict that this new (as yet undiscovered) species of elephant will be gray. QUESTION: If this new species of elephant is green, what does this do to our hypothesis that all elephants are gray? Some terminology: Null Hypothesis: A statement encapsulating “no effect” Alternative Hypothesis: A statement encapsulating “an effect”” Fisher: Null hypothesis only Neyman and Pearson: H0 and H1, Weigh risk of of false positive against the false negative We use a hyprid approach Figure 1.4: Hypothetico-deductive view of the scientific method. Photo Source: LSE Library Not all hypotheses are created equal. Consider the following two hypotheses: H\\(_{1}\\): There are vultures in the local park H\\(_{2}\\): There are no vultures in the local park Which of these two ways of framing the null hypothesis can be rejected by data? Hypothesis can only be rejected, they can never be accepted! “Based on the data obtained, we reject the null hypothesis that…” or “Based on the data obained, we fail to reject the null hypothesis that…” More terminology Population: Entire collection of individuals a researcher is interested in. Model: Mathematical description of the quantity of interest. It combines a general description (functional form) with parameters (population parameters) that take specific values. Population paramater: Some measure of a population (mean, standard deviation, range, etc.). Because populations are typically very large this quantity is unknown (and usually unknowable). Sample: A subset of the population selected for the purposes of making inference for the popualtion. Sample Statistic: Some measure of this sample that is used to infer the true value of the associated populatipn parameter. An example: Population: Fish density in a lake Sample: You do 30 net tows and count all the fish in each tow Model: \\(Y_i \\sim Binom(p,N)\\) The basic outline of statistical inference sample(data) \\(\\rightarrow\\) sample statistics \\(\\rightarrow\\) ESTIMATOR \\(\\rightarrow\\) population parameter \\(\\rightarrow\\) underlying distribution Estimators are imperfect tools Bias: The expected value \\(\\neq\\) population parameter Not consistent: As \\(n \\to \\infty\\) sample statistic \\(\\neq\\) population parameter Variance "],["week-1-lab.html", "2 Week 1 Lab 2.1 Using R like a calculator 2.2 The basic data structures in R 2.3 Writing functions in R 2.4 Writing loops and if/else 2.5 (A short diversion) Bias in estimators 2.6 Some practice writing R code 2.7 A few final notes", " 2 Week 1 Lab In lab today, we will cover just a few of the basic elements of using R. If you are not already fluent with R, you should work through all of Logan Chapter 1, as there are many important elements covered in that chapter that we will not have time to go through in lab. I will assume everyone is using RStudio to run R - exercises and associated code will be written accordingly. For the purposes of lab, we will be entering all of the commands directly on the command line at the prompt. In general, however, you should get into the habit of writing scripts with all of your code. This will allow you to save your work and go back and easily use code that you have written in the past. When you are doing actual analyses for publication, it is essential that you have all of your code in well commented scripts that could be understood by another researcher in your field. All of your analysis should be fully reproducible long after the paper is published. 2.1 Using R like a calculator R can be used like a basic calculator with commands entered at the R prompt ‘&gt;’ 3+5 ## [1] 8 3*5 ## [1] 15 (The “##” is the html editor’s mechanism for indicating R output. The [1] is something that actually appears in the R output. It indicates that what you see is the first element of the output and in this case it can be ignored.) While R is fairly clever about the order of operations 3*5+7 ## [1] 22 it is good practice to be explicit (3*5)+7 ## [1] 22 Notice that the following two expressions are equivalent 4^(1/2) ## [1] 2 sqrt(4) ## [1] 2 The former expression uses the ^ to signify an exponent, whereas the latter uses the built-in R function sqrt() for the square-root. Note the difference between -4^(1/2) ## [1] -2 and (-4)^(1/2) ## [1] NaN In the first instance, R does the sqrt() and THEN applied the negative, whereas in the second case, it is trying to take the square-root of a negative number and it spits back NaN for ‘Not a Number’. To use scientific notation, use e. 2.2e3 ## [1] 2200 For large numbers, R automatically uses scientific notation although the threshold for scientific notation is something you can change using the R function ‘options’ (advanced use only - don’t worry about it for now). 2.2e3*5e5 ## [1] 1.1e+09 2.2 The basic data structures in R There are several basic types of data structures in R. VECTORS: One-dimensional arrays of numbers, character strings, or logical values (T/F) FACTORS: One-dimensional arrays of factors (Stop - Let’s discuss factors) DATA FRAMES: Data tables in which the various columns may be of different type MATRICES: In R, matrices can only be 2-dimensional, anything higher dimension is called an array (see below). Matrix elements are typically (but not necessarily) numerical, but the key difference from a data frame is that every element has to have the same type. Some functions, like the transpose function t(), only work on matrices. ARRAYS: higher dimensional matrices are called arrays in R LISTS: lists can contain any type of object as list elements. You can have a list of numbers, a list of matrices, a list of characters, etc., or any combination of the above. Vectors: Vectors can be column vectors or row vectors but we are almost always talking about column vectors which are defined with a c(). One example of a vector would be a sequence of numbers. There are many ways to generate sequences in R. Lets say you want to define an object x as the following sequence of numbers (1,2,3,4,5,6,7) You could do this this long way x&lt;-c(1,2,3,4,5,6,7) Notice here I have used the &lt;- to “assign” the column vector (hence “c”) of values to the variable x. I could also do this using x&lt;-1:7 or I could explicitly use the seq() function as follows x&lt;-seq(from=1,to=7) The sequence function actually has three inputs, but I have left the last off because the default is that you want to step in increments of 1. The full version would be x&lt;-seq(from=1,to=7,by=1) Make sure this works by printing out the value for x x Try changing it up a little with x&lt;-seq(from=1,to=16,by=3) STOP: Spend a few minutes making vectors and using some of the basic R commands. What happens if you pass a vector to one of R’s built-in functions? R can do a host of logical operations. x&lt;7 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE We can turn that into a binary vector in at least two ways as.numeric(x&lt;7) ## [1] 1 1 0 0 0 0 or 1*(x&lt;7) ## [1] 1 1 0 0 0 0 The former forces R to return the values of x as a numerical vector, and by default False maps to 0 and True to 1. The latter version does the same thing, by multiplying the logical vector by a number. This trick comes in handy all the time. For example, if you want to know how many values of x are less than 7, you can simply do the following sum(as.numeric(x&lt;7)) ## [1] 2 You can also ask which elements satisfy certain criteria. In other words, you can type which(x&lt;7) ## [1] 1 2 This is telling you that the first and second elements of the vector are less than 7. We can take a random set of numbers y&lt;-c(4,8,6,3,6,9,2) and sort them sort(y) ## [1] 2 3 4 6 6 8 9 or reverse sort them rev(sort(y)) ## [1] 9 8 6 6 4 3 2 To pull up the help file for the R command ‘sort’: ?sort STOP: Let’s take this opportunity to go through all the elements of an R help file. We can also print out the rank of each value rank(y) ## [1] 3.0 6.0 4.5 2.0 4.5 7.0 1.0 Notice that ties got averaged. Elements of vectors in R are addressed using [] as follows First lets make a vector z z&lt;-seq(from=1,to=15,by=2) We can find the 4th element by simply typing z[4] ## [1] 7 or we can find the 3rd and 4th elements by typing z[c(3,4)] ## [1] 5 7 In this more complicated case, we create a vector of the indices we want, and feed that into the brackets. We can do the opposite as well, instead of pulling out a set of elements you want, you can excise a set of elements and print everything else. In other words, if you wanted all the elements BUT element 3, you would use the minus sign z[-3] ## [1] 1 3 7 9 11 13 15 Factors: To explore factors, we will use the dataset Prestige.csv. To simplify everything that follows, I will set the working directory to my own folder for this week’s lab. This will allow me to reference files within this folder without the entire file name. Load the data Prestige&lt;-read.csv(&quot;_data/Prestige.csv&quot;) We can look at the entire data set by typing the name at the command prompt, but we can also just look at the first few lines using the ‘head’ function head(Prestige) ## X education income women prestige census type ## 1 gov.administrators 13.11 12351 11.16 68.8 1113 prof ## 2 general.managers 12.26 25879 4.02 69.1 1130 prof ## 3 accountants 12.77 9271 15.70 63.4 1171 prof ## 4 purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## 5 chemists 14.62 8403 11.68 73.5 2111 prof ## 6 physicists 15.64 11030 5.13 77.6 2113 prof or the last few lines using the ‘tail’ function tail(Prestige) ## X education income women prestige census type ## 97 train.engineers 8.49 8845 0.00 48.9 9131 bc ## 98 bus.drivers 7.58 5562 9.47 35.9 9171 bc ## 99 taxi.drivers 7.93 4224 3.59 25.1 9173 bc ## 100 longshoremen 8.37 4753 0.00 26.1 9313 bc ## 101 typesetters 10.00 6462 13.58 42.2 9511 bc ## 102 bookbinders 8.55 3617 70.87 35.2 9517 bc We can also get the dimensions of the data set using the ‘dim’ function dim(Prestige) ## [1] 102 7 or use the ‘length’ function to figure out the length of one of the columns length(Prestige[,1]) ## [1] 102 Note: Depending on your version of R and operating system, and how you loaded the dataset Prestige, R may not be holding Prestige as a simple dataframe, and if so, it may tell you that the length(Prestige[,1])=1. In this case, we need to tell R that it should force it into a data.frame (more generally in programming, this is called ‘casting’) and we can do this as Prestige&lt;-as.data.frame(Prestige) What this does is take the object Prestiage and change it into a data.frame and then assign that to the variable Prestige (i.e. it replaces Prestiage with the new object). Now try the ‘length’ command above again and it should yield a more sensible result. Factors are character labels which take fixed values. First just look at the data. Prestige ## X education income women prestige census type ## 1 gov.administrators 13.11 12351 11.16 68.8 1113 prof ## 2 general.managers 12.26 25879 4.02 69.1 1130 prof ## 3 accountants 12.77 9271 15.70 63.4 1171 prof ## 4 purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## 5 chemists 14.62 8403 11.68 73.5 2111 prof ## 6 physicists 15.64 11030 5.13 77.6 2113 prof ## 7 biologists 15.09 8258 25.65 72.6 2133 prof ## 8 architects 15.44 14163 2.69 78.1 2141 prof ## 9 civil.engineers 14.52 11377 1.03 73.1 2143 prof ## 10 mining.engineers 14.64 11023 0.94 68.8 2153 prof ## 11 surveyors 12.39 5902 1.91 62.0 2161 prof ## 12 draughtsmen 12.30 7059 7.83 60.0 2163 prof ## 13 computer.programers 13.83 8425 15.33 53.8 2183 prof ## 14 economists 14.44 8049 57.31 62.2 2311 prof ## 15 psychologists 14.36 7405 48.28 74.9 2315 prof ## 16 social.workers 14.21 6336 54.77 55.1 2331 prof ## 17 lawyers 15.77 19263 5.13 82.3 2343 prof ## 18 librarians 14.15 6112 77.10 58.1 2351 prof ## 19 vocational.counsellors 15.22 9593 34.89 58.3 2391 prof ## 20 ministers 14.50 4686 4.14 72.8 2511 prof ## 21 university.teachers 15.97 12480 19.59 84.6 2711 prof ## 22 primary.school.teachers 13.62 5648 83.78 59.6 2731 prof ## 23 secondary.school.teachers 15.08 8034 46.80 66.1 2733 prof ## 24 physicians 15.96 25308 10.56 87.2 3111 prof ## 25 veterinarians 15.94 14558 4.32 66.7 3115 prof ## 26 osteopaths.chiropractors 14.71 17498 6.91 68.4 3117 prof ## 27 nurses 12.46 4614 96.12 64.7 3131 prof ## 28 nursing.aides 9.45 3485 76.14 34.9 3135 bc ## 29 physio.therapsts 13.62 5092 82.66 72.1 3137 prof ## 30 pharmacists 15.21 10432 24.71 69.3 3151 prof ## 31 medical.technicians 12.79 5180 76.04 67.5 3156 wc ## 32 commercial.artists 11.09 6197 21.03 57.2 3314 prof ## 33 radio.tv.announcers 12.71 7562 11.15 57.6 3337 wc ## 34 athletes 11.44 8206 8.13 54.1 3373 &lt;NA&gt; ## 35 secretaries 11.59 4036 97.51 46.0 4111 wc ## 36 typists 11.49 3148 95.97 41.9 4113 wc ## 37 bookkeepers 11.32 4348 68.24 49.4 4131 wc ## 38 tellers.cashiers 10.64 2448 91.76 42.3 4133 wc ## 39 computer.operators 11.36 4330 75.92 47.7 4143 wc ## 40 shipping.clerks 9.17 4761 11.37 30.9 4153 wc ## 41 file.clerks 12.09 3016 83.19 32.7 4161 wc ## 42 receptionsts 11.04 2901 92.86 38.7 4171 wc ## 43 mail.carriers 9.22 5511 7.62 36.1 4172 wc ## 44 postal.clerks 10.07 3739 52.27 37.2 4173 wc ## 45 telephone.operators 10.51 3161 96.14 38.1 4175 wc ## 46 collectors 11.20 4741 47.06 29.4 4191 wc ## 47 claim.adjustors 11.13 5052 56.10 51.1 4192 wc ## 48 travel.clerks 11.43 6259 39.17 35.7 4193 wc ## 49 office.clerks 11.00 4075 63.23 35.6 4197 wc ## 50 sales.supervisors 9.84 7482 17.04 41.5 5130 wc ## 51 commercial.travellers 11.13 8780 3.16 40.2 5133 wc ## 52 sales.clerks 10.05 2594 67.82 26.5 5137 wc ## 53 newsboys 9.62 918 7.00 14.8 5143 &lt;NA&gt; ## 54 service.station.attendant 9.93 2370 3.69 23.3 5145 bc ## 55 insurance.agents 11.60 8131 13.09 47.3 5171 wc ## 56 real.estate.salesmen 11.09 6992 24.44 47.1 5172 wc ## 57 buyers 11.03 7956 23.88 51.1 5191 wc ## 58 firefighters 9.47 8895 0.00 43.5 6111 bc ## 59 policemen 10.93 8891 1.65 51.6 6112 bc ## 60 cooks 7.74 3116 52.00 29.7 6121 bc ## 61 bartenders 8.50 3930 15.51 20.2 6123 bc ## 62 funeral.directors 10.57 7869 6.01 54.9 6141 bc ## 63 babysitters 9.46 611 96.53 25.9 6147 &lt;NA&gt; ## 64 launderers 7.33 3000 69.31 20.8 6162 bc ## 65 janitors 7.11 3472 33.57 17.3 6191 bc ## 66 elevator.operators 7.58 3582 30.08 20.1 6193 bc ## 67 farmers 6.84 3643 3.60 44.1 7112 &lt;NA&gt; ## 68 farm.workers 8.60 1656 27.75 21.5 7182 bc ## 69 rotary.well.drillers 8.88 6860 0.00 35.3 7711 bc ## 70 bakers 7.54 4199 33.30 38.9 8213 bc ## 71 slaughterers.1 7.64 5134 17.26 25.2 8215 bc ## 72 slaughterers.2 7.64 5134 17.26 34.8 8215 bc ## 73 canners 7.42 1890 72.24 23.2 8221 bc ## 74 textile.weavers 6.69 4443 31.36 33.3 8267 bc ## 75 textile.labourers 6.74 3485 39.48 28.8 8278 bc ## 76 tool.die.makers 10.09 8043 1.50 42.5 8311 bc ## 77 machinists 8.81 6686 4.28 44.2 8313 bc ## 78 sheet.metal.workers 8.40 6565 2.30 35.9 8333 bc ## 79 welders 7.92 6477 5.17 41.8 8335 bc ## 80 auto.workers 8.43 5811 13.62 35.9 8513 bc ## 81 aircraft.workers 8.78 6573 5.78 43.7 8515 bc ## 82 electronic.workers 8.76 3942 74.54 50.8 8534 bc ## 83 radio.tv.repairmen 10.29 5449 2.92 37.2 8537 bc ## 84 sewing.mach.operators 6.38 2847 90.67 28.2 8563 bc ## 85 auto.repairmen 8.10 5795 0.81 38.1 8581 bc ## 86 aircraft.repairmen 10.10 7716 0.78 50.3 8582 bc ## 87 railway.sectionmen 6.67 4696 0.00 27.3 8715 bc ## 88 electrical.linemen 9.05 8316 1.34 40.9 8731 bc ## 89 electricians 9.93 7147 0.99 50.2 8733 bc ## 90 construction.foremen 8.24 8880 0.65 51.1 8780 bc ## 91 carpenters 6.92 5299 0.56 38.9 8781 bc ## 92 masons 6.60 5959 0.52 36.2 8782 bc ## 93 house.painters 7.81 4549 2.46 29.9 8785 bc ## 94 plumbers 8.33 6928 0.61 42.9 8791 bc ## 95 construction.labourers 7.52 3910 1.09 26.5 8798 bc ## 96 pilots 12.27 14032 0.58 66.1 9111 prof ## 97 train.engineers 8.49 8845 0.00 48.9 9131 bc ## 98 bus.drivers 7.58 5562 9.47 35.9 9171 bc ## 99 taxi.drivers 7.93 4224 3.59 25.1 9173 bc ## 100 longshoremen 8.37 4753 0.00 26.1 9313 bc ## 101 typesetters 10.00 6462 13.58 42.2 9511 bc ## 102 bookbinders 8.55 3617 70.87 35.2 9517 bc Notice that the last column assigns a type of professional status to the different occupations. We can have R list all those by printing just the last column. We do that by using the $ followed by the name of that column: Prestige$type ## [1] &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; ## [11] &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; ## [21] &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;prof&quot; &quot;bc&quot; &quot;prof&quot; &quot;prof&quot; ## [31] &quot;wc&quot; &quot;prof&quot; &quot;wc&quot; NA &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; ## [41] &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; ## [51] &quot;wc&quot; &quot;wc&quot; NA &quot;bc&quot; &quot;wc&quot; &quot;wc&quot; &quot;wc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; ## [61] &quot;bc&quot; &quot;bc&quot; NA &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; NA &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; ## [71] &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; ## [81] &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; ## [91] &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;prof&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; &quot;bc&quot; ## [101] &quot;bc&quot; &quot;bc&quot; Notice that in addition to just listing that column, R also tells you what all the factor values are. We can do this with numerical values too, but be careful because R will interpret the numerical values as characters: IMPORTANT: By default, R will rank factors alphabetically. R will do this also when doing modeling and it is almost never what you want. In this case, you likely want to think of the factors arranged as bc\\(&lt;\\)wc\\(&lt;\\)prof. To do this you: Prestige$type&lt;-factor(Prestige$type,levels=c(&quot;bc&quot;,&quot;wc&quot;,&quot;prof&quot;)) levels(Prestige$type) ## [1] &quot;bc&quot; &quot;wc&quot; &quot;prof&quot; Data frames: R has a special object called a dataframe which is, as the name suggests, designed to hold data. Unlike a matrix, in which all the elements have to be the same type (typically numbers), dataframes are more like spreadsheets - each column can be its own datatype. So you can have a column of numbers associated with a second column of treatment types (character). Let’s make a data frame to play around with, which we will make the ranking of the top three girls and boys names for 2010. our.data.frame&lt;-data.frame(rank=seq(1:3),boys.names=c(&quot;Jacob&quot;,&quot;Ethan&quot;,&quot;Michael&quot;), girls.names=c(&quot;Isabella&quot;,&quot;Sophia&quot;,&quot;Emma&quot;)) our.data.frame ## rank boys.names girls.names ## 1 1 Jacob Isabella ## 2 2 Ethan Sophia ## 3 3 Michael Emma Now while I would encourage everyone to use the command line at all times, its worth pointing out that R does have a very basic data editor. To change a value in our.data.frame using the data editor, use the command ‘fix’ fix(our.data.frame) Note that the command ‘edit’ looks like it should do the same thing but it does not. In fact, ‘edit’ does not change the original data frame but it makes a changed copy which must be assigned another name. In the following example, the changes are stored in new.data.frame. new.data.frame&lt;-edit(our.data.frame) NOTE: R allows you to ‘attach’ a dataframe to a workspace so that you can refer to the individual columns without having to type in the name of the dataframe. I think this is terrible practice and makes your code impossible to read by your future self. Matrices: You make a matrix as follows (here we populate the matrix with a sequence from 1:12): test.matrix&lt;-matrix(1:12,nrow=3,ncol=4) test.matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that in general, you do not need to include the label names for input parameters to functions. This gives the same answer: test.matrix&lt;-matrix(1:12,3,4) test.matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 but I highly suggest leaving all labels for clarity of coding. R indexes matrices by ROW THEN COLUMN. So, for example, try test.matrix[2,3] ## [1] 8 R has all the functions you could ever want for matrix algebra, such as transposing: trans.test.matrix&lt;-t(test.matrix) See what happens when you try 1-test.matrix Notice that R automatically translates the 1 into a matrix of 1s such that the calculation makes sense. Arrays: Arrays are just higher dimensional matrices and since we will not use them much, I won’t get into details here. Lists: A list is a one-dimensional structure of potentially heterogeneous data types. list.1&lt;-list(data=seq(1,15),mat1=test.matrix,mat2=trans.test.matrix) We can reference elements of the list by name list.1$data ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 or by position list.1[[2]] ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that list indexing requires double brackets. 2.3 Writing functions in R Using R, you are not limited to functions that have been written for you, you can write your own functions! The basic template is straightforward: square&lt;-function(x) { x*x } We can use our function now as follows: square(5) ## [1] 25 You can also have more than one argument for an R function: product&lt;-function(x,y) { x*y } product(3,5) ## [1] 15 A few notes about using R. What makes R special is not the base package but the “Contributed packages” which make up the bulk of R’s utility. We will be using a variety of these contributed packages along the way, so you need to feel comfortable downloading them from the web. I have posted a handout on Blackboard to cover this. 2.4 Writing loops and if/else The R language is very good at doing operations on vectors or matrices, and when possible, this is always the preferred method of doing an operation mulitple times. However, sometimes this is not possible and you have to write a loop to perform some operation on elements taken one at a time. There are two different kinds of loops in R. A ‘for loop’ executes once for each step through the looping index. The basic syntax for a ‘for loop’ in R is: for (i in 1:6) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 The indexing variable does not need to be called “i”, it could be anything. What follows the “in” can be any sequence of numbers; they need not be consecutive. What appears inside the brackets is the chunk of code that will be executed at each iteration. This code can, but need not, actually use the indexing variable. Another example illustrating these points is: for (blah in c(1,3,5,6)) { print(4+blah) } ## [1] 5 ## [1] 7 ## [1] 9 ## [1] 10 A ‘while loop’ is open ended; it will execute the loop indefinitely until the ‘while’ condition is no longer met. The basic syntax for a ‘while loop’ in R is: i=1 while(i &lt;= 8) { y &lt;- i*i i &lt;- i + 1 # What would happen if we left this line out? print(y) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 Sometimes, you want to make R check some condition before executing a command. An ‘if’ statement will check a statement and execute a chunk of code if the statement evaluates to TRUE. If the statement evaluates to FALSE, the code is simply skipped. An if/else statement allows a second chunk of code to be executed as an alternative to the first. The syntax for each is as follows: a&lt;-3 if (a&lt;4) print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; if (a &lt; 4) { print(&quot;Hello&quot;) } else print(&quot;Goodbye&quot;) ## [1] &quot;Hello&quot; if (a &gt; 4) { print(&quot;Hello&quot;) } else print(&quot;Goodbye&quot;) ## [1] &quot;Goodbye&quot; 2.5 (A short diversion) Bias in estimators Now we will stop for a short digression about how to calculate the population variance (i.e. the variance assuming the data I have is from the entire population) and how to estimate the sample variance (i.e. the variance assuming what I have is a sample from the population, and I want to infer the variance of the underlying but unknown population), since we can now use R to convince ourselves that the naive estimator for variance is biased. The population variance is the variance of a population which, by definition, means that every single individual of interest has been measured. Remember, in this case there is no inference going on. When we have measured every single individual of interest, all we can do (statistically) is describe that population. The population variance describes the variation in the quantity of interest for that population you have completely sampled). \\[ \\sum^{n}_{i=1}{\\frac{(Y_{i}-\\bar{Y})^{2}}{n}} \\] The sample variance answers the question “If this data I have come from a larger population, and I want to use these data to estimate the population variance in that larger population, what is the best unbiased estimator for that (unknown) population variance?” The formula for the sample variance (think of it like “the estimate of the variance from the sample”): \\[ \\sum^{n}_{i=1}{\\frac{(Y_{i}-\\bar{Y})^{2}}{n-1}} \\] We can see this in practice using a little simulation. Type the following into an R script and run it in R: n.iter&lt;-100 data&lt;-rnorm(n.iter,0,2) sum&lt;-0 for (j in 1:length(data)) { sum&lt;-sum+((data[j]-mean(data))*(data[j]-mean(data))) } population.variance&lt;-sum/length(data) sample.variance&lt;-sum/(length(data)-1) What is the ratio of the sample variance to the population variance? Are either close to what we know the true variance to be? What happens if we change n.iter to 1000? Do the values get closer to the correct value? What happens to the ratio of the sample variance to the population variance? What does the R function var() give you? 2.6 Some practice writing R code We will be using a cloud-seeding dataset from: Simpson, Alsen, and Eden. (1975). A Bayesian analysis of a multiplicative treatment effect in weather modification. Technometrics 17, 161-166. The data consist of data on the amount of rainfall (in acre-feet) from unseeded clouds vs. those seeded with silver nitrate. Here and throughout I have assumed the data resides in a local folder and the code below has my pathnames but you will have to change the code according to your own file structure. (I am using a Mac, but getting the pathname correct I have included a .txt file and a .csv file to show you the differences in inputting your data: Method 1: cloud.data&lt;-read.table(&quot;_data/clouds.txt&quot;) Notice that that doesn’t work because the headers have become part of the data. cloud.data&lt;-read.table(&quot;_data/clouds.txt&quot;, header=T) Remember that we need to add the “header=T” or it will assume the headers are actually the first line of data. Method 2: cloud.data&lt;-read.table(&quot;_data/clouds.csv&quot;, header=T) This doesn’t work because R does know what the delimiter is. You have to specify the delimiter: cloud.data&lt;-read.table(&quot;_data/clouds.csv&quot;, header=T,sep=&quot;,&quot;) or use the command ‘read.csv’ which automatically assumes its comma delimited. cloud.data&lt;-read.csv(&quot;_data/clouds.csv&quot;, header=T) There are two ways to refer to the first column of data. Because we have column headers, we can refer to them by name using the “$” as follows: cloud.data$Unseeded_Clouds ## [1] 1202.6 830.1 372.4 345.5 321.2 244.3 163.0 147.8 95.0 87.0 ## [11] 81.2 68.5 47.3 41.1 36.6 29.0 28.6 26.3 26.1 24.4 ## [21] 21.7 17.3 11.5 4.9 4.9 1.0 but we can also just ask for a specific column of the data, in this case the first column cloud.data[,1] ## [1] 1202.6 830.1 372.4 345.5 321.2 244.3 163.0 147.8 95.0 87.0 ## [11] 81.2 68.5 47.3 41.1 36.6 29.0 28.6 26.3 26.1 24.4 ## [21] 21.7 17.3 11.5 4.9 4.9 1.0 Note that you can always print the data using just the name cloud.data ## Unseeded_Clouds Seeded_Clouds ## 1 1202.6 2745.6 ## 2 830.1 1697.8 ## 3 372.4 1656.0 ## 4 345.5 978.0 ## 5 321.2 703.4 ## 6 244.3 489.1 ## 7 163.0 430.0 ## 8 147.8 334.1 ## 9 95.0 302.8 ## 10 87.0 274.7 ## 11 81.2 274.7 ## 12 68.5 255.0 ## 13 47.3 242.5 ## 14 41.1 200.7 ## 15 36.6 198.6 ## 16 29.0 129.6 ## 17 28.6 119.0 ## 18 26.3 118.3 ## 19 26.1 115.3 ## 20 24.4 92.4 ## 21 21.7 40.6 ## 22 17.3 32.7 ## 23 11.5 31.4 ## 24 4.9 17.5 ## 25 4.9 7.7 ## 26 1.0 4.1 or, if its easier, can use the data editor as described above. Lets calculate the variance of each treatment. For now, I will do this step-by-step, defining intermediate variables along the way. For simplicity, I redefine the two columns worth of data as “A” and “B”: A&lt;- cloud.data$Unseeded_Clouds mean.A&lt;-mean(A) diff.from.mean.A&lt;- A-mean.A n.A&lt;-length(A) # Here I am just calculating the sample size to use in next line s2.A&lt;-sum(diff.from.mean.A^2)/(n.A-1) s2.A ## [1] 77521.26 Redo the calculation for the Seeded clouds to get “s2.B”. We could have saved ourselves a lot of effort by using the R function “var”: s2.A&lt;-var(A) s2.A ## [1] 77521.26 Is the variance for the Seeded clouds the same as the Unseeded clouds? How close (to equal) is close enough? What is the null hypothesis? 2.7 A few final notes I mentioned at the outset that all of your code should be kept in a script (some kind of text file; it could be a .R file but it could be a simple .txt file) and that your code should be clearly commented. Comments can be added to code using the # sign. For example a&lt;-3+5 #This is a comment everything after the # is not executed by R and is simply for your use in understanding the code. **Short digression on brackets and good coding practices. "],["week-2-lecture.html", "3 Week 2 Lecture 3.1 Week 2 Readings 3.2 Today’s Agenda 3.3 Hypothesis testing 3.4 Permutation tests 3.5 Parameter estimation 3.6 Method #1: Non-parametric bootstrap 3.7 Parametric bootstrap 3.8 Jackknife 3.9 Jackknife-after-bootstrap 3.10 By the end of Week 2, you should understand…", " 3 Week 2 Lecture 3.1 Week 2 Readings For this week, I suggest reading Aho Sections 5.4 and 6.6.1 and Johnson (1995). Those interested in a historical but still relevant perspective on how we learn from data should check out Tukey (1969). 3.2 Today’s Agenda This week we will learn about two concepts: 1) Hypothesis testing, and 2) Parameter estimation. You can think of these two concepts as two sides of the same coin, since they each allow us to learn from data. Hypothesis testing is a bit like the game 50-questions (Are you red? Are you blue? Each question is a null hypothesis to be rejected…). One example would be estimating the change in infection rate for patients receiving a Covid vaccine. The null hypothesis is: The vaccine provides no benefit. The test statistic would be the infection rate, and a mathematical way to state the null hypothesis would be \\(H_{0}: Rate_{Unvaxxed} - Rate_{Vaxxed}=0\\). A hypothesis test would allow us to potential reject that null hypothesis. Parameter estimation is more direct, it just asks “What are you?”. In the example provided above, we would estimate the parameter of interest, which is \\(Rate_{Unvaxxed} - Rate_{Vaxxed}\\), and our uncertainty about that parameter of interest. If our estimate of that parameter was so large (or the uncertainty so small) that we could be sure that the parameter was not zero, then this would be equivalent to rejecting the null hypothesis. 3.3 Hypothesis testing There are few topics in statistics more controversial than the various philosophies behind null hypothesis testing. Over the next two weeks we will learn about the two paradigms (Fisher vs. Neyman-Pearson), the hybrid approach mostly commonly used in ecology, and criticisms of the whole enterprise. Bayesian statistics takes an entirely different approach than either Fisher or Neyman-Pearson, and the Bayesian approach resolves many of the inconsistencies involved with frequentist statistics, but at the expense of increased computation (and the use of prior information…). We frame decision-making in terms of a null and an alternative hypothesis. \\(H_{0}\\) vs. \\(H_{A}\\) To take Karl Popper’s famous example: \\(H_{0}\\): There are no vultures in the park. \\(H_{A}\\): There are vultures in the park. Note that the data may reject the null hypothesis (for example, finding vultures in the park), or the data may fail to reject the null hypothesis, but it can never prove the null hypothesis. We cannot prove there are no vultures in the park. We can only say that we were not able to find any vultures in the park, and therefore cannot reject the null hypothesis. Fisher’s original context for developing significance testing was agricultural experiments that could be easily replicated. Fisher’s threshold of 0.05 was an arbitrary threshold for an effect to be considered worthy of continued experimentation. Any experiment that failed to reach this threshold would not be pursued. Experiments that gave “significant” results would be subject to additional experiments. These additional experiments may prove the original effect to be a fluke (and experiments would cease) or the additional experiments may provide confirmatory evidence that the effect was real. Null hypothesis testing (as I will teach it) involves 6 steps. Step #1: Specify a null hypothesis \\(H_{0}\\) (Note that I do not include specification of the alternative hypothesis \\(H_{A}\\) here. While the alternative hypothesis is useful as a mental construct, the basic approach deals only with \\(H_{0}\\) and does not require a \\(H_{A}\\).) Example: In an experiment on vaccine efficacy, the null hypothesis would be that the probability of coronovirus infection is the same in the vaccinated group as in the control group that received only a placebo. Mathematically, this could be stated: \\(H_{0}: P_{\\small{\\mbox{infection}}}^{\\small{\\mbox{vaccinated}}}=P_{\\small{\\mbox{infection}}}^{\\small{\\mbox{control}}}\\) Step #2: Specific an appropriate test statistic T. A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic. generic \\(T=f(X)\\) specific \\(T^{*}\\)=T(\\(X_{1}\\),\\(X_{2}\\),…,\\(X_{n}\\)) We will introduce many more test statistics in the weeks to come. Example: In our example with the vaccine trial, a reasonable test statistic would be \\(T=P_{\\small{\\mbox{infection}}}^{\\small{\\mbox{vaccinated}}}-P_{\\small{\\mbox{infection}}}^{\\small{\\mbox{control}}}\\). (This would be the best test statistic if probabilities were Normally distributed, but they are not. However, this test statistic is reasonable and highly intuitive and illustrates the basic point.) Step #3: Determine the distribution of the test statistic under the null hypothesis \\(H_{0}\\). A test statistic is a statistical quantity that has a statistical distribution. \\(f(T│H_{0})\\) Notice that this is the probability of obtaining the test statistic T given the null distribution, it is not \\(f(H_{0}│T)\\). The test statistic and its distribution under the null hypothesis is the statistical test. Test = Test statistic + Distribution of test statistic under \\(H_{0}\\) Step #4: Collect data and calculate T* Collect data by taking random samples from your population and calculate the test statistic from the sample data. Step #5: Calculate a p-value Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis \\(P(T^{*}│H_{0})\\)=p-value Step #6: Interpret the p-value Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected) Example: In our vaccination example, we would look at the difference in these two populations (control vs. vaccinated) and if the difference between those two probabilities was larger than we would expect to occur under the null hypothesis (which, but assuming that the vaccine is no better than a placebo, assumes that these two groups are equivalent and therefore that any differences are due to random chance alone), then we would reject the null hypothesis that the vaccine is equivalent to the placebo. (Note that I haven’t said anything about whether the vaccine lowers infection rates; in a few minutes we’ll discuss one-tailed vs. two-tailed tests.) These steps apply for both parametric and non-parametric statistics. Here we are introducing hypothesis testing through the lens of randomization procedures, but the same steps will be used again when we get into statistics involving parametric distributions (i.e. statistical distributions of known form and described by a finite number of parameters) and their properties. As you will see in a few weeks, most standard statistical tests involve a test statistic with a known distribution under the null hypothesis; here the distribution under the null hypothesis needs to be generated by randomization (randomization test). (We are starting with the randomization-based procedures because there is no math involved and it is more intuitive.) The basic idea underlying all statistical tests: What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the “p-value”). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis. p-value = \\(P(data|H_{0})\\) Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is not a statement about the probability that the null hypothesis is true. This logic can go wrong!! Example: If a person is an American, he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American. Let’s draw a null distribution. In order to interpret the statistical test, we need to know whether we want a one-tailed test or a two-tailed test. In a one-tailed test, we would reject the null hypothesis only if the test statistic is larger than expected under in the null in one direction (5\\(%\\) in one tail). In a two-tailed test, we would reject the null if the test statistic is larger in either direction (2.5\\(%\\) in both tails). Example: Let’s say I’m looking at the change in auto accident mortalities after a ban is enacted on driving while texting. We would expect that auto accident mortality would decrease after a ban on texting while driving. Let’s say, for arguments sake, that our test statistic T is the change in accident deaths \\(H_{0}\\): T=0 (no change in deaths) \\(H_{A}\\): T&lt;0 (decline in deaths) Another possible formulation of the null and alternative hypotheses is \\(H_{0}\\): T=0 (no change in deaths) \\(H_{A}\\): T \\(\\neq\\) 0 (increase or decline in deaths) Why does it matter? Consider the first case. To reject the null hypothesis, you would have to show that the measured decline \\(T^*\\) was so large as to be very unlikely to have occurred by random chance assuming there was no true change in death rate. Therefore, you would require \\(P(T \\geq T^{*}│H_{0})&lt;0.05\\) to be true for you to decide to reject the null hypothesis. This is a one-tailed test. Consider the second case. To reject the null hypothesis, you would accept values of \\(T^{*}\\) as significant if they were either very large or very small, and would divide the 5% critical region between the two tails \\(P(T \\geq T^{*}│H_{0})&lt;0.025\\) \\(P(T \\leq T^{*}│H_{0})&lt;0.025\\) Notice that it now becomes a more stringent test. If \\(T^{*}\\) is large, it now has to be even larger to qualify as “significant”. This is a two-tailed test. TWO KEY POINTS: 1. If you are using a one-tailed test, you have to be willing to accept a result that is opposite in sign of what was expected as being PURELY BY CHANCE!! In other words, if traffic deaths went UP after the texting ban, you would have to be willing to accept that that was by pure chance and you would then fail to reject the null hypothesis of NO CHANGE. This is in fact what happened, by the way: Texting bans actually increase traffic deaths – WHY? 2. Before using the more “lenient” one-tailed test, make sure you really believe that results opposite to what you expect are only random You cannot do a one-tailed test, find the answer to have the wrong sign and then do a two-tailed test. While probably quite common, this is not statistically valid. You cannot use the data to generate the test! Not all tests are created equal!! Tests differ in their power to detect differences, and their efficiency. The balance between power and efficiency depends on the specific situation; we will discuss this more next week. We are going to introduce the idea of hypothesis testing through the practice of permutation tests, because it allows us to get into the flow of testing hypotheses without the burden of a lot of complicated mathematics. Moreover, in doing so, we introduce the more general concept of “generative models”, which generate outcomes through simulation. For example, we can think of the statement \\(X \\sim Pois(\\lambda)\\) as a generative model because it allows us to generate datasets that follow the distribution \\(Pois(\\lambda)\\). If we had a dataset and we wanted to know whether it came from a Poisson distribution, we could imagine generating lots of datasets using a generative model (i.e. drawn from \\(Pois(\\lambda)\\)) and asking ourselves whether any of the generated datasets look anything like the dataset we have. In the same way, we can think about testing a null hypothesis \\(H_{0}\\) by generating data under that null hypothesis, calculating some test statistics from that generated data, and asking whether our geneterated test statsitics “look like” the test statistic obtained from our real data. If not, then we can reject the null hypothesis. Simulations like this are enormously powerful tools for testing hypotheses and are often far more intuitive than the alternative “parametric” statistical tests we will learn in Weeks 3 and 4. 3.4 Permutation tests Let’s say we have two random samples drawn from possibly different probability distributions F and G, \\(F \\rightarrow z=\\{z_{1},z_{2},...,z_{n}\\}\\) \\(G \\rightarrow y=\\{y_{1},y_{2},...,y_{m}\\}\\) Having observed z and y, we wish to test the null hypothesis \\(H_{0}\\) of no difference between F and G, \\(H_{0}:F=G\\). Note that the equality F=G means that the two distributions are exactly the same across their entire distribution, not just that their means are the same. If \\(H_{0}\\) is true, than there is no probabilistic difference between drawing random values from F and drawing random values from G. What are some possible test statistics that we might use in this case? There are many test statistics that we could use to test this null hypothesis but lets use the difference in means as the test statistic. If there is a large difference in their means, than we can probably reject the null hypothesis that they represent the same underlying distribution. \\(T=E[F]-E[G]=\\bar{z}-\\bar{y}\\) The way to do this is to lump all the data together and to randomly permute the labels so that data are randomly assigned to a group (z vs. y). Important: We are not sampling with replacement here. We are simply permuting the labels to “erase” any possible correlation between group and the values of the data. We then calculate the mean of the “fake z” group and the mean of the “fake y” group and take the difference. That is the result of ONE permutation. If we do that many many times (say, 10000 times) then the distribution of those differences reflects the distribution under the null hypothesis of no difference between F and G. We will do an example like this in the problem set. 3.5 Parameter estimation Hypothesis testing answers a very direct question relating to the probability of the data under the null hypothesis, and this can be used to judge whether the data are consistent with the null hypothesis or whether the data are inconsistent with the null hypothesis (in which case we would “reject” it). Parameter estimation represents a more direct approach by directly estimating the parameter of interest. First, a bit of vocabulary: Estimators are tools that produce estimates of population statistics from sample statistics. The basic outline of “statistical inference”: Data = sample \\(\\rightarrow\\) sample statistics \\(\\rightarrow\\) ESTIMATOR \\(\\rightarrow\\) population parameters We generally use the word “statistic” when discussing the data, and “parameter” when discussing the underlying distribution. An “estimator” or “point estimate” is a statistic (that is, a function of the data) that is used to infer the value of an unknown parameter in a statistical model. If the parameter is denoted \\(\\theta\\) then the estimator is typically written by adding a “hat” over the symbol: \\(\\hat{\\theta}\\). Being a function of the data, the estimator is itself a random variable; a particular realization of this random variable is called the “estimate”. Sometimes the words “estimator” and “estimate” are used interchangeably, but I will try and be consistent in using the word “estimator” for the function in the generic, and the word “estimate” for the result of applying that function to the data at hand. We have not been formally introduced to statistical distributions yet, but the Normal (a.k.a. Gaussian) distribution is the classic “bell-shaped” curve you may be familiar with. It has two parameters, \\(\\mu\\) (the location of the center of the bell curve) and \\(\\sigma\\) (the width of the bell curve). Let’s say we have a random variable X that follows a Normal distribution: \\[ X \\sim N(\\mu,\\sigma^{2}) \\] Parameter estimation involves using the data to estimate the true value of \\(\\mu\\) and \\(\\sigma\\). We define the “estimator” for \\(\\mu\\) as \\[ \\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] Therefore, the “estimate” \\(\\hat{\\mu}\\) is \\[ \\hat{\\mu}=\\bar{X} =\\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] Estimators are imperfect tools, and they can suffer from bias and/or variance (or, equivalently, standard error). Bias: As \\(n \\rightarrow \\infty\\), sample statistic does not converge to the population parameter Standard error: Each individual estimate may be too low or too high from the true value (this can occur even if the long run average value is correct, i.e. unbiased). In other words, there is sample-to-sample variance in the estimates obtained from a given sample. It turns out that bias and variance trade-off, and this trade-off is controlled by the complexity of the model you are trying to fit. We will return to these ideas in Week 13 but I suggest reading through this nice (and short) explanation of bias and variance (written from the perspective of machine learning, which is just another kind of model building enterprise). Why are estimators associated with a standard error? If you were to do your experiment all over again, say 1000 times, the value of your estimate would be different each time. Your 1000 estimates would have a statistical distribution with some spread, and the spread of these 1000 estimates is quantified by the standard error. (A good example of this from the Week 1 problem set is the question asking you the probability of getting your bag of M&amp;Ms from a random sample of M&amp;Ms. Let’s say you calculated this probability by sampling 100,000 times and you got 4 “matches”. This gives you an estimate of 0.00004. But if you repeated this experiement a second time, with another sample of 100,000 bags, you might get 7 matches or 2 matches or none at all! So 0.00004 is your estimate of the probability, but it is an estimate with some uncertainty because you don’t know how variable that probability would be if you did the experiment multiple times. This “spread” of the estimates is the standard error of your estimate. Computationally, we can calculate the standard error by doing lots of experiments, but in practice, this is not always possible. Therefore, as we’ll see in the next few weeks, we usually use the properties of statistical distributions to calculate the standard error of an estimate.) How do we estimate the bias and variance (related to standard error) of an estimator? While there are other methods that we will discuss in a few weeks, now we are going to introduce the idea through two non-parametric approaches: bootstrap and jackknife. First we need to stop and discuss what it means to sample from an empirical distribution. Let’s say I have a bunch of lotto balls in an urn \\(X=\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\) and I want to draw sets of 5 lotto numbers from that urn. I can sample with replacement or without replacement. If you sample with replacement, we may get some numbers more than once. It also means that if you draw n balls out of an urn with n numbers, there are some numbers you will never draw. STOP: Do you understand sample-with-replacement and sample-without-replacement? 3.6 Method #1: Non-parametric bootstrap The basic idea behind bootstrap sampling is that even if we don’t know what the distribution is that underlies the data, we can “pull ourselves up by our bootstraps” and generate the distribution by resampling WITH REPLACEMENT from the data itself. Say we have original data drawn from an unknown distribution G \\(X=\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\) \\[ X \\sim G() \\] We don’t know the underlying distribution, but we can substitute the empirical distribution \\(\\hat{G}\\) which is defined by \\(\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\). In other words, we model the underlying “true” unknown distribution as a multinomial where every value in X is given a probability \\(\\frac{1}{n}\\) of occurring. Let’s say we want to compute a statistic of the probability distribution \\(\\theta=f(G)\\), which could be the mean or the median or the standard error of the standard deviation (anything at all!!). BTW: \\(\\theta\\) is analogous to the test statistic T used for hypothesis testing, and it will be used in the same way. However, I will use the symbol \\(\\theta\\) to be consistent with the Efron and Tibshirani and other literature on the bootstrap. The “plug-in” principle states that for every parameter of the underlying distribution, we can estimate that function by simply plugging in the empirical distribution \\[ \\hat{\\theta}=f(\\hat{G})=f(X) \\] This is exactly what we would do intuitively. If we have a bunch of numbers and we want to know the mean of the distribution from whence they came, we would use as the best estimate the mean of those numbers. The “plug-in” principle simply formalizes the idea that these summary statistics can be used to make inference about the generating distribution. In the development to follow, we will assume that we have NO other information about a distribution other than a single sample from that distribution. Summary statistics are easy enough to compute, but we don’t have any way of knowing how accurate those summary statistics might be. The bootstrap gives us a way to calculate the accuracy of our summary statistics \\(\\hat{\\theta}\\). The bootstrap works NO MATTER HOW COMPLICATED THE FUNCTION, IT IS COMPLETELY AUTOMATIC, AND REQUIRES NO THEORETICAL CALCULATIONS. (I’m simplifying a little. The bootstrap fails in rare cases, which we won’t get into.) First we need the idea of a bootstrap sample. A bootstrap sample is any sample drawn randomly WITH REPLACEMENT from the empirical distribution. BOOTSTRAP = SAMPLE WITH REPLACEMENT \\(X^{*}=\\{\\mbox{n values drawn with replacement from } X\\}\\) n = size of the bootstrap sample = size of the original dataset We draw k such bootstrap samples: \\[ X_{1}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] \\[ X_{2}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] etc. \\[ X_{k}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] Important: Because we are sampling WITH REPLACEMENT, some of the original values will be represented more than once in any given bootstrap sample and others not at all. We calculate our statistic of interest on each bootstrap sample: \\[ \\theta_{1}^{*}=f(X_{1}^{*}) \\] \\[ \\theta_{2}^{*}=f(X_{2}^{*}) \\] etc. \\[ \\theta_{k}^{*}=f(X_{k}^{*}) \\] We will number the different bootstrap sample statistics as \\[ \\theta_{1}^{*},θ_{2}^{*},θ_{3}^{*},...,θ_{k}^{*} \\] k = number of bootstrap samples, you can choose the number of bootstrap samples, more sample = better estimates Now that we have our collection of k bootstrapped estimates of the statistic, what do we do with them? Remember: The goal was to calculate the bias and standard error of our estimator. \\[ \\widehat{Bias_{boot}}=\\left( \\frac{1}{k}\\sum_{i=1}^{k}\\theta_{i}^{*}\\right)-\\hat{\\theta} \\] In other words, the Bias of our estimator is simply the mean of the bootstrapped sample statistics minus the statistic as calculated for the original data. (For unbiased estimators, our estimate of bias goes to zero as the sample size n gets very large.) We can also use these bootstrapped statistics to calculate the standard error of the estimator: \\[ \\widehat{se_{boot}}=\\sqrt{\\frac{1}{k-1}\\sum_{i=1}^{k}(\\theta_{i}^{*}-\\bar{\\theta^{*}})^{2}} \\] This is just the standard deviation of the distribution of \\(\\theta\\). This is a really important point that is worth dwelling on for a bit. Our uncertainty about the value is captured by how much variation there is when I draw a different sub-sample of the data, which mimics re-doing the experiment altogether. In this case, I call the standard deviation of those \\(\\theta^{*}\\) values a standard error, because they represent my uncertainty (my potential error) about \\(\\hat{\\theta}\\). Do not confuse standard deviation and standard error. A standard deviation is a statistic (something calculated from data) about the spread of the data. A standard error is the standard deviation of my estimates, and therefore is a measure of how uncertain I am about my estimate. We will work through a few examples in the lab. Knowing how to draw bootstrap replicates gets more complicated when you have multivariate datasets. For example, lets start with a dataset comparing average LSAT scores and GPA for the incoming classes for 15 law schools Figure 0.1: Source: Efron and Tibshirani (1994) Lets say we want to estimate the true correlation coefficient between LSAT scores and GPA. We haven’t covered this yet, but one estimator for the true correlation coefficient is Pearson’s product moment correlation coefficient r \\[ r=\\frac{cov(a,b)}{\\sqrt{var(a)×var(b)}} \\] Therefore, in this case \\[ \\hat{r} = \\frac{cov(LSAT,GPA)}{\\sqrt{var(LSAT)*var(GPA)}} \\] (In R, we would write this as r.est = cor.test(LSAT,GPA)$estimate.) If LSAT and GPA both come from a normal distribution, then we could use the theory of normal distributions to calculate the standard error of \\(\\hat{r}\\). (We will learn this in Week 9.) But, we know LSAT and GPA can’t be from normal distributions. At the very least, GPA is bounded on (0,4), so it cannot be Normally distributed. So, how do we calculate the standard error of \\(\\hat{r}\\)? Here we sample with replacement from the bivariate PAIRS of data. In other words, we sample \\[ X_{1}^{*}=(LSAT_{i},GPA_{i}), \\mbox{where i=sample with replacement 1...n} \\] \\[ X_{2}^{*}=(LSAT_{i},GPA_{i}), \\mbox{where i=sample with replacement 1...n} \\] and so forth, and then calculate the correlation of each simulated dataset. Question: Why not sample with replacement from the two datasets independently? What question would that be answering? If we do this many times, say k=10,000 times, then we can draw a histogram of these bootstrapped correlation coefficients. We can calculate the standard error of our estimate for the correlation coefficient \\[ \\hat{se}_{boot} = \\sqrt{\\left(\\frac{1}{k-1}\\right)\\sum_{i=1}^{k}(r_{i}^{*}-\\bar{r^{*}})^{2}} \\] Therefore, using R, we would calculate the parametric correlation coefficient as: r.est ± 1.96*s.e.boot (VERSION 1) Even better, we can calculate the 95th percentile confidence interval of this distribution: quantile(all.cor,c(0.025,0.975)) (VERSION 2) Note that while VERSION 1 is common, VERSION 2 is preferred because there is no guarantee that the distribution of bootstrap statistics is even vaguely Normal. Bootstrapping can deal with even more complex cases, and is particularly useful when dealing with spatial or temporal autocorrelation. Take for instance a time series of hormone levels: Figure 1.1: Source: Efron and Tibshirani (1994) If you wanted to do some time series analysis of this data, say to calculate the correlation between each datapoint and the last datapoint, you would have a difficult time doing so because of the complex temporal autocorrelation. Bootstrap can help in this case, but its not at all obvious how to bootstrap from this time series and preserve the essential temporal autocorrelation structure of the data. One approach would be to do a “moving blocks” bootstrap. Figure 1.2: Source: Efron and Tibshirani (1994) This is more advanced, but it makes the point that a) bootstrap can be enormously useful in a variety of complicated analyses and b) you have to think carefully about what to sample in order to preserve the essential element of the data. R has numerous functions for doing bootstrapping, although bootstrapping is so easy its often just as easy (and more transparent) to simply write your own code to do it. We will go over some examples in lab. Note that the procedure we have described is called the non-parametric bootstrap estimate because it is based only on the non-parametric empirical distribution G ̂. If we had assumed some kind of distributional form for G, it would be considered a parametric bootstrap. 3.7 Parametric bootstrap The parametric bootstrap is similar to the non-parametric bootstrap except that instead of drawing our bootstrap samples from the original data, we fit a distribution to the data first, and then draw our samples from that. We haven’t covered how to fit a distribution to data yet, nor have we introduced any of the univariate distributions, so I won’t show you how to do a parametric bootstrap now but we’ll get some practice in the Week 3 problem set. Why would we ever do a parametric bootstrap? We might use a parametric distribution if our original sample size was so small that we did not think it could “stand in” for the underlying parametric distribution. For example, if your dataset for coin age just so happens not to have any coins made in 1990, you may be uncomfortable having all your bootstrapped datasets also be missing coins made in 1990. (Remember: Bootstrapping is, in some way, supposed to mimic redoing your experiment. Do you really think that you’d never get a coin made in 1990?) To get around this problem, you might do a parametric bootstrap. Note that, if you use MLEs to get the parameters for the parametric bootstrap, those parameter estimates assume large sample sizes (the formula are asymptotically correct for large sample sizes) and so you have to be a little caution that your parametric bootstrap might not be capturing the true underlying distribution. While parametric bootstrap is often done when sample sizes are too small, occasionally it may also be used when you have some strong theoretical justification for a particular distribution but the statistics you are interested in have no simple formula. (In other words, maybe the distribution is known, but the statistical properties of the specific parameter you are interested in is not known but could be derived through parametric bootstrapping.) 3.8 Jackknife Jackknifing is another method of assessing bias and standard error of sample statistics. Jackknife can also be used to establish the influence of each datapoint in your dataset. The procedure simply involves leaving out each datapoint and recalculating the statistic of interest. If your dataset involves the set \\[ \\{x_{1},x_{2},x_{3}\\} \\] then the jackknife samples are \\[ \\{x_{1},x_{2}\\},\\{x_{1},x_{3}\\},\\{x_{2},x_{3}\\} \\] The traditional notation is that the estimate based on the dataset when the ith element is removed is (\\(\\widehat{\\theta_{(i)}}\\)). The jackknife estimate of bias is given by \\[ \\widehat{Bias_{jack}}=(n-1)(\\hat{\\theta_{(.)}}-\\hat{\\theta}) \\] where \\[ \\hat{\\theta}_{(.)}=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(i)} \\] You can convince yourself of this formula by working out the case where \\(\\hat{\\theta}\\) is the mean. You can also see intuitively why you would have to multiply the jackknife estimate of bias by (n-1) since the deviation of the jackknifed samples from the full sample is much smaller than the standard deviation of the bootstrapped samples. (DOES EVERYONE SEE WHY?) The jackknife estimate of standard error is given by \\[ \\hat{se}_{jack}=\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^{n}(\\hat{\\theta}_{(i)}-\\hat{\\theta}_{(.)})^{2}} \\] With the pennies example we will do in lab, we have 5 pennies and we have only 5 possible jackknifed samples. Do you see why? Note that while bootstrapping can involve simulating an arbitrarily large number of pseudosamples (k), there are only n possible jackknife replicates for a dataset of size n. Exercise: Use your pennies to calculate \\(\\widehat{Bias}_{jack}\\) and \\(\\widehat{se}_{jack}\\). Both bootstrap and jackknife can estimate the standard error of a statistic, and in this way, their use can often be interchangeable. However, the jackknife can ONLY compute the bias and standard error whereas the bootstrap calculates the entire distribution of the statistic from which the standard error can be inferred. Bootstrapping is often more computer intensive, but with modern computers this is hardly a drawback. 3.9 Jackknife-after-bootstrap Jackknife-after-bootstrap is one method of assessing the standard error of bootstrap statistics. For example, jackknife-after-bootstrap can give us the standard error of the bootstrap standard error: \\[ \\widehat{se}_{jack}(\\widehat{se}_{boot}) \\] To do this there are two steps: Leave out data point i and use the remaining data in a bootstrap analysis to calculate (s.e.) ̂_(boot(i)) Define \\[ \\widehat{se}_{jack}(\\widehat{se}_{boot})=\\sqrt{\\left(\\frac{n-1}{n}\\right)\\sum_{i=1}^{n}(\\hat{se}_{boot(i)}-\\hat{se}_{boot(.)})^{2}} \\] Notice that because there are always some bootstrap samples that do not include i, you do not actually have to do any extra computation to do jackknife-after-bootstrap, but the precise details of using the bootstrap samples you already have are a bit complicated. In R, this can be done using the ‘jack.after.boot’ function in the package “boot”. Discuss: Why are hypothesis testing and parameter estimation two sides of the same coin? 3.10 By the end of Week 2, you should understand… the 6 steps of Null Hypothesis Significance Testing here the interpretation of a p-value (hint: it is not the probability that the null hypothesis is correct) the difference between a one-tailed and two-tailed test how to use permutation-based methods to test a statistical hypothesis the relationship between testing a hypothesis and estimating an effect size the definition of an estimator how to use bootstrapping (both parametric and non-parametric) to estimate a test statistic and calculate the bias and standard error of your estimate when to use parametric bootstrap vs. non-parametric bootstrap (hint: rarely) how to use jackknife to estimate bias and standard errors of an estimate "],["week-2-lab.html", "4 Week 2 Lab 4.1 Confidence intervals 4.2 Testing hypotheses through permutation 4.3 Basics of bootstrap and jackknife 4.4 Calculating bias and standard error 4.5 Parametric bootstrap", " 4 Week 2 Lab 4.1 Confidence intervals Before getting too far, we need to formally define a confidence interval. A 95th percentile confidence interval say “If I repeat this procedure 100 times using 100 different datasets, 95% of the time my confidence intervals will capture the true parameter”. It does NOT say that there is a 95% chance that the parameter is in the interval. Quiz time! (Don’t worry, not a real quiz) Important note: This is an area where Aho is WRONG. Aho is correct on only one point. It is true that once the 95th CI has been constructed, it is no longer possible to assign a \\(\\%\\) to the probability that that CI contains the true value or not. Because that CI, once created, either DOES or DOES NOT contain the true value. However, we often talk about the interval in the abstract. When we say “There is a 95\\(\\%\\) chance that the interval contains the true value” what we mean is that there is a 95\\(\\%\\) probability that a CI created using that methodology would contain the true value. Do not let Week 2 pass by without fundamentally understanding the interpretation of a confidence interval. So now we know the general idea behind confidence intervals but we don’t yet know how to calculate them. To do that, we’ll actually walk through an example of bootstrap using pennies. Each of you should have gathered the ages of five pennies. (If a penny was made in 2021, that penny would be 1 year old, etc.) Data: 5 pennies that the students have Test statistic: Median Lets say we are trying to find the median age of all pennies in circulation. We can’t figure this out exactly, because we can’t collect all the pennies in circulation, but we each have a sample of 5 pennies. The median age of the pennies in our sample is a reasonable estimate for the median age of all pennies in circulation. What is our uncertainty about that number? How far might our estimate of the median age be from the true median age? In this case, we don’t know the underlying distribution of penny ages. (Let’s brainstorm this for a bit. Do we have any guesses what this distribution might look like? What might be a reasonable distribution to describe the shape of penny age?) Let’s use bootstrapped samples to calculate the s.e. associated with that estimate. Procedure: 1. Sample WITH REPLACEMENT a group of 5 pennies. (To sample with replacement you will have to sample one penny, write down the age, and repeat that 5 times.) 2. Calculate the median age from that sample of pennies. 3. Repeat Do this a few times with your actual physical pennies, and then once you get the idea, you can make a vector in R of your penny ages and use R to speed up the sampling. Don’t forget to sample with replacement. Gather a collection of 100 median estimates, each one calculated using a different bootstrapped dataset. Calculate the Bias and the Variance of the estimator for the Median. We now want to take this one step further and estimate the confidence intervals for the median age of a penny in circulation. We actually have two primary mechanisms for generating confidence intervals for the statistic. Method #1: Simply take the quantiles directly from the distribution of \\(\\hat{\\theta}^{*}\\): \\[ \\theta_{LL} = \\mbox{2.5th percentile of } \\hat{\\theta}^{*} \\] \\[ \\theta_{UL} = \\mbox{97.5th percentile of } \\hat{\\theta}^{*} \\] Notice that (by construction) 95\\(%\\) of the \\(\\hat{\\theta}^{*}\\) values fall in the interval \\((\\theta_{LL},\\theta_{UL})\\). This is the very definition of the 95th percentile confidence interval. OR Method #2: We can use the Normal approximation: We have a second method that won’t make 100% sense until next week, but it turns out that if we assume the bootstrapped estimates follow a Normal distribution, \\[ \\hat{\\theta^{*}} \\sim N(\\hat{\\theta},\\hat{se}^{2}) \\] we can use the fact that the 95th percentile confidence interval is approximately given by: \\[ \\hat{\\theta}_{LL}=\\hat{\\theta}-1.96*\\hat{se} \\] \\[ \\hat{\\theta}_{UL}=\\hat{\\theta}+1.96*\\hat{se} \\] It turns out that 95\\(\\%\\) of the probability for a Standard Normal distribution lies between (-1.96\\(\\sigma\\),+1.96\\(\\sigma\\)). (We will show this more formally next week.) NB: If you are going to go through the trouble of doing the bootstrap sampling, I don’t know why you would make a Normal approximation at the very end to construct the CIs. I recommend Method #1. Checkpoint #1: Use your penny data to calculate the 95th percentile confidence interval using Method #1 and Method #2. What did you get? 4.2 Testing hypotheses through permutation These examples use data on the speeds of the top 20 racing pigeons from a race in Alma, GA on February 7,2021. Example #1: Use permutation methods to test whether Cock or Hen birds fly at different speeds (speeds are in meters-per-minute) (in other word: \\(H_{0}\\): No difference in speeds between the C and H groups): C=\\(\\{1359.8,1355.3,1355.1,1353.0,1349.8,1348.8,1345.2\\}\\) H=\\(\\{1357.5,1356.4,1355.1,1353.5,1353.2,1352.5,1350.0,1349.8,1346.2,1344.9,1344.4,1343.9,1342.6\\}\\) Checkpoint #2: Is this a one-tailed or a two-tailed test? Make sure that you understand what is being done here, as this example is very closely related to the problem set. Example #2: Using the same data, provide a 95% confidence interval for the difference in mean speed based on 1000 bootstrap samples Note that these two approaches are very closely related. Do you see why either approach can be used to test the null hypothesis? Checkpoint #3: What is the null hypothesis here? Example #3: Now we will do one slightly more complicated example from Phillip Good’s book “Permutation tests: A practical guide to resampling methods and testing hypotheses”: Holmes and Williams (1954) studied tonsil size in children to verify a possible association with the virus . Test for an association between status and tonsil size. (Note that you will need to come up with a reasonable test statistic.) Figure 0.1: Data on tonsil size and S. pyrogenes status. Source: Good (1994) Now lets consider the full dataset, where tonsil size is divided into three categories. How would we do the test now? Checkpoint #4: What is the new test statistic? (There are many options.) What ‘labels’ do you permute? Figure 1.1: Fill dataset on tonsil size and S. pyrogenes status. Source: Good (1994) 4.3 Basics of bootstrap and jackknife To get started with bootstrap and jackknife techniques, we start by working through a very simple example. First we simulate some data x&lt;-seq(0,9,by=1) This will constutute our “data”. Let’s print the result of sampling with replacement to get a sense for it… table(sample(x,size=length(x),replace=T)) ## ## 0 1 2 3 4 5 8 9 ## 1 1 1 1 3 1 1 1 Now we will write a little script to take bootstrap samples and calculate the means of each of these bootstrap samples xmeans&lt;-vector(length=1000) for (i in 1:1000) { xmeans[i]&lt;-mean(sample(x,replace=T)) } The actual number of bootstrapped samples is arbitrary at this point but there are ways of characterizing the precision of the bootstrap (jackknife-after-bootstrap) which might inform the number of bootstrap samples needed. In practice, people tend to pick some arbitrary but large number of bootstrap samples because computers are so fast that it is often easy to draw far more samples than are actually needed. When calculation of the statistic is slow (as might be the case if you are using the samples to construct a phylogeny, for example), then you would need to be more concerned with the number of bootstrap samples. First, lets just look at a histogram of the bootstrapped means and plot the actual sample mean on the histogram for comparison hist(xmeans,breaks=30,col=&quot;pink&quot;) abline(v=mean(x),lwd=2) 4.4 Calculating bias and standard error From these we can calculate the bias and standard deviation for the mean (which is the “statistic”): \\[ \\widehat{Bias_{boot}} = \\left(\\frac{1}{k}\\sum^{k}_{i=1}\\theta^{*}_{i}\\right)-\\hat{\\theta} \\] bias.boot&lt;-mean(xmeans)-mean(x) bias.boot ## [1] -0.0314 hist(xmeans,breaks=30,col=&quot;pink&quot;) abline(v=mean(x),lwd=5,col=&quot;black&quot;) abline(v=mean(xmeans),lwd=2,col=&quot;yellow&quot;) \\[ \\widehat{s.e._{boot}} = \\sqrt{\\frac{1}{k-1}\\sum^{k}_{i=1}(\\theta^{*}_{i}-\\bar{\\theta^{*}})^{2}} \\] se.boot&lt;-sd(xmeans) We can find the confidence intervals in two ways: Method #1: Assume the bootstrap statistics are normally distributed LL.boot&lt;-mean(xmeans)-1.96*se.boot #where did 1.96 come from? UL.boot&lt;-mean(xmeans)+1.96*se.boot LL.boot ## [1] 2.660374 UL.boot ## [1] 6.276826 Method #2: Simply take the quantiles of the bootstrap statistics quantile(xmeans,c(0.025,0.975)) ## 2.5% 97.5% ## 2.7000 6.3025 Let’s compare this to what we would have gotten if we had used normal distribution theory. First we have to calculate the standard error: se.normal&lt;-sqrt(var(x)/length(x)) LL.normal&lt;-mean(x)-qt(0.975,length(x)-1)*se.normal UL.normal&lt;-mean(x)+qt(0.975,length(x)-1)*se.normal LL.normal ## [1] 2.334149 UL.normal ## [1] 6.665851 In this case, the confidence intervals we got from the normal distribution theory are too wide. Checkpoint #6: Does it make sense why the normal distribution theory intervals are too wide? Because the original were were uniformly distributed, the data has higher variance than would be expected and therefore the standard error is higher than would be expected. There are two packages that provide functions for bootstrapping, ‘boot’ and ‘boostrap’. We will start by using the ‘bootstrap’ package, which was originally designed for Efron and Tibshirani’s monograph on the bootstrap. To test the main functionality of the ‘bootstrap’ package, we will use the data we already have. The ‘bootstrap’ function requires the input of a user-defined function to calculate the statistic of interest. Here I will write a function that calculates the mean of the input values. library(bootstrap) theta&lt;-function(x) { mean(x) } results&lt;-bootstrap(x=x,nboot=1000,theta=theta) results ## $thetastar ## [1] 3.9 3.9 4.3 4.1 2.8 5.8 4.1 3.5 5.1 4.1 5.2 4.6 3.5 3.6 5.5 4.7 3.1 4.9 ## [19] 4.7 3.8 3.7 3.9 3.1 5.3 4.3 3.7 3.7 3.9 4.2 4.6 5.5 4.1 2.5 5.4 4.9 6.3 ## [37] 3.4 4.5 3.8 4.0 4.6 5.0 4.1 3.1 3.1 3.5 4.6 3.5 4.9 3.6 4.2 3.3 5.3 6.0 ## [55] 4.7 5.8 4.9 4.3 4.7 4.0 3.7 3.1 3.8 4.5 4.3 5.1 5.1 5.8 2.6 4.5 4.4 5.5 ## [73] 4.0 5.0 5.0 4.8 5.7 5.2 3.9 3.1 6.1 4.8 5.4 4.5 4.2 2.7 4.0 6.0 5.4 4.7 ## [91] 4.2 4.2 7.6 4.2 3.3 3.8 5.6 4.4 5.1 3.7 3.4 5.3 2.6 5.8 4.4 2.9 4.7 5.2 ## [109] 4.7 4.0 3.1 4.7 2.6 4.2 4.1 4.6 4.5 4.1 4.3 5.6 4.3 3.9 4.1 4.6 5.5 4.2 ## [127] 4.2 5.3 5.4 4.0 5.5 6.3 3.9 3.8 5.1 2.7 4.1 5.3 4.5 3.5 4.9 3.3 5.2 4.7 ## [145] 3.9 4.0 6.2 2.7 4.1 4.3 2.9 6.6 4.0 3.7 3.9 5.4 4.5 5.4 5.1 4.5 4.2 4.7 ## [163] 3.3 5.0 3.7 5.1 6.1 5.2 4.9 6.3 4.9 4.2 4.6 4.1 3.5 5.7 3.7 3.7 7.0 4.8 ## [181] 6.1 5.2 2.0 4.4 4.1 4.6 3.1 5.6 5.5 4.8 4.2 3.1 2.5 4.7 4.5 3.2 4.5 3.6 ## [199] 4.3 3.9 3.7 4.4 5.1 4.0 4.9 4.6 4.3 3.5 4.8 4.2 3.4 3.6 5.4 4.7 4.2 4.0 ## [217] 4.7 7.0 3.1 5.4 4.0 4.6 5.4 4.4 5.2 4.0 5.0 5.5 5.0 3.5 5.4 3.9 1.8 5.3 ## [235] 4.3 4.4 5.2 4.8 5.5 5.8 4.2 4.2 4.6 7.6 4.9 5.9 4.3 5.3 5.4 5.8 3.2 6.1 ## [253] 4.3 3.2 3.6 4.3 4.6 4.0 5.0 5.8 5.1 3.8 4.9 5.4 3.6 3.4 2.5 5.1 4.2 4.6 ## [271] 5.1 4.5 3.8 2.7 4.2 4.8 4.3 3.0 2.1 3.4 4.9 3.7 4.8 3.2 4.1 4.9 3.9 4.3 ## [289] 5.3 5.5 4.9 4.8 6.0 4.7 3.4 6.6 4.9 4.3 5.0 4.0 4.1 5.5 5.0 5.3 3.9 5.0 ## [307] 3.0 3.6 4.7 4.9 3.3 2.9 2.9 4.6 4.2 4.2 6.1 6.1 4.0 5.1 4.3 5.5 4.4 5.7 ## [325] 4.4 5.1 4.7 4.7 2.8 3.3 4.3 3.3 4.2 3.8 5.3 5.5 2.9 3.1 4.2 3.5 4.3 5.2 ## [343] 2.7 2.0 5.5 3.7 4.6 5.3 4.8 3.2 4.8 5.0 3.5 5.2 4.0 5.2 4.5 3.9 3.8 4.5 ## [361] 4.6 3.3 5.6 4.2 4.2 4.6 3.2 2.9 4.9 6.4 4.6 5.3 5.1 4.8 3.6 2.8 5.2 5.0 ## [379] 6.2 2.8 7.1 4.2 3.7 4.7 3.8 4.2 5.1 6.2 3.3 4.7 4.5 4.9 4.2 3.8 5.6 5.7 ## [397] 3.2 3.6 3.6 4.4 3.8 4.6 3.3 3.2 4.9 4.8 5.6 3.7 4.6 5.9 3.7 4.5 3.9 3.2 ## [415] 3.8 5.5 5.5 3.7 4.0 4.0 3.8 4.7 3.9 4.4 2.9 4.2 5.1 4.1 3.4 4.9 4.5 5.3 ## [433] 3.7 5.7 3.9 4.2 4.3 3.3 4.6 3.7 4.0 3.9 4.3 4.3 5.9 3.8 4.8 5.2 3.7 3.9 ## [451] 3.0 5.0 4.0 4.3 6.0 4.6 4.4 3.8 3.9 5.6 3.5 4.8 5.0 3.9 6.4 2.2 2.8 4.4 ## [469] 3.1 3.3 5.0 5.5 4.2 2.6 3.5 3.1 5.7 4.6 3.8 6.2 4.3 3.4 4.6 4.4 3.3 4.9 ## [487] 3.6 4.6 2.9 5.1 4.8 3.2 5.2 2.9 4.2 3.6 4.7 4.4 3.7 4.8 4.1 6.5 3.9 5.6 ## [505] 3.6 4.1 4.8 5.2 6.4 4.8 4.8 5.5 3.5 4.7 4.5 4.5 4.9 3.0 3.5 4.2 4.0 5.1 ## [523] 4.5 3.9 4.5 4.8 5.9 2.5 4.8 4.2 5.9 6.1 4.0 5.9 5.0 5.0 4.1 4.9 5.8 5.7 ## [541] 4.7 4.3 5.5 5.0 6.2 4.2 6.9 4.0 4.9 6.0 4.5 3.0 4.4 4.8 4.4 4.2 4.0 4.6 ## [559] 5.0 5.1 5.6 4.4 3.3 4.0 3.6 5.9 6.4 5.7 4.0 5.7 4.8 4.8 4.9 3.9 4.0 4.9 ## [577] 2.4 3.8 4.6 5.5 4.8 5.0 5.0 2.9 3.8 3.6 4.1 5.5 3.3 5.7 4.0 4.9 4.1 5.0 ## [595] 3.4 4.6 4.5 3.4 3.7 4.0 5.6 5.2 4.4 4.7 4.2 4.5 4.8 4.8 3.9 3.2 6.1 4.6 ## [613] 3.9 5.8 4.9 5.2 3.9 3.1 3.8 5.8 4.8 3.8 5.5 5.7 5.2 4.6 4.9 4.3 4.6 5.3 ## [631] 2.9 4.0 5.0 3.9 3.6 5.1 4.4 4.8 4.1 4.1 5.4 3.6 3.4 2.9 5.5 5.3 3.9 2.6 ## [649] 6.0 6.2 3.0 4.6 4.1 5.6 3.4 3.8 4.3 3.8 4.4 4.8 5.2 4.6 3.3 5.2 5.5 4.8 ## [667] 4.4 4.1 5.3 5.7 5.5 4.1 4.3 4.3 4.7 3.8 4.5 4.0 4.0 4.3 3.5 3.2 3.2 6.0 ## [685] 5.3 4.6 4.1 5.2 6.1 5.1 5.2 4.5 4.2 4.2 3.6 3.2 4.0 3.1 3.3 4.3 4.3 5.6 ## [703] 5.9 5.0 4.2 4.3 5.4 4.7 3.8 5.2 3.8 6.4 3.0 4.3 3.7 4.7 7.3 3.8 3.9 4.7 ## [721] 4.4 5.2 5.3 5.4 3.8 4.0 3.1 4.5 3.4 4.0 3.2 3.6 3.7 4.7 4.4 4.9 2.8 5.0 ## [739] 4.8 4.6 3.7 5.3 4.9 3.3 5.4 4.7 5.3 4.5 4.0 5.7 3.5 5.1 6.7 4.1 4.9 4.2 ## [757] 4.0 5.1 3.9 5.7 4.2 5.4 4.2 4.8 3.2 6.5 3.3 5.4 3.4 4.5 4.4 3.8 4.1 3.1 ## [775] 4.5 3.3 3.3 5.0 5.1 4.8 4.3 4.8 5.2 5.2 4.6 4.2 4.4 4.8 5.3 3.1 6.8 4.9 ## [793] 4.6 3.5 4.4 5.9 4.5 4.8 4.8 3.4 4.4 4.3 5.1 5.7 6.1 4.2 2.7 3.3 3.9 4.6 ## [811] 4.0 3.9 3.0 5.4 5.8 3.6 5.2 4.4 5.1 5.3 5.2 4.7 4.4 4.2 4.7 4.3 5.0 3.5 ## [829] 3.6 2.7 5.0 5.4 5.3 3.3 4.8 3.6 6.2 4.5 3.9 4.4 3.9 3.2 3.8 3.0 6.4 5.1 ## [847] 4.0 3.4 5.5 5.8 4.4 4.2 4.7 3.7 4.3 4.3 4.3 4.5 3.7 6.6 4.9 2.8 3.5 5.9 ## [865] 5.2 4.4 3.4 3.9 4.0 3.9 2.3 4.1 5.3 3.1 4.9 3.8 4.7 5.6 4.3 5.4 4.2 3.8 ## [883] 4.2 4.5 4.1 4.8 4.3 5.7 3.4 4.8 4.1 3.9 5.9 4.3 6.0 4.1 1.5 3.3 5.1 4.4 ## [901] 4.9 4.7 4.5 4.5 4.1 3.7 4.7 5.0 5.6 3.2 4.2 5.0 4.5 3.8 3.2 3.8 3.8 6.7 ## [919] 5.6 3.7 4.6 4.2 3.5 3.6 5.6 5.7 3.4 4.2 4.4 5.2 4.8 5.5 4.2 4.3 3.9 4.4 ## [937] 2.9 4.5 5.3 2.2 4.7 3.9 4.0 4.7 5.2 6.0 5.4 4.3 5.2 5.3 4.3 4.5 3.9 3.9 ## [955] 4.7 4.9 2.6 2.9 4.0 4.8 6.4 6.1 5.6 6.0 3.9 4.3 4.2 5.3 4.4 5.1 4.9 4.1 ## [973] 5.2 4.7 5.5 3.0 5.0 3.7 4.1 6.5 4.5 5.4 4.0 5.6 5.6 4.0 3.2 4.6 5.1 3.2 ## [991] 4.6 4.2 4.1 4.2 4.0 6.5 5.2 3.2 3.1 2.7 ## ## $func.thetastar ## NULL ## ## $jack.boot.val ## NULL ## ## $jack.boot.se ## NULL ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta) quantile(results$thetastar,c(0.025,0.975)) ## 2.5% 97.5% ## 2.7 6.3 Notice that we get exactly what we got last time. This illustrates an important point, which is that the bootstrap functions are often no easier to use than something you could write yourself. You can also define a function of the bootstrapped statistics (we have been calling this theta) to pull out immediately any summary statistics you are interested in from the bootstrapped thetas. Here I will write a function that calculates the bias of my estimate of the mean (which is 4.5 [i.e. the mean of the number 0,1,2,3,4,5,6,7,8,9]) bias&lt;-function(x) { mean(x)-4.5 } results&lt;-bootstrap(x=x,nboot=1000,theta=theta,func=bias) results ## $thetastar ## [1] 3.6 4.8 3.9 3.9 3.2 5.5 4.9 4.2 3.3 3.5 5.4 4.0 5.0 4.7 3.9 4.9 4.5 6.7 ## [19] 5.6 3.2 5.8 6.5 3.4 5.4 4.9 4.5 4.6 4.3 5.7 2.8 3.4 4.5 5.4 4.0 4.5 4.9 ## [37] 3.1 3.6 5.1 3.2 4.2 4.1 5.0 4.0 3.9 5.2 4.2 4.3 3.9 4.4 4.2 6.4 3.4 5.3 ## [55] 4.6 3.1 3.4 4.8 5.3 4.0 3.5 4.6 4.7 5.7 4.5 4.4 2.7 5.0 4.7 4.9 5.2 4.6 ## [73] 5.7 4.0 5.0 3.6 5.8 3.9 4.9 5.9 6.2 4.8 4.7 5.1 5.1 5.7 4.3 4.0 5.2 3.9 ## [91] 6.2 5.1 3.8 3.2 3.5 4.3 4.7 6.5 3.6 3.9 4.8 4.3 4.4 5.9 4.1 5.2 4.5 4.7 ## [109] 3.9 3.2 4.8 5.4 3.6 5.1 4.1 3.3 3.7 5.8 4.5 4.4 5.9 4.6 4.2 4.1 3.1 5.0 ## [127] 4.4 3.5 5.4 3.7 4.5 3.9 4.7 6.2 4.5 4.3 5.4 4.6 3.9 4.6 3.6 2.6 5.5 5.6 ## [145] 5.3 3.6 4.2 6.6 4.1 5.0 4.2 3.4 2.3 3.9 3.9 4.0 4.2 4.0 5.4 5.0 2.9 4.5 ## [163] 3.5 5.9 3.7 5.5 5.1 5.5 5.9 4.5 4.6 5.1 5.0 4.2 5.0 4.3 4.1 4.4 5.4 5.6 ## [181] 5.9 4.8 6.4 4.8 6.7 4.2 3.9 6.2 5.1 4.4 3.6 3.4 3.1 3.9 4.4 5.2 5.4 3.8 ## [199] 5.7 3.8 5.1 4.8 3.9 3.5 3.5 4.5 4.3 4.8 5.8 3.4 3.9 3.2 5.8 2.5 5.3 4.4 ## [217] 4.8 5.5 5.2 4.6 4.9 4.7 4.5 4.5 4.5 5.6 3.9 5.5 3.0 3.3 4.1 4.6 6.2 4.8 ## [235] 4.5 5.1 4.8 3.7 3.7 3.7 4.3 4.7 4.5 4.0 3.6 5.1 4.7 6.5 3.4 3.6 4.3 3.5 ## [253] 5.2 2.7 4.2 4.7 5.9 3.9 5.6 5.2 3.8 3.9 3.5 4.7 4.2 4.4 4.7 4.7 3.0 5.5 ## [271] 4.7 3.6 3.7 6.4 6.1 3.4 3.4 3.0 4.0 3.8 3.3 4.4 4.2 4.3 5.7 4.7 4.6 4.0 ## [289] 5.3 5.4 4.6 4.7 5.9 5.4 4.4 4.1 3.6 4.7 3.0 4.8 4.1 3.3 7.3 5.1 3.4 4.8 ## [307] 5.1 3.6 2.8 4.2 3.8 3.7 5.4 4.0 4.9 5.3 3.4 3.9 4.4 3.7 4.3 5.2 4.8 4.6 ## [325] 4.7 4.9 3.9 4.5 5.7 3.9 3.6 4.6 5.6 5.1 4.9 3.6 3.8 4.2 6.3 4.2 5.9 5.3 ## [343] 4.3 3.6 2.9 3.7 3.2 5.2 4.3 4.8 5.0 4.2 5.1 4.0 4.9 5.2 5.4 4.2 3.3 4.1 ## [361] 3.9 5.1 5.6 6.1 3.3 4.2 5.5 4.8 4.8 4.5 5.1 4.2 4.3 3.7 3.7 5.6 4.6 5.4 ## [379] 5.5 4.9 4.3 3.9 3.2 5.2 5.1 3.8 6.0 5.1 4.9 4.9 3.5 4.9 4.5 2.2 4.7 3.2 ## [397] 4.2 5.3 5.1 5.0 3.6 4.2 4.4 5.4 5.7 4.5 5.6 4.5 4.4 4.6 6.4 6.0 4.0 5.1 ## [415] 4.9 4.1 4.5 4.5 4.9 4.3 4.4 5.8 3.4 2.5 4.6 4.5 6.5 4.4 5.1 3.3 4.3 4.6 ## [433] 4.5 4.4 5.9 4.4 3.6 3.0 4.4 5.2 3.6 4.4 3.6 5.5 2.1 3.2 3.4 3.4 5.2 5.2 ## [451] 3.9 4.8 3.3 5.7 4.5 4.1 5.6 3.6 4.7 4.5 4.2 3.5 5.1 5.0 3.8 5.5 4.6 5.8 ## [469] 5.9 4.1 4.4 5.1 4.8 4.1 4.4 4.4 3.7 3.5 5.0 3.5 4.9 5.7 4.4 3.8 3.8 4.9 ## [487] 4.6 3.8 4.2 6.0 2.6 4.6 4.6 5.6 4.3 3.1 3.3 4.6 2.6 4.7 3.6 3.0 3.6 4.5 ## [505] 3.8 2.9 4.8 3.9 5.9 4.9 4.2 4.9 3.9 5.6 4.5 3.6 4.2 3.6 4.6 2.7 5.5 3.9 ## [523] 4.7 4.3 2.9 2.8 5.0 5.7 4.1 5.1 3.9 3.4 5.1 4.3 3.8 3.6 3.6 5.6 6.1 2.8 ## [541] 6.2 4.9 5.9 4.6 3.1 5.8 4.4 3.7 5.1 3.6 4.9 3.4 2.2 6.0 5.6 5.9 5.6 5.0 ## [559] 4.5 4.2 5.1 5.6 4.9 4.9 5.1 3.3 3.2 3.2 4.2 5.2 3.5 4.5 5.3 5.6 3.1 3.3 ## [577] 5.4 5.9 5.2 5.6 4.8 5.4 4.7 5.7 6.2 4.3 4.9 5.0 4.5 4.7 4.4 3.3 4.4 3.8 ## [595] 5.0 5.4 5.4 4.9 4.3 3.5 5.2 3.7 4.8 5.0 4.0 4.1 5.1 4.2 4.8 3.8 4.2 3.5 ## [613] 3.8 4.5 4.0 4.6 6.9 4.7 3.3 3.9 5.3 3.6 5.4 3.8 3.7 3.8 3.1 4.7 4.9 5.6 ## [631] 2.6 5.4 3.8 5.3 4.9 4.7 5.2 5.9 6.4 4.3 4.3 2.2 4.5 4.9 4.7 5.7 3.9 5.0 ## [649] 4.8 6.0 4.8 5.3 4.3 3.4 4.4 3.1 4.6 2.4 4.9 5.2 4.6 5.9 3.7 4.6 3.7 3.6 ## [667] 3.9 5.1 2.4 4.9 4.2 4.6 6.8 4.0 4.5 3.4 4.8 4.3 6.1 2.9 5.0 4.1 4.1 4.6 ## [685] 4.8 4.6 2.6 4.0 5.0 4.3 4.6 4.1 6.0 4.1 5.5 3.6 5.2 4.1 4.1 4.2 2.8 3.5 ## [703] 4.2 4.4 2.9 4.4 5.1 4.7 5.0 4.6 3.7 4.2 3.4 4.1 3.2 5.0 3.7 4.5 3.9 5.5 ## [721] 5.0 4.4 4.5 3.0 4.0 5.2 3.8 3.2 3.8 4.2 3.7 2.7 4.1 4.9 4.8 3.8 5.3 5.5 ## [739] 4.9 5.1 3.7 5.0 3.2 4.4 4.2 3.4 4.4 5.9 3.6 5.0 5.0 4.4 4.4 4.3 5.1 4.8 ## [757] 4.9 5.2 5.0 4.0 5.1 4.4 4.5 2.7 5.9 3.9 4.7 5.1 4.2 5.1 4.7 4.2 4.1 4.4 ## [775] 4.8 4.4 5.0 4.2 5.1 3.9 5.2 6.1 3.8 4.2 5.7 4.8 3.1 6.3 3.0 4.8 5.4 5.0 ## [793] 5.3 5.2 4.6 4.0 3.3 5.0 4.2 5.6 5.6 4.5 3.0 5.2 4.7 4.8 3.1 4.4 4.4 6.0 ## [811] 5.5 4.5 4.3 3.7 4.1 4.9 4.8 5.3 4.8 4.6 4.5 4.4 2.3 4.2 4.5 5.8 6.7 3.9 ## [829] 5.0 5.3 5.1 5.8 5.0 5.1 6.2 6.3 5.0 5.6 4.4 4.5 3.7 3.4 3.5 6.5 4.6 5.1 ## [847] 5.3 4.8 4.3 5.2 5.8 3.1 5.0 6.6 4.4 4.9 5.2 2.9 6.1 4.8 4.1 5.0 3.5 4.8 ## [865] 4.4 3.1 3.7 3.8 3.5 4.7 5.5 6.0 4.4 4.3 4.3 4.9 4.7 4.9 4.0 4.3 5.0 3.5 ## [883] 5.1 4.1 3.7 4.7 2.7 3.6 3.9 3.7 5.6 4.2 4.6 5.3 5.3 5.0 5.8 4.4 4.7 5.5 ## [901] 5.2 4.4 4.2 5.5 5.0 5.2 5.2 5.1 3.9 4.6 5.9 5.3 5.2 6.1 2.6 5.2 4.4 5.0 ## [919] 5.0 4.0 4.5 4.1 4.9 3.4 2.1 3.9 5.1 4.9 4.2 4.9 3.9 5.2 4.3 6.4 4.0 4.2 ## [937] 4.9 5.2 5.0 4.6 4.7 4.3 4.8 3.2 5.3 4.2 5.4 4.2 4.3 5.4 3.9 4.1 4.2 3.4 ## [955] 4.3 6.2 5.0 5.8 6.0 4.0 5.3 4.3 4.7 3.5 5.4 4.5 6.2 4.9 3.3 4.0 3.9 4.9 ## [973] 4.8 3.3 4.5 4.9 4.7 2.7 3.4 2.1 4.1 5.3 3.5 3.9 3.9 3.6 1.9 3.2 4.3 4.6 ## [991] 4.9 4.3 3.9 5.6 3.8 6.3 4.7 5.7 4.9 6.1 ## ## $func.thetastar ## [1] 0.0067 ## ## $jack.boot.val ## [1] 0.48534483 0.32922636 0.23501484 0.12005208 0.06184971 -0.03854447 ## [7] -0.09093750 -0.28225352 -0.39371429 -0.43738872 ## ## $jack.boot.se ## [1] 0.8762613 ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta, func = bias) Compare this to ‘bias.boot’ (our result from above). Why might it not be the same? Try running the same section of code several times. See how the value of the bias ($func.thetastar) jumps around? We should not be surprised by this because we can look at the jackknife-after-bootstrap estimate of the standard error of the function (in this case, that function is the bias) and we can see that it is not so small that we wouldn’t expect some variation in these values. Remember, everything we have discussed today are estimates. The statistic as applied to your data will change with new data, as will the standard error, the confidence intervals - everything! All of these values have sampling distributions and are subject to change if you repeated the procedure with new data. Note that we can calculate any function of \\(\\theta^{*}\\). A simple example would be the 72nd percentile: perc72&lt;-function(x) { quantile(x,probs=c(0.72)) } results&lt;-bootstrap(x=x,nboot=1000,theta=theta,func=perc72) results ## $thetastar ## [1] 4.1 4.7 4.1 3.8 2.6 4.9 5.8 5.2 4.5 4.4 4.1 4.7 3.8 3.5 5.8 3.6 3.2 5.5 ## [19] 4.6 4.8 5.5 6.4 5.2 3.9 4.5 3.2 6.0 5.1 3.7 5.5 5.7 4.0 3.9 4.6 4.1 3.4 ## [37] 6.4 4.6 3.3 2.8 2.3 4.8 5.1 3.1 3.8 3.7 4.2 5.0 4.3 3.5 4.6 2.9 3.6 4.6 ## [55] 2.8 4.3 3.7 4.9 5.6 4.8 3.9 4.4 4.1 4.2 5.9 3.6 5.3 4.3 4.2 5.0 5.1 5.3 ## [73] 3.7 4.6 4.2 6.4 5.2 5.4 4.4 4.0 4.6 4.5 6.3 4.3 4.3 5.1 4.1 5.2 5.2 5.0 ## [91] 5.4 5.4 4.1 4.8 1.9 4.0 4.4 3.7 6.6 2.2 3.8 4.7 3.8 6.4 6.8 5.3 2.8 3.3 ## [109] 3.4 3.0 4.6 3.4 4.9 3.7 3.2 4.2 3.7 4.0 2.9 5.1 3.8 4.0 3.0 4.3 6.0 5.3 ## [127] 4.0 4.4 6.4 2.8 4.7 5.6 4.4 5.8 3.3 5.9 4.4 3.8 5.2 3.3 3.8 3.9 3.5 4.3 ## [145] 5.3 4.6 3.5 2.9 4.7 5.8 3.5 5.0 4.8 3.6 5.0 4.3 5.2 6.2 5.4 4.8 4.6 5.1 ## [163] 3.8 4.2 3.4 5.0 5.0 3.7 6.6 3.2 4.0 4.7 2.8 5.3 3.1 3.7 3.1 4.5 6.0 3.7 ## [181] 4.9 4.3 5.2 3.3 4.6 5.0 4.9 4.9 4.0 4.8 4.5 4.1 4.2 4.2 4.2 2.8 5.4 3.8 ## [199] 3.8 3.6 4.5 5.1 3.5 4.3 4.0 3.6 6.2 4.7 5.0 5.1 5.3 3.6 2.8 5.1 5.0 3.0 ## [217] 5.9 2.9 3.9 5.4 2.3 4.7 4.6 4.5 4.1 4.0 4.4 4.5 3.6 4.3 4.9 3.9 4.2 5.4 ## [235] 5.4 3.9 5.7 3.2 5.1 5.8 4.3 4.7 5.1 5.0 4.1 3.8 4.5 4.8 4.6 5.8 3.7 5.2 ## [253] 4.4 4.9 4.9 3.3 6.3 6.7 4.9 5.2 5.0 4.1 4.6 4.6 4.1 4.3 4.2 6.0 4.2 4.6 ## [271] 6.1 2.9 3.0 4.8 4.9 4.7 4.6 4.6 3.3 5.0 5.2 3.5 3.1 4.9 6.0 3.4 5.1 5.0 ## [289] 4.7 6.5 5.5 4.9 4.5 3.6 7.2 3.5 6.5 2.5 4.2 4.3 4.8 5.9 5.4 5.4 4.2 5.0 ## [307] 5.2 4.5 6.2 4.6 4.7 4.6 4.1 4.2 4.3 6.1 4.7 4.4 3.9 4.2 4.4 5.8 4.2 4.8 ## [325] 6.0 5.2 4.5 4.7 3.9 3.8 5.5 4.8 3.6 3.7 5.1 5.1 5.1 5.2 3.3 5.9 4.4 3.9 ## [343] 4.1 3.9 5.1 3.4 5.0 3.4 3.5 5.3 6.4 4.6 3.1 4.7 5.3 3.3 5.2 3.5 4.9 3.0 ## [361] 4.3 4.9 5.3 3.6 4.5 5.9 2.7 6.2 2.8 3.9 5.7 1.6 5.8 4.3 5.1 5.5 5.3 3.7 ## [379] 5.4 4.9 3.4 4.6 4.5 4.2 4.4 4.7 3.4 5.0 4.8 3.2 6.1 3.5 3.4 4.5 5.1 4.7 ## [397] 3.9 4.3 4.6 3.3 4.3 4.2 5.3 5.3 5.4 6.0 5.2 4.5 5.3 4.3 3.8 4.2 3.8 5.0 ## [415] 2.6 5.3 4.8 4.1 4.0 4.3 3.9 4.5 5.5 4.7 4.2 5.0 3.7 4.4 4.9 5.3 4.2 4.2 ## [433] 5.1 3.7 5.0 4.0 3.6 5.6 5.9 6.1 3.1 2.5 4.6 6.1 3.5 4.7 3.8 5.3 5.7 6.2 ## [451] 4.3 1.7 3.3 4.8 4.6 5.4 4.8 3.0 4.2 6.3 3.7 5.6 3.7 5.2 3.9 5.9 5.7 2.6 ## [469] 6.1 3.8 3.5 5.0 2.8 4.6 3.9 5.0 3.3 4.0 5.3 4.2 4.0 3.7 5.0 3.2 4.9 3.8 ## [487] 3.7 5.0 2.8 3.4 4.9 4.2 2.1 4.2 4.0 4.9 5.5 5.2 4.6 5.2 4.9 5.2 5.0 4.6 ## [505] 3.6 3.8 4.5 3.7 5.0 4.0 4.2 3.9 4.3 3.6 5.0 6.7 5.0 4.9 2.4 5.2 3.5 3.0 ## [523] 5.3 5.3 4.4 4.8 4.2 4.0 4.2 2.5 4.7 5.5 4.9 3.2 3.2 4.9 4.0 4.8 4.0 5.7 ## [541] 4.1 4.8 3.6 5.6 4.5 4.2 5.5 3.8 7.2 3.9 4.8 3.5 6.1 4.4 4.5 3.5 3.3 4.0 ## [559] 3.6 3.5 4.0 6.2 4.6 4.8 5.4 5.4 5.0 5.5 3.9 4.2 5.0 6.7 5.2 4.4 6.6 3.6 ## [577] 4.1 6.0 4.3 4.8 5.3 3.9 5.2 4.3 3.9 4.5 4.5 3.4 5.2 4.2 5.6 3.8 5.4 5.6 ## [595] 4.6 4.0 5.1 3.9 5.8 4.7 4.4 4.1 3.6 4.8 3.7 4.3 4.2 6.1 5.7 5.0 5.3 4.0 ## [613] 3.4 4.6 3.7 3.8 4.3 3.5 5.3 4.8 3.9 3.3 3.5 3.9 5.4 5.6 3.4 5.2 5.1 5.3 ## [631] 4.3 4.5 5.1 5.2 3.9 4.2 5.3 5.5 3.1 5.5 4.2 4.8 5.2 3.8 4.2 5.1 5.2 4.5 ## [649] 5.6 5.5 5.4 4.8 3.5 4.7 1.8 4.4 4.6 5.6 4.4 4.2 3.9 3.6 5.4 3.6 4.8 3.7 ## [667] 4.8 4.4 2.4 4.0 4.5 3.9 3.6 5.3 3.3 3.4 4.1 2.3 5.2 5.1 3.4 5.2 4.7 5.0 ## [685] 3.7 5.6 4.8 5.7 5.5 5.9 3.8 4.8 2.6 3.9 4.8 2.6 4.8 4.4 4.3 3.5 4.9 4.7 ## [703] 5.7 5.2 4.5 4.5 5.6 5.2 5.1 4.5 3.6 4.9 5.5 4.5 5.7 3.6 4.1 5.6 3.7 4.6 ## [721] 6.2 5.2 3.2 5.7 3.8 4.9 3.9 3.7 6.0 5.1 4.0 3.6 5.5 3.1 5.3 5.7 3.9 5.2 ## [739] 6.4 4.4 5.1 4.5 5.1 5.0 4.4 3.6 3.6 4.5 3.6 4.3 4.6 4.8 3.3 2.8 4.3 4.1 ## [757] 4.6 5.0 4.8 4.7 4.9 4.5 4.1 3.3 4.5 4.7 3.2 4.1 4.7 5.8 5.9 5.3 4.0 3.5 ## [775] 6.6 5.2 4.2 6.4 4.6 4.1 5.4 5.4 6.1 3.5 6.5 3.7 5.1 6.4 4.2 3.4 5.1 5.0 ## [793] 5.7 4.7 4.0 4.3 4.3 4.9 4.8 4.1 2.9 4.3 3.8 4.1 4.7 4.1 4.9 3.0 4.5 5.9 ## [811] 4.4 3.4 5.9 6.2 5.1 5.2 5.6 5.3 6.3 4.1 3.9 5.5 4.2 5.7 3.7 5.2 4.9 5.7 ## [829] 4.0 3.8 4.9 4.0 3.8 4.9 3.6 3.9 4.4 6.0 3.6 6.3 4.6 4.1 3.4 4.5 5.4 5.7 ## [847] 5.4 4.8 4.4 5.8 3.5 4.5 4.8 2.9 5.8 4.7 3.1 4.6 4.9 4.6 4.9 4.3 6.2 5.0 ## [865] 3.9 5.3 3.8 5.2 3.8 4.8 5.4 4.6 5.0 3.1 5.3 5.5 5.8 5.0 6.0 2.0 3.5 5.0 ## [883] 4.5 4.1 4.4 5.6 4.0 5.5 3.7 6.9 5.0 4.2 4.4 2.3 3.6 3.7 3.3 3.8 5.2 4.0 ## [901] 5.3 3.5 5.6 5.3 4.7 4.2 3.3 6.3 4.9 4.2 3.4 5.5 3.8 4.5 3.4 5.3 3.6 4.0 ## [919] 4.7 5.3 3.7 6.2 4.5 5.4 3.7 3.7 4.8 5.0 3.5 4.6 4.3 5.9 3.9 4.8 3.9 5.7 ## [937] 4.6 3.8 4.5 4.1 4.6 3.4 4.5 3.7 3.2 5.4 5.3 4.1 4.3 4.5 3.9 5.1 5.3 4.7 ## [955] 3.8 4.6 5.6 4.5 4.7 5.3 5.1 5.4 2.5 4.4 5.7 3.7 5.1 2.8 6.5 5.6 2.1 4.6 ## [973] 5.1 3.7 3.3 3.5 5.2 4.2 4.3 5.5 5.0 3.3 4.6 4.5 5.3 5.7 3.9 2.4 4.8 5.0 ## [991] 3.1 3.9 2.9 6.0 4.5 4.0 5.4 6.1 4.8 4.9 ## ## $func.thetastar ## 72% ## 5.1 ## ## $jack.boot.val ## [1] 5.500 5.400 5.308 5.200 5.200 4.900 5.000 4.800 4.600 4.500 ## ## $jack.boot.se ## [1] 0.961975 ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta, func = perc72) On Tuesday we went over an example in which we bootstrapped the correlation coefficient between LSAT scores and GPA. To do that, we sampled pairs of (LSAT,GPA) data with replacement. Here is a little script that would do something like that using (X,Y) data that are independently drawn from the normal distribution xdata&lt;-matrix(rnorm(30),ncol=2) Everyone’s data is going to be different. With such a small sample size, it would be easy to get a positive or negative correlation by random change, but on average across everyone’s datasets, there should be zero correlation because the two columns are drawn independently. n&lt;-15 theta&lt;-function(x,xdata) { cor(xdata[x,1],xdata[x,2]) } results&lt;-bootstrap(x=1:n,nboot=50,theta=theta,xdata=xdata) #NB: xdata is passed to the theta function, not needed for bootstrap function itself Notice the parameters that get passed to the ‘bootstrap’ function are: (1) the indexes which will be sampled with replacement. This is different that the raw data but the end result is the same because both the indices and the raw data get passed to the function ‘theta’ (2) the number of bootrapped samples (in this case 50) (3) the function to calculate the statistic (4) the raw data. Lets look at a histogram of the bootstrapped statistics \\(\\theta^{*}\\) and draw a vertical line for the statistic as applied to the original data. hist(results$thetastar,breaks=30,col=&quot;pink&quot;) abline(v=cor(xdata[,1],xdata[,2]),lwd=2) 4.5 Parametric bootstrap Let’s do one quick example of a parametric bootstrap. We haven’t introduced distributions yet (except for the Gaussian, or Normal, distribution, which is the most familiar), so lets spend a few minutes exploring the Gamma distribution, just so we have it to work with for testing out parametric bootstrap. All we need to know is that the Gamma distribution is a continuous, non-negative distribution that takes two parameters, which we call “shape” and “rate”. Lets plot a few examples just to see what a Gamma distribution looks like. (Note that the Gamma distribution can be parameterized by “shape” and “rate” OR by “shape” and “scale”, where “scale” is just 1/“rate”. R will allow you to use either (shape,rate) or (shape,scale) as long as you specify which you are providing. Let’s generate some fairly sparse data from a Gamma distribution original.data&lt;-rgamma(10,3,5) and calculate the skew of the data using the R function ‘skewness’ from the ‘moments’ package. library(moments) theta&lt;-skewness(original.data) head(theta) ## [1] 0.04114946 What is skew? Skew describes how assymetric a distribution is. A distribution with a positive skew is a distribution that is “slumped over” to the right, with a right tail that is longer than the left tail. Alternatively, a distribution with negative skew has a longer left tail. Here we are just using it for illustration, as a property of a distribution that you may want to estimate using your data. Lets use ‘fitdistr’ to fit a gamma distribution to these data. This function is an extremely handy function that takes in your data, the name of the distribution you are fitting, and some starting values (for the estimation optimizer under the hood), and it will return the parameter values (and their standard errors). We will learn in a couple weeks how R is doing this, but for now we will just use it out of the box. (Because we generated the data, we happen to know that the data are gamma distributed. In general we wouldn’t know that, and we will see in a second that our assumption about the shape of the data really does make a difference.) library(MASS) fit&lt;-fitdistr(original.data,dgamma,list(shape=1,rate=1)) # fit&lt;-fitdistr(original.data,&quot;gamma&quot;) # The second version would also work. fit ## shape rate ## 6.656619 10.895447 ## ( 2.905390) ( 4.939441) Now lets sample with replacement from this new distribution and calculate the skewness at each step: results&lt;-c() for (i in 1:1000) { x.star&lt;-rgamma(length(original.data),shape=fit$estimate[1],rate=fit$estimate[2]) results&lt;-c(results,skewness(x.star)) } head(results) ## [1] 0.1553965 -0.1666818 0.8353146 1.2848476 0.3346622 0.4606029 hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) Now we have the bootstrap distribution for skewness (the \\(\\theta^{*}\\) s), we can compare that to the equivalent non-parametric bootstrap: results2&lt;-bootstrap(x=original.data,nboot=1000,theta=skewness) results2 ## $thetastar ## [1] 1.248644e-01 9.314880e-01 4.073348e-01 2.378928e-01 8.266331e-02 ## [6] -3.533511e-01 5.051411e-01 -3.571094e-01 -1.913844e-01 -3.294101e-01 ## [11] 2.202674e-01 8.974588e-02 1.414782e+00 -2.155514e-01 4.208976e-02 ## [16] -4.862939e-01 5.955147e-01 9.955894e-01 -1.115795e-01 7.085356e-01 ## [21] 3.205781e-01 -2.078646e-01 -4.814372e-01 2.794813e-02 2.045696e-02 ## [26] -1.905275e-01 -4.293409e-01 1.786431e-01 2.265448e-01 4.103153e-01 ## [31] 2.897949e-01 -7.382220e-02 2.394541e-01 -4.170754e-02 -1.193985e-01 ## [36] 1.824685e-01 5.579152e-01 -3.203382e-01 -1.353472e-01 -1.780656e-01 ## [41] 3.642300e-01 3.861753e-01 3.510501e-01 2.572288e-01 3.591180e-01 ## [46] 1.656825e-01 1.814247e-01 4.424430e-01 3.949627e-01 5.445965e-01 ## [51] 4.922059e-01 -2.742142e-01 -2.941932e-01 8.717411e-03 -2.738183e-01 ## [56] -1.928523e-01 -4.923623e-01 -2.167510e-01 6.256877e-01 3.438848e-01 ## [61] 8.774208e-01 1.175291e-01 1.654373e-01 3.992319e-01 1.709403e-01 ## [66] 4.135156e-01 1.888118e-02 2.459348e-01 1.312449e+00 3.579083e-01 ## [71] 8.250533e-01 -7.638472e-01 1.156150e-01 4.770586e-01 8.514371e-01 ## [76] -1.503285e-01 -2.538610e-01 -7.028305e-01 -3.250747e-01 -6.846366e-01 ## [81] 5.122879e-02 -4.065244e-02 -2.094236e-01 3.893420e-01 8.425690e-01 ## [86] 7.921210e-02 -8.269614e-01 4.121731e-01 1.574053e-01 -5.293748e-01 ## [91] -7.222991e-02 -4.306529e-01 4.635685e-01 3.472950e-01 -1.658945e-01 ## [96] -8.081375e-02 -2.309428e-01 1.581909e-02 1.082174e-01 5.780280e-01 ## [101] 5.038373e-02 4.170105e-02 -5.084781e-01 2.518883e-01 5.585445e-01 ## [106] 5.566665e-01 -1.521346e-02 2.413007e-02 1.452438e-01 1.964994e-01 ## [111] 3.732075e-01 3.145648e-01 2.961068e-02 -8.399738e-01 3.346222e-01 ## [116] -3.267885e-01 6.074581e-01 3.547913e-01 -5.936889e-01 2.040338e-01 ## [121] 1.052222e-01 -9.678901e-02 -4.091247e-01 4.609701e-01 -1.831913e-01 ## [126] 1.580043e-02 5.987812e-03 -2.775563e-01 -3.114722e-02 1.890425e-01 ## [131] -1.759072e-03 1.870425e-01 -3.787889e-01 5.541130e-01 1.162460e-01 ## [136] -6.887075e-01 2.559066e-01 4.763079e-01 3.162122e-02 5.294080e-01 ## [141] 3.166246e-01 -2.220326e-01 -4.521537e-01 3.450133e-01 -3.099922e-02 ## [146] 3.191262e-01 -1.230909e-01 1.140408e-01 4.269207e-01 -3.388864e-02 ## [151] 1.584520e-01 -2.364627e-01 3.495156e-02 -3.177141e-01 -2.106420e-01 ## [156] -2.839368e-01 1.309849e-01 3.088261e-01 -9.782363e-02 -9.718164e-01 ## [161] -4.030697e-01 -2.414182e-01 1.309157e-01 -2.237797e-01 5.263765e-02 ## [166] -1.497970e-01 -3.839083e-01 6.157136e-02 7.726468e-01 1.545159e-01 ## [171] 3.035914e-01 -6.401525e-01 1.086757e+00 -2.374413e-01 -5.022676e-01 ## [176] 9.660528e-02 2.879287e-01 4.264407e-01 8.432167e-01 -1.983929e-01 ## [181] 1.146590e-01 1.184714e-01 -1.843569e-01 -2.727618e-01 6.087651e-01 ## [186] -2.219773e-01 2.746503e-01 6.534690e-01 -2.934281e-01 -1.497970e-01 ## [191] -6.446975e-01 -1.109201e-01 7.141352e-01 -4.421196e-01 4.001989e-01 ## [196] -2.964118e-01 4.321816e-01 2.183214e-01 -3.723561e-01 9.581893e-01 ## [201] 1.010467e-01 -8.799866e-01 -2.971426e-01 1.508439e-01 6.475756e-01 ## [206] 6.727600e-01 2.695367e-01 -3.425819e-02 -2.062653e-01 -5.302281e-01 ## [211] 2.455836e-01 2.996791e-01 1.843802e-01 -5.263403e-02 6.247957e-02 ## [216] 7.491237e-02 6.020301e-01 -4.081955e-01 1.156506e-01 -5.275556e-01 ## [221] -2.447835e-01 -1.552401e-01 2.616693e-03 -2.320397e-01 -8.605298e-02 ## [226] -9.390373e-02 -4.065388e-01 3.062912e-01 2.834600e-01 2.364160e-01 ## [231] 9.518717e-03 -3.424963e-01 -1.645669e-01 -2.467611e-01 8.715084e-02 ## [236] 3.485269e-01 -1.152603e-01 1.326714e-01 4.983940e-01 -1.017600e+00 ## [241] -2.664269e-01 4.867535e-01 1.529716e-01 6.056231e-02 -5.716596e-01 ## [246] -1.859530e-01 -2.662146e-01 6.577878e-01 -1.913514e-01 3.151215e-01 ## [251] -6.441208e-02 -5.008276e-01 3.901033e-01 2.701210e-01 -4.511517e-01 ## [256] 8.369751e-01 -2.182860e-02 -1.098572e-01 -5.640160e-01 -3.692367e-01 ## [261] -1.083904e-01 4.918764e-01 5.657681e-01 1.422781e+00 2.059368e-01 ## [266] 2.464526e-01 -6.069979e-02 7.352476e-01 7.573062e-02 -1.837349e-01 ## [271] 2.717695e-01 3.973784e-01 5.156218e-01 6.669092e-01 1.308618e-01 ## [276] 8.057089e-02 -6.413022e-01 8.309105e-01 -3.657907e-02 -2.358106e-01 ## [281] -2.016347e-01 -4.248948e-02 2.843475e-01 1.496299e-01 -3.602500e-01 ## [286] 8.239766e-02 2.082902e-01 -2.188489e-01 -3.042078e-01 -1.431625e-01 ## [291] -1.434201e-01 3.088261e-01 5.629421e-01 -1.886604e-01 6.368222e-02 ## [296] -1.018080e+00 1.212746e-01 3.838362e-01 1.781905e-01 -1.170547e-01 ## [301] 2.554187e-01 3.625323e-02 -2.162986e-01 -1.701998e-01 3.374118e-01 ## [306] -1.790809e-01 9.155555e-01 -1.396869e-01 5.892840e-02 -4.453472e-01 ## [311] 1.309843e-01 -8.117614e-02 8.504970e-02 1.653034e-01 1.015547e-01 ## [316] -1.987246e-01 -5.693203e-03 6.754464e-02 8.239480e-01 9.548867e-01 ## [321] 3.240652e-01 1.797978e-01 -3.402353e-01 -6.066168e-02 6.450630e-01 ## [326] 1.163647e-01 -3.291981e-01 6.339764e-01 -4.838957e-01 -3.034210e-02 ## [331] -2.533818e-01 1.582971e-01 2.724611e-01 1.704497e+00 3.763495e-01 ## [336] 4.868161e-02 2.558870e-01 -1.206402e-02 3.897220e-01 2.721395e-02 ## [341] 2.683224e-01 -1.422803e-01 -2.415594e-01 9.443097e-01 -9.898072e-02 ## [346] 4.972773e-01 4.948946e-01 3.966577e-02 -1.187744e-01 8.077573e-02 ## [351] 6.710371e-01 -4.774707e-01 4.797046e-02 -1.876154e-01 -3.163537e-01 ## [356] -1.973102e-01 4.847050e-01 6.911744e-01 -3.986466e-01 3.740613e-02 ## [361] 5.637046e-01 -1.075719e-02 -1.152653e-01 4.842758e-01 -3.989131e-01 ## [366] 5.091892e-02 -8.621422e-03 5.364278e-01 5.144800e-01 -2.279375e-01 ## [371] -3.486371e-01 -1.422803e-01 -1.154849e-01 3.424102e-01 2.457238e-01 ## [376] 1.738421e-02 -4.035489e-01 2.506348e-01 7.464224e-01 3.194035e-01 ## [381] 1.467205e-01 6.735876e-01 -1.189970e-01 -7.443386e-02 -1.160767e-01 ## [386] -5.388494e-01 5.252815e-01 -4.056910e-01 -3.401969e-01 6.930169e-01 ## [391] 1.268772e-01 5.562805e-01 3.896700e-01 4.206781e-01 -4.199309e-01 ## [396] -8.103005e-01 -3.949297e-01 6.904120e-01 -6.390776e-02 -2.372341e-02 ## [401] 2.841102e-01 1.448630e-01 -4.420579e-01 2.721395e-02 6.227130e-03 ## [406] -2.035279e-01 6.710551e-01 2.030492e-01 2.578370e-01 -3.190975e-02 ## [411] -1.400007e-01 6.976915e-01 4.179956e-01 -1.559013e-01 5.961827e-01 ## [416] -1.419932e-01 3.887220e-01 9.473812e-02 3.414052e-01 1.703224e-01 ## [421] -1.097610e-01 3.858096e-01 -8.945620e-01 3.237908e-01 -7.696088e-01 ## [426] -4.281654e-01 -6.456887e-01 -4.443648e-01 1.797966e-01 -2.954959e-01 ## [431] -6.978314e-02 -1.451705e-01 4.970269e-01 -8.667838e-01 -1.527576e-01 ## [436] -2.604573e-01 -2.027552e-01 4.087747e-05 -1.363681e-01 3.181191e-01 ## [441] 6.016724e-01 -1.816238e-01 2.502152e-02 -2.483552e-01 -3.151872e-01 ## [446] 5.861081e-01 1.139262e+00 1.042715e+00 2.387030e-01 -2.732968e-02 ## [451] 7.241888e-01 7.048301e-01 -6.271520e-01 4.603385e-01 4.731864e-01 ## [456] -1.174834e-01 1.086313e-01 -2.966985e-01 2.119484e-01 -2.815533e-01 ## [461] 4.104239e-01 -1.281513e-02 1.153444e-01 -3.403658e-01 -2.751879e-01 ## [466] -4.979693e-01 1.017588e-01 -3.034210e-02 -6.090338e-01 5.617861e-01 ## [471] 3.924255e-01 -9.751015e-02 2.000377e-02 2.715883e-01 3.904577e-01 ## [476] 5.124111e-01 3.010978e-01 -3.018022e-02 -3.164824e-01 1.929243e-02 ## [481] 9.201919e-02 6.172151e-02 1.310875e-01 -3.011664e-01 7.213053e-01 ## [486] -3.856500e-01 8.285179e-01 -1.961384e-02 -2.336423e-01 -4.771875e-01 ## [491] 1.065887e-02 -5.892143e-01 3.352533e-01 -3.083062e-01 9.740582e-01 ## [496] 3.453765e-01 7.692003e-01 1.093766e+00 1.275486e+00 1.186484e-02 ## [501] -3.486705e-01 -5.628846e-01 3.180200e-02 4.265278e-01 4.574450e-01 ## [506] 1.043782e+00 -1.239138e-01 5.004795e-02 3.880515e-01 2.458727e-01 ## [511] 6.247957e-02 -7.659328e-01 2.417840e-02 -4.094927e-01 2.553775e-01 ## [516] -6.643443e-01 -5.200480e-01 9.598052e-03 1.067281e-01 1.075926e-01 ## [521] 2.363310e-01 -1.857210e-01 -1.062320e-01 -1.079995e+00 4.104206e-01 ## [526] -5.063296e-01 -2.882670e-01 -7.959798e-01 3.501653e-01 -1.886079e-01 ## [531] 4.056751e-02 7.303893e-01 1.857033e-02 -1.907467e-02 1.263335e-01 ## [536] -1.282509e-01 -4.413792e-01 9.457422e-02 -1.961340e-01 1.522869e-01 ## [541] -3.999287e-02 -1.153334e+00 -1.037723e-01 -5.437438e-01 3.777805e-03 ## [546] -1.618819e-01 3.440226e-01 1.838540e-01 -8.344381e-02 -4.030442e-02 ## [551] 1.998703e-02 -2.543803e-01 -1.908346e-01 1.501070e-01 4.482658e-01 ## [556] 3.562999e-02 -1.678402e-01 2.115631e-01 1.235467e-01 4.758801e-01 ## [561] 3.474977e-01 6.056974e-01 -1.431625e-01 1.254459e-01 6.008068e-01 ## [566] -3.177834e-01 -1.959285e-01 6.954754e-01 6.738760e-02 1.600044e-01 ## [571] 4.195127e-02 -6.995380e-01 6.276648e-01 1.829404e-01 -4.048921e-02 ## [576] 6.260712e-01 8.338629e-01 -2.067092e-02 -1.973213e-01 3.530552e-01 ## [581] 1.886135e-01 4.253090e-02 -9.002235e-02 -8.135975e-01 3.741419e-01 ## [586] -4.156237e-01 -2.292646e-01 -2.788867e-02 1.486134e+00 2.674690e-01 ## [591] 4.556571e-01 -3.190361e-02 1.767877e-01 7.264505e-01 1.474538e-01 ## [596] 9.167117e-01 -2.190197e-01 2.928686e-01 -2.548745e-01 1.229455e-01 ## [601] -4.957571e-01 6.070731e-02 5.063525e-01 1.859398e-01 4.151664e-01 ## [606] -1.921279e-01 -2.379486e-02 1.013450e-01 1.283272e+00 -2.527790e-01 ## [611] 2.098880e-01 7.199293e-01 -5.258517e-01 -9.026300e-02 -1.295224e-01 ## [616] -1.156899e-01 -1.300923e-03 -5.906785e-01 -2.910042e-01 -3.364946e-01 ## [621] -3.290679e-01 8.859849e-01 9.885939e-02 -7.612059e-01 -5.074031e-01 ## [626] -3.425718e-01 2.238583e-01 6.272427e-01 -3.743701e-01 6.721334e-02 ## [631] 3.257035e-02 -7.096340e-01 2.091665e-01 -8.253722e-01 -2.514599e-01 ## [636] -2.733685e-01 2.868372e-01 -2.849232e-03 -3.368171e-01 8.884872e-02 ## [641] -1.511044e-01 2.360786e-01 -8.223686e-01 -5.284757e-03 3.424102e-01 ## [646] -3.931425e-01 -1.109518e-02 1.884263e-01 8.236490e-01 3.271381e-01 ## [651] -9.181520e-03 7.301034e-01 6.512305e-02 -1.428352e-01 5.426722e-01 ## [656] -1.314187e-01 -8.728848e-01 3.574990e-01 -6.759306e-02 9.610148e-01 ## [661] 1.280078e-01 2.503548e-01 9.746505e-02 -3.276406e-01 -6.013361e-01 ## [666] 2.897949e-01 -4.849842e-01 4.611736e-01 4.424430e-01 -3.280715e-01 ## [671] 2.144834e-01 -4.551418e-01 -1.369689e-01 2.266359e-01 -3.596636e-02 ## [676] -1.595079e-01 1.291688e-01 6.841450e-01 9.082464e-01 1.974766e-01 ## [681] -2.895754e-02 -1.141932e-01 -1.553693e-01 7.115959e-02 1.256677e+00 ## [686] 3.258557e-01 1.919837e-01 4.069130e-02 9.274356e-01 -2.974921e-01 ## [691] 2.804515e-01 6.381541e-01 -1.115150e-01 9.141509e-01 4.560778e-01 ## [696] 7.143779e-01 -1.577128e-02 -2.763202e-02 -9.123873e-01 -1.927630e-01 ## [701] 7.266396e-02 4.960342e-01 -4.286933e-01 5.366887e-02 2.759993e-01 ## [706] -1.711207e-01 -3.359031e-01 -1.759740e-01 3.192273e-01 1.214959e+00 ## [711] 1.281842e-02 -2.570641e-01 5.808135e-01 3.493827e-01 8.049280e-01 ## [716] 1.482182e+00 -5.114536e-01 -3.212951e-01 4.134157e-01 8.334917e-02 ## [721] -1.841290e-01 -3.556498e-01 -6.230101e-01 1.077536e-01 1.667808e-01 ## [726] 5.072021e-01 1.617718e-01 4.377634e-01 2.402678e-01 1.250824e-01 ## [731] -9.623305e-01 -2.443577e-01 -3.079924e-01 -7.848592e-01 2.674690e-01 ## [736] 9.216179e-01 4.925973e-01 3.363684e-01 3.186036e-01 -2.353333e-01 ## [741] -6.377721e-02 -4.218349e-01 -2.994571e-01 -1.429816e-01 -5.473788e-01 ## [746] -6.853907e-01 -2.994571e-01 3.320121e-02 2.192663e-01 -3.366174e-02 ## [751] 2.952837e-01 2.610721e-01 -2.427738e-01 -2.562390e-02 -7.315184e-02 ## [756] -2.934281e-01 2.597937e-01 -3.639489e-01 -3.257705e-01 7.708416e-01 ## [761] 4.834132e-01 1.063369e-01 -1.786780e-01 4.873975e-01 -5.562248e-01 ## [766] 1.774033e-01 -3.153819e-01 -3.128627e-01 1.403747e-01 1.568480e-01 ## [771] -1.056475e+00 6.375678e-02 2.021748e-01 -7.000794e-01 7.141352e-01 ## [776] 2.233926e-01 -4.350367e-01 -2.040888e-01 9.115927e-01 7.394643e-01 ## [781] -5.953510e-01 8.886907e-01 -9.511924e-01 1.077014e-01 -1.583000e-01 ## [786] 1.144425e+00 -5.720915e-01 9.079095e-01 4.008283e-01 7.587584e-01 ## [791] -2.458345e-01 3.908220e-01 8.915185e-01 -3.506075e-01 -2.053645e-01 ## [796] 4.788584e-01 -1.506216e-01 3.070224e-01 4.555659e-01 -8.247519e-01 ## [801] 9.746505e-02 3.079980e-02 2.671842e-01 2.897883e-01 5.793987e-01 ## [806] -1.726866e-01 1.063188e+00 5.997109e-02 1.842992e-01 9.448066e-01 ## [811] -2.926740e-02 1.036759e-01 2.670848e-01 -5.640098e-01 1.132922e-01 ## [816] 1.803301e-01 4.450598e-01 -1.170942e+00 6.201095e-02 1.460078e-01 ## [821] 5.515187e-01 -3.706986e-01 -1.413669e+00 -6.143736e-01 9.362339e-02 ## [826] -2.014446e-01 -3.800446e-01 -3.720739e-01 -9.547799e-01 -1.957961e-02 ## [831] -1.408761e-01 1.531324e-01 3.607887e-01 -4.279954e-02 3.312578e-01 ## [836] -1.118093e-01 3.833885e-01 1.720975e-01 9.962475e-01 -5.804727e-02 ## [841] -3.618603e-01 4.191415e-01 -1.272739e-01 4.784384e-01 6.051012e-01 ## [846] -1.285297e-01 3.503864e-01 1.496272e-01 -8.453685e-02 7.761837e-01 ## [851] 5.195394e-02 3.589825e-02 -1.927029e-01 -6.850059e-02 -5.726941e-01 ## [856] 1.208852e-01 6.352304e-01 -2.081140e-01 8.360055e-01 -2.812535e-01 ## [861] 7.581578e-02 -2.303046e-01 -4.171230e-01 -1.855443e-01 -3.188598e-02 ## [866] -3.233671e-02 4.036727e-02 -1.456390e-01 -2.868441e-01 -6.884831e-02 ## [871] -3.692943e-01 8.706125e-01 1.753361e-01 4.593184e-01 -5.469941e-01 ## [876] -7.609199e-01 -2.977485e-01 -3.961164e-02 4.224436e-01 5.507437e-01 ## [881] 2.358282e-01 6.756979e-01 -2.303824e-01 -2.787903e-01 2.307915e-01 ## [886] 1.478655e-01 8.411069e-02 5.678260e-01 3.833709e-03 -7.652341e-01 ## [891] 4.312812e-01 -2.616233e-01 1.076824e-02 8.585340e-01 8.911838e-02 ## [896] 2.565970e-01 7.381687e-01 6.438408e-02 2.679837e-01 3.620335e-01 ## [901] 1.479967e-01 -2.826696e-02 -1.373891e-01 9.038053e-01 9.915928e-02 ## [906] -1.583000e-01 -1.698026e-01 4.844224e-01 4.226974e-01 -2.410375e-01 ## [911] -4.790420e-02 -3.648473e-01 4.287314e-02 5.238145e-01 -4.136940e-02 ## [916] 3.388182e-01 -3.903311e-01 8.606698e-01 9.853265e-02 1.103981e-01 ## [921] -2.349247e-01 4.213026e-02 -5.518392e-02 6.700601e-01 1.410134e-01 ## [926] -7.210331e-02 -2.223201e-01 4.555659e-01 -2.826696e-02 -1.239138e-01 ## [931] -6.085538e-02 -7.070413e-01 4.834431e-01 -3.839720e-01 -2.787665e-03 ## [936] 9.520477e-02 8.784016e-02 -6.544963e-01 -4.332027e-02 1.368047e-01 ## [941] 5.461364e-01 2.062403e-01 3.173848e-01 -5.214915e-01 -1.369689e-01 ## [946] -1.010460e+00 -2.449769e-02 5.035939e-01 -4.729381e-01 3.038303e-01 ## [951] 6.892540e-01 -5.394823e-01 7.164238e-02 -5.485711e-01 5.525376e-01 ## [956] -1.790809e-01 2.194707e-02 -1.667449e-01 -4.513501e-01 4.311467e-01 ## [961] 8.197330e-01 -4.957235e-02 2.367200e-01 1.019947e+00 1.274830e-01 ## [966] -4.033014e-01 3.053956e-01 4.479244e-01 8.938677e-02 5.194186e-01 ## [971] 5.466676e-01 3.757059e-01 3.268810e-01 -4.186304e-01 5.627300e-01 ## [976] 4.368797e-01 -3.922603e-01 2.210392e-01 9.413269e-01 1.133069e-01 ## [981] -1.128590e-01 1.300679e-01 -4.881767e-01 6.339642e-01 1.181456e-01 ## [986] 1.509870e-01 -4.126112e-01 5.581431e-02 3.668597e-01 -1.052699e+00 ## [991] -7.483330e-01 3.179200e-01 8.883326e-01 4.095123e-01 -3.173120e-02 ## [996] 2.246878e-01 3.871033e-01 7.323513e-02 1.948507e+00 -3.439961e-01 ## ## $func.thetastar ## NULL ## ## $jack.boot.val ## NULL ## ## $jack.boot.se ## NULL ## ## $call ## bootstrap(x = original.data, nboot = 1000, theta = skewness) hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) hist(results2$thetastar,breaks=30,border=&quot;purple&quot;,add=T,density=20,col=&quot;purple&quot;,freq=F) What would have happened if we would have fit a normal distribution instead of a gamma distribution? fit2&lt;-fitdistr(original.data,dnorm,start=list(mean=1,sd=1)) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced fit2 ## mean sd ## 0.61094862 0.22427622 ## (0.07092237) (0.05014552) results.norm&lt;-c() for (i in 1:1000) { x.star&lt;-rnorm(length(original.data),mean=fit2$estimate[1],sd=fit2$estimate[2]) results.norm&lt;-c(results.norm,skewness(x.star)) } head(results.norm) ## [1] -0.42242374 -1.54207026 0.05638018 -0.20603893 -0.18264197 0.12816589 hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) hist(results.norm,breaks=30,col=&quot;lightgreen&quot;,freq=F,add=T) hist(results2$thetastar,breaks=30,border=&quot;purple&quot;,add=T,density=20,col=&quot;purple&quot;,freq=F) All three methods (two parametric and one non-parametric) really do give different distributions for the bootstrapped statistic, so the choice of which method is best depends a lot on the situation, how much data you have, and what you might already know about the underlying distribution. Jackknifing is just as easy at bootstrapping. Here we will do a trivial example for illustration. We will write a little function for the mean even though you could put the function in directly with ‘jackknife(x,mean)’ theta&lt;-function(x) { mean(x) } x&lt;-seq(0,9,by=1) results&lt;-jackknife(x=x,theta=theta) results ## $jack.se ## [1] 0.9574271 ## ## $jack.bias ## [1] 0 ## ## $jack.values ## [1] 5.000000 4.888889 4.777778 4.666667 4.555556 4.444444 4.333333 4.222222 ## [9] 4.111111 4.000000 ## ## $call ## jackknife(x = x, theta = theta) Checkpoint #7: Why do we not have to tell the ‘jackknife’ function how many replicates to do? Let’s compare this with what we would have obtained from bootstrapping results2&lt;-bootstrap(x,1000,theta) mean(results2$thetastar)-mean(x) #this is the bias ## [1] -0.0506 sd(results2$thetastar) #the standard deviation of the theta stars is the SE of the statistic (in this case, the mean) ## [1] 0.8887686 Everything we have done to this point used the R package ‘bootstrap’ - now lets compare that with the R package ‘boot’. To avoid any confusion (a.k.a. masking) between the two packages, I recommend detaching the bootstrap package from the workspace with detach(&quot;package:bootstrap&quot;) The ‘boot’ package is now recommended over the ‘bootstrap’ package, but they give the same answers and to some extent it is personal preference which one prefers to use. We will still use the mean as the statistic of interest, but we will have to write a new function for it because the syntax of the ‘boot’ package is slightly different: library(boot) theta&lt;-function(x,index) { mean(x[index]) } boot(x,theta,R=999) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = x, statistic = theta, R = 999) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 4.5 0.001001001 0.9154754 One of the main advantages to the ‘boot’ package over the ‘bootstrap’ package is the nicer formatting of the output. Going back to our original code, lets see how we could reproduce all of these numbers: table(sample(x,size=length(x),replace=T)) ## ## 0 2 3 4 7 8 ## 1 1 1 1 2 4 xmeans&lt;-vector(length=1000) for (i in 1:1000) { xmeans[i]&lt;-mean(sample(x,replace=T)) } mean(x) ## [1] 4.5 bias&lt;-mean(xmeans)-mean(x) se.boot&lt;-sd(xmeans) bias ## [1] 0.0306 se.boot ## [1] 0.898839 Why do our numbers not agree exactly with those of the boot package? This is because our estimates of bias and standard error are just estimates, and they carry with them their own uncertainties. That is one of the reasons we might bother doing jackknife-after-bootstrap. The ‘boot’ package has a LOT of functionality. If we have time, we will come back to some of these more complex functions later in the semester as we cover topics like regression and glm. "],["week-3-lecture.html", "5 Week 3 Lecture 5.1 Week 3 Readings 5.2 Overview of probability distributions 5.3 Normal (Gaussian) Distribution 5.4 Standard Normal Distribution 5.5 Log-Normal Distribution 5.6 Intermission: Central Limit Theorem 5.7 Poisson Distribution 5.8 Binomial Distribution 5.9 Beta Distribution 5.10 Gamma Distribution 5.11 Some additional notes: 5.12 By the end of Week 3, you should understand…", " 5 Week 3 Lecture 5.1 Week 3 Readings For this week, I suggest reading Aho Chapter 3. The context of the problem set will make more sense if you skim Viswanathan et al. (2008) before doing the problem set (those interested in animal movement models should read the whole paper). There is also a handout for this week here. Important - This figure has a typo. Please print it out and fix it. The arrow heads connecting the Normal distribution and the Standard Normal distribution should be reversed. We will discuss this in class as well so you understand why these arrows need to be reversed. 5.2 Overview of probability distributions We will cover 7 distributions this week, and several more next week. Please refer to the handout if univariate distributions, and note that the arrows between Normal and Standard Normal need to be reversed. For each distribution, there are five things I want you to know. 1 - Probability density function 2 - General Shape 3 - The expected value \\(E[X]\\) 4 - The variance \\(E[X-E[X]]^2\\) 5 - Relationship to other distributions What is a probability density? if \\(f(x \\mid params)\\) is a probablity density function: \\[ P(a&lt;x&lt;b)=\\int_a^bf(x \\mid params)dx \\] Probablity at a single point is always zero but probablity density is not. The probability density function is not restricted to being \\(\\le1\\) The integral over it’s range is always 1. 5.3 Normal (Gaussian) Distribution The outcome is produced by small number effects acting additively and independently. Normally distributed errors is the most common assumption of linear models. Central Limit theorem! The probability density function of the Normal distribution is given by \\[ f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] \\[ x \\in \\mathbb{R} \\\\ \\mu \\in \\mathbb{R} \\\\ \\sigma &gt; 0 \\] The shape of the Normal distribution can be illustrated by a few examples The expected value of the Normal distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_{-\\infty}^{\\infty}{X \\cdot f(X)dX} \\\\ &amp;= \\int_{-\\infty}^{\\infty} x \\cdot f(x \\mid \\mu, \\sigma) = \\int_{-\\infty}^{\\infty}\\frac{x}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\mu \\end{align} \\] The variance of the Normal distribution is given by \\[ \\begin{align} Var[X] &amp;= E[(X- E[X])^2] \\\\ &amp;= E[(X - \\mu)^2] \\\\ &amp;= E[X^2] - \\mu^2 \\\\ &amp;= \\left( \\int_{-\\infty}^{\\infty} x^2 \\cdot f(x \\mid \\mu, \\sigma) = \\int_{-\\infty}^{\\infty}\\frac{x^{2}}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx \\right) - \\mu^2 \\\\ &amp;= \\sigma^2 \\end{align} \\] 5.4 Standard Normal Distribution Raw data rarely fits standard normal. Mostly useful as a theoretical construct in hypothesis testing. The probability density function of the Standard Normal distribution is given by \\[ Z = \\frac{X-\\mu}{\\sigma} \\] \\[ f(z \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2} \\] While the letter used to represent a random variable is usually arbitrary (X usually, maybe Y), we use Z (or its specific incarnation z) to represent a random variable drawn from a Standard Normal distribution. The expected value and variance of the Standard Normal distribution are given by E[X] = 0 Var[X] = 1 Note that the Standard Normal distribution is a linear transformation of the Normal distribution (centered on zero with variance equal to 1). 5.5 Log-Normal Distribution The outcome is produced by small number effects acting multiplicatively and independently. Often used for things where small grows slowly and big grows quickly, such as forest fires or insurence claims. The probability density function of the Log-Normal distribution is given by \\[ \\begin{align} log(X) &amp;\\sim N(\\mu,\\sigma) \\\\ X &amp;\\sim LN(\\mu,\\sigma) \\end{align} \\] \\[ f(x \\mid \\mu, \\sigma) = \\frac{1}{x\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(log(x)-\\mu)^2}{2\\sigma^2}} \\\\ x \\in \\{0,\\infty\\} \\\\ \\mu \\in \\mathbb{R} \\\\ \\sigma &gt; 0 \\] The shape of the Log-Normal distribution can be illustrated with a few examples The expected value and variance of the Log-Normal distribution is given by \\(\\mu\\) is no longer the mean! \\[ E[X] = e^{\\mu + \\frac{\\sigma^2}{2}} \\] \\(\\sigma\\) is no longer the variance! \\[ Var[X] = e^{2(\\mu + \\sigma^2) - (2\\mu + \\sigma^2)} \\] Note that if the log of a variable (X) has a Normal distribution \\[ log(X) \\sim N(\\mu,\\sigma^{2}) \\] than the variable X follows a Log-Normal distribution. NB: Be careful when using the Log-Normal distribution. In particular, keep in mind that the sum of Log-normally distributed variables is NOT Log-Normally distributed. 5.6 Intermission: Central Limit Theorem \\[ X_1,X_2,X_3,...,X_k \\sim N(\\mu,\\sigma^2) \\\\ S_n = \\frac{1}{n} (X_1 + X_2 + X_3,...,X_n) \\\\ \\lim_{n \\to \\infty} S_n \\to N(\\mu,\\frac{\\sigma^2}{n}) \\] X is i.i.d X can be drawn from any distribution (with some very limited exceptions; distribution has to have finite moments)! 5.7 Poisson Distribution The Poisson distribution arises principally in 3 situations: 1 - In the description of random spatial point patterns (disease events, complete spatial randomness) 2 - As the frequency distribution of rare but independent events 3 - As the error distribution in linear models of count data The probability mass function of the Poisson distribution is given by \\[ P(x \\mid \\lambda)= \\frac{e^{-\\lambda} \\cdot \\lambda^x}{x!} \\\\ \\lambda&gt;0 \\\\ x \\in \\mathbb{N} \\cup \\{0\\} \\] Note that when variables are discrete (i.e. when the distribution only produces integer numbers), we call the probability density function a probability mass function. The PDF and PMF play the same role in both cases. The shape of the Poisson distribution is illustrated by a few examples The expected value and variance of the Poisson distribution is given by \\[ \\begin{align} E[X] &amp;= \\sum_{x=1}^{\\infty} x \\frac{e^{-\\lambda} \\cdot \\lambda^x}{x!} \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{x=1}^{\\infty} x \\frac{\\lambda^{x-1}}{x!} \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{x=1}^{\\infty} \\frac{\\lambda^{x-1}}{(x-1)!}\\\\ &amp;\\mbox{define } y = x-1 \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{y=0}^{\\infty} \\frac{\\lambda^{y}}{y!} \\mbox{ (the sum is now the expansion of the exponential)}\\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot e^{\\lambda} \\\\ &amp;= \\lambda\\end{align} \\] \\[ Var[X] = \\lambda \\] The Poisson distribution has the following relationship to the Normal distribution: \\[ \\lim_{\\lambda \\to \\infty} Pois(\\lambda) \\to N(\\lambda, \\lambda) \\] The above limit can be understood in terms of ‘moment matching’. What are moments? Think of moments as being like characteristics of the distribution, but in decreasing order of importance. The first moment in the mean of the distribution (or, equivalently, the expected value E[X]). The second moment is the variance of the distribution, or Var[X]. The third moment is the skew of a distribution (assemmetry of left and right tails), the fourth momnent is the kurtosis (fatness of the tails), etc. Consider this, if I wanted to construct a Normal distribution that “looked” like a Poisson, I would want a Normal distribution that had the same moments as possible to the Poisson, so at the very least I would want that Normal distribution to have the same mean and variance. This is easy done with the Normal distribution because the mean and variance are two parameters that can be controlled independently, so we can simply set these memonts to match one another (just like the Poisson) and now we have a Normal distribution that has the key characteristic of the Poisson which is E[X]=Var[X]. Since we use the Greek letter \\(\\lambda\\) to represent this parameter for the Poisson, we can use that same letter in the Normal to emphasize how these two distributions relate as \\(\\lambda \\to \\infty\\). Note also that as \\(\\lambda \\to \\infty\\), the values of the draws from Pois(\\(\\lambda\\)) get very large and so the discreteness of the values approximates a continuous distribution like the Normal. Note that the sum of Poisson distributed variables is itself Poisson distributed. \\[ \\begin{align} X &amp;\\sim Pois(1) \\\\ Y &amp;= \\sum_{i=1}^{\\lambda} X_i \\\\ Y &amp;\\sim Pois(\\lambda) \\end{align} \\] Applying the Central Limit Theorem \\[\\begin{align} \\bar{X} = \\frac{Y}{\\lambda} &amp;\\sim N(1,\\frac{1}{\\lambda}) \\\\ Y &amp;\\sim N(\\lambda, \\lambda) \\end{align} \\] 5.8 Binomial Distribution The probability mass function of the Binomial distribution is given by \\[ P(x \\mid p,n) = \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\\\ n \\in \\mathbb{N} \\cup \\{0\\} \\\\ x \\in \\{0,1,2,3,...,n\\} \\\\ p \\in [0,1] \\] The shape of the Binomial distribution is illustrated by the following examples The expected value and variance of the Binomial distribution is given by \\[ \\begin{align} E[X] &amp;= \\sum_{x=1}^n x \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\\\ &amp;=np \\\\ Var[X] &amp;= np(1-p) \\end{align} \\] The Binomial distribution has the following relationship to the Normal distribution \\[ \\lim_{n \\to \\infty} Binom(n,p) \\to N(np,np(1-p)) \\] 5.9 Beta Distribution One of the few distributions that is restricted to a finite interval (0 and 1). Can be used to model proportions. The probability density function of the Beta distribution is given by \\[ f(x \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\ \\alpha&gt;0 \\\\ \\beta&gt;0 \\\\ x \\in (0,1) \\] Gamma Function: if n is a positive integer \\(\\Gamma(n)=(n-1)!\\) The shape of the Beta distribution is illustrated by the following examples The expected value and variance of the Beta distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_0^1x\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}dx \\\\ &amp;= \\frac{\\alpha}{\\alpha + \\beta} \\end{align} \\] \\[ Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 \\cdot(\\alpha + \\beta + 1)} \\] The Beta Distribution’s relationship to the Normal and Uniform distributions are given by \\(Beta(1,1)\\) is the same as \\(Uniform(0,1)\\) \\[ f(x \\mid 1,1) = \\frac{\\Gamma(2)}{\\Gamma(1) + \\Gamma(1)}x^{0}(1-x)^{0} = 1 \\] In the limit that \\(\\alpha\\) and \\(\\beta\\) are the same and growing large, the Beta distribution has the following relationship to the Normal distribution. \\[ \\lim_{\\alpha=\\beta \\to \\infty} \\to N(\\frac{1}{2}, \\frac{1}{8\\alpha + 4}) \\] 5.10 Gamma Distribution Useful for variables that have a positive skew. it is often used to model “waiting times”, such as the time before a device or machine fails. The probability density function of the Gamma distribution (not to be confused with the Gamma Function) is given by \\[ f(x \\mid \\alpha, \\beta) = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{(\\alpha-1)} e^\\frac{-x}{\\beta} \\\\ \\alpha&gt;0 \\\\ \\beta &gt;0 \\\\ x&gt;0\\] The shape of the Gamma distribution is illustrated by the following examples The expected value and variance of the Gamma distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_0^\\infty x\\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{(\\alpha-1)} e^\\frac{-x}{\\beta} \\\\ &amp;= \\alpha\\beta \\end{align} \\] \\[ Var[X] = \\alpha\\beta^2 \\] The relationship between the Gamma distribution and the Normal distribution is \\[ \\lim_{\\alpha \\to \\infty} Gamma(\\alpha,\\beta) \\to N(\\alpha \\beta,\\alpha\\beta^2) \\] 5.11 Some additional notes: Please skim through the reading that has been posted “The Algebra of Expectations”. Focus on the “rules”. I want to make sure everyone is clear on the distribution of Normal random variables. If \\[ X\\sim N(\\mu,\\sigma^{2}) \\] then the distribution of a new variable c*X (where c is a constant) is given by \\[ cX\\sim N(c\\mu,c^{2}\\sigma^{2}) \\] The mean is simply multiplied by \\(c\\), and the variance is multiplied by \\(c^{2}\\). To understand why this is, let’s first go into a more detailed derivation of the variance of the distribution of Normal random variables than we did in class. To begin with, you will need to know that one property of expectations is the following: \\[ E[ g(x) ] = \\int g(x) f(x) \\ dx \\] With this knowledge, let’s approach the derivation of the variance, where we are essentially defining \\(g(x) = (X-E[X])^2\\) and using some basic properties of algebra involving integrals. Note that in this derivation we are treating \\(E[X]\\) as a constant, and not as a function of \\(x\\). \\[ \\begin{eqnarray} Var[X]&amp;=&amp;E[(X-E[X])^{2}] \\\\ &amp;=&amp;\\int (x - E(X))^2 f(x) \\ dx \\\\ &amp;=&amp;\\int (x^2 -2xE(X) + E(X)^2 ) f(x) \\ dx \\\\ &amp;=&amp;\\int x^2 f(x) \\ dx - 2E(X) \\int x f(x) \\ dx + \\int E(X)^2 f(x) \\ dx \\\\ &amp;=&amp; E(X^2) -2 E(X)^2 + E(X)^2 \\\\ &amp;=&amp; E(X^2) - E(X)^2 \\end{eqnarray} \\] 5.12 By the end of Week 3, you should understand… the shape of each of the distributions introduced (including the range of support for x) and the number of parameters for each distribution. You should also know if the parameters have restrictions (e.g., non-negative, etc.) the process by which you can find the E[X] and Var[X] for any parametric distribution. (In some cases, the integral might not be tractable.) the relationships between the distributions the PDFs for the Normal distribution, Standard Normal, Log-Normal, Binomial, and Poisson distributions "],["week-3-lab.html", "6 Week 3 Lab 6.1 Exploring the univariate distributions with R 6.2 Standard deviation vs. Standard error 6.3 The Central Limit Theorem", " 6 Week 3 Lab 6.1 Exploring the univariate distributions with R As a review of last week’s lecture, we can ask a number of things about a statistical distribution: Look at the probability density function: What is the probability of obtaining X (discrete) or a number in the interval (X1,X2) (continuous)? Look at the cumulative probability: What is the probability of obtaining \\(X &lt; X^{*}\\)? Look at the quantiles of the distributions: The inverse of the cumulative distribution - What is \\(X^{*}\\) such that the cumulative probability of obtaining \\(X &lt; X^{*}\\) is the specified quantile? Quantiles can have any size: Quartiles, deciles, percentiles, etc. Look at samples from the distribution: What does the distribution “look like”? There are four basic functions in R: d = probability density function p = cumulative probability q = quantiles of the distribution r = random numbers generated from the distribution We combine these letters with the function names to make all the function calls: For example, Normal distribution: dnorm, pnorm, qnorm, rnorm Log-normal distribution: dlnorm, plnorm, qlnorm, rlnorm Poisson: dpois, ppois, qpois, rpois First we’ll play around with the normal distribution because we know what the answers should be. Then we’ll move onto distributions we may be less familiar with: First, lets draw a couple of random values from the standard normal. We can take 100 random draws from the Standard Normal N(0,1) using the R function ‘rnorm’. data&lt;-rnorm(100,mean=0,sd=1) head(data) ## [1] -1.2252941 -0.2706581 0.4527726 -2.1286491 0.4179466 -1.1680336 Note that you could have left off the “mean” and “sd” since R knows the order of inputs, that is you could simply write head(rnorm(100,0,1)) ## [1] 0.04689724 0.46133908 1.44288990 0.61679270 -0.98129554 -0.60597035 or even head(rnorm(100)) ## [1] -1.3324963 1.1131091 -2.1533460 -0.6384104 1.0264202 -1.1139911 since mean=0, sd=1 is the default. Until you are 100% comfortable with R, its better to leave all the options spelled out. Make a histogram of data hist(data) Play around with the hist command using different numbers of ‘breaks’ or try leaving that option off altogether. You will get a sense for how many breaks you need for the histogram to “look right” but I prefer to use more breaks than R defaults to. Also, compare this last plot with this one: hist(data,freq=FALSE) To really play around with these distributions, lets combine these commands into a single command: hist(rnorm(100,mean=0,sd=1),col=&quot;plum4&quot;) Play around with different means and sd and convince yourself that ‘rnorm’ really does work. You can look at the graphics options by doing ?hist and you can explore the list of named colors by typing colors() What happens if you add the flag “plot=F”? hist(rnorm(1000,mean=0,sd=1),plot=F) ## $breaks ## [1] -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ## [16] 3.5 4.0 4.5 ## ## $counts ## [1] 1 2 4 13 46 100 146 191 200 143 97 31 16 8 1 0 1 ## ## $density ## [1] 0.002 0.004 0.008 0.026 0.092 0.200 0.292 0.382 0.400 0.286 0.194 0.062 ## [13] 0.032 0.016 0.002 0.000 0.002 ## ## $mids ## [1] -3.75 -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ## [13] 2.25 2.75 3.25 3.75 4.25 ## ## $xname ## [1] &quot;rnorm(1000, mean = 0, sd = 1)&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; Note that you can assign that to a variable and then use those results later in a calculation or another plot. Next, lets play around with pnorm quantiles&lt;-seq(-3.5,3.5,0.01) #These are the quantiles density&lt;-dnorm(quantiles,mean=0,sd=1) #dnorm gives the pdf for a given quantile plot(quantiles,density,type=&quot;l&quot;,ylab=&quot;Probability density&quot;) This is the probability density function for the Standard Normal. (We are getting a little ahead of ourselves because we won’t discuss graphics until next week, but this syntax is fairly straightforward.) Let’s use the same vector ‘quantiles’ and try the function pnorm: cumulative&lt;-pnorm(quantiles,mean=0,sd=1) plot(quantiles,cumulative,type=&quot;l&quot;,ylab=&quot;Probability&quot;) This gives us the cumulative distribution function! Finally, lets look at qnorm probability&lt;-seq(0,1,0.001) quantiles&lt;-qnorm(probability,mean=0,sd=1) plot(probability,quantiles,type=&quot;l&quot;,ylab=&quot;Quantiles&quot;) This plots the quantiles for each probability between 0 and 1. In other words, what value \\(Y^{*}\\) is associated with the cumulative probability of \\(X^{*}\\). Lets make sure this makes sense by plotting on top of this line another representing the quantiles for a normal distribution with smaller variance quantiles2&lt;- qnorm(probability,mean=0,sd=0.2) plot(probability,quantiles,type=&quot;l&quot;,ylab=&quot;Quantiles&quot;) lines(probability,quantiles2,col=&quot;red&quot;) Notice that because the variance of the new distribution is smaller, you get from a cumulative probability of 0 to 1 over a smaller range of values. Now the issue of quantiles gets a bit complicated when we consider discrete distributions. A nice online post on this can be found here. Let’s consider data drawn from a very simple fake dataset and plot the quantiles. count&lt;-c(1,2,3,4,5) plot(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01))) This is confusing because the default definition of the quantiles here makes it look like the data are continuous and non-integer values are possible, but the actual data are discrete. In other words, if you asked what the “10th percentile” of this data were, this plot tells you that the 10th percentile is ~1.5, but that’s not even a value in the original dataset. This behavior comes about because of the ambiguity described in the blog post above. It turns out there are at least 9 ways to define quantiles for discrete data, and you can see a decription of them here and here. The default behavior in R is to use the “type 7” quantile, which is a weighted average of the two closest values. We can see them all plotted together as follows: count&lt;-c(1,2,3,4,5) plot(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=1),typ=&quot;l&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=2),col=&quot;red&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=3),col=&quot;orange&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=4),col=&quot;yellow&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=5),col=&quot;green&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=6),col=&quot;blue&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=7),col=&quot;purple&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=8),col=&quot;gray&quot;,lwd=2) lines(seq(0,1,0.01),quantile(count,probs=seq(0,1,0.01),type=9),col=&quot;pink&quot;,lwd=2) 6.2 Standard deviation vs. Standard error Many people struggle with the distinction between the standard deviation of a sample (or a population), and the standard error of the mean of the sample (or population). The standard deviation is a measure of the average spread of the data. Since the standard deviation is a measure of the average spread of the data, adding more data does not appreciably change the standard deviation. (Make sure this makes sense!) The standard error can be understood as follows: If you repeated your experiment many times, and calculated the mean of each of the samples (one sample from each “experiment”), the standard deviation of the means would represent the uncertainty in the estimate of the mean coming from any one sample. The standard deviation of those means is called the standard error of the mean (or SEM). The SEM decreases as the size of each sample increases because each sample is now more representative of the underlying distribution. More precisely, the standard error is the standard deviation of the sampling distribution of a statistic. Standard errors can be calculated for any statistic. For example, if we fit a Beta(\\(\\alpha\\),\\(\\beta\\)) distribution to a dataset, we want to estimate the parameter values \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and thier standard errors, which we might denote s.e.\\(_{\\hat{\\alpha}}\\) and s.e.\\(_{\\hat{\\beta}}\\). Checkpoint #1: Does everyone in your group understand the definition of a standard error? We can use the Poisson distribution to illustrate the difference between a standard deviation of a distribution and the standard deviation of the mean: First we want to plot the distribution for three different sample sizes: par(mfrow=c(3,1)) #Use ?par to see what this command does - more on this later sample1&lt;-rpois(1000,lambda=3) sample2&lt;-rpois(10000,lambda=3) sample3&lt;-rpois(100000,lambda=3) hist(sample1) hist(sample2) hist(sample3) sd(sample1) ## [1] 1.697575 sd(sample2) ## [1] 1.75765 sd(sample3) ## [1] 1.73252 Notice that the standard deviation has not appreciably changed as we have increased the sample size. Now lets run some code to calculate the standard error of the mean: sample.size&lt;-1000 means&lt;-c() for (i in 1:2000) { means&lt;-c(means,mean(rpois(sample.size,lambda=3))) } hist(means) s.e.1&lt;-sqrt(var(rpois(sample.size,lambda=3))/sample.size) s.e.2&lt;-sd(means) s.e.1 ## [1] 0.05536216 s.e.2 ## [1] 0.05540869 Note that the number of experiments I looped through (2000 in this case) is not relevant. It just has to be big enough that you get a sense of what the distribution of means looks like. Now go back and modify the code so that sample.size=10000. Checkpoint #2: How does increasing the sample size change the result? On Tuesday we discussed the probability mass function for the binomial. While an individual flip of the coin can be thought of as a success/failure, the binomial is answering the question “How many succesess do I expect if I try n times.” We can plot this for varying numbers of trials assuming p=0.5 par(mfrow=c(3,3)) p=0.5 plot(seq(0,10),dbinom(x=seq(0,10),size=1,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=1&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),1*p,1*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=2,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=2&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),2*p,2*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=3,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=3&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),3*p,3*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=4,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=4&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),4*p,4*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=5,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=5&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),5*p,5*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=6,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=6&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),6*p,6*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=7,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=7&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),7*p,7*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=8,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=8&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),8*p,8*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=9,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=9&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),9*p,9*p*(1-p)),col=&quot;red&quot;,lwd=2) and we can see how this might change for p=0.1 par(mfrow=c(3,3)) p=0.1 plot(seq(0,10),dbinom(x=seq(0,10),size=1,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=1&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),1*p,1*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=2,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=2&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),2*p,2*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=3,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=3&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),3*p,3*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=4,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=4&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),4*p,4*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=5,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=5&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),5*p,5*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=6,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=6&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),6*p,6*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=7,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=7&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),7*p,7*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=8,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=8&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),8*p,8*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=9,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=9&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),9*p,9*p*(1-p)),col=&quot;red&quot;,lwd=2) If you look back at our notes from Tuesday, we see that the gamma and the Poisson distributions look quite similar (ignoring that one is discrete and the other continuous). We can use R to show us the differences are: First we will draw from the Poisson distribution, then we will use R’s very handy function ‘fitdistr’ to fit the gamma distribution to that data and compare. We haven’t yet covered HOW this function works, but for now let’s just take for granted that this function is able to find the parameter estimates that will give you the best fit to your data. First, install the library ‘MASS’. library(MASS) #this loads the library into the workspace sample.pois&lt;-rpois(1000,lambda=20) fit&lt;-fitdistr(sample.pois,&quot;gamma&quot;,start=list(shape=20,scale=1)) fit ## shape scale ## 19.12980662 1.05583920 ## ( 0.84808051) ( 0.04742662) (Sometimes you get a warnings message about NAs when using fitdistr. The best explanation I can find says that this means R “encountered some difficulties during fitting”. I can find no difference in the fits when you get the warning and when you don’t, and the same sample.pois will sometimes give a warning and sometimes not, so it appears independent of the data itself. Do not ignore warnings() in R but don’t be paralized by them, especially in a context where R is searching parameter space during an optimization. Be sure to search around for an explanation and make sure you are confident that R is still giving reasonable answers.) Notice that we are fitting a gamma distribution to this data, and we specify what distribution we want to fit using the name of the distribution in quotes. Remember that rgamma can take as inputs shape,scale or shape,rate=1/scale. I am using scale as the input because it is consistent with the way I introduced the gamma distribution in class. Be aware that some people will use rate and some will use scale and you always have to check. When we print the object fit, we get the estimates and the standard errors, but at first it isn’t obvious how to extract the estimates (and errors) so we can use them in other calculations. We start by using the function names to “get inside” this object and see what it is made up of. names(fit) ## [1] &quot;estimate&quot; &quot;sd&quot; &quot;vcov&quot; &quot;loglik&quot; &quot;n&quot; We now look at fit$estimate ## shape scale ## 19.129807 1.055839 and notice that we can pull out the two estimates as fit$estimate[1] ## shape ## 19.12981 fit$estimate[2] ## scale ## 1.055839 Now we want to plot the data, and the best fit line: x.vals&lt;- seq(from=5,to=40,by=1) hist(sample.pois,breaks=x.vals) lines(x.vals,dgamma(x.vals,shape=fit$estimate[1],scale=fit$estimate[2])*1000,col=&quot;blue&quot;) Two things to note here: 1. I created x.vals just as a mechanism for plotting a relatively smooth line for the best-fit distribution 2. I multiplied for 1000 because I had originally drawn 1000 values, and this puts my best-fit line on the same scale as the histogram. NOTE: We can guess at starting values by making sure the mean and variance of the gamma match the mean and variance of the data. This method is an example of “moment matching”. In other words, we take two distributions and get a close fit between them by requiring that they have the same mean and, if possible, the same variance. We can see that if we generate data from a Poisson, it can be fit very well by a gamma distribution. It can also be fit quite well by a Normal distribution hist(sample.pois,breaks=x.vals) fit2&lt;-fitdistr(sample.pois,&quot;normal&quot;) lines(x.vals,dnorm(x.vals,mean=fit2$estimate[1],sd=fit2$estimate[2])*1000,col=&quot;red&quot;) Checkpoint #3: We see here that the data, which were generated by a Poisson distribution, are fit quite well by the Normal distribution. If this data were research data, what would be one major clue that the Poisson distribution was more appropriate than the Normal distribution? So we have shown that given certain parameters, a Gamma distribution can approximate a Poisson, and we have shown that the Normal can approximate the draws from a Poisson distribution. This latter fact shouldn’t come as a surprise because \\[ \\lim_{\\lambda\\rightarrow\\infty} Pois(\\lambda)\\rightarrow N(\\lambda,\\lambda) \\] The function ‘fitdistr’ is one of the MOST HANDY functions that exist for probabilities in R. Notice that ‘fitdistr’ also gives the estimated standard errors in parentheses. The next few weeks will be dedicated to learning more about the interpretation and creation of these standard errors or, equivalently, confidence intervals. Finally, I want to introduce the idea of a QQ-plot. A QQ-plot has the quantiles of two distributions plotted against one another. If the two distributions are quite similar, the QQ-plot will fall roughly on the 1:1 line. We can compare the Poisson data to the gamma distribution fit using a QQ-plot of the original Poisson sample and an equally sized sample from our best-fit gamma distribution. qqplot(x=sample.pois, y=rgamma(1000,shape=fit$estimate[1],scale=fit$estimate[2])) abline(a=0,b=1,col=&quot;red&quot;,lwd=2) In lab today, we will dive into using R to understand the properties of the univariate distributions, but first we’ll take a short detour to discuss the Central Limit Theorem (or CLT). 6.3 The Central Limit Theorem QUESTION: Why is the normal distribution so fundamental to statistics? ANSWER: The central-limit theorem. Let \\(X_{1}\\), \\(X_{2}\\),…, \\(X_{n}\\) be independently and identically distributed random variables with mean \\(\\mu\\) and finite, non-zero variance \\(\\sigma^{2}\\), \\[ X_{1},X_{2},...,X_{n} \\sim N(\\mu,\\sigma^{2}) \\] and the average of these variable \\(S_{n}\\) be defined as \\[ S_{n} = \\frac{1}{n}(X_{1}+X_{2}+X_{3}+...+X_{n}) \\] Then the Central Limit Theorem states: \\[ \\lim_{n\\rightarrow\\infty} S_{n} \\rightarrow N \\left( \\mu,\\frac{\\sigma^{2}}{n} \\right) \\] The Central Limit Theorem tells us something very important about how well we can estimate the mean of a set of random i.i.d. numbers. Our uncertainty of the mean is given by the variance of \\(S_{n}\\) \\[ \\mbox{variance of estimate of } \\mu = \\frac{s^{2}}{n} \\] where \\[ s^{2} = \\frac{(X-\\bar{X})^{2}}{n-1} \\] is our unbiased estimate of \\(\\sigma^{2}\\). Therefore, we define the STANDARD ERROR of our estimate of \\(\\mu\\) as \\[ \\mbox{s.e. of } \\mu = \\sqrt{\\frac{s^{2}}{n}} \\] Our uncertainty regarding our estimate of \\(\\mu\\) goes down as the \\(\\sqrt{n}\\). Note that in the more general case, where draws are not from a Normal distribution to start with, we should think of this as \\[ \\mbox{s.e. of } mean = \\sqrt{\\frac{\\mbox{variance of the data}}{n}} \\] DO NOT CONFUSE STANDARD ERROR AND STANDARD DEVIATION. Standard errors are just the standard deviation of a parameter estimate, it expresses uncertainty about the estimate. Standard deviations of a population simply reflect the spread in values. As you increase sample size, standard errors (i.e. standard deviations of the parameter estimate) get smaller and smaller, but standard deviations of the population values do not get smaller with increasing sample size. Here I have illustrated the CLT using the normal distribution, but the variables X can be drawn from ANY distribution (as long as the X are i.i.d. from a distribution with finite mean and variance), which is remarkable. For example, X could be drawn from a Bernoulli! Checkpoint #4: Write a short script to draw Bernoulli distributed data. Play around with data of different sizes. Roughly how large does \\(n\\) need to get before \\(S_{n}\\) starts to “look” like a Normal distribution? What effect does the parameter \\(p\\) have? The application of the CLT to Bernoulli and Binomially-distributed variables can be confusing, because we have to be very careful about what \\(\\textit{n}\\) is. Consider 100 draws from a Bernoulli distribution. Each draw is either 0 or 1. The CLT says that the mean of those 100 Bernoulli draws is as follows: \\[ \\lim_{n\\rightarrow\\infty} S_{n} \\rightarrow N \\left( \\mu,\\frac{p(1-p)}{100} \\right) \\] and therefore the standard error of the mean (i.e. the uncertainty in the mean of those 100 draws) is \\(\\sqrt{\\frac{p(1-p)}{100}}\\). We can code that experiment as follows: mean_of_flip_outcomes&lt;-c() for (j in 1:1000) { vector_of_flip_outcomes&lt;-c() for (i in 1:100) { flip_outcome&lt;-rbinom(1,1,0.5) vector_of_flip_outcomes&lt;-c(vector_of_flip_outcomes,flip_outcome) } mean_of_flip_outcomes&lt;-c(mean_of_flip_outcomes,mean(vector_of_flip_outcomes)) } hist(mean_of_flip_outcomes,breaks=30) sd(mean_of_flip_outcomes) ## [1] 0.04910584 Notice that the standard error is what we would have expected. Now consider 100 draws from a Binomial distribution where each draw represents 5 coin flips. Each sample now represents the number of “heads” across the 5 flips and is in the set {0,1,2,3,4,5}. The CLT says that the mean of those 100 Binomial draws is as follows: \\[ \\lim_{n\\rightarrow\\infty} S_{n} \\rightarrow N \\left( \\mu,\\frac{50p(1-p)}{100} \\right) \\] and therefore the standard error of the mean (i.e. the uncertainty in the mean of those 100 draws) is \\(\\sqrt{\\frac{50p(1-p)}{100}}\\). mean_of_flip_outcomes&lt;-c() for (j in 1:1000) { vector_of_flip_outcomes&lt;-c() for (i in 1:100) { flip_outcome&lt;-rbinom(1,50,0.5) #this is the only line that changed vector_of_flip_outcomes&lt;-c(vector_of_flip_outcomes,flip_outcome) } mean_of_flip_outcomes&lt;-c(mean_of_flip_outcomes,mean(vector_of_flip_outcomes)) } hist(mean_of_flip_outcomes,breaks=30) sd(mean_of_flip_outcomes) ## [1] 0.3546413 Once again, the standard error is what we would have expected. Note that we could have written the code for this binomial experiment more compactly, as follows: mean_of_flip_outcomes&lt;-c() for (j in 1:1000) { flip_outcome&lt;-rbinom(100,50,0.5) #here we are doing all 100 draws at once mean_of_flip_outcomes&lt;-c(mean_of_flip_outcomes,mean(flip_outcome)) } hist(mean_of_flip_outcomes,breaks=30) sd(mean_of_flip_outcomes) ## [1] 0.3672288 And again, our estimate of the standard error is what we expect. The key with Bernoulli and the Binomial is just being really clear on what \\(\\textit{n}\\) is, because we have 1) the number of coin flips in each experiment (this determines what the variance \\(\\sigma^2\\) is in the numerator part of the CLT) and 2) the number of experiments (which determines what the denominator is in the CLT). Finally, while the CLT is a very general statement, do not forget the requirements that the mean and standard deviation exist (i.e. are finite). The Cauchy distribution, which is used all the time in atomic physics but rarely in ecology, has NO MEAN and NO SD – therefore, the CLT would not apply. "],["week-4-lecture.html", "7 Week 4 Lecture 7.1 Week 4 Readings 7.2 t-distribution 7.3 Chi-squared distribution 7.4 F distribution 7.5 Estimating confidence intervals - 5 special cases 7.6 To recap", " 7 Week 4 Lecture 7.1 Week 4 Readings For this week, I suggest reading Aho Sections 4.4.2, 5.1, 5.2, 5.3-5.3.4 and Logan Chapter 3. I also strongly recommend reading Sections 1.1, 2.1, 3.1, and 4.1 of Bolker Chapter 6. Note that most students dislike this chapter by Bolker but it covers some fundamental information that will be important throughout your statistics journey and many advanced students come around to Bolker’s way of thinking over time, so bear with it. 7.2 t-distribution Let \\(X_{1},X_{2},...,X_{n}\\) be independently and identically distributed random variables with mean \\(\\mu\\) and finite, non-zero variance \\(\\sigma^{2}\\), and the average of these variable \\(\\bar{X}\\) be defined as \\[ \\bar{X} = \\frac{1}{n}(X_{1}+X_{2}+X_{3}+...+X_{n}) \\] Then the Central Limit Theorem states: \\[ lim_{n \\rightarrow \\infty} \\bar{X} \\rightarrow N\\left(\\mu,\\frac{\\sigma^{2}}{n}\\right) \\] and therefore that \\[ lim_{n \\rightarrow \\infty} \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^{2}/n}} \\rightarrow N(0,1) \\] (Note that I could use \\(s^{2}\\) synonymously with \\(\\sigma^{2}\\) here because \\(s^{2}\\) is an unbiased estimator of \\(\\sigma^{2}\\), which means that \\(lim_{n \\rightarrow \\infty} s^{2}\\rightarrow \\sigma^{2}\\).) What is the distribution of \\(\\bar{X}\\) for finite n when we don’t know what the population variance \\(\\sigma^{2}\\) is and have to substitute \\(s^{2}\\) instead? The t-distribution. The t-distribution has one parameter, n-1, where n=the number of degrees of freedom. (Just like \\(\\mu\\) and \\(\\sigma^{2}\\) are the parameters of the Normal, the parameter of the t is called the “degrees of freedom”. Do not let the name confuse you. Also, keep in mind that there is no relationship between the number of draws from the t distribution and the parameter for the t distribution. You can have n=100 draws from a \\(t_{n-1=10}\\) or n=10 draws from a \\(t_{n-1=100}\\). The two “n”s here are different and independent of one another.) Notice that as n gets large, both the numerator and the denominator get small. The ratio of these two small numbers is the t-distribution, which is symmetric about 0 because the sample mean could end up either slightly too low or slightly too high. Where did we lose the degree of freedom? Click for Answer We lost one degree of freedom because we had to use the sample mean in the estimation of the sample variance. Sometimes \\(s^{2}\\) is smaller than \\(\\sigma^{2}\\), and sometimes it is larger, which explains why the t-distribution is flatter in the middle but fatter in the tails. For large sample sizes, the t-distribution is indistinguishable from the normal distribution, but for small sample sizes, the t-distribution is flatter in the middle and fatter in the tails. The p.d.f. looks like \\[ p.d.f. = f(x|p=n-1=d.o.f.) \\sim \\frac{\\Gamma(\\frac{p+1}{2})}{\\Gamma(\\frac{p}{2})}\\frac{1}{(p\\pi)^{1/2}}\\frac{1}{(1+\\frac{x^{2}}{p})^{(p+1)/2}} \\] Side note: I mentioned that there were some distributions for which I was not requiring you to memorize the pdf. Note that for some of the remaining pdfs (like the t, F, and chi-squared) I may give you the pdf and ask you to identify it. Fair warning!! The expected value is given by \\[ E[X]=0 \\mbox{ if p}&gt;1 \\] The variance is given by \\[ Var[X]=\\frac{p}{p-2} \\mbox{ if p}&gt;2 \\] 7.3 Chi-squared distribution The chi-squared distribution is related to the standard normal distribution as follows: Draw \\[ X_{1},X_{2},X_{3},...,X_{n} \\sim N(0,1) \\] Create a new quantity \\[ Y= X_{1}^{2}+X_{2}^{2}+X_{3}^{2}+...+X_{n}^{2} \\] \\[ Y \\sim χ_{n}^{2} \\] In other words, the sum of squared standard deviates is distributed as a \\(\\chi^{2}\\) distribution with n degrees of freedom. If we construct standard normal by transforming normal distributions with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), we can re-write the sum above as \\[ Y = \\frac{1}{\\sigma^{2}}(X_{1}-\\mu)^{2}+\\frac{1}{\\sigma^{2}}(X_{2}-\\mu)^{2}+\\frac{1}{\\sigma^{2}}(X_{3}-\\mu)^{2}+...+\\frac{1}{\\sigma^{2}}(X_{n}-\\mu)^{2} \\] \\[ Y = \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}{(X_{i}-\\mu)^{2}} \\sim \\chi_{n}^{2} \\] where \\(\\mu\\) is the population mean and \\(\\sigma^{2}\\) is the population variance. To reiterate, it is important to remember the following properties of the chi-squared distribution: If \\(X \\sim N(0,1)\\), then \\(X^{2} \\sim \\chi_{1}^{2}\\). In other words, the square of a standard normal random variable is a chi-squared random variable. chisqhist&lt;-hist((rnorm(1000,0,1))^2,freq=F,breaks=30,main=&quot;Chi-squared with df=1&quot;) lines(chisqhist$mids,dchisq(chisqhist$mids,1),col=&quot;red&quot;,lwd=2) If \\(X_{p1},X_{p2},X_{p3}... \\sim \\chi^{2}\\) distribution, then \\(\\sum_{i}{X_{pi}} \\sim \\chi^{2}_{\\sum{p_{i}}}\\). In other words, independent chi-squared random variables sum to a chi-squared random variable, and the degrees of freedom also add. df1&lt;-5 df2&lt;-2 df3&lt;-3 chisqhist2&lt;-hist(rchisq(1000,df1)+rchisq(1000,df2)+rchisq(1000,df3),freq=F,breaks=30,main=&quot;Sum of chi-squared distributions&quot;) lines(chisqhist2$mids,dchisq(chisqhist2$mids,df1+df2+df3),col=&quot;red&quot;,lwd=2) While above I used the symbol “n” to designate the parameter of the \\(\\chi^{2}\\) distribution because it is intuitive how that parameter is equal to the number of (squared) random variables are being summed, here I will switch to the letter \\(\\nu\\), which is slightly more traditional. The chi-squared distribution has an ugly pdf, which I include here for completeness. \\[ f(x \\mid \\nu=n) = \\frac{1}{2^\\frac{\\nu}{2} \\Gamma(\\frac{\\nu}{2})} x^{(\\frac{\\nu}{2}-1)} e^\\frac{-x}{2} \\] Remembering that the Gamma distribution is given by \\[ f(x|\\alpha,\\beta) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}x^{\\alpha-1}e^{-x/\\beta} \\] With this we can see that the \\(\\chi^{2}\\) distribution is a special case of the gamma distribution with \\(\\alpha = \\nu/2\\) and \\(\\beta=2\\). The shape of the \\(\\chi^{2}\\) distribution can be seen with a few examples: The expected value and variance are given by: \\[ E[X] = \\nu \\\\ Var[X] =2\\nu \\] I don’t derive these, but they follow from the E[X] and Var[X] of the gamma distribution. 7.4 F distribution As you will see in detail throughout the semester, the F distribution is critical to modelling variances. The F distribution is related to the \\(\\chi^{2}\\) distribution, in fact the ratio of two chi-squared distributions is the F-distribution. \\[ \\frac{\\chi^{2}_{n-1}/(n-1)}{\\chi^{2}_{m-1}/(m-1)} \\sim F_{n-1,m-1} \\] The F distribution has two parameters: the d.o.f. of both samples being compared (n-1 and m-1). The pdf of the F distribution is given by \\[ f(x|r=n-1,s=m-1)=\\frac{r\\Gamma(\\frac{1}{2}(r+s))}{s\\Gamma(\\frac{1}{2}r)\\Gamma(\\frac{1}{2}s)}\\frac{\\frac{rx}{s}^{\\frac{r}{2}-1}}{(1+\\frac{rx}{s})^{\\frac{r+s}{2}}} \\] (noting that I have used r=n-1 and s=m-1 to make the equation slightly more readable) and the shape of the F distribution looks like \\[ E[X] = \\frac{m-1}{m-3} \\] and the variance is just ugly. Why would we ever need to use the F-distribution? It turns out that the F-distribution is critical for assessing variances. Let’s say you have two datasets… Data set A, with n data points: \\[ X_{1},X_{2},X_{3},...,X_{n} \\sim N(\\mu_{A},\\sigma_{A}^{2}) \\] and data set B, with m data points: \\[ Y_{1},Y_{2},Y_{3},...,Y_{m} \\sim N(\\mu_{B},\\sigma_{B}^{2}) \\] Now you have two sets of data, both normally distributed but with different means and variances. You want to know whether the variance of data set A is bigger or smaller than the variance for data set B. We want to know the ratio \\(\\sigma_{A}^{2}/\\sigma_{B}^{2}\\) but all we have at our disposal is the ratio of the sample variances \\(s_{A}^{2}/s_{B}^{2}\\). It turns out the F-distribution can help us. We start with the fact that you know from our discussion a few minutes ago that \\[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n{(X_i-\\mu)^2} \\sim \\chi^2_n \\] If we have to estimate \\(\\mu\\) using \\(\\bar{X}\\) than we lose one degree of freedom \\[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n{(X_i-\\bar{X})^2} \\sim \\chi^2_{n-1} \\] but we know that \\[ s^2 = \\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{n-1} \\] Therefore \\[ s^2(n-1) = \\sum_{i=1}^n(X_i-\\bar{X})^2 \\] So we can write \\[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n(X_i-\\bar{X})^2 = \\frac{s^2(n-1)}{\\sigma^2} \\sim \\chi^2_{n-1} \\] Therefore, \\[ \\frac{s^2_A/\\sigma^2_A}{s^2_B/\\sigma^2_B} \\sim \\frac{\\chi^2_{n-1}/(n-1)}{\\chi^2_{m-1}/(m-1)} \\] is a ratio of scaled chi-squared distributions which is, as you now know, also an F-distribution: \\[ \\frac{s^2_A/\\sigma^2_A}{s^2_B/\\sigma^2_B} \\sim F_{n-1,m-1} \\] So, if we know the population variances (\\(\\sigma^2\\)) and we calculate the sample variances (\\(s^2\\)), than the ratio above has the F-distribution. While we don’t usually know the population variances, we are often testing a hypothesis that the two population variances are equal. If we assume that \\(\\sigma_{A}^{2}=\\sigma_{B}^{2}\\), than the expression above becomes \\[ \\frac{s^2_A}{s^2_B} \\sim F_{n-1,m-1} \\] This fact will be used in a few weeks when we discuss F-tests. (Stay tuned…) The t-, F-, and chi-squared distributions are all closely related. I will not work through the proofs here, but I will highlight a couple important relationships that will come in handy later. Relationship 1: We’ve already shown that the F distribution is a ratio of scaled chi-squared distributions. \\[ \\frac{\\chi^2_{n-1}/(n-1)}{\\chi^2_{m-1}/(m-1)} \\sim F_{n-1,m-1} \\] Relationship 2: If \\(X \\sim t_{n-1}\\), then \\(X^2 \\sim F_{1,n-1}\\). In words, this means that if you square a t-distributed variable, you get an F distributed variable. Once you decide on a distribution for your data, you need some way of estimating the best-fit parameters. This brings us to the next major topic of Biometry. 7.5 Estimating confidence intervals - 5 special cases We will now learn how to fit a model to data, that is, to estimate the parameter values for the distribution being used to model the data. Since parameter estimates are useless without their corresponding confidence intervals, we will also learn out to estimate confidence intervals for these parameters. Let’s review for a second what is meant by a confidence interval. A 95th percentile confidence interval is an interval estimated from the data that we are 95\\(\\%\\) certain contains the true but unknown value for the parameter. Because this interval is estimated from data, it has its own confidence interval. While we usually we don’t worry too much about this, keep in mind that a different dataset would also produce a different parameter estimate and a different confidence interval. How do we calculate confidence intervals when we are using a parametric distribution to model some data? We are going to use what we know about these distributions to derive an analytical expression for the distribution of the parameter of interest, and then use the quantiles of that distribution to calculate the appropriate confidence intervals (typically we use the \\(\\alpha⁄2\\) and 1-\\(\\alpha⁄2\\) confidence intervals, where \\(\\alpha\\)=0.05). The following five examples use the same methodology: Step #1: Start with an expression involving the parameter of interest that you know is true based on what we already know about the properties of the univariate distributions Step #2: Algebraically re-arrange that expression to isolate the parameter of interest on one side of the equation. You now have an expression for the statistical distribution describing the estimate of that parameter Step #3: Replace the distribution with the appropriate quantiles to generate the lower bound and upper bound of interest. The first four examples involve the following model \\[ X \\sim N(\\mu,\\sigma^{2}) \\] If we want to find confidence intervals for \\(\\mu\\), we can do so in the case where \\(\\sigma\\) is known (unrealistic) or where \\(\\sigma\\) is unknown. If we want to find confidence intervals for \\(\\sigma\\), we can do so in cases where \\(\\mu\\) is known (unrealistic) or where \\(\\mu\\) is unknown. These form the first four of the five examples we will cover. Example 1: Confidence intervals for \\(\\mu\\) assuming \\(\\sigma\\) is known. Let’s say that we have some data \\(X_{1},X_{2},X_{3},....,X_{n}\\). To use a concrete example, let’s say that these data represent the population growth rate of some bird colonies that I am monitoring. I am going to model growth rate by a Normal distribution: \\[ X \\sim N(\\mu,\\sigma^{2}) \\] Let’s assume for the moment that I already know what the variance of growth rate is, so \\(\\sigma\\) is already known and does not have to be estimated from the data, but I do need to use the data to estimate the mean of the distribution \\(\\mu\\). Let’s start with the model we have for the data \\[ X \\sim N(\\mu,\\sigma^{2}) \\] From this, follows \\[ \\bar{X} \\sim N(\\mu,\\sigma^{2}/n) \\] Note that this is exactly true if the original data come from a Normal distribution but it is also approximately true as long as the Central Limit Theorem holds (and it does for almost all distributions that you are likely to encounter in your research). Subtracting off the parameter \\(\\mu\\) from both sides we get \\[ \\bar{X}-\\mu \\sim N(0,\\sigma^{2}/n) \\] and \\[ \\bar{X}-\\mu \\sim \\sqrt{\\frac{\\sigma^{2}}{n}}N(0,1) \\] We can re-arrange this expression to get the distribution for \\(\\mu\\) \\[ \\mu-\\bar{X} \\sim \\sqrt{\\frac{\\sigma^{2}}{n}}N(0,1) \\] (Why can I just reverse the signs on the left hand side?) \\[ \\mu \\sim \\bar{X}+\\sqrt{\\frac{\\sigma^{2}}{n}}N(0,1) \\] We now have the distribution for the parameter \\(\\mu\\) in terms of quantities we already know (\\(\\sigma\\), which is assumed known, sample size n, and the average of the data \\(\\bar{X}\\)). We can use this expression to get confidence intervals for \\(\\mu\\), by plugging in the quantiles of the standard Normal distribution on the right hand side. The lower limit is defined by the [\\(\\alpha\\)/2] quantile of N(0,1), and the upper limit is defined by 1-[\\(\\alpha\\)/2] quantile of N(0,1). By tradition, we call the quantiles of the standard normal ‘’z’’. \\[ P(\\bar{X}+\\sqrt{\\frac{\\sigma^{2}}{n}}z_{\\alpha/2} \\leq \\mu \\leq \\bar{X}+\\sqrt{\\frac{\\sigma^{2}}{n}}z_{1-\\alpha/2}) = 0.95 \\] Because the N(0,1) is symmetric about zero, \\[ z_{\\alpha/2}= -z_{1-\\alpha/2} \\] Figure 7.1: Standard normal diagram illustrating the symmetry of the distribution and the quantiles for the left and right tails. this is the same as \\[ P(\\bar{X}-\\sqrt{\\frac{\\sigma^{2}}{n}}z_{1-\\alpha/2} \\leq \\mu \\leq \\bar{X}+\\sqrt{\\frac{\\sigma^{2}}{n}}z_{1-\\alpha/2}) = 0.95 = 1-\\alpha \\] This second version makes it easier to see that there is a quantity that is subtracted off \\(\\bar{X}\\) for the lower limit, and added to \\(\\bar{X}\\) for the upper limit. The confidence intervals in this case are symmetric about \\(\\bar{X}\\). Side note: This is called the “equal-tails” method, because we have constructed the confidence interval using equal amounts in each tail of the distribution. It is the most common way of constructing a confidence interval, but not the only way. Notice that \\[ z_{1-\\alpha/2}=qnorm(0.975)=1.96 \\] from which we arrive at a form for the confidence interval that might look familiar, that is, estimate \\(\\pm\\) 2 \\(\\times\\) SE (the standard error). Note: We often use this approach for the population mean even if we do not know the exact distribution because the CLT tells us that for large sample sizes, the mean of the distribution is normally distributed. But this approach is quite limited because it doesn’t tell us how to get estimates and CIs for other parameters, nor does it address the problem of estimates when the sample size is small and the CLT doesn’t apply. Example #2: Confidence intervals for \\(\\mu\\) assuming \\(\\sigma\\) is unknown. What about a (much more common) situation where you need to use the data to estimate both the mean and the variance? In this case we have to use the sample variance \\(s^{2}\\) to estimate the parametric variance \\(\\sigma^{2}\\). We start with the following fact \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{s^{2}}{n}}} \\sim t_{n-1} \\] Therefore \\[ \\bar{X}-\\mu \\sim \\sqrt{\\frac{s^{2}}{n}} t_{n-1} \\] \\[ P(\\bar{X}-\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[n-1]} \\leq \\mu \\leq \\bar{X}+\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[n-1]}) = 1-\\alpha \\] It is almost never the case that you know \\(\\sigma^{2}\\), so you should always use the quantiles of the t-distribution for building confidence intervals for \\(\\bar{X}\\). People incorrectly use the normal approximation because back in the day of tables, the normal distribution was easier. In the age of computers, no excuse – use the t-distribution. One final note - next week we will learn about t-tests and depending on the assumptions being made, the appropriate t-distribution may have a degree-of-freedom different from n-1, so I’m including the most general statement about the confidence interval for the mean of a Normally distriuted population here: \\[ P(\\bar{X}-\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[dof]} \\leq \\mu \\leq \\bar{X}+\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[dof]}) = 1-\\alpha \\] Where does the Central Limit Theorem come in here? In this case, if you are going to invoke the Central Limit Theorem (because your data are not actually Normally distributed) to get at confidence intervals for the mean of the distribution, you would need the sample size to be large anyways (because the CLT only says that the mean takes a Normal distribution in the limit of large sample sizes) and in this case \\(s^{2} \\rightarrow \\sigma^{2}\\) so you can use Case 1 above. Example #3: Estimating the confidence intervals for the variance \\(\\sigma^{2}\\) assuming \\(\\mu\\) is known. Based on the definition of a \\(\\chi^{2}\\) distribution, we know that if we have data \\(X \\sim N(\\mu,\\sigma^{2})\\), then \\[ \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2} \\sim \\chi^{2}_{n} \\] Note that this is only true if the underlying data are actually Normally distributed. (In other words, the parameter \\(\\sigma\\) is a parameter of the Normal distribution, so Examples 3-5 require that the original data are Normally distributed, otherwise the concept of putting CI on the parameter \\(\\sigma\\) doesn’t make sense.) Here we will use the fact that if \\(\\mu\\) is known, than the sample variance is calculated as \\[ \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2} = s^{2} \\] Notice that because \\(\\mu\\) is known and does not have to be estimated from the data, we retain all n degrees of freedom in the denominator. We can re-arrange this equation to get \\[ \\sum_{i=1}^{n}(X_{i}-\\mu)^{2} = ns^{2} \\] which we plug into our \\(\\chi^{2}\\) expression to yield \\[ \\frac{1}{\\sigma^{2}}ns^{2} \\sim \\chi^{2}_{n} \\] We then re-arrange a bit further to isolate \\(\\sigma^{2}\\) \\[ \\sigma^{2} \\sim \\frac{ns^{2}}{\\chi^{2}_{n}} \\] Now that we have the sampling distribution for \\(\\sigma^{2}\\) we can simply insert the appropriate quantiles to get the lower and upper limits of the confidence interval. Note that because the \\(\\chi^{2}\\) is in the denominator, the larger quantile is associated with the lower limit, and vice versa. \\[ P\\left(\\frac{ns^{2}}{\\chi^{2}_{(1-\\alpha/2)[n]}} \\leq \\sigma^{2} \\leq \\frac{ns^{2}}{\\chi^{2}_{(\\alpha/2)[n]}}\\right) = 1-\\alpha \\] Example #4: Estimating the confidence intervals for the variance \\(\\sigma^{2}\\) assuming \\(\\mu\\) is unknown. This example proceeds similarly to the one above, except we now need to estimate \\(\\mu\\) from the data, and this means we lose of degree of freedom both in the \\(\\chi^{2}\\) distribution and in the estimate of the sample variance. \\[ \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\sim \\chi^{2}_{n-1} \\] Why n-1? Because we lost a degree of freedom when we had to estimate \\(\\bar{X}\\). As before (except with \\(\\mu\\) unknown, and estimated by \\(\\bar{X}\\)) \\[ \\frac{1}{(n-1)}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} = s^{2} \\] so we can rearrange to show that \\[ \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}=\\frac{(n-1)s^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1} \\] Using the above expression, we re-arrange to get the sampling distribution of \\(\\sigma^{2}\\) \\[ \\sigma^{2} \\sim \\frac{(n-1)s^{2}}{\\chi^{2}_{n-1}} \\] from which we get the confidence intervals \\[ P(\\frac{(n-1)s^{2}}{\\chi^{2}_{(1-\\alpha/2)[n-1]}} \\leq \\sigma^{2} \\leq \\frac{(n-1)s^{2}}{\\chi^{2}_{(\\alpha/2)[n-1]}}) = 1-\\alpha \\] Example #5: Estimating the ratio of two variances \\(\\sigma^{2}_{A}/\\sigma^{2}_{B}\\) We start with this expression, which we derived above: \\[ \\frac{s^2_A/\\sigma^2_A}{s^2_B/\\sigma^2_B} \\sim F_{n-1,m-1} \\] We can rewrite this equation as \\[ \\frac{s^2_A/s^2_B}{\\sigma_{A}^{2}/\\sigma^2_B} \\sim F_{n-1,m-1} \\] Now we are going to invert the fraction on the left hand side in order to get the \\(\\sigma\\)s in the numerator. The right hand side is still an F-distribution, but because we have flipped numerator and denominator, we need to switch the order of the two parameters \\[ \\frac{\\sigma_{A}^{2}/\\sigma^2_B}{s^2_A/s^2_B} \\sim F_{m-1,n-1} \\] We isolate the ratio of \\(\\sigma\\)s in the numerator by multiplying the sample variances across, as follows \\[ \\frac{\\sigma_{A}^{2}}{\\sigma^2_B} \\sim \\frac{s^2_A}{s^2_B} F_{m-1,n-1} \\] Now, as before, we have the sampling distribution for the quantity we want, and we obtain the confidence intervals by substituting in the appropriate quantiles. \\[ \\frac{s_{A}^{2}}{s_{B}^{2}}F_{(\\alpha/2)[m-1,n-1]} \\leq \\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}} \\leq \\frac{s_{A}^{2}}{s_{B}^{2}}F_{(1-\\alpha/2)[m-1,n-1]} \\] 7.6 To recap We used the Normal distribution to derive the confidence interval for the parametric mean \\(\\mu\\) when \\(\\sigma\\) is known. We used the t-distribution to derive the confidence interval for the parametric mean \\(\\mu\\) when \\(\\sigma\\) is unknown (much more common). We used the \\(\\chi^{2}\\) distribution to derive the confidence interval for the parametric variance \\(\\sigma^{2}\\) when \\(\\mu\\) is known. We used the \\(\\chi^{2}\\) distribution to derive the confidence interval for the parametric variance \\(\\sigma^{2}\\) when \\(\\mu\\) is unknown (much more common). We used the F distribution to derive the confidence interval for the ratio of two variances. "],["week-4-lab.html", "8 Week 4 Lab", " 8 Week 4 Lab On Tuesday we discussed a few ways of getting confidence intervals for parameters under special cases where you have a limiting distribution that allows you to solve for it. Another, much more general, way of obtaining parameter estimates and confidence intervals is to use maximum likelihood. There are few more important subjects in applied statistics. Maximum likelihood and probability distributions are intimately related, for reasons that will become apparent. To serve as an example, we’ll use the Normal Distribution \\(N(\\mu,\\sigma^{2})\\): The probability density of the normal distribution is given by \\[ f(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(x-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] Remember that for variables that are i.i.d., the joint probability \\((X_{1},X_{2},X_{3})\\) is simply the product of the three p.d.f.s \\[ P(X_{1}\\cap X_{2} \\cap X_{3})=P(X_{1})\\times P(X_{2})\\times P(X_{3}) \\] \\[ f(X_{1},X_{2},...,X_{n}|\\mu, \\sigma) = \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] Taken as a probability density, this equation denotes the probability of getting unknown data \\({X_{1},X_{2},...,X_{n}}\\) given (|) the known distribution parameters \\(\\mu\\) and \\(\\sigma\\). However, it can be rewritten as a likelihood simply by reversing the conditionality: \\[ L(\\mu,\\sigma|X_{1},X_{2},...,X_{n}) = \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] The likelihood specifies the probability of obtaining the known data \\({X_{1},X_{2},...,X_{n}}\\) by a certain combination of the unknown parameters \\(\\mu\\) and \\(\\sigma\\). pdf: parameters known, data varies likelihood: data known, parameters vary In this way, the relationship between the joint probability density and the likelihood function is a bit like the relationship between the young woman and the old maid in this famous optical illusion: Figure 0.1: Optical illusion known as “My Wife and my Mother-in-Law”. Source: Wikimedia Commons Parameter estimates may be found by maximum likelihood simply by finding those parameters that make your data most likely (among all possible data sets). Conceptually, it helps to remember the Week #1 problem set. The likelihood of obtaining your exact set of colors was very small even when using the true underlying probabilities of each color. Likelihoods are always very small numbers - even the maximum likelihood estimates (MLEs) are very unlikely to produce your dataset, simply because there are so many possible datasets that could be produced. The MLEs are simply those parameters that make your dataset more likely than any other dataset. The magnitude of the likelihood means nothing. The actual value of the likelihood depends on the size of the “sample space” (how many possible datasets could you imagine getting?), so we can only assign meaning to the relative size of likelihoods among different combinations of parameter values. We can say whether one set of parameter values is more likely to be the “true” population values than other possible sets of parameter values. We will now discuss how to go about finding MLEs. First we will calculate the MLE for the normal parameters by hand, and then we will use two different methods of calculating the maximum likelihood estimators using R. First, we are going to do it manually. The likelihood function for X drawn from \\(N(\\mu,\\sigma^{2})\\) is \\[ L(\\mu,\\sigma|X_{1},X_{2},...,X_{n})= \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] Because likelihoods are very small, and we are only interested in relative values, we use the log-likelihood values which are easier to work with (for reasons that will become clear) The log-likelihood (LL) is \\[ LL = \\sum_{i}\\left(-\\frac{1}{2}log(2\\pi\\sigma^{2})-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right) \\] We want to maximize the LL, which is usually done by minimizing the negative-LL (NLL).To make the algebra easier, I will define \\(A=\\sigma^{2}\\). \\[ NLL=\\sum_{i}\\left(\\frac{1}{2}log(2\\pi A)+\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{A}\\right) \\] \\[ \\frac{\\partial NLL}{\\partial \\mu} = \\sum_{i}\\left(\\frac{-(X_{i}-\\hat{\\mu})}{A}\\right)=0 \\] Notice that when I set the left-hand side to 0, the notation changes from \\(\\mu\\) to \\(\\hat{\\mu}\\) because the MLE \\(\\hat{\\mu}\\) is that value that makes that statement true. \\[ \\frac{\\partial NLL}{\\partial \\mu} =\\sum_{i}-(X_{i}-\\hat{\\mu})=0=\\Sigma_{i}(X_{i}-\\hat{\\mu}) \\] \\[ n\\hat{\\mu}-\\sum_{i}X_{i}=0 \\] \\[ \\hat{\\mu}=\\frac{1}{n}\\sum_{i}X_{i} \\] Now we do the same for \\(A=\\sigma^{2}\\) \\[ NLL=\\sum_{i}\\left(\\frac{1}{2}log(2\\pi A)+\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{A}\\right) \\] \\[ \\frac{\\partial NLL}{\\partial A} = \\sum_{i}\\left(\\frac{1}{2}\\frac{2\\pi}{2\\pi\\hat{A}}-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\hat{A}^{2}}\\right)=0 \\] \\[ \\sum_{i}\\left(1-\\frac{(X_{i}-\\mu)^{2}}{\\hat{A}}\\right)=0 \\] \\[ n-\\frac{1}{\\hat{A}}\\sum_{i}\\left((X_{i}-\\mu)^{2}\\right)=0 \\] \\[ \\hat{A}=\\hat{\\sigma^{2}}=\\frac{1}{n}\\sum_{i}(X_{i}-\\mu)^{2} \\] The MLEs are not necessarily the best estimates, or even unbised estimates. In fact, the MLE for \\(\\sigma^{2}\\) is biased (the unbiased estimator replaces n with n-1). To do this in R, we have to write a function to define the NLL: neg.ll&lt;-function(x,mu,sigma2) { sum(0.5*log(2*pi*sigma2)+0.5*((x-mu)^2)/sigma2) } For the purposes of a simple example, lets generate some fake “data” by drawing random samples from a \\(N(\\mu=1,\\sigma=2)\\). x&lt;-rnorm(1000,mean=1,sd=2) mu.test.values&lt;-seq(-2,4,0.1) sigma2.test.values&lt;-seq(1,11,0.1) Next, we will make a matrix to store the values of the likelihood for a grid of potential \\(\\mu\\) and \\(\\sigma^{2}\\) values. likelihood.matrix&lt;-matrix(nrow=length(mu.test.values),ncol=length(sigma2.test.values)) Now we will search parameter space by brute force, calculating the likelihood on a grid of potential \\(\\mu\\) and \\(\\sigma^{2}\\) values. for (i in 1:length(mu.test.values)) { for (j in 1:length(sigma2.test.values)) { likelihood.matrix[i,j]&lt;-neg.ll(x,mu.test.values[i],sigma2.test.values[j]) } } We can plot the results using the functions ‘image’ and ‘contour’, and place on top of this plot the maximum likelihood as found by the grid search as well as the known parameter values. image(mu.test.values,sigma2.test.values,likelihood.matrix,col=topo.colors(100)) contour(mu.test.values,sigma2.test.values,likelihood.matrix,nlevels=30,add=T) max.element&lt;-which(likelihood.matrix==min(likelihood.matrix),arr.ind=T) points(mu.test.values[max.element[1]],sigma2.test.values[max.element[2]],pch=16,cex=2) points(1,4,pch=&#39;x&#39;,cex=2) Now we can plot the likelihood “slices”, which show cross sections across the search grid for fixed values of \\(\\mu\\) or \\(\\sigma^{2}\\). The right hand panels are just zoomed in versions of the left hand panels so you can see what the NLL function looks like in the vicinity of the MLE. par(mfrow=c(2,2)) plot(mu.test.values,likelihood.matrix[,max.element[2]],typ=&quot;b&quot;,col=&quot;darkred&quot;) plot(mu.test.values,likelihood.matrix[,max.element[2]],typ=&quot;b&quot;,xlim=c(0.5,1.5),ylim=c(2090,2160),col=&quot;darkred&quot;) plot(sigma2.test.values,likelihood.matrix[max.element[1],],typ=&quot;b&quot;,col=&quot;darkred&quot;) plot(sigma2.test.values,likelihood.matrix[max.element[1],],typ=&quot;b&quot;,xlim=c(3.5,4.5),,ylim=c(2080,2140),col=&quot;darkred&quot;) Notice how the likelihood curve for \\(\\sigma^{2}\\) is not symmetric. While a horizontal line drawn at some higher value (which represents the likelihood of an alternative hypothesis) yields a fairly symmetric confidence interval for \\(\\mu\\), the assymetry of the likelihood surface yields a highly assymetric confidence interval for \\(\\sigma^{2}\\). Confidence intervals do not have to be symmetric! However, immediately in the vicinity of the minimum, the likelihood surface is approximately quadratic and symmetric. We will come back to this in a second. In this case, the bivariate likelihood surface shows no correlation between \\(\\mu\\) and \\(\\sigma^{2}\\), but this is not always the case. Sometimes you get strong correlations among parameter estimates and get diagonal “ridges” in parameter space. In this case, it is important to distinguish between the likelihood profile and likellihood slices. (see Bolker!) The likelihood surface need not even have a single maximum; there could be several peaks which makes it difficult to define the MLE or its confidence intervals. If there are strong tradeoffs between parameter values, it is often better to discuss the MLEs in terms of a confidence region, which is the envelop of parameter space that you are [insert confidence limit here] percent certain contains the true combination of population parameter values. R has a function ‘optim’ which optimizes functions (and is thus much better than a simple grid search ‘brute force’ approach we just did) and is very handy for minimizing the LL. We take advantage of the R function that gives us the probability density function, which saves us having to hard code that into R. Make sure the use of ‘dnorm’ in the code below makes sense! neg.ll.v2&lt;-function(x,params) { mu=params[1] sigma=params[2] -sum(dnorm(x,mean=mu,sd=sigma,log=TRUE)) } Notice that I used the “log-TRUE” option to take the log inside the dnorm command, which saves me taking it later. I also had to pass the parameters as one variable since that is what ‘optim’ is expecting. Take a second to convince yourself that the neg.ll and neg.ll.v2 functions give the same answer. We still need a way to maximize the log-likelihood and for this we use the function ‘optim’: opt1&lt;-optim(par=c(1,1),fn=neg.ll.v2,x=x) opt1 ## $par ## [1] 1.033575 1.969664 ## ## $value ## [1] 2096.855 ## ## $counts ## function gradient ## 53 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL An even easier way is to use the ‘fitdistr’ command we already learned about, but the ‘optim’ function comes in handy all the time and is the only option you have when fitting non-tranditional distributions not covered by ‘fitdistr’. library(MASS) fitdistr(x,&quot;normal&quot;) ## mean sd ## 1.03397484 1.96976847 ## (0.06228955) (0.04404536) Notice that this function outputs the SE as well, whereas our function and ‘optim’ only give the MLE. Note that we can use the SE provided by ‘optim’ to calculate a confidence interval. For example, the 95th percentile CI would be given by \\((\\mbox{MLE estimate} - 1.96*SE, \\mbox{MLE estimate} + 1.96*SE)\\). But notice that using the SE in this way yields symmetric confidence intervals (LL and UL both the same distance from the MLE) and yet we just showed above that the actual CI may be asymmetric (like it was for \\(\\sigma\\)). This is because ‘optim’ is approximating the likelihood surface as a quadratic surface in the vicinity of the MLE, and this is only a good approximation if the likelihood surface is indeed quadratic and symmetric in the vicinity of the MLE. So ‘optim’’s assumption that the NLL profile is symmetric and well fit by a quadratic function is a reasonable function for the \\(\\mu\\) parameter but its a bad approximation for the \\(\\sigma\\) parameter. When you are doing “mission critical” analysis for your research, it is better to find the correct CI by using the likelihood profile and not the SE provided by ‘optim’. Keep in mind that the likelihood is a relative concept that only makes sense relative to other possible datasets. The absolute magnitude depends on the “sample space” of the data and sometimes even the maximum likelihood is a very small value. So all we can do is compare relative likelihoods. We now know how to use maximum likelihood to calculate the “best” parameter value (in the sense that it is the parameter value that maximizes the likelihood, or minimizes the negative log-likelihood.) But we know that parameter estimates by themselves are useless. We need to somehow calculate the uncertainty in our maximum likelihood estimate, i.e. the confidence interval. For example, if I have a one-parameter model (let’s call the parameter \\(\\theta\\)) and I want to find a confidence interval on \\(\\theta\\), then I want to ask the question: What values of estimated theta are reasonable under the null hypothesis that \\(\\hat{\\theta}\\) is the true value? We want to find a window of \\(\\theta\\) values around \\(\\hat{\\theta}\\) that are similar enough to \\(\\hat{\\theta}\\) that I would not reject the null hypothesis (that \\(\\hat{\\theta}\\) is the true value). Since \\(\\hat{\\theta}\\) is the one that minimizes the NLL surface, any other \\(\\theta\\) value would have a higher NLL. It turns out that, if one model represents a special case of another likelihood, the ratio between two likelihoods is related to a \\(\\chi^{2}\\)-distribution. \\[ -2log\\left(\\frac{\\widehat{\\mbox{L}_{r}}}{\\widehat{\\mbox{L}}}\\right) \\sim \\chi^{2}_{r} \\] where \\(r\\) is the number of “constrained” parameters for the smaller model. So if I compare a model where \\(\\theta\\) is allowed to vary (the original model) to one in which I fix \\(\\theta\\) arbitrarily (somewhere higher on the NLL surface), then the likelihood ratio (\\(\\times\\) -2) follows a \\(\\chi^{2}_{1}\\). Therefore, we can set a cut-off for the difference in log-likelihoods based on the 95th percentile of the \\(\\chi^{2}\\) distribution. qchisq(0.95,1)/2 ## [1] 1.920729 which equals 1.92 log-likelihood units if you are looking at only one parameter to be estimated. Therefore, if we have only one parameter, then we simply calculate the NLL over a range of parameter values, and find the CIs representing those parameter estimates which have &lt;1.92 increase in NLL from the MLE. The correct cut-off (i.e. the “depth” of the water filling the NLL) depends on the number of parameters being estimated as well as the desired width of the CI. The Poisson distribution you will look at in the problem set has only a single parameter \\(\\lambda\\) but the Normal distribution has two parameters (\\(\\mu\\) and \\(\\sigma\\)) and so we would use a different cut-off for these two distributions. Table 8.1: The difference between the minimum NLL at the MLE and the NLL associated with the upper and lower confidence interval limits. Parameters CI_90th CI_90th CI_90th 1 qchisq(0.90,1)/2 = 1.353 qchisq(0.95,1)/2 = 1.921 qchisq(0.99,1)/2 = 3.317 2 qchisq(0.90,2)/2 = 2.303 qchisq(0.95,2)/2 = 2.996 qchisq(0.99,2)/2 = 4.605 3 qchisq(0.90,3)/2 = 3.126 qchisq(0.95,3)/2 = 3.907 qchisq(0.99,3)/2 = 5.672 4 qchisq(0.90,4)/2 = 3.890 qchisq(0.95,4)/2 = 4.744 qchisq(0.99,4)/2 = 6.638 You will explore this more in the problem set. There is also more discussion of this in Bolker’s Chapter #6. While the above method (finding the bounds on the parameter within which the increase in the NLL is less than or equal to the threshold determined by the number of parameters and the desired critical value \\(\\alpha\\)) is the best method of finding the confidence intervals for a parameter, it’s worth coming back to the idea that “fitdistr” returns an estimate of the parameter standard errors. How does “fitdistr” do that? It turns out that the standard error is related to the inverse of the second derivative of the NLL right in the vicinity of the minimum where the curve is approximately quadratic. (The second derivative of the NLL has to be positive, do you see why?) Figure 8.1: Comparing steep and shallow NLL functions, and its impact on the estimated standard errors. A large second derivative is associated with a steep curve in the NLL, and this results in a small standard error (does this make sense?). Conversely, a very flat NLL that gently slopes up would be associated with a large standard error. So, if “fitdistr” estimates the standard error, why not just create the confidence intervals using [MLE-1.96\\(\\times\\)SE, MLE+1.96\\(\\times\\)SE]? Well, you could, but this assumes that the NLL is symmetric around its minimum and that the confidence interval is correspondingly symmetric around the maximum likelihood estimate. For some parameters for some distributions, this assumption will be fine, but for other parameters this may be a poor assumption and so the confidence intervals created using the standard error will be only approximate (and the approximation may not be that accurate, especially if you want 90th or 50th percentile confidence intervals [do you see why the approximation gets worse as you estimate larger CI intervals?]). Using the NLL function directly in the manner described about involves no approximation and will always be more correct. "],["week-5-lecture.html", "9 Week 5 Lecture 9.1 Week 5 Readings 9.2 Statistical power 9.3 The single sample t test 9.4 The unpaired two sample t test 9.5 Pooling the variances 9.6 The paired two sample t test 9.7 The F test 9.8 Comparing two proportions 9.9 Comparing two distributions 9.10 A bit more detail on the Binomial 9.11 Side-note about the Wald test 9.12 Chi-squared goodness-of-fit test 9.13 Chi-squared test of independence", " 9 Week 5 Lecture 9.1 Week 5 Readings For this week, I suggest reading Aho Sections 6.1-6.5 and Logan Chapter 6. I also strongly recommend reading Johnson (2002), Simberloff (1990), and Brosi and Biber (2009). There are additional readings that I highlight here because they may be of interest, or may provide additional perspectives on the themes of the week, including this paper on the irreproducibility of results based on p-values, this article on p-values from The Scientist Magazine, this paper on p-hacking, and the official statement on p-values by the American Statistical Association. 9.2 Statistical power In any hypothesis test, there are 4 possible outcomes. Figure 0.1: Type I and Type II errors B and C are the correct answer. If \\(H_{0}\\) is false, we want to reject the null hypothesis. A = Probability of rejecting \\(H_{0}\\) when \\(H_{0}\\) is true = Type I error = This is your \\(\\alpha\\)! D = Probability of not rejecting \\(H_{0}\\) when \\(H_{0}\\) is false = Type II error The power of a statistical test is defined as \\[ \\mbox{Power} = \\frac{C}{C+D} \\] In words, power is the probability of correctly rejecting the null hypothesis. Power calculations boil down to this unavoidable fact: When variance is high, you need larger samples. When the differences are small, you need larger samples. There is a tradeoff between Type I and Type II errors. As a general rule of thumb, people aim for Type I errors of 0.05 and Power = 0.80. Question for the class: Why do we worry more about Type I errors than Type II errors? When would a Type II error be really serious? (For example, failing to detect a disease agent…) Power calculations only make sense before an experiment, not after. If you found a significant effect, then clearly you had enough power, and if no significant effect, you clearly do not enough power. The main utility of power calculations is to get some intuition for the necessary sample size required while designing an experiement. In order to plan a study, you need to know how many samples you need to detect as significant difference of a certain magnitude. For a single sample comparison (against a fixed value) \\[ n = \\left(\\frac{\\sigma(z_{1-\\alpha}+z_{1-\\beta})}{\\delta}\\right)^{2} \\] You will be derive this as a group in lab this week but for now its enough to note that in the very common case that \\(\\alpha\\)=0.05 and power=1-\\(\\beta\\)=0.80, this can be approximated by \\[ n = \\frac{8\\sigma^{2}}{\\delta^{2}} \\] Note that some authors use the symbol \\(\\sigma^{2}\\) under the assumption that this is known (or assumed) prior to calculation, whereas others substitute \\(s^{2}\\) for \\(\\sigma^{2}\\) in recognition that this is calculated from the data, although they leave the formula unchanged. As long as you understand what is meant by the symbol, than it is not important which you use here. Example: If you want to be able to detect a change of 2.0 in a population with variance \\(\\sigma^{2}\\)=10.0, then we would need 8x10/\\(2^{2}\\) = 20 replicates. For a two sample comparison, the sample size (for each group!) required is simply twice \\[ n = 2\\left(\\frac{\\sigma(z_{1-\\alpha}+z_{1-\\beta})}{\\delta}\\right)^{2} \\] or \\[ n = \\frac{16\\sigma^{2}}{\\delta^{2}} \\] I won’t say a lot more about power calculations because I think that these calculations typify the old way of thinking because they depend so heavily on the arbitrary cut-off of “significance” that is so problematic in null hypothesis testing. Such power calculations can give you some general rules of thumb for whether an experiment is well designed, but even then it required some major assumptions about the size of the unknown variance that I’m not sure how helpful they are. A bit of review: What have we done so far this semester? We learned a bit about probability theory (joint probability, union and intersection, etc.). We learned about hypothesis testing and parameter estimation, using two randomization-based procedures that gave us the main idea behind these two key concepts We learned a suite of discrete and continuous probability distributions. These probability distributions allow us to describe the relative probability of various outcomes. We often use probability distributions to model our data. We learned various ways of expressing probability distributions (e.g., cumulative distributions, quantiles, etc.), and we learned how to calculate various properties of a distribution (e.g., the expected value E[X] and the variance Var[X]). We learned two different ways to estimate the parameters of a parametric distribution given data: A) Using the Central Limit Theorem and other “theory” about distributions; B) Using maximum likelihood. While we discussed four special cases where we can use Method A, Method B is MUCH more general, and can be used under all circumstances. Maximum likelihood is the “go-to” approach for fitting models to data if you wish to fit a parametric distribution to your data. An important sidebar: Now that we’ve covered maximum likelihood estimation, and we know how to use ML to estimate a parameter and its confidence intervals, lets cycle back to what we learned about randomization-based procedures in Week 2. In Week 2, we use bootstrap and jackknife to estimate the confidence intervals (or the standard error, if you prefer to think of it that way) for a parameter estimate. So why might we prefer one approach (ML or bootstrap/jackknife) over the other? ML is the standard approach if you know the joint likelihood of your data, as it is computationally much more efficient and it has well known properties because you have specified the distribution of your data using well described distributions. However, sometimes you don’t know the joint distribution of your dataset? Why not? It may be that each individual data point does not come from a known distribution (or, put another way, none of the known paramteric distributions fit your data well) It may be that each individual data point does come from a known distribution but your data are not independent and you are not able to describe the JOINT distribution of your data In these cases, we tend to fall back on non-parametric methods like bootstrap and jackknife, such as what was covered in Week 2. We will now learn how to combine our knowledge of all the univariate distributions with our understanding of hypothesis testing to test hypotheses about the parameters of a distribution. (In other words, we will learn how to use statistics to pose and answer quantitatively rigorous questions about our data.) First, a few reminders about statistical hypothesis testing… We frame decision-making in terms of a null and an alternative hypothesis: \\(H_{0}\\) vs. \\(H_{A}\\). Let’s say that we are measuring the growth rates of bird colonies, and as before, we use a Normal distribution \\(N(\\mu,\\sigma^2)\\). One reasonable null hypothesis might be \\(H_{0}:\\mu=0\\). So we collect data on several bird colonies, and we might find that our confidence interval for \\(\\mu\\) contains 0. In this case, we cannot reject the null hypothesis that \\(\\mu=0\\). But we also cannot affirmatively prove the null hypothesis. We simply “cannot reject” the null hypothesis. There are two reasons we “cannot reject” the null hypothesis. It might be that \\(\\mu\\) really is equal to 0. It is also possible that \\(\\mu \\neq 0\\) but we did not have enough data to shrink the confidence intervals sufficiently to reject the null hypothesis. In this latter scenario, we would say that we did not have enough “statistical power” to reject the null hypothesis. The six steps of null hypothesis testing are: Step #1: Specify a null hypothesis \\(H_{0}\\). Step #2: Specify an appropriate test statistic T. A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic. generic=\\(T\\)=f(X) specific=\\(T^{*}\\)=\\(T(X_{1},X_{2},...,X_{n})\\) We are familiar with some test statistics already, for example the use of \\(\\bar{X}\\) as a measure of the mean of a normally distributed population. Step #3: Determine the distribution of the test statistic under the null hypothesis \\(H_{0}\\). A test statistic is a statistical quantity that has a statistical distribution (\\(f(T│H_{0})\\)). Remember that this is the probability of obtaining the test statistic T GIVEN the null distribution, it is NOT \\(f(H_{0}│T)\\). The test statistic and its distribution under the null hypothesis form the statistical test. Test = Test statistic + Distribution of test statistic under \\(H_{0}\\). Step #4: Collect data and calculate \\(T^{*}\\). Collect data by taking random samples from your population and calculate the test statistic from the sample data. Step #5: Calculate a p-value. Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis \\(P(T^{*}│H_{0})\\)=p-value. Step #6: Interpret the p-value. Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected). Note that these steps apply for both parametric and non-parametric statistics. The same basic steps also apply whether the test statistic follows a known distribution under the null hypothesis or whether the distribution under the null hypothesis needs to be generated by randomization (randomization test). The basic idea underlying all statistical tests: What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the ‘’p-value’’). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis. p-value = P(data|H0) Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is NOT a statement about the probability that the null hypothesis is true. Not all tests are created equal!! Tests differ in their power to detect differences and their “efficiency”. The balance between power and efficiency depends on the specific situation; we will discuss this more next week. To get some practice in constructing and executing hypothesis tests, we are going to go over 4 classic and frequently used hypothesis tests: The t-test The F-test Test of binomial proportions Test of two distributions The t-test is used to make inference on the means of normally distributed variables. There are three varieties of the t-test – one to test whether the mean of some normally distributed variable is equal to some hypothesized value, one to test whether the means of two unpaired samples are equal, and one to test whether the means of two paired samples are equal. 9.3 The single sample t test (The t-test is often called the Student’s t-test, as it is named after the pseudonym under which the original paper was submitted. However, I will refer to it as just the t-test for simplicity.) The single sample t-test is used when you want to compare the mean of a distribution to a prescribed value. \\(H_{0}: \\mu = c\\) Let’s say we have normally distributed data: \\[ X \\sim N(\\mu,\\sigma^{2}) \\] The Central Limit Theorem says: \\[ \\bar{X} \\sim N(\\mu,\\sigma^{2}/n) \\] which means that \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^{2}/n}} \\sim N(0,1) \\] and if we do not know \\(\\sigma^{2}\\), that \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{s^{2}/n}} \\sim t_{n-1} \\] Remember that: \\[ H_{0}: \\mu=c \\] \\[ H_{A}: \\mu \\neq c \\] Therefore, under the NULL HYPOTHESIS \\[ \\frac{\\bar{X}-c}{\\sqrt{s^{2}/n}} \\sim t_{n-1} \\] In this case, our “test statistic” is \\(T=\\frac{\\bar{X}-c}{\\sqrt{s^{2}/n}}\\) and this test statistics follow the \\(t_{n-1}\\) distribution. In other words, under the null hypothesis (if \\(\\mu\\) really is c), the value of the test statistic for any particular dataset of size n will be drawn from this distribution. Let’s say when I actually calculate T for the data I have, I get \\(T^{*}\\). \\[ P(T \\leq (-|T^{*}|) \\mbox{ OR } T \\geq (|T^{*}|)|H_{0}) = p-value \\] If p&lt;0.05, we say that there is \\(&lt;5\\%\\) probability that we would obtain something as or more ‘extreme’ than \\(T^{*}\\) if the null hypothesis was true, we therefore REJECT the null hypothesis If p&gt;0.05, we say that there is a \\(&gt;5\\%\\) probability we would obtain something as or more ‘extreme’ than \\(T^{*}\\) if the null hypothesis is true, and therefore we DO NOT REJECT the null hypothesis. Note that the t-test assumes that the data are Normally distributed, but it turns out that the t-test is fairly robust to violations of this assumption. In fact, the Central Limit Theorem says that (with a few minor requirements) the means of data have as their limit (for large sample sizes) the Normal distribution, so the t-test is often valid even if the original data is not Normally distributed. Keep in mind that the t-test is intimately connected to the idea of calculating a confidence interval for the mean of a Normally distributed population. Last week, we derived this formula: \\[ P(\\bar{X}-\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[dof]} \\leq \\mu \\leq \\bar{X}+\\sqrt{\\frac{s^{2}}{n}}t_{(1-\\alpha/2)[dof]}) = 1-\\alpha \\] So we now can test a hypothesis regarding the value of the mean and we can generate confidence intervals on the true (but unknown) mean of the population we are studying. 9.4 The unpaired two sample t test The unpaired two-sample t test is used when you have data from two groups and you want to test whether these groups have the same mean value (i.e. \\(H_{0}: \\mu_{A} = \\mu_{B}\\). As before, we will assume that the data are at least approximately Normally distributed. \\[ X_{A} \\sim N(\\mu_{A},\\sigma_{A}^{2}) \\] \\[ X_{B} \\sim N(\\mu_{B},\\sigma_{B}^{2}) \\] If datasets A and B are independent (in other words, each draw from A is not correlated to a corresponding draw from B), then the difference between these two datasets is given by \\[ X_{A}-X_{B} \\sim N(\\mu_{A}-\\mu_{B},\\sigma_{A}^{2}+\\sigma_{B}^{2}) \\] Why? It’s worth going back to review the Algebra of Expectations, but in brief, when you add or subtract independent variables, their variances add. It follows (but I leave the few steps of algebra for you): \\[ \\bar{X_{A}}-\\bar{X_{B}} \\sim N(\\mu_{A}-\\mu_{B},\\frac{\\sigma_{A}^{2}}{n_{A}}+\\frac{\\sigma_{B}^{2}}{n_{B}}) \\] Note that in addition to making no assumption about sample variances, we make no assumption of equal sample sizes between the two datasets being compared. Therefore,the standard error of the difference between means is given by \\[ SE = \\sqrt{\\frac{\\sigma_{A}^{2}}{n_{A}}+\\frac{\\sigma_{B}^{2}}{n_{B}}} \\] which we have to estimate from the data using the sample variances \\(s^{2}\\): \\[ SE = \\sqrt{\\frac{s_{A}^{2}}{n_{A}}+\\frac{s_{B}^{2}}{n_{B}}} \\] So our test statistic in this case is \\[ T = \\frac{\\bar{X_{A}}-\\bar{X_{B}}}{\\sqrt{\\frac{s_{A}^{2}}{n_{A}}+\\frac{s_{B}^{2}}{n_{B}}}} \\] Now all we need is the distribution of the test statistics under the null hypothesis which is, by definition (this is a t-test after all) the t distribution. The degress of freedom for this distribution is a bit complicated: \\[ dof = \\frac{\\left(\\frac{s_{A}^{2}}{n_{A}}+\\frac{s_{B}^{2}}{n_{B}}\\right)^{2}}{\\frac{\\left[\\frac{s_{A}^{2}}{n_{A}}\\right]^2}{n_A-1}+\\frac{\\left[\\frac{s_{B}^{2}}{n_{B}}\\right]^2}{n_B-1}} \\] Note that there are some simpler formulas that apply if you assume equal sample size and/or equal variances. In the case of equal variances, you can pool the data to find a pooled estimate of the common variance \\(s^{2}\\), but I will not go into the details here. Note that the default for R is to assume unequal sample sizes and unequal variances. 9.5 Pooling the variances If the variances are assumed equal, this simplifies somewhat \\[ T = \\frac{\\bar{X_{A}}-\\bar{X_{B}}}{SE_{diff}} = \\frac{\\bar{X_{A}}-\\bar{X_{B}}}{\\sqrt{s^{2}_{pooled}\\left(\\frac{1}{n_{A}}+\\frac{1}{n_{B}}\\right)}} \\] How do we calculate \\(s^{2}_{pooled}\\)? \\[ s^{2}_{pooled} = \\frac{1}{n_{A}+n_{B}-2}(SS_{A}+SS_{B}) \\] where \\(SS_{A}\\) is the sums-of-squares for dataset A and \\(SS_{B}\\) is the sums-of-squares for dataset B. Why bother assuming the variances are equal? By combining the data in the estimate of their pooled variance, we get a better estimate of \\(s_{pooled}^{2}\\) than either \\(s_{A}^{2}\\) or \\(s_{B}^{2}\\). 9.6 The paired two sample t test Paired data occurs when each datapoint in set A corresponds to a datapoint in set B. Examples might be the strength of the left vs. right leg in a sample of individuals, or the blood sugar of husbands vs. wifes in a sample of married couples. In these cases, the question at hand is whether the difference between the two datasets is equal to some value or not. In other words, the null hypothesis is \\(H_{0}: X_{A}-X_{B} = c\\). The test statistic is the same as with the unpaired test \\[ T = \\frac{\\bar{X_{A}}-\\bar{X_{B}}}{SE_{diff}} \\] but now our calculation of the \\(SE_{diff}\\) changes because we are no longer assuming the two datasests are independent. In this case, the variances do not simply add, and the correct expression is \\[ X_{A}-X_{B} \\sim N\\left(\\mu_{A}-\\mu_{B},\\sigma^{2}_{A}+\\sigma^{2}_{B}-2Cov(A,B)\\right) \\] (Remember that \\(\\sigma^{2}_{A}\\) is just Var(A) or, put another way, the Cov(A,A). So this is the more general formula for the difference between two random Normally distributed variables, because if the two datasests are in fact independent, Cov(A,B)=0 and we end up with the simpler formula we introduced earlier. If you want more information on how to calculate the Covariance, you can jump ahead to the notes in Week 9. Working through the algebra a little \\[ \\bar{X_{A}}-\\bar{X_{B}} \\sim N(\\mu_{A}-\\mu_{B},\\frac{\\sigma^{2}_{A}+\\sigma^{2}_{B}-2Cov(A,B)}{n}) \\] So now the test statistic looks like \\[ T = \\frac{\\bar{X_{A}}-\\bar{X_{B}}}{SE_{diff}} \\] as before, but the \\(SE_{diff}\\) is given by \\[ SE_{diff} = \\sqrt{\\frac{\\sigma^{2}_{A}+\\sigma^{2}_{B}-2Cov(A,B)}{n}} \\] The last term represents the covariance between sample A and sample B. When this covariance is positive, the variance of the difference is reduced, which means that any given difference found between the two samples is actually more significant. Therefore, if the data are paired, a paired t-test will yield more significant results because it is a more powerful test for paired data. We will see this in action in lab on Wednesday. Note that a paired two-sample t-test is equivalent to a one-sample t-test where you create the one sample dataset by subtracting the paired data. In other words, \\[ Y = X_{A}-X_{B} \\] Now you can do a one-sample t-test on Y just as we did before. This is usually the easiest way to deal with paired data. The t-test does assume that the data are Normally distributed and, depending on the form we choose to use, we may be assuming that the variances are the same. Given a real dataset, you wouldn’t know for sure whether the variance are the same, and so you would need to first test whether the variances are the same (or, rather, whether you can reject the null hypothesis that they are the same). How do we test whether two datasests come from populations with the same variance - the F-test! (That’s coming now…) 9.7 The F test (This is often referred to as Fisher’s F test, but I will just stick with F test.) Its fairly obvious why someone would want to compare two means, but less obvious why you would want to compare two variances. As mentioned just a second ago, one of the biggest uses of the F-test is to determine whether two samples violate the equal-variances assumption underlying the t-test. Another major use is when comparing two nested models to determine which model fits a dataset better. (We will see this again when we cover ANOVA.) The null hypothesis for the F test is \\(H_{0}: \\sigma^{2}_{A} = \\sigma^{2}_{B}\\). Here I assume the dataset with the larger sample variance is sample A, so the implied alternative is \\(H_{A}: \\sigma^{2}_{A} &gt; \\sigma^{2}_{B}\\). In this case, I am testing whether we can reject the null hypothesis that the two parametric variances are actually the same (even if the sample variance of A is larger) [this is the one-tailed test, we will discuss the two-tailed test at the end]. Remember from last week: \\[ \\frac{s_{A}^{2}/\\sigma_{A}^{2}}{s_{B}^{2}/\\sigma_{B}^{2}} \\sim \\frac{\\chi^{2}_{n-1}/(n-1)}{\\chi^{2}_{m-1}/(m-1)} \\sim F_{n-1,m-1} \\] Therefore, under the null hypothesis, we get \\[ \\frac{s_{A}^{2}}{s_{B}^{2}} \\sim F_{n-1,m-1} \\] The left hand side of this equation, the ratio of the sample variances, is the test statistic for the F test and we call it the F statistic. Under the null distribution \\[ (F_{s}=\\frac{s_{A}^{2}}{s_{B}^{2}}|H_{0}) \\sim F_{n-1,m-1} \\] That that (by convention) the larger variance is placed in the numerator. Under the null hypothesis that the two samples are drawn from populations with the same variance, the F-statistic \\(F_{s}\\) is distributed as the F-distribution. The F-distribution is peaked around 1.0 because the two variances are samples estimates of the same quantity. The only difference between \\(s_{1}^{2}\\) and \\(s_{2}^{2}\\) is the sample size. The F-distribution depends on two degrees of freedom, n-1 and m-1. There exists a separate F-distribution for each combination of n and m. As before, we would reject the null hypothesis is our test statistic is an ‘extreme’ (and hence unlikely) value to get from the null distribution. What should we use as the critical value for this test? If you are testing Case 1: \\(H_{0}: \\sigma_{A}^{2}= \\sigma_{B}^{2}\\) vs. \\(H_{A}:\\sigma_{A}^{2} \\neq \\sigma_{B}^{2}\\) then you need a two-tailed test and you use the \\(\\alpha/2\\) quantile for the F distribution. If you are testing Case 2: \\(H_{0}: \\sigma_{A}^{2}= \\sigma_{B}^{2}\\) vs. \\(H_{A}:\\sigma_{A}^{2} &gt; \\sigma_{B}^{2}\\) then you need a one-tailed test and you use the \\(\\alpha\\) quantile for the F distribution. The R function for testing whether the variances of two samples are different is ‘’var.test’’. We will be using this function in lab this week. 9.8 Comparing two proportions There are a number of different ways to do the proportion test, but I will only expect you to know one. I will call the true underlying proportion \\(\\theta\\). In this case, \\(H_{0}: \\theta = \\theta_{0}\\), and for the two-tailed test, \\(H_{A}:\\theta \\neq \\theta_{0}\\). The approach we will go over is called the Wald test (see also 9.10), and it uses the large sample normal approximation to the Binomial distribution. Recall that \\[ \\mbox{lim}_{n \\rightarrow \\infty} Binom(n,\\theta) \\rightarrow N(n\\theta,n\\theta(1-\\theta)) \\] from which it follows (details below) \\[ \\mbox{lim}_{n \\rightarrow \\infty} \\hat{p} \\rightarrow N(\\theta,\\frac{\\theta(1-\\theta)}{n}) \\] So under the null hypothesis that \\(\\theta=\\theta_{0}\\) \\[ \\frac{\\hat{p}-\\theta_{0}}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\sim N(0,1) \\] (Technically, since we estimated the s.e. using the data, this is t-distributed, but since we are already assuming \\(n \\rightarrow \\infty\\), then we typically use the standard normal here.) Therefore, we can compare out test statistic against the standard normal to decide if the observed value is EXTREME, i.e. (if \\(T^{*}\\) is positive [see handout]) \\[ P(T \\geq T^{*}│H_{0})+P(T \\leq (-T^{*}) │H_{0}) = \\mbox{p-value for 2-tailed test} \\] Correspondingly, we can derived confidence intervals on the true proportion \\(\\theta\\). \\[ P\\left(\\hat{p}-z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\leq \\theta \\leq \\hat{p}+z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right) = 1-\\alpha \\] This approximation works if \\(\\theta \\sim 0.5\\), and sample size is large. Side note: Another test for proportions is called the “score” test, defined as \\[ \\frac{\\hat{p}-\\theta_{0}}{\\sqrt{\\frac{\\theta_{0}(1-\\theta_{0})}{n}}} \\sim N(0,1) \\] Notice that the s.e. here is a function of value \\(\\theta_{0}\\) in our null hypothesis. It turns out this is a “better test” (coverage more closely 1-\\(\\theta\\)) but it is less commonly used because the expression for the confidence interval is very complicated. Question: What can go wrong with the Wald test if \\(\\theta \\sim\\) 0 or 1? Answer: We can easily get confidence intervals that extend beyond (0,1). We can truncate the CIs at 0 or 1, but the “coverage” of the CI is no longer 1-\\(\\alpha\\). There are MANY methods statisticians have devised to obtain true 1-\\(\\alpha\\) CIs that do not go outside of (0,1) and which work for all \\(\\theta\\). I have posted a paper under “Interesting statistics papers” that address 7 methods, including the Wald approximation, but I will only expect you to know the Wald approximation. 9.9 Comparing two distributions There are many ways that we might compare two distributions. Many of these methods focus on the difference between the empirical cumulative density functions (i.e. \\(P(X \\leq X^{*})\\)), and they differ in the loss function (that is, the function used to weight differences of different magnitudes) used when comparing the CDFs. Here I introduce the Kolmogorov-Smirnov test (or the K-S test), which is one of the most common. The K-S test can be used to compare two empirical distributions OR an empirical distribution against a parametric distribution. The K-S statistic D is the maximum difference between the two CDFs or, more formally, \\[ D=sup_{x} |CDF_{1}(x)-CDF_{2}(x)| \\] Figure 1.1: Illustration of the empirical CDF (blue) and the CDF of the distribution being tested (red). Source: Wikimedia Commons The expected distribution of D under the null hypothesis is complicated and not one you need to know. We will go over R’s functions to do these tests on Thursday. 9.10 A bit more detail on the Binomial Above I skipped over some details about deriving the Binomial from the Bernoulli. Here I fill in those details. Let’s assume X follows a Bernoulli distribution: \\[ X \\sim Bernoulli(\\theta) \\] and that Y represents the sum of multiple Xs. \\[ Y = \\sum_{i=1}^{n}X_{i} \\sim Binomial \\] The Central Limit Theorem states that \\[ \\sum_{i=1}^{n}X_{i} \\rightarrow N(mean=\\sum_{i=1}^{n}E[X_{i}],variance = \\sum_{i=1}^{n}Var[X_{i}],) \\] If we add up all the coin flips (i.e., all the \\(X_{i}\\)) than get \\(n\\hat{p}\\) because the empirical probability (what we actually get out of the coin flip experiment, which we use to estimate the theoretical population parameter \\(\\theta\\)) of getting \\(X_{i}=1\\) is just \\(\\hat{p}\\) and we flip n coins. \\[ n\\hat{p} \\rightarrow N(mean=n\\theta,var=n\\theta(1-\\theta)) \\] So if we divide through by n \\[ \\hat{p} \\rightarrow N(mean=\\theta,var=\\theta(1-\\theta)/n) \\] in the limit that \\(n \\rightarrow \\infty\\). 9.11 Side-note about the Wald test We introduced the Wald test in the context of the binomial test, but the Wald test is a much more general test about the statistical significance of a maximum likelihood estimate. We often use the Wald test even when we haven’t proven that the estimate in question is actually the MLE. In the case of the proportion test, the MLE for the binomial parameter p is just \\(\\hat{p}\\) (=# heads)⁄n. The Wald test states that if you have a parameter estimate \\(\\hat{\\theta}\\) and you want to test it against the null hypothesis value \\(\\theta_{0}\\), you can use the following (approximate) relationship \\[ \\frac{\\hat{\\theta}-\\theta_{0}}{se(\\hat{\\theta})} \\sim N(0,1) \\] or, equivalently, \\[ \\frac{(\\hat{\\theta}-\\theta_{0})^{2}}{var(\\hat{\\theta})} \\sim \\chi^{2}_{1} \\] The standard error of a maximum likelihood estimate \\(se(\\hat{\\theta})\\) is usually approximated using the inverse of the second derivative of the log-likelihood (if you are interested, this is called the Fisher Information matrix). This makes intuitive sense because if the negative log-likelihood surface is steep around the minimum (and the second derivative large), the uncertainty about the MLE is small (narrow CI). I won’t get into detail about this, because often you have some knowledge of the parameter’s variance and can use the Wald test with little fuss or calculation. 9.12 Chi-squared goodness-of-fit test \\(H_{0}\\) is a table (need not be 2 \\(\\times\\) 2) of predicted probabilities such as Figure 1.2: 2 x 2 contingency table and you want to test whether the data are consistent with the null hypothesis of known probabilities. The chi-squared test statistic is \\[ X^{2} = \\sum_{\\mbox{cell} i}\\frac{(O_{i}-E_{i})^2}{E_{i}} \\] \\[ X^{2}|H_{0} \\sim \\chi^{2}_{(r \\times c) -1} \\] We have lost a single degree of freedom because the total sample size of the observed data constrains the value of one of the cells given the other three. When we have a single proportion (e.g., percentage of men vs. women in class) we can use this as an alternative to the binomial test we discussed in class. (When the expected frequencies are small, the binomial test is preferred over the chi-squared goodness of fit test.) If all you are given are marginal probabilities, you have to assume independence to get the probabilities for individual cells. 9.13 Chi-squared test of independence \\(H_{0}\\) is a table (need not be 2 \\(\\times\\) 2) of marginal probabilities Figure 1.3: 2 x 2 contingency table with marginal probabilities only or a table of observed data from which marginal probabilities can be calculated Figure 1.4: 2 x 2 contingency table The chi-squared test statistic is \\[ X^{2} = \\sum_{\\mbox{cell} i}\\frac{O_{i}-E_{i}}{E_{i}} \\] \\[ X^{2}|H_{0} \\sim \\chi^{2}_{(r-1) \\times (c-1)} \\] We have one degree of freedom from each row and column because the total sample size in each row and column is fixed by the marginal totals. This test is used to test whether the characters are independent. "],["week-5-lab.html", "10 Week 5 Lab 10.1 t-test 10.2 F-test 10.3 Comparing two proportions 10.4 Comparing two distributions", " 10 Week 5 Lab 10.1 t-test To learn more about R`s functions for doing the hypothesis tests introduced on Tuesday, we will simulate some data. Simulating data with known properties is always the best way of exploring a statistical test before applying it to your own data. Although we could simulate data using the “rnorm” function, we will use the “mvrnorm” function from the MASS package so we can consider the role of correlation between the two samples. library(MASS) data&lt;-mvrnorm(n=100,c(1,1.2),matrix(c(3,0,0,3),2,2)) The call here is similar to the one used for rnorm except now we are drawing two samples at the same time with the following properties: \\[ \\left[ \\begin{array}{c} X_{A} \\\\ X_{B} \\end{array} \\right] \\sim N\\left(\\left(\\begin{array}{c} \\mu_{A} \\\\ \\mu_{B} \\end{array}\\right),\\left(\\begin{array}{cc} Cov(A,A) &amp; Cov(A,B) \\\\ Cov(A,B) &amp; Cov(B,B) \\end{array}\\right)\\right) \\] Note that the variances have to be non-negative, and because Cov(A,B)=Cov(B,A), the covariance matrix has to be symmetric. (CAREFUL: mvrnorm does not enforce this!) sample.a&lt;-data[,1] sample.b&lt;-data[,2] plot(sample.a,sample.b) Notice that because Cov\\((X_{A},X_{B})\\)=0, the scatterplot has no trend. The t-test will answer the question, “Can we reject the null hypothesis that these two populations have the same mean”? t.test(sample.a,sample.b) ## ## Welch Two Sample t-test ## ## data: sample.a and sample.b ## t = 0.7491, df = 197.81, p-value = 0.4547 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3108335 0.6916329 ## sample estimates: ## mean of x mean of y ## 1.1667366 0.9763369 Notice that the degrees of freedom need not necessarily be an integer. R assumes unequal variances so we have to use the more complicated formula for the degrees of freedom. (In this situation, the number of degrees of freedom is not very intuitive; however, if you have \\(s_{A}^{2}=s_{B}^{2}\\) and \\(n_{A}=n_{B}\\), then the formula for the d.o.f. simplifies to 2n-2 which is what you would expect. ) Because we are simulating data, everyone’s output is going to look slightly different. In fact, some of us may get p&lt;0.05 and others p&gt;0.05. Here we are looking for a relatively small difference (compared to the standard deviations) with only 100 random draws. We would get more consistent results if we had larger sample sizes, larger mean differences, or smaller standard deviations. Checkpoint #1: The output of t.test includes 7 quantities - can you reproduce all 7 quantities based on what we learned on Tuesday? The default in R is to assume a two-sided test. In other words, by default, R tests \\[ H_{0}:\\mu_{1}=\\mu_{2}, H_{A}:\\mu_{1}\\neq\\mu_{2} \\] The lower limit of the two-sided 95th percentile CI represents the quantile below which 2.5\\(\\%\\) of the probability falls and vice versa. However, when we are thinking about a one-tailed test, we would put all 5\\(\\%\\) in one tail and then we would reject all test statistics that fall inside this one tail. If we are estimating effect sizes and their confidence intervals, the confidence intervals are no longer bounded on both sides. In fact, for a one-tailed test where we are only interested in the “greater” alternative hypothesis (for example, \\(X_{A}&gt;X_{B}\\)), there is only a lower bound (the upper bound is infinity) because the question then becomes whether the smallest value of the test statistic consistent with the data is below zero (in which case you cannot reject the null hypothesis and the difference is not significant) or above zero (in which case the values consistent with the data are all above zero and therefore you can reject the null hypothesis). In other words, when you are only interested in the alternative hypotheses \\(X_{A}&gt;X_{B}\\), you don’t care how much bigger \\(X_{A}\\) is than \\(X_{B}\\), you just want to know whether at its smallest the difference is still larger than zero. You can force R to do a one-sided test, either \\[ H_{A}:\\mu_{1}&gt;\\mu_{2} \\] or \\[ H_{A}:\\mu_{1}&lt;\\mu_{2} \\] by using the options t.test(sample.a,sample.b,alternative=&quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: sample.a and sample.b ## t = 0.7491, df = 197.81, p-value = 0.2273 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -0.2296421 Inf ## sample estimates: ## mean of x mean of y ## 1.1667366 0.9763369 t.test(sample.a,sample.b,alternative=&quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: sample.a and sample.b ## t = 0.7491, df = 197.81, p-value = 0.7727 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.6104415 ## sample estimates: ## mean of x mean of y ## 1.1667366 0.9763369 Try both of these options and see how the p-value and the confidence intervals change. Make sure you understand why they make sense. Remember, if you are going to use a one-tailed test, you should be prepared to accept that a large difference opposite to what was expected is pure random chance. Spend some time going back and experimenting with different sets of random variables. Make the means more or less different. Change the variances. Make sure you understand why the t-test results change as you alter the data. Checkpoint #2: What happens if you make the sample sizes smaller or larger? Now let’s see what happens when we simulate a new dataset with correlations between the two samples. data&lt;-mvrnorm(n=100,c(1,1.2),matrix(c(3,2,2,3),2,2)) sample.a&lt;-data[,1] sample.b&lt;-data[,2] plot(sample.a,sample.b) Notice that now the two samples are positively correlated. t.test(sample.a,sample.b,paired=T) ## ## Paired t-test ## ## data: sample.a and sample.b ## t = -0.70233, df = 99, p-value = 0.4841 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3420054 0.1631881 ## sample estimates: ## mean of the differences ## -0.08940864 Compare this with t.test(sample.a,sample.b,paired=F) ## ## Welch Two Sample t-test ## ## data: sample.a and sample.b ## t = -0.37918, df = 194.87, p-value = 0.705 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.5544505 0.3756332 ## sample estimates: ## mean of x mean of y ## 1.166717 1.256126 which is the same as t.test(sample.a,sample.b) ## ## Welch Two Sample t-test ## ## data: sample.a and sample.b ## t = -0.37918, df = 194.87, p-value = 0.705 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.5544505 0.3756332 ## sample estimates: ## mean of x mean of y ## 1.166717 1.256126 since the default is to assume unpaired samples. To convince ourselves of the formulas we learned for paired t-tests, we will define a new variable z&lt;-sample.a-sample.b mean(z) ## [1] -0.08940864 var(z) ## [1] 1.620605 var(sample.a)+var(sample.b)-2*cov(sample.a,sample.b) ## [1] 1.620605 Spend some time going back and experimenting with different sets of random variables. In particular, change the covariances (makign sure that the covariance matrix is symmetric and the variances positive). Make sure you understand why the t-test results change as you alter the data. Checkpoint #3: What happens to the t-test results when you make the correlation weak? 10.2 F-test One of the conditions for using the t-test is that the variances of the two samples are the same. One way of determining if the two variances are the same is the F-test. To review what we learned on Tuesday, under the null hypothesis that two populations have the same \\(\\sigma^{2}\\), the ratio of their sample variances will follow an F-distribution \\[ F_{s}=\\frac{s_{A}^{2}}{s_{B}^{2}} \\] If the value of the test statistic would be unusual for the F-distribution, we would reject the null hypothesis. (In other words, such a ratio of variances would be unlikely to occur by chance if the two variances were in fact the same.) As with the t-test, we can do a one-tailed or two-tailed F-test. The one-tailed F-test presumes that we want to test whether one variance is significantly larger than another variance, i.e. \\[ H_{0}: \\sigma_{A}^{2} = \\sigma_{B}^{2} \\] \\[ H_{\\mbox{A implied}}: \\sigma_{A}^{2} &gt; \\sigma_{B}^{2} \\] For a one-tailed test, we would assume that if the data were to show that \\[ s_{B}^{2} &gt; s_{A}^{2} \\] then we would chalk that up to random chance and we would NOT interpret that as \\[ \\sigma_{B}^{2} &gt; \\sigma_{A}^{2}. \\] If we are doing a one-tailed F-test where we are only interested in \\(\\sigma_{A}^{2} &gt; \\sigma_{B}^{2}\\), then our test statistic is \\(\\frac{s_{A}^{2}}{s_{B}^{2}}\\) (regardless of which is bigger), and we would set our critical value so that \\(\\alpha\\) of the distribution of the test statistic would fall into the right-hand tail of the F-distribution. We would then compare our test statistic \\(T^{*}\\) to the value of the (1-\\(\\alpha\\)) quantile of the F-distribution, and the question becomes \\[ \\frac{s_{A}^{2}}{s_{B}^{2}} &gt; F_{[1-\\alpha](n-1,m-1)}? \\] However, if we are open to either \\[ H_{\\mbox{A implied}}: \\sigma_{A}^{2} &gt; \\sigma_{B}^{2} \\] or \\[ H_{\\mbox{A implied}}: \\sigma_{A}^{2} &lt; \\sigma_{B}^{2} \\] then we want to do a two-tailed test. In this case, by tradition and to make life simpler, the larger sample variance always goes in the numerator, and we check this value against the critical value obtained by putting only 2.5% in the right hand tail. Now we are asking the question \\[ \\frac{s_{A}^{2}}{s_{B}^{2}} &gt; F_{[1-\\frac{\\alpha}{2}](n-1,m-1)}? \\] In other words, because the F-distribution is always non-negative, it is easier to put the larger sample variance in the numerator to create a test statistic \\(T^{*}&gt;1\\) and then compare that to \\(F_{[1-\\frac{\\alpha}{2}](n-1,m-1)}\\) than it is to worry about computing both critical values (one for a ratio \\(&lt;1\\) and a separate one for a ratio \\(&gt;1\\)). Let’s explore this by simulating some data sample.a&lt;-rnorm(100,mean=5,sd=4) sample.b&lt;-rnorm(100,mean=2,sd=3) Lets assume we are only interested in the implied alternative hypothesis \\[ H_{A implied}: \\sigma_{A}^{2} &gt; \\sigma_{B}^{2} \\] We can calculate this one-tailed F-test by hand first var.A&lt;-var(sample.a) var.B&lt;-var(sample.b) F.ratio&lt;-var.A/var.B F.ratio ## [1] 1.73414 Note that we didn’t check that var.A was actually bigger than var.B. Because we are only interested in a one-tailed test, we want var.A in the numerator and we will compare that to the right-hand side of the F-distribution. Now we know what the F-ratio of our simulated data is, but we don’t know what the critical value of the test statistic is for the ratio to be considered “significant”. qf(p=0.95,df1=99,df2=99) ## [1] 1.394061 Checkpoint #4: Do you understand why df1=99 and df2=99? We see that the F-ratio for our data is greater than the critical value for the 95th percentile. Therefore we can say that the two samples are unlikely to come from populations from the same variance. We can also flip the question around to ask: “What is the probability of obtaining this F-ratio if the null hypothesis were true?” 1-pf(F.ratio,df1=99,df2=99) ## [1] 0.003314422 and we see that it is small (&lt;0.05). Remember, pf gives the cumulative probability of getting any value less than or equal to the percentile being queried, so 1-pf gives the probability of getting a value as or more extreme. R gives us an easier way to test equality of variances (the F test): var.test(sample.a,sample.b) ## ## F test to compare two variances ## ## data: sample.a and sample.b ## F = 1.7341, num df = 99, denom df = 99, p-value = 0.006629 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.166802 2.577338 ## sample estimates: ## ratio of variances ## 1.73414 The output of var.test includes 7 quantities - make sure you can calculate each and every one of these quantities. How did R arrive at its p-value? Notice that it is twice what we calculated when doing the F-test by hand. The reason is that R’s default is to do a two-sided test, and we only calculated the probability that you got a ratio as big or larger than the \\(F^{*}\\). You can get the two-tailed probability by either doubling that value or by adding the probability that you would get an F-ratio smaller than 1/\\(F^{*}\\). Notice also that to get the confidence intervals on the F-ratio, you multiply by the quantiles of the F-distribution. qf(0.025,df1=99,df2=99)*F.ratio ## [1] 1.166802 qf(0.975,df1=99,df2=99)*F.ratio ## [1] 2.577338 10.3 Comparing two proportions On Tuesday I introduced two different test statistics that are frequently used to test a binomial proportion. The Wald test is based on the idea that near the minimum of the likelihood function (in the vicinity of the MLE), deviations from the MLE will be normally distributed. Therefore, the Wald test is a very general statement \\[ \\frac{\\hat{\\theta}-\\theta_{0}}{se(\\hat{\\theta})}\\sim N(0,1) \\] which we can apply in this specific case as \\[ \\frac{\\hat{p}-\\theta_{0}}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\\sim N(0,1) \\] The ‘score’ test uses the value of the binomial proportion under the null hypothesis in place of the empirical probability in estimating the standard error in the denominator. \\[ \\frac{\\hat{p}-\\theta_{0}}{\\sqrt{\\frac{\\theta_{0}(1-\\theta_{0})}{n}}}\\sim N(0,1) \\] We can easily invert the Wald test to arrive at the following confidence interval for p: \\[ P\\left(\\hat{p}-z_{1-(\\alpha/2)}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\leq \\theta \\leq \\hat{p}+z_{1-(\\alpha/2)}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right)=1-\\alpha \\] However, if you look at the score test, you see that it is quite difficult to invert and isolate \\(\\hat{p}\\). This is why the confidence intervals for the score test are so complicated. We will not go into more detail here because in practice you would not be calculating it by hand anyways (but the general idea behind inverting the test to get a CI should be clear!). We will now explore R’s functions for estimating the probability \\(\\hat{p}\\) (and its confidence interval) for a binomial distribution. First we simulate some data to work with heads&lt;-rbinom(1,size=100,prob=0.5) heads ## [1] 49 Now we will use R’s function for the proportion test prop.test(heads,100) #continuity correction true by default ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 0.01, df = 1, p-value = 0.9203 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3894281 0.5913488 ## sample estimates: ## p ## 0.49 The continuity correction is used because we are approximating a discrete distribution (the binomial) with its normal approximation. While generally recommended, it rarely makes a large difference and for transparency I suggest we turn it off. prop.test(heads,100,correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 0.04, df = 1, p-value = 0.8415 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3942200 0.5865199 ## sample estimates: ## p ## 0.49 To make sure we understand this, let’s look at the help file for prop.test ?prop.test There are a couple of additional pieces of information you need to understand and/or reconstruct the output for prop.test, and unfortunately the help file for this function is fairly unhelpful. R uses the score test, so the confidence interval computed by R won’t be quite the same as what we would calculate using the Wald test method introduced in lecture. (But they will be close.) The test statistic that prop.test reports is the square of the score test statistic introduced in lecture \\[ \\left[\\frac{\\hat{p}-\\theta_{0}}{\\sqrt{\\frac{\\theta_{0}(1-\\theta_{0})}{n}}}\\right]^{2} \\] This is fine. Remember that there is no “correct” test statistic. The only challenge here is that we need to find the distribution of the test statistic under the null hypothesis. In this case, we remember from Week 3 that if we have a random variable X \\[ X \\sim N(0,1) \\] then the square of X \\[ X^{2} \\sim \\chi^{2}_{1} \\] Therefore, the test statistic output by prop.test (assuming \\(H_{0}: \\theta_{0}=0.5\\)) is n&lt;-100 theta0&lt;-0.5 test.statistic&lt;-(((heads/n)-theta0)/(sqrt(theta0*theta0/n)))^2 test.statistic ## [1] 0.04 and the p-value is given by 1-pchisq(test.statistic,df=1) ## [1] 0.8414806 Notice that because \\(X^{2}\\) is always positive, the test is a one-tailed test (“extreme” values of this test statistic are always large and positive). We could get the same result comparing the original (non-squared) test statistic against the Standard Normal, as long as you make sure to do the two-tailed test (“extreme” for X includes large and positive and large and negative). 2*(1-pnorm(sqrt(test.statistic))) ## [1] 0.8414806 Notice in the helpfile that R will return a clipped version of the confidence interval, so the confidence interval is bounded [0,1]. There is another function in R called “binom.test” which does an “exact test” based on the actual binomial distribution, rather than the normal approximation to it we introduced on Tuesday. Notice that there is no continuity correction because we are not approximating the binomial distribution with the normal distribution. binom.test(heads,100) ## ## Exact binomial test ## ## data: heads and 100 ## number of successes = 49, number of trials = 100, p-value = 0.9204 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.3886442 0.5919637 ## sample estimates: ## probability of success ## 0.49 One of the caveats in using the Wald method for binomial proportion is that it can give unrealistic values for the CIs when the underlying probability is very close to either 0 or 1 (especially if the sample size is small). To see this, lets calculate the Wald confidence intervals by hand assuming we got 2 successes out of 100 trials. p.hat&lt;-2/100 n&lt;-100 se&lt;-sqrt(p.hat*(1-p.hat)/n) z&lt;-qnorm(0.975) LL&lt;-p.hat-se*z UL&lt;-p.hat+se*z LL ## [1] -0.007439496 UL ## [1] 0.0474395 The lower limit of the CI is actually negative which makes no sense since the binomial proportion is bounded [0,1]. We can clip the CI to have a LL of zero, but there is no guarentee that the “coverage” of the CI contains 95%. To see whether this is true, lets simulate multiple datasets with a true underlying probability of 0.02, calculate confidence intervals from those random samples, and see what proportion of the CIs actually contain the true value (0.02). data&lt;-rbinom(n=1000,size=100,prob=0.02) #draw 1000 random datasets-100 trials each p.hat&lt;-data/100 #vector of empirical probabilities p.hat n&lt;-100 se&lt;-sqrt(p.hat*(1-p.hat)/n) z&lt;-qnorm(0.975) LL&lt;-p.hat-se*z #vector of LLs UL&lt;-p.hat+se*z #vector of UL sum(as.numeric((0.02&lt;=UL)&amp;(0.02&gt;=LL)))/1000 #proportion of times the CI include the true value ## [1] 0.882 We see that the CIs are actually too narrow! The Wald test is commonly used but as we have demonstrated, it is not very good in practice. R and its packages make it easy to get better CIs for a binomial proportion and in real analysis, you should use these more sophisticated methods. One of the better functions for doing tests of binomial proportions is provided by Dr. Dirk Enzmann. He has posted a function online that calculates 11 different versions of the binomial confidence interval. I have pasted the function here: # Calculate confidence intervals for a single proportion according to one of 11 # methods (default: log likelihood ratio) for a level of alpha (default: .05) # and with significant decimals (default: 3) given a proportion and the sample # size (see: Newcombe, 1998; Brown et al., 2001). Except for the asymptotic and # Agresti&#39;s methods all methods can yield asymmetric intervals around p: # # Methods are: # &#39;likelihood&#39; : log likelihood ratio interval (default) # &#39;asymptotic&#39; : simple &quot;classical text book&quot; or Wald method interval # &#39;asymptotic.cc : asymptotic with Yates&#39; continuity correction # &#39;score&#39; : score or Wilson method interval (= prop.test(...,correct=F), # i.e. without Yates&#39; continuity correction) # &#39;score.cc&#39; : score method with continuity correction (= default of # prop.test()) # &#39;binomial&#39; : &quot;exact&quot; or Clopper-Pearson method interval (= binom.test()) # &#39;binomial.midp&#39; : binomial mid-p &quot;quasi-exact&quot; interval # &#39;Jeffreys&#39; : Jeffreys prior interval # &#39;Agresti-Coull&#39; : Agresti-Coull method adding z?/2 successes # &#39;Agresti.2_2 : Agresti-Coull method adding 2 successes and 2 failures # &#39;logit&#39; : logit interval # # References: # Brown, L.D., Cai, T.T. &amp; DasGupta, A. (2001). Interval estimation for a # binomial proportion. Statistical Science, 16, 101-133. # Newcombe, R.G. (1998). Two-sided confidence intervals for the single propor- # tion: Comparison of seven methods. Statistics in Medicine, 17, 857-872. prop.CI = function(p,n,alpha=.05,digits=3,method=&quot;likelihood&quot;) { # Asymptotic (or Wald) interval: z = qnorm(1-alpha/2) if (method==&#39;asymptotic&#39;) { se = sqrt(p*(1-p)/n) CI = list(p=p,CI=c((p-z*se),(p+z*se)),n=n,level=1-alpha,method=method) } # Asymptotic (or Wald-test) CIs with continuity correction: if (method==&#39;asymptotic.cc&#39;) { se = sqrt(p*(1-p)/n) CI = list(p=p,CI=c((p-z*se-1/(2*n)),(p+z*se+1/(2*n))),n=n,level=1-alpha, method=method) } # Score test (or Wilson) interval: if (method==&#39;score&#39;) { term1 = 2*n*p + z**2 term2 = z*sqrt(z**2 + 4*n*p*(1-p)) term3 = 2*(n + z**2) CI = list(p=p,CI=c((term1-term2)/term3,(term1+term2)/term3),n=n, level=1-alpha,method=method) } # Score test (or Wilson) interval with continuity correction: if (method==&#39;score.cc&#39;) { term1 = 2*n*p + z**2 if (p&gt;0) { term2L = z*sqrt(z**2 - 2 - 1/n + 4*p*(n*(1-p)+1)) } if (p&lt;1) { term2U = z*sqrt(z**2 + 2 - 1/n + 4*p*(n*(1-p)-1)) } term3 = 2*(n + z**2) if ((p&gt;0) &amp; (p&lt;1)) { CI = list(p=p,CI=c((term1-1-term2L)/term3,(term1+1+term2U)/term3),n=n, level=1-alpha,method=method) } if (p==0) { CI = list(p=p,CI=c(0,CIU=(term1+1+term2U)/term3),n=n,level=1-alpha, method=method) } if (p==1) { CI = list(p=p,CI=c((term1-1-term2L)/term3,1),n=n,level=1-alpha, method=method) } } # Binomial (&#39;exact&#39; or Clopper-Pearson) interval: if (method==&#39;binomial&#39;) { conf.int=binom.test(round(p*n),n,conf.level=1-alpha)$conf.int CI = list(p=p,CI=c(conf.int[1],conf.int[2]),n=n,level=1-alpha,method=method) } # Binomial mid-p quasi-exact interval: if (method==&#39;binomial.midp&#39;) { x = round(p*n) uplim = 1 lowlim = 0 if (x == 0) uplim = 1-alpha**(1/n) if (x == n) lowlim = alpha**(1/n) if (x &gt; 0 &amp; x &lt; n) { pp = seq(0.000001,0.999999,length=100000) a2 = 0.5*pbinom(x-1,n,pp) + 0.5*pbinom(x,n,pp) uplim = pp[max(which(a2&gt;(alpha/2)))] lowlim = pp[min(which(a2&lt;(1-alpha/2)))] } CI = list(p=p,CI=c(lowlim,uplim),n=n,level=1-alpha,method=method) } # Log-likelihood-ratio interval: if (method==&#39;likelihood&#39;) { x = round(p*n) k = -qchisq(1-alpha,1)/2 pp = seq(0.000001,0.999999,length=100000) lik = dbinom(x,size=n,pp) logLR = log(lik/max(lik)) conf.int=range(pp[logLR &gt; k]) CI = list(p=p,CI=c(conf.int[1],conf.int[2]),n=n,level=1-alpha,method=method) } # Jeffreys prior interval: if (method==&#39;Jeffreys&#39;) { x = round(p*n) conf.int=qbeta(c(alpha/2,1-alpha/2),x+.5,n-x+.5) CI = list(p=p,CI=c(conf.int[1],conf.int[2]),n=n,level=1-alpha,method=method) } # Agresti-Coull (adding z?/2 successes) interval # (see: http://www.stat.ufl.edu/~aa/cda/R/one_sample/R1/index.html ) if (method==&#39;Agresti-Coull&#39;) { x = round(p*n) tr = z**2 suc = tr/2 pp = (x+suc)/(n+tr) se = sqrt(pp*(1-pp)/(n+tr)) CI = list(p=p,CI=c((pp-z*se),(pp+z*se)),n=n,level=1-alpha,method=method) if (CI$CI[1] &lt; 0) CI$CI[1]=0 if (CI$CI[2] &gt; 1) CI$CI[2]=1 } # Agresti-Coull (adding 2 successes and 2 failures) interval: # (see: http://www.stat.ufl.edu/~aa/cda/R/one_sample/R1/index.html ) if (method==&#39;Agresti.2_2&#39;) { x = round(p*n) pp = (x+2)/(n+4) se = sqrt(pp*(1-pp)/(n+4)) CI = list(p=p,CI=c((pp-z*se),(pp+z*se)),n=n,level=1-alpha,method=method) if (CI$CI[1] &lt; 0) CI$CI[1]=0 if (CI$CI[2] &gt; 1) CI$CI[2]=1 } # Logit interval: if (method==&#39;logit&#39;) { lambda = log(p/(1-p)) x = round(p*n) V = n/(x*(n-x)) conf.int = (c(lambda - z*sqrt(V),lambda + z*sqrt(V))) conf.int = exp(conf.int)/(1+exp(conf.int)) CI = list(p=p,CI=c(conf.int[1],conf.int[2]),n=n,level=1-alpha,method=method) } cat(&#39;p ? &#39;,100*(1-alpha),&#39;%-CI = &#39;,round(p,digits),&#39; (&#39;, round(CI$CI[1],digits),&#39;; &#39;,round(CI$CI[2],digits),&#39;)\\n&#39;,sep=&#39;&#39;) CI } # The following example reproduces the data of Table I of Newcombe (1998, # p. 861). Other methods to calculate confidence intervals of a single propor- # tion as discussed in Brown et al. (2001) will also be demonstrated. # # To run, de-comment the following line: # source(&quot;http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Software/ex_prop.CI.r&quot;) I will use this code to calculate the confidence intervals for the binomial proportion if I get 2 successes out of 100 trials. ci.1C1 =prop.CI(2/100,100,digits=4,method=&#39;asymptotic&#39;)$CI ## p ? 95%-CI = 0.02 (-0.0074; 0.0474) ci.2C1 =prop.CI(2/100,100,digits=4,method=&#39;asymptotic.cc&#39;)$CI ## p ? 95%-CI = 0.02 (-0.0124; 0.0524) ci.3C1 =prop.CI(2/100,100,digits=4,method=&#39;score&#39;)$CI ## p ? 95%-CI = 0.02 (0.0055; 0.07) ci.4C1 =prop.CI(2/100,100,digits=4,method=&#39;score.cc&#39;)$CI ## p ? 95%-CI = 0.02 (0.0035; 0.0774) ci.5C1 =prop.CI(2/100,100,digits=4,method=&#39;binomial&#39;)$CI ## p ? 95%-CI = 0.02 (0.0024; 0.0704) ci.6C1 =prop.CI(2/100,100,digits=4,method=&#39;binomial.midp&#39;)$CI ## p ? 95%-CI = 0.02 (0.0034; 0.0645) ci.7C1 =prop.CI(2/100,100,digits=4)$CI ## p ? 95%-CI = 0.02 (0.0034; 0.0605) ci.8C1 =prop.CI(2/100,100,digits=4,method=&#39;Jeffreys&#39;)$CI ## p ? 95%-CI = 0.02 (0.0042; 0.0626) ci.9C1 =prop.CI(2/100,100,digits=4,method=&#39;Agresti-Coull&#39;)$CI ## p ? 95%-CI = 0.02 (0.0011; 0.0744) ci.10C1=prop.CI(2/100,100,digits=4,method=&#39;Agresti.2_2&#39;)$CI ## p ? 95%-CI = 0.02 (0.0015; 0.0754) ci.11C1=prop.CI(2/100,100,digits=4,method=&#39;logit&#39;)$CI ## p ? 95%-CI = 0.02 (0.005; 0.0764) We see that most of these methods correctly constrain the CIs to be bounded by [0,1]. 10.4 Comparing two distributions To perform the K-S test to compare two distributions, we need to simulate some data. We will plot their (empirical) cumulative distributions using the function “ecdf”. x&lt;-rnorm(50,mean=0,sd=1) y&lt;-runif(30,0,1) ks.test(x,y) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x and y ## D = 0.56667, p-value = 4.58e-06 ## alternative hypothesis: two-sided plot(ecdf(x)) lines(ecdf(y),col=&quot;red&quot;) "],["week-6-lecture.html", "11 Week 6 Lecture 11.1 Week 6 Readings 11.2 Family-wise error rates 11.3 How do we sort the signal from the noise?", " 11 Week 6 Lecture 11.1 Week 6 Readings For this week, I suggest reading Aho Sections 6.5. In class this week, we will also discuss Bender and Lange (2001), Berger and Berry (1988), Cohen (1994), Gawande (1999), Johnson (1999), and Nuzzo (2014). Yes, there are a lot of papers this week, but I would encourage you to read efficiently - get the gist but don’t get bogged down in the details. There is an overarching theme among these papers I want to make sure you really take to heart. There are additional readings that I highlight here because they may be of interest, or may provide additional perspectives on the themes of the week, including this paper on the problems of null hypothesis testing, this cheeky analysis of disease and astrological sign, this paper on the hazards of sequential Bonferroni, and this review of the past, present, and future of null hypothesis testing. 11.2 Family-wise error rates The ‘’family-wise error rate’’ is \\[ \\alpha = 1-(1-\\alpha^{’})^k \\] where \\(\\alpha^{&#39;}\\) is the ‘’per-comparison error rate’’. This is an important formula, don’t just memorize it - make sure you understand it! Solution #1: One solution is to narrow down the model a priori to a single model that you think best captures the relevant biology of the system. Solution #2: Another work-around is to lower the threshold for significance so that the Type I error rate for the whole family of comparisons is still 5\\(\\%\\). There are several ways to do this, of which we will discuss two: Method #1: The first method simply involves rearranging the formula above \\[ \\alpha=1-(1-\\alpha^{&#39;})^{k} \\] \\[ \\alpha^{&#39;}=1-(1-\\alpha)^{1/k} \\] This method, called the Dunn-Šidák method, simply sets the per comparison error rate as above. This assumes that all the comparisons are independent. Method #2: If the comparisons are not independent, then it is better to use a more conservative method, called Bonferroni’s method (of the Bonferroni correction) \\[ \\alpha^{&#39;}=\\alpha⁄k \\] However, this can set the per-comparison Type I error rate so low that it severely inflates the probability of a Type II error. Solution #3: Sequential Bonferroni’s test – Let’s say you start with 10 comparisons. You would test each comparison, rank them in order of their p-values, and discard the least significant (highest p value) if it was not significant at the \\(\\alpha\\)/10 level, the next least significant if it was not significant at the \\(\\alpha\\)/9 level, and so forth until all remaining comparisons were significant using a Bonferroni adjusted critical value for the number of remaining comparisons. Solution #4: Resampling-based corrected p-values – \\[ P_{corrected} = P(min(p) \\leq p-observed|H_{0}) \\] In other words, if you were to simulate data under the null hypothesis, what is the probability that the smallest p-value among the set of comparisons made is smaller than or equal to the p-value that you actually obtained. (In other words, the adjusted p-value asks “how extreme is my most extreme p-value when compared against the most extreme p-values I would expect under the null hypothesis?) 11.3 How do we sort the signal from the noise? One of the challenges we address this week is that almost all research programs involve multiple scientific hypotheses or exploratory analyses that are sifting through myriad potential causal drivers for observed phenomena. As a result, by the very structure of null hypothesis significance testing, we end up with a large number of false positive results from which it is often difficult to identify the ones most likely to represent real causal relationships. Though post-hoc studies can establish correlations, we know that correlation does not equal causation, and in fact only careful designed experiments can really confidently establish a causal relationship between a given variable and some observed response. That said, there are clues that we can use to help us identify what is likely to be “real” amidst a sea of significant p-values. In 1965, Sir Austin Bradford Hill established a series of criteria in the Proceedings of the Royal Society of Medicine now known as the Bradford Hill criteria (Hill 1965). These criteria are summarized as follows (the attendant commentary is my own and may not hew exactly to Hill’s original argument, which was focused on medical applications): Strength or Effect Size: Covariates that have a larger effect on responses are more likely to have a causal relationship. Consistency: Results that are reproducible by other researchers are more likely to be causal, and this is particularly true for results that can be reproduced in different systems and in different locations. (In other words, reproducible in this context also contains the idea of “extensibility”.) Specificity: Relationships that are very specific to a system in which no other plausible explanations exist are more likely to be causal. Temporality: The response has to occur after the purported causal driver, and the time scale of the response has to make biological sense. Biological gradient: This captures the idea that causal drivers are usually dose-dependent, so greater exposure to the driver should result in a greater response. Plausibility: There needs to be some plausible mechanism by which the driver could yield the response, though it may be that the exact mechanism involved in unknown. Coherence: Findings in the field that more closely align with findings from controlled experiments are more likely to be causal. Analogy: Proposed causal relationships are more likely to be causal if there are well-established analogs of similar relationships. One of the applications in which correlation and causation is particularly difficult to establish is in the investigation of possible cancer clusters. Applying Hill’s criteria to this application, we have to ask ourselves whether the hypothesized cancer causing agent 1) creates an increase in cancer incidence that is large enough to be biologically meaningful, 2) also causes an increase in cancers in other scenarios, 3) is specific to the population affected and the cancers involved have other causes, 4) the population was exposed to this hypothesized agent before the cancers were detected and that the time scale between exposure and the development of cancers is biologically reasonable, 5) people with greater exposure to the agent should have a greater risk than those with less exposure, 6) there is come mechanism by which the agent could possibly cause the cancer, 7) whether there is any experimental or laboratory findings to support the proposed mechanism, and 8) whether there are any other similar cancers that are caused by similar agents. "],["week-6-lab.html", "12 Week 6 Lab", " 12 Week 6 Lab Today we will do a short exercise to illustrate the permutation method of dealing with multiple comparisons. First, we will simulate 10 groups of data (say, heights) from the same distribution using the normal distribution and put the data for each group in a list for easy access: data.all&lt;-list() for (i in 1:10) { data.all[[i]]&lt;-rnorm(5) #Note the double brackets for a list } Now we can compare each group pairwise using a t.test. p.values&lt;-matrix(ncol=10,nrow=10) for (i in 1:9) { for (j in (i+1):10) { p.values[i,j]&lt;-t.test(data.all[[i]],data.all[[j]])$p.value } } p.values ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] NA 0.06368416 0.5838991 0.98661149 0.71020561 0.80142955 0.411223410 ## [2,] NA NA 0.1998734 0.04681936 0.05871088 0.05717086 0.008488879 ## [3,] NA NA NA 0.56756431 0.41004976 0.69884641 0.191583175 ## [4,] NA NA NA NA 0.68351075 0.79247038 0.339998589 ## [5,] NA NA NA NA NA 0.54198588 0.790097835 ## [6,] NA NA NA NA NA NA 0.191032672 ## [7,] NA NA NA NA NA NA NA ## [8,] NA NA NA NA NA NA NA ## [9,] NA NA NA NA NA NA NA ## [10,] NA NA NA NA NA NA NA ## [,8] [,9] [,10] ## [1,] 0.49355952 0.395523139 0.69709846 ## [2,] 0.10054760 0.008454453 0.02125135 ## [3,] 0.99025089 0.183158943 0.35009801 ## [4,] 0.44721912 0.314594947 0.65052915 ## [5,] 0.34135945 0.792145357 0.94027986 ## [6,] 0.59165670 0.161926176 0.45374938 ## [7,] 0.06047482 0.981255523 0.61678331 ## [8,] NA 0.041445929 0.20695639 ## [9,] NA NA 0.59820494 ## [10,] NA NA NA Now we can see how many of these p.values are “significant”. We know these are false positives, because all the data were generated from the same distribution. false.positives&lt;-sum(p.values&lt;0.05,na.rm=T) false.positives ## [1] 5 We could correct this using the Bonferonni method: k&lt;-45 new.threshold.B&lt;-0.05/k new.threshold.B ## [1] 0.001111111 false.positives.B&lt;-sum(p.values&lt;new.threshold.B,na.rm=T) false.positives.B ## [1] 0 We could correct this using the Dunn-Sidak method: k&lt;-45 new.threshold.DS&lt;-1-((1-0.05)^(1/k)) new.threshold.DS ## [1] 0.001139202 false.positives.DS&lt;-sum(p.values&lt;new.threshold.DS,na.rm=T) false.positives.DS ## [1] 0 We could correct this using the randomization method. This requires simulating data under the null hypothesis to generate a null distribution of p-values. p.values.all&lt;-c() min.p.values.all&lt;-c() for (k in 1:1000) { data.null&lt;-list() for (i in 1:10) { data.null[[i]]&lt;-rnorm(10) #Note the double brackets for a list } p.values.null&lt;-matrix(ncol=10,nrow=10) for (i in 1:9) { for (j in (i+1):10) { p.values.null[i,j]&lt;-t.test(data.null[[i]],data.null[[j]])$p.value } } p.values.all&lt;-c(p.values.all,c(p.values.null)[!is.na(c(p.values.null))]) min.p.values.all&lt;-c(min.p.values.all,min(c(p.values.null)[!is.na(c(p.values.null))])) } new.threshold.R&lt;-quantile(min.p.values.all,probs=c(0.05)) new.threshold.R ## 5% ## 0.001793704 false.positives.R&lt;-sum(p.values&lt;new.threshold.R,na.rm=T) false.positives.R ## [1] 0 If you were to do this experiment (all of the code in the preceding clock) 100 times, you should get at least 1 false positive 5 times, since we have set the threshold such that we have a 5% chance that the smallest p-value in that set of 45 comparisons will be smaller than the threshold we set. "],["week-7-lecturelab.html", "13 Week 7 Lecture/Lab 13.1 Week 7 Readings 13.2 Introduction to plotting in R 13.3 Box plots 13.4 Two-dimensional data 13.5 Three-dimensional data 13.6 Multiple plots", " 13 Week 7 Lecture/Lab 13.1 Week 7 Readings For this week, I suggest reading Logan Chapter 5. There are many papers that might be helpful in the design of effective visualizations. Two that I would recommend highly are Wainer (1984) and Rougier et al. (2014). There are a suite of additional papers you should at least open and skim, papers on: three-dimensional visualizations, callouts and labels, axes, ticks, and grids, error bars, and box plots. 13.2 Introduction to plotting in R We have already covered most of the basic elements of plotting, but here I’ll go over some elements of plotting that you may not have learned already this semester. You will need the packages ‘ggplot2’,‘gplots’, and ‘ade4’ so you might as well install them now. library(ggplot2) library(gplots) ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess library(ade4) 13.3 Box plots A boxplot is a convenient way of summarizing univariate data; the top and bottom of the rectangle represent the upper and lower quartiles of the data, the median by a horizontal line in the rectangle. There are several conventions for the whiskers, so the meaning of the whiskers should be explained clearly in the legend. One convention holds that the whisker extend to the further point that is no more than 1.5 times the interquartile range (75th-25th). In this case, outlying datapoints are shown with their own dot or star. You can also draw whiskers that extend out to the furthest datapoint. Let’s illustrate this using some randomly drawn data. boxplot(rnorm(1000,mean=0,sd=1),rnorm(1000,mean=0,sd=3)) We can play around with some of the options, such as naming the boxes and coloring the borders. boxplot(rnorm(1000,mean=0,sd=1),rnorm(1000,mean=0,sd=3),names=c(&quot;cats&quot;,&quot;dogs&quot;), border=c(&quot;red&quot;,&quot;green&quot;)) 13.4 Two-dimensional data Read in the following dataset describing fox fur production as a function of year fox.data&lt;-read.csv(&quot;_data/FoxFurProduction.csv&quot;, header=F) We could do our entire analysis referring to the two columns as [,1] and [,2] but to make our code more readable, lets add some column names colnames(fox.data)&lt;-c(&quot;year&quot;,&quot;production&quot;) Now we can refer to the two columns of data as $year and $production. Now let’s plot the data: plot(fox.data$year,fox.data$production) abline(a=200,b=0) abline(h=200) points(fox.data$year[fox.data$production&gt;200],fox.data$production[fox.data$production&gt;200],cex=2,pch=16) large.production&lt;- fox.data$production&gt;200 points(fox.data$year[large.production],fox.data$production[large.production],cex=2,pch=16) Notice that with this code I am highlighting all the points where fur production is &gt;200, and I added a horizontal bar at 200 using the function “abline”. (Stop: Discuss the options for abline.) All software packages make horrible default plots and R is no exception. Make the following changes to make a more information plot and to explore what R can do. Label the x and y axes. Try plotting as a line using “typ=”l”“” Plot both points and lines suing “typ=”b”” Change the plotting symbol using “pch=XX” where XX is a number between 1 and 25. While black is the logical default color, play around with color using the option “col=XX” where XX can be either a number or a name. We can get R to list all of the named colors using colors() ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; ## [4] &quot;antiquewhite1&quot; &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; ## [7] &quot;antiquewhite4&quot; &quot;aquamarine&quot; &quot;aquamarine1&quot; ## [10] &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; ## [16] &quot;azure3&quot; &quot;azure4&quot; &quot;beige&quot; ## [19] &quot;bisque&quot; &quot;bisque1&quot; &quot;bisque2&quot; ## [22] &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; ## [25] &quot;blanchedalmond&quot; &quot;blue&quot; &quot;blue1&quot; ## [28] &quot;blue2&quot; &quot;blue3&quot; &quot;blue4&quot; ## [31] &quot;blueviolet&quot; &quot;brown&quot; &quot;brown1&quot; ## [34] &quot;brown2&quot; &quot;brown3&quot; &quot;brown4&quot; ## [37] &quot;burlywood&quot; &quot;burlywood1&quot; &quot;burlywood2&quot; ## [40] &quot;burlywood3&quot; &quot;burlywood4&quot; &quot;cadetblue&quot; ## [43] &quot;cadetblue1&quot; &quot;cadetblue2&quot; &quot;cadetblue3&quot; ## [46] &quot;cadetblue4&quot; &quot;chartreuse&quot; &quot;chartreuse1&quot; ## [49] &quot;chartreuse2&quot; &quot;chartreuse3&quot; &quot;chartreuse4&quot; ## [52] &quot;chocolate&quot; &quot;chocolate1&quot; &quot;chocolate2&quot; ## [55] &quot;chocolate3&quot; &quot;chocolate4&quot; &quot;coral&quot; ## [58] &quot;coral1&quot; &quot;coral2&quot; &quot;coral3&quot; ## [61] &quot;coral4&quot; &quot;cornflowerblue&quot; &quot;cornsilk&quot; ## [64] &quot;cornsilk1&quot; &quot;cornsilk2&quot; &quot;cornsilk3&quot; ## [67] &quot;cornsilk4&quot; &quot;cyan&quot; &quot;cyan1&quot; ## [70] &quot;cyan2&quot; &quot;cyan3&quot; &quot;cyan4&quot; ## [73] &quot;darkblue&quot; &quot;darkcyan&quot; &quot;darkgoldenrod&quot; ## [76] &quot;darkgoldenrod1&quot; &quot;darkgoldenrod2&quot; &quot;darkgoldenrod3&quot; ## [79] &quot;darkgoldenrod4&quot; &quot;darkgray&quot; &quot;darkgreen&quot; ## [82] &quot;darkgrey&quot; &quot;darkkhaki&quot; &quot;darkmagenta&quot; ## [85] &quot;darkolivegreen&quot; &quot;darkolivegreen1&quot; &quot;darkolivegreen2&quot; ## [88] &quot;darkolivegreen3&quot; &quot;darkolivegreen4&quot; &quot;darkorange&quot; ## [91] &quot;darkorange1&quot; &quot;darkorange2&quot; &quot;darkorange3&quot; ## [94] &quot;darkorange4&quot; &quot;darkorchid&quot; &quot;darkorchid1&quot; ## [97] &quot;darkorchid2&quot; &quot;darkorchid3&quot; &quot;darkorchid4&quot; ## [100] &quot;darkred&quot; &quot;darksalmon&quot; &quot;darkseagreen&quot; ## [103] &quot;darkseagreen1&quot; &quot;darkseagreen2&quot; &quot;darkseagreen3&quot; ## [106] &quot;darkseagreen4&quot; &quot;darkslateblue&quot; &quot;darkslategray&quot; ## [109] &quot;darkslategray1&quot; &quot;darkslategray2&quot; &quot;darkslategray3&quot; ## [112] &quot;darkslategray4&quot; &quot;darkslategrey&quot; &quot;darkturquoise&quot; ## [115] &quot;darkviolet&quot; &quot;deeppink&quot; &quot;deeppink1&quot; ## [118] &quot;deeppink2&quot; &quot;deeppink3&quot; &quot;deeppink4&quot; ## [121] &quot;deepskyblue&quot; &quot;deepskyblue1&quot; &quot;deepskyblue2&quot; ## [124] &quot;deepskyblue3&quot; &quot;deepskyblue4&quot; &quot;dimgray&quot; ## [127] &quot;dimgrey&quot; &quot;dodgerblue&quot; &quot;dodgerblue1&quot; ## [130] &quot;dodgerblue2&quot; &quot;dodgerblue3&quot; &quot;dodgerblue4&quot; ## [133] &quot;firebrick&quot; &quot;firebrick1&quot; &quot;firebrick2&quot; ## [136] &quot;firebrick3&quot; &quot;firebrick4&quot; &quot;floralwhite&quot; ## [139] &quot;forestgreen&quot; &quot;gainsboro&quot; &quot;ghostwhite&quot; ## [142] &quot;gold&quot; &quot;gold1&quot; &quot;gold2&quot; ## [145] &quot;gold3&quot; &quot;gold4&quot; &quot;goldenrod&quot; ## [148] &quot;goldenrod1&quot; &quot;goldenrod2&quot; &quot;goldenrod3&quot; ## [151] &quot;goldenrod4&quot; &quot;gray&quot; &quot;gray0&quot; ## [154] &quot;gray1&quot; &quot;gray2&quot; &quot;gray3&quot; ## [157] &quot;gray4&quot; &quot;gray5&quot; &quot;gray6&quot; ## [160] &quot;gray7&quot; &quot;gray8&quot; &quot;gray9&quot; ## [163] &quot;gray10&quot; &quot;gray11&quot; &quot;gray12&quot; ## [166] &quot;gray13&quot; &quot;gray14&quot; &quot;gray15&quot; ## [169] &quot;gray16&quot; &quot;gray17&quot; &quot;gray18&quot; ## [172] &quot;gray19&quot; &quot;gray20&quot; &quot;gray21&quot; ## [175] &quot;gray22&quot; &quot;gray23&quot; &quot;gray24&quot; ## [178] &quot;gray25&quot; &quot;gray26&quot; &quot;gray27&quot; ## [181] &quot;gray28&quot; &quot;gray29&quot; &quot;gray30&quot; ## [184] &quot;gray31&quot; &quot;gray32&quot; &quot;gray33&quot; ## [187] &quot;gray34&quot; &quot;gray35&quot; &quot;gray36&quot; ## [190] &quot;gray37&quot; &quot;gray38&quot; &quot;gray39&quot; ## [193] &quot;gray40&quot; &quot;gray41&quot; &quot;gray42&quot; ## [196] &quot;gray43&quot; &quot;gray44&quot; &quot;gray45&quot; ## [199] &quot;gray46&quot; &quot;gray47&quot; &quot;gray48&quot; ## [202] &quot;gray49&quot; &quot;gray50&quot; &quot;gray51&quot; ## [205] &quot;gray52&quot; &quot;gray53&quot; &quot;gray54&quot; ## [208] &quot;gray55&quot; &quot;gray56&quot; &quot;gray57&quot; ## [211] &quot;gray58&quot; &quot;gray59&quot; &quot;gray60&quot; ## [214] &quot;gray61&quot; &quot;gray62&quot; &quot;gray63&quot; ## [217] &quot;gray64&quot; &quot;gray65&quot; &quot;gray66&quot; ## [220] &quot;gray67&quot; &quot;gray68&quot; &quot;gray69&quot; ## [223] &quot;gray70&quot; &quot;gray71&quot; &quot;gray72&quot; ## [226] &quot;gray73&quot; &quot;gray74&quot; &quot;gray75&quot; ## [229] &quot;gray76&quot; &quot;gray77&quot; &quot;gray78&quot; ## [232] &quot;gray79&quot; &quot;gray80&quot; &quot;gray81&quot; ## [235] &quot;gray82&quot; &quot;gray83&quot; &quot;gray84&quot; ## [238] &quot;gray85&quot; &quot;gray86&quot; &quot;gray87&quot; ## [241] &quot;gray88&quot; &quot;gray89&quot; &quot;gray90&quot; ## [244] &quot;gray91&quot; &quot;gray92&quot; &quot;gray93&quot; ## [247] &quot;gray94&quot; &quot;gray95&quot; &quot;gray96&quot; ## [250] &quot;gray97&quot; &quot;gray98&quot; &quot;gray99&quot; ## [253] &quot;gray100&quot; &quot;green&quot; &quot;green1&quot; ## [256] &quot;green2&quot; &quot;green3&quot; &quot;green4&quot; ## [259] &quot;greenyellow&quot; &quot;grey&quot; &quot;grey0&quot; ## [262] &quot;grey1&quot; &quot;grey2&quot; &quot;grey3&quot; ## [265] &quot;grey4&quot; &quot;grey5&quot; &quot;grey6&quot; ## [268] &quot;grey7&quot; &quot;grey8&quot; &quot;grey9&quot; ## [271] &quot;grey10&quot; &quot;grey11&quot; &quot;grey12&quot; ## [274] &quot;grey13&quot; &quot;grey14&quot; &quot;grey15&quot; ## [277] &quot;grey16&quot; &quot;grey17&quot; &quot;grey18&quot; ## [280] &quot;grey19&quot; &quot;grey20&quot; &quot;grey21&quot; ## [283] &quot;grey22&quot; &quot;grey23&quot; &quot;grey24&quot; ## [286] &quot;grey25&quot; &quot;grey26&quot; &quot;grey27&quot; ## [289] &quot;grey28&quot; &quot;grey29&quot; &quot;grey30&quot; ## [292] &quot;grey31&quot; &quot;grey32&quot; &quot;grey33&quot; ## [295] &quot;grey34&quot; &quot;grey35&quot; &quot;grey36&quot; ## [298] &quot;grey37&quot; &quot;grey38&quot; &quot;grey39&quot; ## [301] &quot;grey40&quot; &quot;grey41&quot; &quot;grey42&quot; ## [304] &quot;grey43&quot; &quot;grey44&quot; &quot;grey45&quot; ## [307] &quot;grey46&quot; &quot;grey47&quot; &quot;grey48&quot; ## [310] &quot;grey49&quot; &quot;grey50&quot; &quot;grey51&quot; ## [313] &quot;grey52&quot; &quot;grey53&quot; &quot;grey54&quot; ## [316] &quot;grey55&quot; &quot;grey56&quot; &quot;grey57&quot; ## [319] &quot;grey58&quot; &quot;grey59&quot; &quot;grey60&quot; ## [322] &quot;grey61&quot; &quot;grey62&quot; &quot;grey63&quot; ## [325] &quot;grey64&quot; &quot;grey65&quot; &quot;grey66&quot; ## [328] &quot;grey67&quot; &quot;grey68&quot; &quot;grey69&quot; ## [331] &quot;grey70&quot; &quot;grey71&quot; &quot;grey72&quot; ## [334] &quot;grey73&quot; &quot;grey74&quot; &quot;grey75&quot; ## [337] &quot;grey76&quot; &quot;grey77&quot; &quot;grey78&quot; ## [340] &quot;grey79&quot; &quot;grey80&quot; &quot;grey81&quot; ## [343] &quot;grey82&quot; &quot;grey83&quot; &quot;grey84&quot; ## [346] &quot;grey85&quot; &quot;grey86&quot; &quot;grey87&quot; ## [349] &quot;grey88&quot; &quot;grey89&quot; &quot;grey90&quot; ## [352] &quot;grey91&quot; &quot;grey92&quot; &quot;grey93&quot; ## [355] &quot;grey94&quot; &quot;grey95&quot; &quot;grey96&quot; ## [358] &quot;grey97&quot; &quot;grey98&quot; &quot;grey99&quot; ## [361] &quot;grey100&quot; &quot;honeydew&quot; &quot;honeydew1&quot; ## [364] &quot;honeydew2&quot; &quot;honeydew3&quot; &quot;honeydew4&quot; ## [367] &quot;hotpink&quot; &quot;hotpink1&quot; &quot;hotpink2&quot; ## [370] &quot;hotpink3&quot; &quot;hotpink4&quot; &quot;indianred&quot; ## [373] &quot;indianred1&quot; &quot;indianred2&quot; &quot;indianred3&quot; ## [376] &quot;indianred4&quot; &quot;ivory&quot; &quot;ivory1&quot; ## [379] &quot;ivory2&quot; &quot;ivory3&quot; &quot;ivory4&quot; ## [382] &quot;khaki&quot; &quot;khaki1&quot; &quot;khaki2&quot; ## [385] &quot;khaki3&quot; &quot;khaki4&quot; &quot;lavender&quot; ## [388] &quot;lavenderblush&quot; &quot;lavenderblush1&quot; &quot;lavenderblush2&quot; ## [391] &quot;lavenderblush3&quot; &quot;lavenderblush4&quot; &quot;lawngreen&quot; ## [394] &quot;lemonchiffon&quot; &quot;lemonchiffon1&quot; &quot;lemonchiffon2&quot; ## [397] &quot;lemonchiffon3&quot; &quot;lemonchiffon4&quot; &quot;lightblue&quot; ## [400] &quot;lightblue1&quot; &quot;lightblue2&quot; &quot;lightblue3&quot; ## [403] &quot;lightblue4&quot; &quot;lightcoral&quot; &quot;lightcyan&quot; ## [406] &quot;lightcyan1&quot; &quot;lightcyan2&quot; &quot;lightcyan3&quot; ## [409] &quot;lightcyan4&quot; &quot;lightgoldenrod&quot; &quot;lightgoldenrod1&quot; ## [412] &quot;lightgoldenrod2&quot; &quot;lightgoldenrod3&quot; &quot;lightgoldenrod4&quot; ## [415] &quot;lightgoldenrodyellow&quot; &quot;lightgray&quot; &quot;lightgreen&quot; ## [418] &quot;lightgrey&quot; &quot;lightpink&quot; &quot;lightpink1&quot; ## [421] &quot;lightpink2&quot; &quot;lightpink3&quot; &quot;lightpink4&quot; ## [424] &quot;lightsalmon&quot; &quot;lightsalmon1&quot; &quot;lightsalmon2&quot; ## [427] &quot;lightsalmon3&quot; &quot;lightsalmon4&quot; &quot;lightseagreen&quot; ## [430] &quot;lightskyblue&quot; &quot;lightskyblue1&quot; &quot;lightskyblue2&quot; ## [433] &quot;lightskyblue3&quot; &quot;lightskyblue4&quot; &quot;lightslateblue&quot; ## [436] &quot;lightslategray&quot; &quot;lightslategrey&quot; &quot;lightsteelblue&quot; ## [439] &quot;lightsteelblue1&quot; &quot;lightsteelblue2&quot; &quot;lightsteelblue3&quot; ## [442] &quot;lightsteelblue4&quot; &quot;lightyellow&quot; &quot;lightyellow1&quot; ## [445] &quot;lightyellow2&quot; &quot;lightyellow3&quot; &quot;lightyellow4&quot; ## [448] &quot;limegreen&quot; &quot;linen&quot; &quot;magenta&quot; ## [451] &quot;magenta1&quot; &quot;magenta2&quot; &quot;magenta3&quot; ## [454] &quot;magenta4&quot; &quot;maroon&quot; &quot;maroon1&quot; ## [457] &quot;maroon2&quot; &quot;maroon3&quot; &quot;maroon4&quot; ## [460] &quot;mediumaquamarine&quot; &quot;mediumblue&quot; &quot;mediumorchid&quot; ## [463] &quot;mediumorchid1&quot; &quot;mediumorchid2&quot; &quot;mediumorchid3&quot; ## [466] &quot;mediumorchid4&quot; &quot;mediumpurple&quot; &quot;mediumpurple1&quot; ## [469] &quot;mediumpurple2&quot; &quot;mediumpurple3&quot; &quot;mediumpurple4&quot; ## [472] &quot;mediumseagreen&quot; &quot;mediumslateblue&quot; &quot;mediumspringgreen&quot; ## [475] &quot;mediumturquoise&quot; &quot;mediumvioletred&quot; &quot;midnightblue&quot; ## [478] &quot;mintcream&quot; &quot;mistyrose&quot; &quot;mistyrose1&quot; ## [481] &quot;mistyrose2&quot; &quot;mistyrose3&quot; &quot;mistyrose4&quot; ## [484] &quot;moccasin&quot; &quot;navajowhite&quot; &quot;navajowhite1&quot; ## [487] &quot;navajowhite2&quot; &quot;navajowhite3&quot; &quot;navajowhite4&quot; ## [490] &quot;navy&quot; &quot;navyblue&quot; &quot;oldlace&quot; ## [493] &quot;olivedrab&quot; &quot;olivedrab1&quot; &quot;olivedrab2&quot; ## [496] &quot;olivedrab3&quot; &quot;olivedrab4&quot; &quot;orange&quot; ## [499] &quot;orange1&quot; &quot;orange2&quot; &quot;orange3&quot; ## [502] &quot;orange4&quot; &quot;orangered&quot; &quot;orangered1&quot; ## [505] &quot;orangered2&quot; &quot;orangered3&quot; &quot;orangered4&quot; ## [508] &quot;orchid&quot; &quot;orchid1&quot; &quot;orchid2&quot; ## [511] &quot;orchid3&quot; &quot;orchid4&quot; &quot;palegoldenrod&quot; ## [514] &quot;palegreen&quot; &quot;palegreen1&quot; &quot;palegreen2&quot; ## [517] &quot;palegreen3&quot; &quot;palegreen4&quot; &quot;paleturquoise&quot; ## [520] &quot;paleturquoise1&quot; &quot;paleturquoise2&quot; &quot;paleturquoise3&quot; ## [523] &quot;paleturquoise4&quot; &quot;palevioletred&quot; &quot;palevioletred1&quot; ## [526] &quot;palevioletred2&quot; &quot;palevioletred3&quot; &quot;palevioletred4&quot; ## [529] &quot;papayawhip&quot; &quot;peachpuff&quot; &quot;peachpuff1&quot; ## [532] &quot;peachpuff2&quot; &quot;peachpuff3&quot; &quot;peachpuff4&quot; ## [535] &quot;peru&quot; &quot;pink&quot; &quot;pink1&quot; ## [538] &quot;pink2&quot; &quot;pink3&quot; &quot;pink4&quot; ## [541] &quot;plum&quot; &quot;plum1&quot; &quot;plum2&quot; ## [544] &quot;plum3&quot; &quot;plum4&quot; &quot;powderblue&quot; ## [547] &quot;purple&quot; &quot;purple1&quot; &quot;purple2&quot; ## [550] &quot;purple3&quot; &quot;purple4&quot; &quot;red&quot; ## [553] &quot;red1&quot; &quot;red2&quot; &quot;red3&quot; ## [556] &quot;red4&quot; &quot;rosybrown&quot; &quot;rosybrown1&quot; ## [559] &quot;rosybrown2&quot; &quot;rosybrown3&quot; &quot;rosybrown4&quot; ## [562] &quot;royalblue&quot; &quot;royalblue1&quot; &quot;royalblue2&quot; ## [565] &quot;royalblue3&quot; &quot;royalblue4&quot; &quot;saddlebrown&quot; ## [568] &quot;salmon&quot; &quot;salmon1&quot; &quot;salmon2&quot; ## [571] &quot;salmon3&quot; &quot;salmon4&quot; &quot;sandybrown&quot; ## [574] &quot;seagreen&quot; &quot;seagreen1&quot; &quot;seagreen2&quot; ## [577] &quot;seagreen3&quot; &quot;seagreen4&quot; &quot;seashell&quot; ## [580] &quot;seashell1&quot; &quot;seashell2&quot; &quot;seashell3&quot; ## [583] &quot;seashell4&quot; &quot;sienna&quot; &quot;sienna1&quot; ## [586] &quot;sienna2&quot; &quot;sienna3&quot; &quot;sienna4&quot; ## [589] &quot;skyblue&quot; &quot;skyblue1&quot; &quot;skyblue2&quot; ## [592] &quot;skyblue3&quot; &quot;skyblue4&quot; &quot;slateblue&quot; ## [595] &quot;slateblue1&quot; &quot;slateblue2&quot; &quot;slateblue3&quot; ## [598] &quot;slateblue4&quot; &quot;slategray&quot; &quot;slategray1&quot; ## [601] &quot;slategray2&quot; &quot;slategray3&quot; &quot;slategray4&quot; ## [604] &quot;slategrey&quot; &quot;snow&quot; &quot;snow1&quot; ## [607] &quot;snow2&quot; &quot;snow3&quot; &quot;snow4&quot; ## [610] &quot;springgreen&quot; &quot;springgreen1&quot; &quot;springgreen2&quot; ## [613] &quot;springgreen3&quot; &quot;springgreen4&quot; &quot;steelblue&quot; ## [616] &quot;steelblue1&quot; &quot;steelblue2&quot; &quot;steelblue3&quot; ## [619] &quot;steelblue4&quot; &quot;tan&quot; &quot;tan1&quot; ## [622] &quot;tan2&quot; &quot;tan3&quot; &quot;tan4&quot; ## [625] &quot;thistle&quot; &quot;thistle1&quot; &quot;thistle2&quot; ## [628] &quot;thistle3&quot; &quot;thistle4&quot; &quot;tomato&quot; ## [631] &quot;tomato1&quot; &quot;tomato2&quot; &quot;tomato3&quot; ## [634] &quot;tomato4&quot; &quot;turquoise&quot; &quot;turquoise1&quot; ## [637] &quot;turquoise2&quot; &quot;turquoise3&quot; &quot;turquoise4&quot; ## [640] &quot;violet&quot; &quot;violetred&quot; &quot;violetred1&quot; ## [643] &quot;violetred2&quot; &quot;violetred3&quot; &quot;violetred4&quot; ## [646] &quot;wheat&quot; &quot;wheat1&quot; &quot;wheat2&quot; ## [649] &quot;wheat3&quot; &quot;wheat4&quot; &quot;whitesmoke&quot; ## [652] &quot;yellow&quot; &quot;yellow1&quot; &quot;yellow2&quot; ## [655] &quot;yellow3&quot; &quot;yellow4&quot; &quot;yellowgreen&quot; but we can also explicitly type in RGB values using the rgb() function and values (from 0 to MaxColorValue, which is 1 by default) for each of the red, green, and blue components. plot(fox.data$year,fox.data$production,col=rgb(red=0,green=1,blue=0)) You can extract rgb values from a named color using the col2rgb() function col2rgb(&quot;peachpuff&quot;) ## [,1] ## red 255 ## green 218 ## blue 185 For complex plots which require a suite of colors, I highly recommend using the R package RColorBrewer. This package has a companion website for choosing color schemes (for mapping, for example) www.colorbrewer2.org. RColorBrewer allows you to pick color schemes with certain characteristics (diverging, sequential, etc) and to select a certain number of colors within that color scheme. The website also suggests color schemes that are color-blind friendly (often required for publication). library(RColorBrewer) ## Warning: package &#39;RColorBrewer&#39; was built under R version 4.0.5 mypalette&lt;-brewer.pal(7,&quot;Greens&quot;) image(1:7,1,as.matrix(1:7),col=mypalette,xlab=&quot;Greens (sequential)&quot;, ylab=&quot;&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,bty=&quot;n&quot;) This is a sequential palette. ColorBrewer also offers up a variety of divergent and qualitative palettes such as display.brewer.pal(7,&quot;BrBG&quot;) and display.brewer.pal(7,&quot;Accent&quot;) There are many other options. You can extract the actual hex codes for these colors using brewer.pal(7,&quot;Accent&quot;) ## [1] &quot;#7FC97F&quot; &quot;#BEAED4&quot; &quot;#FDC086&quot; &quot;#FFFF99&quot; &quot;#386CB0&quot; &quot;#F0027F&quot; &quot;#BF5B17&quot; To play around with a few more graphics options, we will use one of the built-in datasets that has R called “mtcars”. I will break one of my own rules here and attach the dataset. attach(mtcars) ## The following object is masked from package:ggplot2: ## ## mpg plot(wt, mpg, main=&quot;Milage vs. Car Weight&quot;,xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;, pch=18, col=&quot;blue&quot;) text(wt, mpg, row.names(mtcars), cex=0.6, pos=4, col=&quot;red&quot;) Note that we can add a legend: # Legend Example boxplot(mpg~cyl, main=&quot;Milage by Car Weight&quot;,yaxt=&quot;n&quot;, xlab=&quot;Milage&quot;, horizontal=TRUE,col=terrain.colors(3)) legend(&quot;topright&quot;, inset=.05, title=&quot;Number of Cylinders&quot;,c(&quot;4&quot;,&quot;6&quot;,&quot;8&quot;), fill=terrain.colors(3), horiz=TRUE) Be aware of overplotting (points that overlap) especially when data are discrete or rounded. One strategy for overcoming this is jittering the points so the density of points can be displayed without overly distorting the underlying relationships. You can use the ‘jitter’ command in the base package. jitter(rep(0, 7)) ## [1] 0.017867798 0.012996409 0.008805254 -0.019457962 0.011128419 ## [6] 0.006089113 0.010632568 Before we launch into three-dimensional plotting, we should introduce ‘ggplot2’ which is quickly becoming “industry standard” for making plots in R. There is so much that can be done with ggplot2 that we will only scratch the surface today, but at least this introduction will illustrate some of the things that ggplot2 can do. The basic function to make plots using ggplot2 is ‘qplot’. We can recreate the scatterplot of the mtcars data using ggplot2 as follows library(ggplot2) qplot(wt, mpg, main=&quot;Milage vs. Car Weight&quot;,xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;, data=mtcars) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. We can also do fancier things, such as mtcars$gear &lt;- factor(mtcars$gear,levels=c(3,4,5), labels=c(&quot;3gears&quot;,&quot;4gears&quot;,&quot;5gears&quot;)) mtcars$am &lt;- factor(mtcars$am,levels=c(0,1), labels=c(&quot;Automatic&quot;,&quot;Manual&quot;)) mtcars$cyl &lt;- factor(mtcars$cyl,levels=c(4,6,8), labels=c(&quot;4cyl&quot;,&quot;6cyl&quot;,&quot;8cyl&quot;)) # Kernel density plots for mpg # grouped by number of gears (indicated by color) qplot(mpg, data=mtcars, geom=&quot;density&quot;, fill=gear, alpha=I(.5), main=&quot;Distribution of Gas Milage&quot;, xlab=&quot;Miles Per Gallon&quot;, ylab=&quot;Density&quot;) There is more to ggplot2 than I can cover today, but if you are serious about making nice plots, its worth investing some time in learning this package. 13.5 Three-dimensional data There are almost an infinite variety of graphical options for R, most of which are available through the contributed R packages. We will use one of them now to demonstrate some of R’s other graphical options. You should already have installed the R package ‘gplots’ but if not, do so now. library(gplots,quietly=TRUE,warn.conflicts=F) For data, we will simply draw samples from the normal distribution. Feel free to choose any two distributions of your own liking. x &lt;- rnorm(2000, sd=4) y &lt;- rnorm(2000, sd=1) hist2d(x,y, same.scale=TRUE) ## ## ---------------------------- ## 2-D Histogram Object ## ---------------------------- ## ## Call: hist2d(x = x, y = y, same.scale = TRUE) ## ## Number of data points: 2000 ## Number of grid bins: 200 x 200 ## X range: ( -14.99486 , 14.92139 ) ## Y range: ( -14.99486 , 14.92139 ) Notice that we had to use “same.scale=T” to make sure that distances along the x and y axes were comparable. Now we will use the hisr2d function to create inputs for a three-dimensional perspective plot. h2d &lt;- hist2d(x,y,show=FALSE, same.scale=TRUE, nbins=c(20,30)) persp( h2d$x, h2d$y, h2d$counts, ticktype=&quot;detailed&quot;, theta=30, phi=30, expand=0.5, shade=0.5, col=&quot;cyan&quot;, ltheta=-30,xlab=&quot;&quot;,ylab=&quot;&quot;,zlab=&quot;&quot;) Make sure you know what these options all mean! Play around with theta and phi and see how that changes the perspective. Let’s use the same data and make a contour plot. contour( h2d$x, h2d$y, h2d$counts, nlevels=10) We can also make a filled contour plot: filled.contour( h2d$x, h2d$y, h2d$counts, nlevels=10, col=gray((10:0)/10)) Play around with “col=rainbow(10)” or “col=topo.colors(10)”. Go back and play around with some of the other distributions from the last few weeks. It is particularly useful to use the 2-dimensional histograms to get a sense for what changing a distribution parameter does to the distribution. We can demonstrate some other kinds of plots R can make using some of the built in datasets. Here we demonstrate both a color plot and a contour plot: x &lt;- 10*(1:nrow(volcano)) x.at &lt;- seq(100, 800, by=100) y &lt;- 10*(1:ncol(volcano)) y.at &lt;- seq(100, 600, by=100) image(x, y, volcano, col=terrain.colors(100),axes=FALSE) contour(x, y, volcano, levels=seq(90, 200, by=5), add=TRUE, col=&quot;brown&quot;) axis(1, at=x.at) axis(2, at=y.at) box() title(main=&quot;Maunga Whau Volcano&quot;, sub = &quot;col=terrain.colors(100)&quot;, font.main=4) 13.6 Multiple plots Now we will discuss how to arrange multiple plots of the same page. With the par() function, you can include the option mfrow=c(nrows,ncols) to make a matrix of nrowsxncols plots that are filled by row. mfcol=c(nrows,ncols) would fill the matrix by columns. # 4 figures arranged in 2 rows and 2 columns par(mfrow=c(2,2)) plot(mtcars$wt,mtcars$mpg, main=&quot;Scatterplot of wt vs. mpg&quot;) plot(mtcars$wt,mtcars$disp, main=&quot;Scatterplot of wt vs disp&quot;) hist(mtcars$wt, main=&quot;Histogram of wt&quot;) boxplot(mtcars$wt, main=&quot;Boxplot of wt&quot;) # 3 figures arranged in 3 rows and 1 column layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE)) hist(mtcars$wt) hist(mtcars$mpg) hist(mtcars$disp) Now for a slightly complicated example that uses much of what we have learned. Childrens IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What proportion of children are expected to have an IQ between 80 and 120? mean&lt;-100 sd&lt;-15 lb&lt;-80 ub&lt;-120 x&lt;-seq(-4,4,length=100)*sd+mean hx&lt;-dnorm(x,mean,sd) plot(x,hx,typ=&quot;n&quot;,xlab=&quot;IQ Values&quot;,ylab=&quot;Density&quot;,main=&quot;Normal Distribution&quot;,axes=F) i&lt;-x&gt;=lb &amp; x&lt;=ub lines(x,hx) polygon(c(lb,x[i],ub),c(0,hx[i],0),col=&quot;red&quot;) As one final example, we will plot scatter plots with marginal histograms. For this we need to install the R package “ade4”. This package has a huge number of useful functions, which we can look at here: The ade4 webpage For now, I just want to use some of its plotting functionality to illustrate the kinds of sophisticated graphics R is capable of. library(ade4) data(rpjdl) First, lets just look at the data using “?rpjdl”. coa1&lt;-dudi.coa(rpjdl$fau,scannf=FALSE,nf=4) s.hist(coa1$li) ## [1] 10 20 30 Finally, just a word of advice about making figures for publication. While R is very flexible, making publication ready figures can be very time consuming in R. If you have any possibility of using Adobe Illustrator, I highly suggest using R to get your figure 90% correct, and then exporting it as an .eps (Encapsulated Postscript) to Illustrator. Except is very rare cases where you have an actual photo or image in your figure, you want to keep your figures as vector graphics throughout the entire process. Postscript (.ps) and Encapsulated Postscript (.eps) are vector file formats, but .tiff, .jpeg, and .png are not. Vector file formats will preserve the clarity pf your figure at any size scale, so your figure remains crisp and clear throughout the creation process. In Illustrator, you can relabel everything (in whatever font the editor prefers), fix the line widths, tweak the colors as needed, arrange multiple panels as needed, add arrows and legends or other details, make one figure an inset of another, or do anything else you could possibly want to do. Illustrator has a learning curve of its own but is well worth the investment. "],["week-8-lecture.html", "14 Week 8 Lecture 14.1 Week 8 Readings 14.2 Warm-up 14.3 The aims of modelling – A discussion of Shmueli (2010) 14.4 Introduction to linear models 14.5 Linear models | example with continuous covariate 14.6 Resolving overparameterization using contrasts 14.7 Effect coding/Treatment constrast 14.8 Helmert contrasts 14.9 Sum-to-zero contrasts 14.10 Polynomial contrasts 14.11 Visualizing hypotheses for different coding schemes 14.12 Orthogonal vs. Non-orthogonal contrasts 14.13 Error structure of linear models", " 14 Week 8 Lecture 14.1 Week 8 Readings For this week, I suggest reading Logan Chapter 7. I have also assigned Shmueli (2010). This is quite an important paper and I encourage you to read it quite carefully. 14.2 Warm-up Which of these is the best model? Why? Figure 0.1: Models of three different complexities. Figure adapted from Grunwald (2007) The Minimum Description Length principle 14.3 The aims of modelling – A discussion of Shmueli (2010) Before we launch into the technical details of building linear models, let’s discuss the Shmueli (2010) paper. Shmueli makes the distinction between three kinds of models Explanatory models Predictive models Descriptive models Explanatory models fit data with the goal of testing causal hypotheses about theoretical constructs. Explanatory models can therefore never be confirmed and are harder to contradict than predictive models. These are models used for inference (inferential models). Predictive models fit data solely to predict new outcomes. Models either work for prediction or don’t. Predictive models may involve parameters that are uninterpretable (e.g., Generalized Additive Models, spline fits, etc.). These models are used for prediction. Descriptive models summarize data, and that’s the only goal. These models are used to describe data. Question: What are the differences among these three types of model? Click for Answer “Explanatory modeling refers…to the application of statistical models to data for testing causal hypotheses about theoretical constructs.” (Shmueli 2010) Using this definition, explanatory modeling is testing a hypothesis or a theory and therefore can never be confirmed and are also harder (than predictive models) to contradict. We have been referring to these models as “inferential models” or models used for “inference”. Predictive models are concerned only with the ability to predict new (measurable) outcomes. Either the models works for prediction or it doesn’t. Predictive models are therefore more utilitarian than explanatory models (use whatever works!). Predictive models may involve predictors that are uninterpretable (more on this later when we discuss Generalized Additive Models and other curve fitting techniques). Example: We can think back to our discussion of population statistics and sample statistics. If we are measuring every individual of a population and we want to summarize the data, we are doing descriptive statistics. Descriptive modeling is aimed at succinctly summarizing a body of data. If we want to use that data to make some inference about a larger population, we are doing inferential statistics. If we want to use our sample to predict a new sample from that larger population, we are doing predictive statistics. As noted by Shmueli, prediction and explanation are conflated in the hypothetical-deductive paradigm of null hypothesis significance testing, because we are saying in essence “If it predicts the data, then it explains the data”. Does everyone see why this is the case? The critical distinction between explanation and prediction can be seen by looking at the Expected Prediction Error (Shmueli 2010; page 293), which we will discuss in more detail below. This distinction is at the core of model selection because, when faced with a number of competing models, we must decide whether we prefer the most accurate model (in terms of capturing the underlying mechanism) or the one that will yield the best predictions. It is also important to note that predictive models are restricted to variables which can actually be measured ahead of time. For example, a model to predict wine quality at the time of harvest cannot depend on the type of cask because the type of cask hasn’t been measured at the time the model is to be run. Likewise, the cost of running an aircraft may depend on the cost of fuel on the day of the flight, but fuel costs on the day of flight cannot be used in a predictive model because it is not possible to know what the cost of fuel will be on that day. Question: If you have to choose one or the other, do you want the most accurate model that captures the underlying mechanism, or the model that will give the best predictions? Click for Answer If we describe “real life” as, \\(\\mathscr{Y} = \\mathscr{F}(\\mathscr{X})\\), where \\(\\mathscr{F}\\) is the mechanism, then in model space, we write, \\(Y = F(X)\\), where \\(F\\) is the model. With explanatory modeling, the goal is \\(F = \\mathscr{F}\\), while with predictive modeling, the goal is \\(Y = \\mathscr{Y}\\). The expected prediction error (\\(EPE\\)) \\[\\text{EPE} = \\mathrm{E} [ (Y - \\hat{f} (X))^2 ]\\] can be broken down into three components: \\[ \\text{EPE} = \\mathrm{Var}(Y) + \\text{bias}^2 + \\mathrm{Var} (\\hat{f} (X)) \\] 1.The variance in the response, or the underlying stochasticity in the process (\\(\\mathrm{Var} (Y)\\)), 2. the bias, which describes to what degree the model is misspecified (\\(\\text{bias}^2\\)), and 3. the estimation variance, which arises because the model is developed from a sample of the data (\\(\\mathrm{Var} (\\hat{f} (X))\\)). While the goal of predictive modeling is to minimize EPE, the goal of explanatory modeling is to minimize bias. Question: Shmueli makes the case that predictive modeling requires a larger sample size than explanatory modeling. Why? Click for Answer In explanatory modeling, you only need enough data to “see whats going on” – in essence, you just need to understand the mean behavior. On the other hand, in predictive modeling, you actually want to be able to make good predictions, which means you need to understand the mean behavior AND the variability around the mean. Understanding the variability requires more data. Also, if the method for testing predictive models is to withhold some of the data at the model building stage, then you need more data. Question: What is data partitioning? When/why is it used? Click for Answer Data partitioning is when you partition the data into a “training” dataset and a “validation” dataset. The model is fit on the “training dataset” and its predictive accuracy tested on the “validation dataset”. Data partitioning is less used in explanatory modeling because by reducing the sample size, it reduces the power of the test. Question: How do explanatory and predictive models differ in their use of “dimension reduction”. (First, what is meant by “dimension reduction”.) Click for Answer Sometimes we want to combine a number of related predictors into a smaller set of predictors that are uncorrelated. The classic example of this would be Principal Components Analysis (PCA). Dimension reduction is often used for predictive modeling, because we can reduce the variance of our predictions by reducing the number of predictors used in the model. However, we do so at the expense of interpretability, since the new predictors are linear combinations of the original variables and can be difficult to interpret. (More on this in the last week of the semester.) The important point here is that in predictive modeling, we don’t care what goes into the model, only that what comes out of the model is optimal is terms of the accuracy of its predictions for new data. Question: Shmueli discusses “algorithmic modeling”. What is it? Click for Answer There are many data mining techniques which can produce predictive models that are useless for explanatory models. One example would be “neural networks”. Question: What are ensemble methods? Click for Answer Ensemble models exploit multiple models simultaneously to achieve predictive accuracy. These models might be completely incompatible (using different predictors for example) but their predictions can be merged into a distribution of predictions, the ensemble of which is better than any of the individual models. Examples include climate change models in which predictions from multiple climate models are combined, and hurricane models where you can look at multiple predicted storm tracks to get an idea of where the storm is most likely to go. Shmueli distinguishes between three types of model checking: “Model validation”: How well does the model fit the underlying causal relationships? “Model fit”: How well does the model fit the data you have? “Generalization”: How well does the model fit new data? Question: This brings us to the problem of overfitting – what is overfitting? Click for Answer Overfitting is when models are fit to the noise in the data as well as the underlying “signal”. This can happen when too many predictor variables are used and is the basis for the idea behind model “parsimony” (use the simplest model that works). Model selection depends on a clear idea of purpose – i.e. whether the model is explanatory or predictive. In explanatory models, predictors that have a theoretical justification may be left in the model even if they are not statistically significant because they are thought to be part of the true underlying model F. On the other hand, predictive models may drop predictors even if they are statistically significant, if their effect size is so small than leaving them out of the model actually increases predictive accuracy. Shmueli illustrates these ideas with two examples: Example #1: The Netflix prize Question: What was the Netflix prize about? Question: What were some of the main conclusions? Missingness is predictively informative. In other words, it is important to know what makes a person decide to rate a movie at all. Data reduction was key, and many variables you would think would be useful in the model were not (i.e. which actors were actually in the movie). You can get better predictions by combining the predictions of several smaller models than by building a single more complex model (i.e. ensemble methods work) Example #2: Online auctions. Question: What was the online auction example about? Question: What were some of the main conclusions? In this example, the goal was explanatory model, so covariates such as the number of bids were excluded because they arose from the bidding process and did not contribute to the bidding process (and therefore could not have a causal effect). R2 as a measure of explanatory power is used to compare models (this is the common metric of explanatory power, more on this next week) The authors retained insignificant variables because the model was built from theory and not from the data itself. Major conclusions: Prediction, explanation, and description are three very different goals for a statistical analysis. R2 is a measure of explanatory power but does not tell you anything about predictive power because you may have fit the model to noise in the original sample. Also, explanatory models tend to be overly optimistic with regards to predictive power. “Checking the model” can include three component: 1) Model validation: how well does the model fit the underlying causal mechanism?; 2) Model fit: how well does the model fit the data you have? (e.g., \\(R^2\\)); 3) Generalization: how well does the model fit new data? (not \\(R^2\\)). Final discussion questions: Must an explanatory model have some level of predictive power to be considered scientifically useful? Must a predictive model have sufficient explanatory power to be scientifically useful? 14.4 Introduction to linear models In the first half of the semester, we were discussing how to identify distributions, and how to fit distributions to data (i.e. parameter estimation). This is a very simple application of fitting a “statistical model”. For example, we would write \\[ Y \\sim N(\\mu,\\sigma^{2}) \\] and then we would use MLE (or other methods) to estimate the two parameters of this distribution \\(\\mu\\) and \\(\\sigma\\). We can also re-write this model as \\[ Y = \\mu + \\epsilon \\mbox{ where } \\epsilon \\sim N(0,\\sigma^{2}) \\] Note that the equation on the left has an equal sign, since Y is strictly equal to the sum of \\(\\mu\\) and \\(\\epsilon\\) and it is \\(\\epsilon\\) that is drawn from a statistical distribution. As the semester proceeds, we will spend a lot of time thinking about the \\(\\epsilon\\), which is the residual representing the difference between the actual response value and what is predicted by the model (\\(\\mu\\) in this case). In essence, we have decomposed Y into a component that is fixed (\\(\\mu\\)) and a component that is stochastic (\\(\\epsilon\\)). We read this equation as “Y is modeled as having a mean \\(\\mu\\) and a random error that is Normally distributed”. This illustrates nicely the general format of a linear model: Response = Deterministic function + Stochastic function Some authors will write this as: Response = Model + Error but I dislike this for two reasons. 1) Your “model” is not just the deterministic function. A model for your data includes the deterministic component and the stochastic component. 2) I don’t like referring to the stochastic component as “error” because that implies that your model is faulty in some way. This stochasticity may be built into the system that you are trying to model, it may be irreducible variability that is not your “fault” and therefore should be considered a legitimate part of what you are trying to model rather than a mistake or an error. Going back for a moment to our original model \\(Y \\sim N(\\mu,\\sigma^{2})\\), we can either leave \\(\\mu\\) as a constant to be estimated or we can try and improve our model by adding complexity to \\(\\mu\\). In other words, if Y represents the size of individuals in a collection of gentoo penguins, we might model mean size as being a function of a continuous variable like age. In that case, our model looks like: \\[ Y = \\beta_{0}+\\beta_{1}Age + \\epsilon \\mbox{ where } \\epsilon \\sim N(0,\\sigma^{2}) \\] or, equivalently, \\[ Y \\sim N(\\beta_{0}+\\beta_{1}Age,\\sigma^{2}) \\] where I’ve used \\(\\beta_{0}\\) to represent the intercept of this linear model and \\(\\beta_{1}\\) to represent the slope (the amount by which Y increases for each year of Age). We may choose to model \\(\\mu\\) as a function of a discrete variable like Colony. In this case, we would write the model as \\[ Y = \\mu_{i} + \\epsilon \\mbox{ where } \\epsilon \\sim N(0,\\sigma^{2}) \\] where each group has its own \\(\\mu\\):\\(\\mu_{1}\\),\\(\\mu_{2}\\),…,\\(\\mu_{n}\\). When the covariates are continuous (the first case), we call this regression. When the covariates represent discrete groups or categories (the second case), we call this ANOVA. But both of these are simply different examples of linear modeling, which is it itself just a special case of fitting distributions, which is what we have been doing all along. In this class, we are not defining linear models as models that can be described by a line. Instead, we define linear models as models in which the parameters can be arranged in a linear combination, and no parameter is a multiplier, divisor, or exponent to any other parameter. For example, \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\epsilon \\text{, } \\epsilon \\sim N(0, \\sigma^2)\\), is a linear model because we can create a new variable \\(Z=X^{2}\\), which makes it more obvious that Y is a linear function of X and this new variable Z. \\(Y = \\beta_0 + \\beta_1 X + \\beta_1^2 X\\) is also a linear model. An example of a nonlinearizable model is \\(Y = \\frac{\\beta_1 + \\beta_2 X}{\\beta_3 + \\beta_4 X}\\). In this class, we will cover a suite of different linear models. Here I introduce them all briefly as a roadmap for the rest of the semester. Variables that are best described as coming from a Gaussian distribution can be modelled by a model of the type introduced just above. Here I just re-use the simple example above where the mean \\(\\mu\\) is a function of a covariate called \\(Age\\). \\[ Y \\sim N(\\mu,\\sigma^{2}) \\\\ \\mu = \\beta_{0}+\\beta_{1}Age \\] However, let’s say \\(Y\\) follows a different distribution, such as a Binomial. In that case the model would look like \\[ Y \\sim Binom(n,p) \\\\ logit(p) = \\beta_{0}+\\beta_{1}Age \\] In this case, the parameter influenced by \\(Age\\) is the probability \\(p\\) and here we have to transform the parameter using a function called the logit() function that we will be introduced to formally in Week 10. But mathematically, all we care about now is that some function of the parameter has a linear relationship to \\(Age\\), just like before. What if \\(Y\\) is best modelled as a Poisson distributed variable? In that case, we have \\[ Y \\sim Pois(\\lambda) \\\\ log(\\lambda) = \\beta_{0}+\\beta_{1}Age \\] where it is the parameter \\(\\lambda\\) (actually, the log of \\(\\lambda\\), stay tuned for more details in Week 10) that is linearly related to the covariate. We can, in fact, write down any number of models like this, for example maybe the data are Beta distributed and we want \\(Age\\) to control the \\(\\alpha\\) parameter. \\[ Y \\sim Beta(\\alpha,\\beta) \\\\ f(\\alpha) = \\beta_{0}+\\beta_{1}Age \\] where f() is some mathematical function. Maybe we think the \\(\\beta\\) parameter is control by some other covariate, say Weight \\[ Y \\sim Beta(\\alpha,\\beta) \\\\ f(\\alpha) = \\beta_{0}+\\beta_{1}Age \\\\ g(\\beta) = \\beta_{2}+\\beta_{3}Weight \\] where f() and g() are some functions that transform the parameter so it is linearly related to the covariates. The point is that all linear modelling takes this general form. You decide on the best distribution to describe the data, and then you build detail into the model by finding covariates that you think describe variation in the parameters. These covariates allow each response to be modelled by a distribution that is tailored for it, accounting for whatever covariates you think describe why some data points \\(Y\\) are larger than other data points. Linear regression and all flavors of ANOVA deal with the model of the basic type introduced first: \\[ Y \\sim N(\\mu,\\sigma^{2}) \\\\ \\mu = \\mbox{covariates added together linearly} \\] All the other models get lumped into “Generalized Linear Models” and we will discuss that in Week 10. 14.5 Linear models | example with continuous covariate Linear models can be written in matrix form (with vectors and matrices), which is more clear when you look at an example. For the model: \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\text{, where } \\epsilon_i \\sim N(0, \\sigma^2) \\] We can expand this model for every data point (\\(i\\)), for a total of five data points: \\[ Y_1 = \\beta_0 + \\beta_1 X_1 + \\epsilon_1 \\text{, where } \\epsilon_1 \\sim N(0, \\sigma^2) \\] \\[ Y_2 = \\beta_0 + \\beta_1 X_2 + \\epsilon_2 \\text{, where } \\epsilon_2 \\sim N(0, \\sigma^2) \\] \\[ Y_3 = \\beta_0 + \\beta_1 X_3 + \\epsilon_3 \\text{, where } \\epsilon_3 \\sim N(0, \\sigma^2) \\] \\[ Y_4 = \\beta_0 + \\beta_1 X_4 + \\epsilon_4 \\text{, where } \\epsilon_4 \\sim N(0, \\sigma^2) \\] \\[ Y_5 = \\beta_0 + \\beta_1 X_5 + \\epsilon_5 \\text{, where } \\epsilon_5 \\sim N(0, \\sigma^2) \\] Note that \\(\\epsilon_{1}\\), \\(\\epsilon_{2}\\),…,\\(\\epsilon_{5}\\) are i.i.d. draws from the same error distribution. In matrix form the model can be written as: \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\\\ Y_5 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_1 \\\\ 1 &amp; X_2 \\\\ 1 &amp; X_3 \\\\ 1 &amp; X_4 \\\\ 1 &amp; X_5 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{bmatrix}\\] where \\(Y\\) is the response vector, \\(X\\) is the design matrix, \\(\\beta\\) is the vector of parameters, and \\(\\epsilon\\) is the error vector. The design matrix tells us which combination of parameters are used to predict each data point. We can simply this notation even further by writing it as \\[ \\overrightarrow{Y} = \\overleftrightarrow{X}\\overrightarrow{\\alpha} + \\overrightarrow{\\epsilon} \\] where \\(\\overrightarrow{Y}\\) is the response vector, \\(\\overleftrightarrow{X}\\) is design matrix, \\(\\overrightarrow{\\alpha}\\) is the vector of parameters (coefficients), \\(\\overrightarrow{\\epsilon}\\) is the vector of residuals (i.e. the remaining stochastic component). (Note: Keeping with traditional mathematical notation, vectors have a single headed arrow whereas as matrices have a double headed arrow. Also, here I use \\(\\overrightarrow{\\alpha}\\) to represent the vector of model coefficients. More traditional would be to use the Greek letter \\(\\overrightarrow{\\beta}\\), but in keeping with my notation above I will stick with \\(\\overrightarrow{\\alpha}\\).) The design matrix is the crucial element here. It tells us what combination of parameters are used to predict each data point (the Ys). In the above example we had one continuous predictor, and the interpretation in this case is fairly straightforward. The interpretation and construction of the design matrix gets more complicated when we have discrete or categorical variables because there are many ways in which to parametrize the design matrix. Rewriting what we had above for a categorical predictor \\[ Y_i = \\alpha_{j(i)} + \\epsilon_i \\text{, where } \\epsilon_i \\sim N(0, \\sigma^2) \\] where \\(\\alpha_{j(i)}\\) is the mean of the group \\(j\\) to which data point \\(i\\) belongs (before we had used the variable \\(\\mu\\) to represent the mean but here we will use \\(\\alpha\\) as the mean for each of the \\(j\\) discrete groups). We can expand this model for every data point (let’s say there are 5 data points and 3 groups): \\[ Y_1 = \\alpha_{1(1)} + \\epsilon_1 \\text{, where } \\epsilon_1 \\sim N(0, \\sigma^2) \\] \\[ Y_2 = \\alpha_{3(2)} + \\epsilon_2 \\text{, where } \\epsilon_2 \\sim N(0, \\sigma^2) \\] \\[ Y_3 = \\alpha_{1(3)} + \\epsilon_3 \\text{, where } \\epsilon_3 \\sim N(0, \\sigma^2) \\] \\[ Y_4 = \\alpha_{2(4)} + \\epsilon_4 \\text{, where } \\epsilon_4 \\sim N(0, \\sigma^2) \\] \\[ Y_5 = \\alpha_{2(5)} + \\epsilon_5 \\text{, where } \\epsilon_5 \\sim N(0, \\sigma^2) \\] Here, data point 1 is in group 1, 2 is in group 3, 3 is in group 1, 4 is in group 2, and 5 is in group 2. Both \\(Y_1\\) and \\(Y_3\\) are in group 1, for example, so will have the same predicted value. However, the error (\\(\\epsilon\\)) for each is different. This is called the “group means”, “cell means”, or “dummy” coding. Think of it like representing each group with a binary indicator telling you if each point is in the group (a “dummy” variable). We can illustrate dummy coding with an example: iris.sub &lt;- iris[sample(x = 1:nrow(iris), size = 12), ] model.matrix( ~ -1 + iris.sub$Species) ## iris.sub$Speciessetosa iris.sub$Speciesversicolor iris.sub$Speciesvirginica ## 1 0 0 1 ## 2 0 1 0 ## 3 0 1 0 ## 4 0 1 0 ## 5 0 1 0 ## 6 0 0 1 ## 7 0 1 0 ## 8 0 0 1 ## 9 0 0 1 ## 10 1 0 0 ## 11 0 1 0 ## 12 0 1 0 ## attr(,&quot;assign&quot;) ## [1] 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$`iris.sub$Species` ## [1] &quot;contr.treatment&quot; iris.means &lt;- ddply(iris.sub, .(Species), summarise, Sepal.Length = mean(Sepal.Length)) dummy &lt;- summary(lm(iris.sub$Sepal.Length ~ -1 + iris.sub$Species)) dummy$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## iris.sub$Speciessetosa 5.000000 0.5652995 8.844869 9.840863e-06 ## iris.sub$Speciesversicolor 5.814286 0.2136631 27.212398 5.922498e-10 ## iris.sub$Speciesvirginica 6.775000 0.2826497 23.969596 1.829390e-09 iris.fig &lt;- ggplot(iris.sub, aes(x = Species, y = Sepal.Length)) + geom_point() + geom_point(data = iris.means, aes(x = Species, y = Sepal.Length, col = Species), size = 3) + labs(y = &quot;Sepal length&quot;) + theme_classic() + theme(text = element_text(size = font.size)) iris.fig Don’t worry about the syntax, but look closely at the figure. The figure is illustrating the values for each group, and the red, green, and blue filled circles represent the mean within each group. Our simple “dummy coding” model simply states that each value (the individual black dots) can be modelled as the mean for that group PLUS the residual difference (i.e. \\(\\epsilon\\)) between the group mean and that individual value. Note that the function we use to fit the linear model ‘lm’ is one that we haven’t formally introduced yet (but that will come soon). For now, just observe how the output of the model represents the parameters we are interested in (in this case, the group means). We write the equation in matrix form as \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\\\ Y_5 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{bmatrix}\\] Like with the continuous example, \\(Y\\) is the response vector of length \\(n\\), the design matrix is an \\(n \\times p\\) matrix of ones and zeroes, the vector of parameters has length \\(p\\), and \\(\\epsilon\\) is the error vector of length \\(n\\). Note that we can still write the equation in the compact form: \\[ \\overrightarrow{Y} = \\overleftrightarrow{X}\\overrightarrow{\\beta} + \\overrightarrow{\\epsilon} \\] Notice that we have a system of equations, one for each group. (We may have several data points in each group – so our model wont fit each data point exactly; the error term takes care of this…) We also have three unknowns, which are the three group means. The three model parameters \\(\\alpha_{1}\\), \\(\\alpha_{2}\\), and \\(\\alpha_{3}\\) are all uniquely specified by the data. However, there are other equivalent ways of writing the same model. For example, we could write \\[ Y_{i} \\sim \\bar{\\alpha} + \\alpha_{j(i)} + \\epsilon_{i} \\mbox{, where } \\epsilon_{i} \\sim N(0,\\sigma^{2}) \\] where now \\(\\bar{\\alpha}\\) represents the average of all the group means and \\(\\alpha_{j(i)}\\) is the DIFFERENCE between the mean of group j and the mean of all the groups. (If the number of data points within each group are equal, than the mean of the group means is simply the mean of all the data, and we often refer to this as the ‘grand’ mean. However, if the groups are unbalanced, this may not be the case.) We can explicitly write out what this model means for each and every data point \\(Y_{i}\\): \\[ Y_1 = \\bar{\\alpha} + \\alpha_{1(1)} + \\epsilon_1 \\text{, where } \\epsilon_1 \\sim N(0, \\sigma^2) \\] \\[ Y_2 = \\bar{\\alpha} + \\alpha_{3(2)} + \\epsilon_2 \\text{, where } \\epsilon_2 \\sim N(0, \\sigma^2) \\] \\[ Y_3 = \\bar{\\alpha} + \\alpha_{1(3)} + \\epsilon_3 \\text{, where } \\epsilon_3 \\sim N(0, \\sigma^2) \\] \\[ Y_4 = \\bar{\\alpha} + \\alpha_{2(4)} + \\epsilon_4 \\text{, where } \\epsilon_4 \\sim N(0, \\sigma^2) \\] \\[ Y_5 = \\bar{\\alpha} + \\alpha_{2(5)} + \\epsilon_5 \\text{, where } \\epsilon_5 \\sim N(0, \\sigma^2) \\] We write the equation in matrix form as \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\\\ Y_5 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\bar{\\alpha} \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\end{bmatrix}\\] We have rewritten the model in terms of an overall mean (the mean of all the group means) and the effect of membership in each of the groups is added to this. Because the differences of each group to the overall mean sum to zero (do you see why this is?) Notice that now we have introduced a fourth parameter without changing the data. We now have three group means and four parameters, so we cannot uniquely estimate all four parameters independently. If we estimate the overall global mean \\(\\bar{\\alpha}\\), then we can only estimate two of the three group differences. The difference associated with the last group is completely constrained. This is analogous to our discussion of the number of degrees of freedom. We say that this model is over parametrized. We can solve this problem is a number of ways. We can remove a parameter altogether (in our first example, we implicitly removed the global mean \\(\\bar{\\alpha}\\)), or we can create new groups (linear combinations of the old groups) that are uniquely parameterized. Parameters that are not estimated because they are fully constrained by the other parameters are said to be aliased. Question: What parameter was removed/aliased in the dummy coding scheme? In that case, \\(\\bar{\\alpha}\\) was removed/aliased. We have now introduced two perfectly “correct” ways of writing this linear model with different design matrices and correspondingly different interpretations of the model coefficients. These different ways of parametrizing the model for categorical variables are called “coding schemes”. While the models represent the same math, the different ways of writing the models down permits different hypothesis tests about the model parameters. In the case where we are looking at group means \\[ Y_{i} \\sim \\alpha_{j(i)} + \\epsilon \\mbox{, where } \\epsilon \\sim N(0,\\sigma^{2}) \\] the (implicit) null hypothesis is that the group means are zero. Mathematically, \\[ H_{0}: \\alpha_{j} = 0 \\] However, in the second case, \\[ Y_{i} \\sim \\bar{\\alpha} + \\alpha_{j(i)} + \\epsilon \\mbox{, where } \\epsilon \\sim N(0,\\sigma^{2}) \\] the same null hypothesis \\[ H_{0}: \\alpha_{j} = 0 \\] means something very different. Therefore, we need to make sure to use coding schemes that will allow us to most directly test the hypothesis of interest. (This is why there is no one “correct” approach.) 14.6 Resolving overparameterization using contrasts We can reduce the number of parameters in our model by creating a new set of parameters, each of which is a linear combination of groups, rather than a single group effect. Each parameter can include contributions from any number of the groups. We will now introduce of series of potential options for doing this, each of which is designed to test a specific hypothesis about how groups may differ. It is occassionally the case that none of these “off the shelf” options are appropriate for your analysis and you may need to design a design matrix that is tailored specifically for your research question, so the options we introduce here are just a few of the options and meant to illustrate the overall idea. The following decision tree is designed to help you choose the correct coding scheme for your specific analysis. We know now that there are many ways in which to write a linear model with categorical predictors, and that the choices made about how to contrast the various levels of the factors is important for interpreting the results. In addition to the “dummy” or “cell means” coding scheme introduced above, we will go over 4 common “off-the-shelf” coding schemes that are used. Keep in mind that these are not all the possible coding schemes that could be used to write a model. Many others exist, and often it is necessary to write your own custom coding schemes to answer a particular question of interest. Before getting into the math that describes the various options, we’ll work through a simple example. Lets say we have a study in which we measure the average lifespan of four species of penguins. For arguments sake, lets say we track 10 individuals in each of four species (40 penguins total). The mean lifespan in each group is 13, 12, 9, and 6. One way to write this model would be to simply estimate the mean of each group. This is the ‘cell means’ coding we described above. Nothing is being contrasted here. In other words, no group is being compared to any other group. Parameter Method\\(\\#1\\) Method\\(\\#2\\) Method\\(\\#3\\) Method\\(\\#4\\) \\(\\mu\\) aliased \\(\\alpha_{1}\\) 13 \\(\\alpha_{2}\\) 12 \\(\\alpha_{3}\\) 9 \\(\\alpha_{4}\\) 6 Now let’s say we want to write the model differently. Now we want to explicitly compare the lifespan of penguins in spp 2, 3, and 4 to the lifespan of penguins of spp 1. In other words, we are contrasting spp 2 vs. spp 1, spp 3 vs. spp 1, and spp 4 vs. spp 1. The estimates we get for these contrasts will allow us to test hypotheses about how each group compares to the first. Parameter Method\\(\\#1\\) Method\\(\\#2\\) Method\\(\\#3\\) Method\\(\\#4\\) \\(\\mu\\) aliased aliased \\(\\alpha_{1}\\) 13 13 \\(\\alpha_{2}\\) 12 -1 \\(\\alpha_{3}\\) 9 -4 \\(\\alpha_{4}\\) 6 -7 Let’s try another set of contrasts. In this case, let’s say that, for whatever reason, we want to compare the Mean(spp 1,spp 2) vs. Mean(spp 1), as well as Mean(spp 1, spp 2, spp 3) vs. Mean(spp 1,spp 2), and the Mean(spp 1, spp 2, spp 3, spp 4) vs. Mean(spp 1, spp 2, spp 3). That would yield the following parameter estimates. (Note that in this case, we have pulled out the group mean as a parameter, and the mean of spp 1 has been aliased, so its not being estimated.) Parameter Method\\(\\#1\\) Method\\(\\#2\\) Method\\(\\#3\\) Method\\(\\#4\\) \\(\\mu\\) aliased aliased 10 \\(\\alpha_{1}\\) 13 13 aliased \\(\\alpha_{2}\\) 12 -1 -0.5 \\(\\alpha_{3}\\) 9 -4 -1.1667 \\(\\alpha_{4}\\) 6 -7 -1.3333 Finally, imagine we want to compare each group to the grand mean (\\(\\mu\\)). In this case we pull out the \\(\\mu\\) and estimate it. Than we compare spp 1 vs. \\(\\mu\\), and spp 2 vs. \\(\\mu\\), and spp 3 vs. \\(\\mu\\). We can’t also do spp 4 vs. \\(\\mu\\) because once we’ve calculated the other four quantities (\\(\\mu\\), and the three contrasts with \\(\\mu\\)), the final quantity (spp 4 vs. \\(\\mu\\)) is completely determined and therefore is aliased. Parameter Method\\(\\#1\\) Method\\(\\#2\\) Method\\(\\#3\\) Method\\(\\#4\\) \\(\\mu\\) aliased aliased 10 10 \\(\\alpha_{1}\\) 13 13 aliased 3 \\(\\alpha_{2}\\) 12 -1 -0.5 2 \\(\\alpha_{3}\\) 9 -4 -1.1667 -1 \\(\\alpha_{4}\\) 6 -7 -1.3333 aliased As a review, the first way of writing the model does not contrast anything. It simply estimates the group means, and the model for group \\(j\\) is simply the group mean for \\(j\\) plus an error term. The other three model parameterizations explicitly contrast the groups, but in different ways. Hopefully, by working through a few examples, you can see that there are many ways we can write a model, and that each way contrasts different things and allows for an explicit hypothesis test on differences that are of interest. In this example, Method #1 is the ‘cell means’ coding that we started with, Method #2 is called ‘effect coding’ or ‘treatment contrasts’, Method #3 is called Helmert contrasts, and Method #4 is called the ‘sum-to-zero’ contrast we already briefly introduced. We’ll walk through a more formal introduction of these contrasts now. 14.7 Effect coding/Treatment constrast In effect coding or treatment contrasts, you set aside one group as the “control” and estimate the difference of every other group from that control group. The estimated parameters therefore represent the “effect” of moving from the control group to any other group (which often represents the impact of a “treatment” relative to the control group). \\[ Y_i = \\alpha_{\\text{control}} + \\alpha_{j(i)} + \\epsilon_i \\text{, where } \\epsilon_i \\sim N(0, \\sigma^2) \\] where \\(\\alpha_{control}\\) is the mean of the control group, and \\(\\alpha_{j&gt;1}\\) is the difference between group \\(j\\) and the control group. If we have \\(k\\) groups, we only have \\(k-1\\) \\(\\alpha_{j}\\) to estimate since we use one group as the control. We can think of \\(\\alpha_{control}\\) as an intercept. We can summary this as Parameter Interpretation Null hypothesis \\(\\alpha_{control}\\) mean of control group (\\(\\mu_1\\)) \\(H_0: \\mu_1 = 0\\) \\(\\alpha_2\\) difference between mean of group 2 and mean of control group (\\(\\mu_2 - \\mu_1\\)) \\(H_0: \\mu_2 - \\mu_1 = 0\\) \\(\\alpha_3\\) difference between mean of group 3 and mean of control group (\\(\\mu_3 - \\mu_1\\)) \\(H_0: \\mu_3 - \\mu_1 = 0\\) This basically uses the first group as the “intercept” of the model, so instead of this \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\] we have \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_{control} = \\mu_1 \\\\ \\alpha_{2} = \\mu_2 - \\mu_{1}\\\\ \\alpha_{3} = \\mu_3 - \\mu_{1} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\] Note that the last two data points come from the same group, and so those two rows will always be the same. In theory, there are many many rows NOT shown. The first version specifies the model in terms of the group means. The second version specifies the model in terms of linear combinations of the group means. The latter approach permits a straightforward way of estimating quantities of biological interest (usually we are interested in testing hypotheses about differences). contrasts(iris.sub$Species) &lt;- &quot;contr.treatment&quot; model.matrix(~ iris.sub$Species) ## (Intercept) iris.sub$Speciesversicolor iris.sub$Speciesvirginica ## 1 1 0 1 ## 2 1 1 0 ## 3 1 1 0 ## 4 1 1 0 ## 5 1 1 0 ## 6 1 0 1 ## 7 1 1 0 ## 8 1 0 1 ## 9 1 0 1 ## 10 1 0 0 ## 11 1 1 0 ## 12 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$`iris.sub$Species` ## [1] &quot;contr.treatment&quot; treatment &lt;- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species)) treatment$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0000000 0.5652995 8.844869 9.840863e-06 ## iris.sub$Speciesversicolor 0.8142857 0.6043306 1.347418 2.107846e-01 ## iris.sub$Speciesvirginica 1.7750000 0.6320240 2.808438 2.042912e-02 In this case, you can see that the model is estimating the mean of the first group (setosa) and then the difference between the second and first group and the difference between the third and first groups. This allows you to test hypotheses about the differences, which is often more meaningful than testing hypotheses about the group means themselves. 14.8 Helmert contrasts Helmert contrasts include a mean treatment effect and then the remaining parameters estimate the group mean relative to the mean treatment effect of the groups before it. This is usually only sensible when the categories have some natural order to them, and are not very common in ecology but they are the default in SPLUS (but not R fortunately). (There are many different versions, but as long as you keep the interpretation of each coefficient straight, it doesn’t matter which you use. I think this version is the clearest.) The equation is difficult to write compactly but the Helmert contrast can be understood using the following example involving three groups. Parameter Interpretation Null hypothesis \\(\\bar{\\alpha}\\) mean of group means \\(H_0: \\frac{\\mu_1 + \\mu_2 + \\mu_3}{3} = 0\\) \\(\\alpha_2\\) difference between mean of groups 1 and 2 and mean of group 1 \\(H_0: \\frac{\\mu_1 + \\mu_2}{2} - \\mu_1 = 0\\) \\(\\alpha_3\\) difference between mean of groups 1-3 and mean of groups 1 and 2 \\(H_0: \\frac{\\mu_1 + \\mu_2 + \\mu_3}{3} - \\frac{\\mu_1 + \\mu_2}{2} = 0\\) This can be expressed as: \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 0 &amp; 2 \\\\ 1 &amp; 0 &amp; 2 \\end{bmatrix} \\begin{bmatrix} \\alpha_{\\small{\\mbox{mean of group means}}} = \\frac{\\mu_{1}+\\mu_{2}+\\mu_{3}}{3} \\\\ \\alpha_{2} = \\frac{\\mu_{1}+\\mu_{2}}{2}-\\mu_{1} \\\\ \\alpha_{3} = \\frac{\\mu_{1}+\\mu_{2}+\\mu_{3}}{3}-\\frac{\\mu_{1}+\\mu_{2}}{2} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\] where I have assumed it is \\(\\alpha_{1}\\) that is aliased (although different books define this differently, so you may find versions in which it is parameterized differently). contrasts(iris.sub$Species) &lt;- &quot;contr.helmert&quot; model.matrix(~ iris.sub$Species) ## (Intercept) iris.sub$Species1 iris.sub$Species2 ## 1 1 0 2 ## 2 1 1 -1 ## 3 1 1 -1 ## 4 1 1 -1 ## 5 1 1 -1 ## 6 1 0 2 ## 7 1 1 -1 ## 8 1 0 2 ## 9 1 0 2 ## 10 1 -1 -1 ## 11 1 1 -1 ## 12 1 1 -1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$`iris.sub$Species` ## [1] &quot;contr.helmert&quot; helmert &lt;- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species)) helmert$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.8630952 0.2223876 26.364306 7.849913e-10 ## iris.sub$Species1 0.4071429 0.3021653 1.347418 2.107846e-01 ## iris.sub$Species2 0.4559524 0.1379189 3.305944 9.142830e-03 14.9 Sum-to-zero contrasts Sum-to-zero contrast set the intercept to the mean treatment effect (the mean of the group means) and then the remaining parameters estimate the mean treatment effect of that group relative to the mean treatment effect. (We introduced this contrast briefly earlier.) \\[ Y_i = \\bar{\\alpha} + \\alpha_{j(i)} + \\epsilon_i \\text{, where } \\epsilon_i \\sim N(0, \\sigma^2) \\] where \\(\\bar{\\alpha}\\) is the mean treatment effect for all groups, and \\(\\alpha_{j}\\) is the difference between group \\(j\\) and \\(\\bar{\\alpha}\\). We can summarize this as Parameter Interpretation Null hypothesis \\(\\bar{\\alpha}\\) mean of group means \\(H_0: \\frac{\\mu_q}{p} = \\frac{\\mu_1 + \\mu_2 + \\mu_3}{3} = 0\\) \\(\\alpha_1\\) difference between mean of group 1 and mean of group means \\(H_0: \\mu_1 - \\frac{\\mu_q}{p} = 0\\) \\(\\alpha_2\\) difference between mean of group 2 and mean of group means \\(H_0: \\mu_2 - \\frac{\\mu_q}{p} = 0\\) This can be expressed as: \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} \\bar{\\alpha} = \\frac{\\mu_{q}}{p} = \\frac{\\mu_{1}+\\mu_{2}+\\mu_{3}}{3} \\\\ \\alpha_{1} = \\mu_{1}-\\frac{\\mu_{q}}{p} \\\\ \\alpha_{2} = \\mu_{2}-\\frac{\\mu_{q}}{p} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\] where we see that it is the last parameter (\\(\\alpha_{3}\\)), the one associated with the group mean of the last group (\\(\\mu_{3}\\)) which is aliased. contrasts(iris.sub$Species) &lt;- &quot;contr.sum&quot; model.matrix(~ iris.sub$Species) ## (Intercept) iris.sub$Species1 iris.sub$Species2 ## 1 1 -1 -1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 0 1 ## 5 1 0 1 ## 6 1 -1 -1 ## 7 1 0 1 ## 8 1 -1 -1 ## 9 1 -1 -1 ## 10 1 1 0 ## 11 1 0 1 ## 12 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$`iris.sub$Species` ## [1] &quot;contr.sum&quot; sumtozero &lt;- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species)) sumtozero$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.86309524 0.2223876 26.3643056 7.849913e-10 ## iris.sub$Species1 -0.86309524 0.3949398 -2.1853845 5.667501e-02 ## iris.sub$Species2 -0.04880952 0.2543100 -0.1919293 8.520582e-01 There is one final “off-the-shelf” contrast that we will learn, and that is polynomial contrasts. 14.10 Polynomial contrasts Polynomial contrasts simply involve fitting polynomial terms of increasing order; polynomial contrasts only make sense if the covariate represents an ordinal value, so that the different levels of the factor are ordered and have equal spacing (income, years of education, etc.). This is really just a form of multiple regression (so a preview of what we will be doing in a couple of weeks…) This one is a little hard to summarize in an equation, but is easily understood by example. Parameter Interpretation Null hypothesis \\(\\beta_0\\) Y intercept \\(H_0: \\beta_0 = 0\\) \\(\\beta_1\\) Partial slope for the linear term \\(H_0: \\beta_1 = 0\\) \\(\\beta_2\\) Partial slope for the quadratic term \\(H_0: \\beta_2 = 0\\) This can be expressed as: \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ Y_4 \\end{bmatrix} = \\begin{bmatrix} 0.58 &amp; -0.71 &amp; 0.41 \\\\ 0.58 &amp; 0 &amp; -0.82 \\\\ 0.58 &amp; 0.71 &amp; 0.41 \\\\ 0.58 &amp; 0.71 &amp; 0.41 \\end{bmatrix} \\begin{bmatrix} \\beta_{0} = \\mbox{mean} \\\\ \\beta_{1} = \\mbox{coefficient for the linear term} \\\\ \\beta_{2} = \\mbox{coefficient for the quadratic term} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\] where the coefficients (\\(\\beta\\)) can be calculated using the values in the contrast matrix (we will call this \\(\\mathbf{P}\\)) associated with each group. Data point 1 is in group 1, 2 in group 3, 3 in group 1, 4 in group 2, and 5 in group 2. \\[ \\beta_{k - 1} = \\frac{\\sum_{j} P_{kj} \\bar{Y}_j}{\\sum_{j} P_{kj}^2} \\] \\[ \\beta_0 = \\frac{1 \\times \\bar{Y}_{grp 1} + 1 \\times \\bar{Y}_{grp 2} + 1 \\times \\bar{Y}_{grp 3}}{1^2 + 1^2 + 1^2} \\] \\[ \\beta_1 = \\frac{-0.71 \\times \\bar{Y}_{grp 1} + 0 \\times \\bar{Y}_{grp 2} + 0.71 \\times \\bar{Y}_{grp 3}}{(-0.71)^2 + 0^2 + 0.71^2} \\] and so forth… The values in the design matrix look complicated, but they aren’t really. Each column is scaled to have length 1 (which means all the denominators are actually just 1). 14.11 Visualizing hypotheses for different coding schemes The different contrasts are different ways of comparing groups to different baselines. You can either compare each group to zero, a control group, to the mean of the group means (often called the grand mean), to the average of all previous groups, etc. baselines &lt;- data.frame(Baseline = c(&quot;Mean of group means&quot;, &quot;Control&quot;, &quot;Zero&quot;), Value = c(mean(iris.means$Sepal.Length), iris.means$Sepal.Length[1], 0)) iris.fig.baselines &lt;- ggplot(iris.sub, aes(x = Species, y = Sepal.Length)) + geom_point() + geom_hline(data = baselines, aes(yintercept = Value), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_text(data = baselines, aes(label = Baseline, x = Inf, y = Value), hjust = 1.0, vjust = 1.0, size = 5) + geom_point(data = iris.means, aes(x = Species, y = Sepal.Length, col = Species), size = 3) + labs(y = &quot;Sepal length&quot;) + theme_classic() + theme(text = element_text(size = font.size)) iris.fig.baselines 14.12 Orthogonal vs. Non-orthogonal contrasts Figure 14.1: The segment AB is orthogonal to the segement CD. Source: Wikipedia In a broad sense, orthogonal refers to things that are perpendicular. For some cases, that’s easy to think about. But what does it mean with respect to categorical comparisons in linear models? In a simple model with one categorical predictor with five different groups (a, b, c, d, and e): \\(Y_i = \\alpha_{j(i)} + \\epsilon_i \\text{, where } \\epsilon_i \\sim N(0, \\sigma^2)\\), we may be interested in comparing a vs. b, a vs. c, or even a, b, and c vs. d and e. There are many ways in which we might want to interpret differences among the groups, and each comparison is called a contrast. There are a large number of contrasts, but few orthogonal contrasts. For example, if we compare a vs. b and a vs. c, then a third comparison, b vs. c is not orthogonal because it is implicity included in the first two contrasts. Note that treatment contrasts are not orthogonal. Mathematically, contrasts are orthogonal if their dot product is equal to zero or, equivalently, if the product of the pairwise coefficients sums to zero. In other words, let’s say we have three groups (three “levels of our factor”). Two possible contrasts would be Group Contrast A Contrast B 1 2 0 2 -1 1 3 -1 1 Contrast A compares Group 1 against the average of Group 2 and 3. Contrast B compares Group 2 vs. Group 3. Are these two contrasts orthogonal? Yes! \\[ \\sum\\limits_{i=1}^{3} \\alpha_{i}\\beta_{i} = (2)(0)+(-1)(1)+(-1)(-1) = 0 \\] If you have \\(k\\) levels of a factor (\\(k\\) numbers of groups), you only have \\(k - 1\\) orthogonal contrasts. Why does this matter? With non-orthogonal contrasts, the order in which you make you comparisons in an Analysis of Variance will change your results and interpretations. See Aho section 10.3.2 for details. 14.13 Error structure of linear models Response = Deterministic function + Stochastic function Only part of a statistical model is deterministic. It’s important to understand whether the stochastic part of the model is properly specified. We have been referring to “error” as being normally distributed, \\(\\epsilon \\sim N(0, \\sigma^2)\\), and this is often assumed for linear models (including regression). Imagine you have modeled the number of plants in quadrats. Is your error normally distributed? Other models for error can be more appropriate, depending on the data and the model, e.g., \\(\\epsilon \\sim \\text{Bernoulli}()\\) or \\(\\epsilon \\sim \\text{Poisson}()\\). Everything to this point has assumed that the residuals for each data point (i.e. \\(\\epsilon_{i}\\)) are independent from one another and identifcally distributed (as \\(N(0,\\sigma^{2}\\))). \\[ Y_i = \\alpha_{j(i)} + \\epsilon_i \\text{, where }\\epsilon_i \\sim N(0, \\sigma^2 \\mathbf{I}) \\] \\[ \\sigma^2 \\mathbf{I} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2 \\end{bmatrix} \\] Diagonal elements represent the \\(\\mathrm{Var}[Y_i]\\) and the off-diagonal elements are \\(\\mathrm{Cov}[Y_i, Y_j]\\). Here, \\(\\mathrm{Cov}[Y_i, Y_j] = 0\\), which means that the error for data point \\(Y_i\\) is independent of the error for data point \\(Y_j\\). But, this is not always the case. Sometimes, a positive error/residual for one data point is coupled with a positive residual for another data point, or vice versa. This happens often in ecology and evolution, because we often work with data that has spatial dependence, temporal dependence, or phylogenetic dependence. Then, the error matrix is no longer diagonal, and may look like: \\[ \\epsilon_i \\sim N(0, \\mathbf{\\Sigma}) \\text{, where } \\mathbf{\\Sigma} = \\begin{bmatrix} 4.5 &amp; 0.3 &amp; -4.7 &amp; 0 &amp; 0 \\\\ 0.3 &amp; 9.0 &amp; 0 &amp; 0 &amp; 0 \\\\ -4.7 &amp; 0 &amp; 0.4 &amp; 0.3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.3 &amp; 5.2 &amp; 0.1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.1 &amp; 0.8 \\end{bmatrix} \\] The matrix is symmetric, because \\(\\mathrm{Cov}[Y_i, Y_j] = \\mathrm{Cov}[Y_j, Y_i]\\). Thus, it is important that we think of the error term as a matrix that describes the correlation structure of the data. Correlated errors will affect model fit. This is important because we want to downweight information from highly correlated data points because the information that each contributes is not independent from one another. "],["week-8-lab.html", "15 Week 8 Lab 15.1 Covariate as number vs. covariate as factor 15.2 Helmert contrasts in R 15.3 Polynomial contrasts in R", " 15 Week 8 Lab Our first task is to go over how to write a linear model in R. Let’s take a moment to go through Logan Table 7.3. 15.1 Covariate as number vs. covariate as factor First we will read in some data on mercury levels in fish. data&lt;-read.table(&quot;_data/fish.txt&quot;,header=T) head(data) ## RIVER STATION LENGTH WEIGHT MERCURY ## 1 0 0 47.0 1616 1.60 ## 2 0 0 48.7 1862 1.50 ## 3 0 0 55.7 2855 1.70 ## 4 0 0 45.2 1199 0.73 ## 5 0 0 44.7 1320 0.56 ## 6 0 0 43.8 1225 0.51 Let’s use Station as a categorical variable, and use it as a covariate for a model of mercury: Station&lt;-data$STATION Mercury&lt;-data$MERCURY What’s the difference between Station ## [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [26] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 ## [51] 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 6 6 6 6 7 7 ## [76] 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 ## [101] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 ## [126] 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 14 14 14 ## [151] 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 and as.factor(Station) ## [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [26] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 ## [51] 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 6 6 6 6 7 7 ## [76] 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 ## [101] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 ## [126] 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 14 14 14 ## [151] 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 ## Levels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 To see what impact this has on the modelling, let’s write a basic linear model in R fit&lt;-lm(Mercury~Station) summary(fit) ## ## Call: ## lm(formula = Mercury ~ Station) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3005 -0.5324 -0.1256 0.4088 2.3669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85564 0.10121 8.454 1.26e-14 *** ## Station 0.04624 0.01161 3.983 0.000101 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7304 on 169 degrees of freedom ## Multiple R-squared: 0.0858, Adjusted R-squared: 0.08039 ## F-statistic: 15.86 on 1 and 169 DF, p-value: 0.0001011 and compare this with fit2&lt;-lm(Mercury~as.factor(Station)) summary(fit2) ## ## Call: ## lm(formula = Mercury ~ as.factor(Station)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3525 -0.3767 -0.1100 0.3592 2.0779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.99300 0.19842 5.005 1.50e-06 *** ## as.factor(Station)1 -0.14879 0.24513 -0.607 0.54475 ## as.factor(Station)2 0.14777 0.26392 0.560 0.57635 ## as.factor(Station)3 0.30800 0.28060 1.098 0.27407 ## as.factor(Station)4 0.02914 0.25979 0.112 0.91083 ## as.factor(Station)5 -0.24300 0.41304 -0.588 0.55717 ## as.factor(Station)6 1.08950 0.37120 2.935 0.00384 ** ## as.factor(Station)7 -0.30300 0.28060 -1.080 0.28190 ## as.factor(Station)8 -0.20657 0.25979 -0.795 0.42774 ## as.factor(Station)9 0.32367 0.41304 0.784 0.43446 ## as.factor(Station)10 0.52129 0.24107 2.162 0.03212 * ## as.factor(Station)11 0.28144 0.28829 0.976 0.33046 ## as.factor(Station)12 -0.50608 0.26392 -1.918 0.05701 . ## as.factor(Station)13 0.22700 0.37120 0.612 0.54175 ## as.factor(Station)14 0.97623 0.26392 3.699 0.00030 *** ## as.factor(Station)15 1.11155 0.27415 4.054 7.93e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6274 on 155 degrees of freedom ## Multiple R-squared: 0.3813, Adjusted R-squared: 0.3214 ## F-statistic: 6.367 on 15 and 155 DF, p-value: 2.1e-10 This second approach is what we are interested in, because we want to use the Stations as categorical factors. Checkpoint #1: Why is the intercept for fit2 not the same as the intercept for fit? We won’t get into linear regression formally until next week, but we can see what’s going on if we plot the data and the best-fit line together on the same plot. plot(Station, Mercury,cex=1,xlab=&quot;Station as a numeric&quot;,ylab=&quot;Mercury&quot;) abline(a=coef(fit)[1],b=coef(fit)[2],col=&quot;red&quot;) segments(x0=-0.5,x1=0.5,y0=coef(fit2)[1],y1=coef(fit2)[1],col=&quot;blue&quot;,lwd=4) The blue segment shows you the mean mercury measurement for the first station (Station 0), whereas the intercept of the red line is a little lower. Why? Because the best fitting line for all the mercury measurements doesn’t necessary pass through the mean value for the first station. To make our consideration of contrasts easy, let’s just redefine the variable accordingly: Station&lt;-as.factor(Station) Let’s examine this output for a second. How do we interpret the intercept? Notice that Station=0 isn’t represented among the factors, it has been turned in to (a.k.a. “aliased by”) the intercept. We can convince ourselves this is true using mean(Mercury[Station==&quot;0&quot;]) ## [1] 0.993 (We have to use quotes now, because the variable “Station” is a factor, which means the levels are now identified by names (characters) which must be put in quotes.) Checkpoint #2: Do you understand what the intercept of fit2 means? Can you predict what the value for as.factor(Station)1 means? We can also convince ourselves of the other model estimates using mean(Mercury[Station==&quot;1&quot;]) ## [1] 0.8442105 mean(Mercury[Station==&quot;2&quot;]) ## [1] 1.140769 When fitting a model involving categorical factors as predictors, R will always use the first factor for the intercept, which is the one that comes first alphabetically or numerically. This may not be what you actually want to do. Why?? Because the hypotheses being tested are: \\[ H_{0}: \\mbox{Factor level 1} = \\mbox{Intercept}=0 \\] \\[ H_{0}: \\mbox{Difference between factor level 2 and factor level 1} = 0 \\] \\[ H_{0}: \\mbox{Difference between factor level 3 and factor level 1} = 0 \\] \\[ H_{0}: \\mbox{Difference between factor level 4 and factor level 1} = 0 \\] etc. In other words, with the exception of the first factor which is the intercept, you are testing the equality of each factor with the first factor, not against zero (or other benchmark). This is the “Treatment contrasts/Dummy coding” approach we discussed in lecture on Tuesday. We can confirm this by having R tell us what its default behavior is when faced with categorical predictors. options()$contrasts ## unordered ordered ## &quot;contr.treatment&quot; &quot;contr.poly&quot; (As a side note, we can just call “options()” to get all sorts of information about R’s defaults.) We can get R to spit out the design matrix for this set of contrasts using contr.treatment(15) ## 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 15 0 0 0 0 0 0 0 0 0 0 0 0 0 1 Another approach would be to take the intercept out of the model, so the coefficients simply represented the group means. We can do this by including a “-1” in the model statement. fit3&lt;-lm(Mercury~Station-1) summary(fit3) ## ## Call: ## lm(formula = Mercury ~ Station - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3525 -0.3767 -0.1100 0.3592 2.0779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Station0 0.9930 0.1984 5.005 1.50e-06 *** ## Station1 0.8442 0.1439 5.865 2.63e-08 *** ## Station2 1.1408 0.1740 6.555 7.80e-10 *** ## Station3 1.3010 0.1984 6.557 7.74e-10 *** ## Station4 1.0221 0.1677 6.095 8.32e-09 *** ## Station5 0.7500 0.3623 2.070 0.040078 * ## Station6 2.0825 0.3137 6.638 5.05e-10 *** ## Station7 0.6900 0.1984 3.478 0.000657 *** ## Station8 0.7864 0.1677 4.690 5.96e-06 *** ## Station9 1.3167 0.3623 3.635 0.000378 *** ## Station10 1.5143 0.1369 11.060 &lt; 2e-16 *** ## Station11 1.2744 0.2091 6.093 8.40e-09 *** ## Station12 0.4869 0.1740 2.798 0.005794 ** ## Station13 1.2200 0.3137 3.889 0.000149 *** ## Station14 1.9692 0.1740 11.316 &lt; 2e-16 *** ## Station15 2.1045 0.1892 11.124 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6274 on 155 degrees of freedom ## Multiple R-squared: 0.8213, Adjusted R-squared: 0.8029 ## F-statistic: 44.53 on 16 and 155 DF, p-value: &lt; 2.2e-16 Notice that all stations are now treated equally, there is no intercept, and the model coefficients (the group means) are tested for significant differences from 0. Let’s pause for a second and make sure we understand how R calculated the estimates and their standard errors. The estimates are pretty straightforward, in fact we have already calculated this. mean(Mercury[Station==&quot;0&quot;]) ## [1] 0.993 But how did R calculate the standard errors? Let’s stop and think for a second as to what the standard error of an estimate represents. We know the standard error of the mean of a (normal) distribution is given by \\[ \\mbox{Standard error of the mean} = \\frac{\\mbox{standard deviation}}{\\sqrt{\\mbox{sample size}}} \\] So the standard error of any estimate is, roughly speaking, the (residual) variation divided by the square root of the sample size. (In the case of the SEM, all the variation is “residual”.) What is the residual variation in this case? One way to measure residual variation within the Station=0 group is to calculate the standard deviation of the Station=0 data. sd(Mercury[Station==&quot;0&quot;]) ## [1] 0.5043158 Using this as the “residual” variation, we would calculate the standard error of the coefficient for the Station=0 group as follows: sd(Mercury[Station==&quot;0&quot;])/sqrt(length(Mercury[Station==&quot;0&quot;])) ## [1] 0.1594787 But this doesn’t match what R has returned!! Why not? Because an even better estimate of the “residual” variation is the pooled estimate of the residual variation, where we use not only the residual variation within the Station=0 group, but within each of the Stations. R returned this value above: “Residual standard error:0.6274”. Using this value, we calculate 0.6274/sqrt(length(Mercury[Station==&quot;0&quot;])) ## [1] 0.1984013 Now we get what R has returned for the standard error on the co-efficient for the Station=0 group. Checkpoint #3: Does this make sense? Its important to recognize that your intuition to use the standard deviation within the Station=0 group to estimate the standard error of the coefficient is not wrong, but that by pooling the data, we can get a more precise estimate of that residual standard deviation. Notice also that the only difference in standard errors between the different stations comes from the different sample sizes. We can write a short script to bootstrap resample data within each station (sample with replacement within station 0, then sample with resamplement within station 1, etc.) and calculate the standard deviation of the estimate for Station 0. This should be at least approximately equal to the standard error of the estimate as reported by the model’s fit using the real data. station0.est&lt;-c() for (i in 1:1000) { data.bootstrap&lt;-data Station.bootstrap&lt;-data.bootstrap$STATION Mercury.bootstrap&lt;-data.bootstrap$MERCURY Mercury.bootstrap[Station.bootstrap==&quot;0&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;0&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;0&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;1&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;1&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;1&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;2&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;2&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;2&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;3&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;3&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;3&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;4&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;4&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;4&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;5&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;5&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;5&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;6&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;6&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;6&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;7&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;7&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;7&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;8&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;8&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;8&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;9&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;9&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;9&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;10&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;10&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;10&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;11&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;11&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;11&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;12&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;12&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;12&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;13&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;13&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;13&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;14&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;14&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;14&quot;]),replace=T) Mercury.bootstrap[Station.bootstrap==&quot;15&quot;]&lt;-sample(Mercury.bootstrap[Station.bootstrap==&quot;15&quot;],length(Mercury.bootstrap[Station.bootstrap==&quot;15&quot;]),replace=T) fit.bs&lt;-lm(Mercury.bootstrap~as.factor(Station.bootstrap)-1) station0.est&lt;-c(station0.est,coef(fit.bs)[1]) } hist(station0.est) sd(station0.est) ## [1] 0.1526854 15.2 Helmert contrasts in R In lecture on Tuesday we discussed several other contrasts. Let’s start with the Helmhert contrast. contrasts(Station)&lt;-&quot;contr.helmert&quot; contr.helmert(15) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 2 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 3 0 2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 4 0 0 3 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 5 0 0 0 4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 6 0 0 0 0 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 ## 7 0 0 0 0 0 6 -1 -1 -1 -1 -1 -1 -1 -1 ## 8 0 0 0 0 0 0 7 -1 -1 -1 -1 -1 -1 -1 ## 9 0 0 0 0 0 0 0 8 -1 -1 -1 -1 -1 -1 ## 10 0 0 0 0 0 0 0 0 9 -1 -1 -1 -1 -1 ## 11 0 0 0 0 0 0 0 0 0 10 -1 -1 -1 -1 ## 12 0 0 0 0 0 0 0 0 0 0 11 -1 -1 -1 ## 13 0 0 0 0 0 0 0 0 0 0 0 12 -1 -1 ## 14 0 0 0 0 0 0 0 0 0 0 0 0 13 -1 ## 15 0 0 0 0 0 0 0 0 0 0 0 0 0 14 and redo the analysis (this time with an intercept) fit4&lt;-lm(Mercury~Station) summary(fit4) ## ## Call: ## lm(formula = Mercury ~ Station) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3525 -0.3767 -0.1100 0.3592 2.0779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.218509 0.057402 21.228 &lt; 2e-16 *** ## Station1 -0.074395 0.122566 -0.607 0.544754 ## Station2 0.074055 0.070951 1.044 0.298229 ## Station3 0.077085 0.055571 1.387 0.167391 ## Station4 -0.009520 0.038068 -0.250 0.802850 ## Station5 -0.051704 0.061812 -0.836 0.404175 ## Station6 0.153426 0.046610 3.292 0.001233 ** ## Station7 -0.058993 0.027179 -2.171 0.031489 * ## Station8 -0.035169 0.020725 -1.697 0.091709 . ## Station9 0.024888 0.036993 0.673 0.502085 ## Station10 0.038329 0.014261 2.688 0.007983 ** ## Station11 0.011954 0.018398 0.650 0.516836 ## Station12 -0.050464 0.014348 -3.517 0.000572 *** ## Station13 0.009108 0.022862 0.398 0.690891 ## Station14 0.057842 0.012338 4.688 6.00e-06 *** ## Station15 0.059069 0.012403 4.763 4.36e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6274 on 155 degrees of freedom ## Multiple R-squared: 0.3813, Adjusted R-squared: 0.3214 ## F-statistic: 6.367 on 15 and 155 DF, p-value: 2.1e-10 Do these coefficients make sense? Let’s work through them using the information learned in Tuesday’s lecture. The first coefficient for the Helmert contrasts is the mean of the group means. This is not the same as the overall mean, because different groups may have different sample sizes. We can find the mean of all the group means by using the output from fit3. Let’s pull out all the coefficients from fit3, and give them a new name for convenience: group.means&lt;-coef(fit3) group.means ## Station0 Station1 Station2 Station3 Station4 Station5 Station6 Station7 ## 0.9930000 0.8442105 1.1407692 1.3010000 1.0221429 0.7500000 2.0825000 0.6900000 ## Station8 Station9 Station10 Station11 Station12 Station13 Station14 Station15 ## 0.7864286 1.3166667 1.5142857 1.2744444 0.4869231 1.2200000 1.9692308 2.1045455 We can confirm the first Helmert parameter estimate by looking at mean(group.means) ## [1] 1.218509 Now the second Helmert parameter is given by \\[ \\frac{\\mu_{1}+\\mu_{2}}{2}-\\mu_{1} \\] which we can calculate using ((group.means[1]+group.means[2])/2)-group.means[1] ## Station0 ## -0.07439474 The third Helmert coefficient is given by \\[ \\frac{\\mu_{1}+\\mu_{2}+\\mu_{3}}{3}-\\frac{\\mu_{1}+\\mu_{2}}{2} \\] which we can calculate using ((group.means[1]+group.means[2]+group.means[3])/3)-((group.means[1]+group.means[2])/2) ## Station0 ## 0.07405466 and so on. Sum-to-zero contrasts can be done using contrasts(Station)&lt;-&quot;contr.sum&quot; contr.sum(15) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ## 15 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 and redo the analysis (this time with an intercept) fit5&lt;-lm(Mercury~Station) summary(fit5) ## ## Call: ## lm(formula = Mercury ~ Station) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3525 -0.3767 -0.1100 0.3592 2.0779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.218509 0.057402 21.228 &lt; 2e-16 *** ## Station1 -0.225509 0.194275 -1.161 0.24752 ## Station2 -0.374299 0.146374 -2.557 0.01151 * ## Station3 -0.077740 0.172608 -0.450 0.65306 ## Station4 0.082491 0.194275 0.425 0.67171 ## Station5 -0.196366 0.167035 -1.176 0.24156 ## Station6 -0.468509 0.343688 -1.363 0.17480 ## Station7 0.863991 0.299023 2.889 0.00441 ** ## Station8 -0.528509 0.194275 -2.720 0.00727 ** ## Station9 -0.432081 0.167035 -2.587 0.01061 * ## Station10 0.098157 0.343688 0.286 0.77557 ## Station11 0.295777 0.140352 2.107 0.03669 * ## Station12 0.055935 0.203888 0.274 0.78419 ## Station13 -0.731586 0.172608 -4.238 3.85e-05 *** ## Station14 0.001491 0.299023 0.005 0.99603 ## Station15 0.750722 0.172608 4.349 2.47e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6274 on 155 degrees of freedom ## Multiple R-squared: 0.3813, Adjusted R-squared: 0.3214 ## F-statistic: 6.367 on 15 and 155 DF, p-value: 2.1e-10 Once again the first parameter to be estimated is the mean of the group means. The second and third parameter estimates can be confirmed as group.means[1]-mean(group.means) ## Station0 ## -0.2255092 group.means[2]-mean(group.means) ## Station1 ## -0.3742987 and so forth. Checkpoint #4: Do you understand how to obtain the sum-to-zero parameter estimates and what they mean in terms of this dataset? 15.3 Polynomial contrasts in R Polynomial contrasts can be done using contrasts(Station)&lt;-&quot;contr.poly&quot; contr.poly(15) ## .L .Q .C ^4 ^5 ## [1,] -4.183300e-01 0.47227028 -4.562564e-01 0.39364141 -3.077236e-01 ## [2,] -3.585686e-01 0.26986873 -6.517949e-02 -0.16870346 3.516842e-01 ## [3,] -2.988072e-01 0.09860588 1.754832e-01 -0.34173265 3.009605e-01 ## [4,] -2.390457e-01 -0.04151827 2.908008e-01 -0.27684671 1.352631e-02 ## [5,] -1.792843e-01 -0.15050372 3.058422e-01 -0.09791879 -2.308696e-01 ## [6,] -1.195229e-01 -0.22835046 2.456765e-01 0.09870529 -3.074162e-01 ## [7,] -5.976143e-02 -0.27505851 1.353728e-01 0.24420711 -2.075060e-01 ## [8,] 1.326970e-17 -0.29062786 -4.453155e-17 0.29729561 -2.660506e-16 ## [9,] 5.976143e-02 -0.27505851 -1.353728e-01 0.24420711 2.075060e-01 ## [10,] 1.195229e-01 -0.22835046 -2.456765e-01 0.09870529 3.074162e-01 ## [11,] 1.792843e-01 -0.15050372 -3.058422e-01 -0.09791879 2.308696e-01 ## [12,] 2.390457e-01 -0.04151827 -2.908008e-01 -0.27684671 -1.352631e-02 ## [13,] 2.988072e-01 0.09860588 -1.754832e-01 -0.34173265 -3.009605e-01 ## [14,] 3.585686e-01 0.26986873 6.517949e-02 -0.16870346 -3.516842e-01 ## [15,] 4.183300e-01 0.47227028 4.562564e-01 0.39364141 3.077236e-01 ## ^6 ^7 ^8 ^9 ^10 ## [1,] 0.21900186 -1.418585e-01 0.08331426 -4.403943e-02 0.020705682 ## [2,] -0.43800373 4.255756e-01 -0.34515908 2.390712e-01 -0.141981819 ## [3,] -0.08423149 -1.855073e-01 0.37994966 -4.360388e-01 0.371109531 ## [4,] 0.26954076 -3.382780e-01 0.14373999 1.664787e-01 -0.392270283 ## [5,] 0.30170187 -3.273658e-02 -0.28473336 3.189229e-01 -0.036178060 ## [6,] 0.07657408 2.728048e-01 -0.25177387 -1.209874e-01 0.356774828 ## [7,] -0.19143520 2.728048e-01 0.11444267 -3.266661e-01 -0.006143444 ## [8,] -0.30629631 3.647875e-16 0.32043947 8.091348e-15 -0.344032870 ## [9,] -0.19143520 -2.728048e-01 0.11444267 3.266661e-01 -0.006143444 ## [10,] 0.07657408 -2.728048e-01 -0.25177387 1.209874e-01 0.356774828 ## [11,] 0.30170187 3.273658e-02 -0.28473336 -3.189229e-01 -0.036178060 ## [12,] 0.26954076 3.382780e-01 0.14373999 -1.664787e-01 -0.392270283 ## [13,] -0.08423149 1.855073e-01 0.37994966 4.360388e-01 0.371109531 ## [14,] -0.43800373 -4.255756e-01 -0.34515908 -2.390712e-01 -0.141981819 ## [15,] 0.21900186 1.418585e-01 0.08331426 4.403943e-02 0.020705682 ## ^11 ^12 ^13 ^14 ## [1,] -8.499378e-03 0.002953738 -8.203890e-04 0.0001578839 ## [2,] 7.163761e-02 -0.029959342 9.844668e-03 -0.0022103751 ## [3,] -2.489103e-01 0.132074283 -5.332528e-02 0.0143674381 ## [4,] 4.322541e-01 -0.323223324 1.706409e-01 -0.0574697522 ## [5,] -3.071918e-01 0.450234056 -3.519469e-01 0.1580418186 ## [6,] -1.469178e-01 -0.273853704 4.692625e-01 -0.3160836371 ## [7,] 3.606165e-01 -0.153172411 -3.519469e-01 0.4741254557 ## [8,] 4.271769e-14 0.389893410 3.717605e-13 -0.5418576636 ## [9,] -3.606165e-01 -0.153172411 3.519469e-01 0.4741254557 ## [10,] 1.469178e-01 -0.273853704 -4.692625e-01 -0.3160836371 ## [11,] 3.071918e-01 0.450234056 3.519469e-01 0.1580418186 ## [12,] -4.322541e-01 -0.323223324 -1.706409e-01 -0.0574697522 ## [13,] 2.489103e-01 0.132074283 5.332528e-02 0.0143674381 ## [14,] -7.163761e-02 -0.029959342 -9.844668e-03 -0.0022103751 ## [15,] 8.499378e-03 0.002953738 8.203890e-04 0.0001578839 and redo the analysis fit6&lt;-lm(Mercury~Station) summary(fit6) ## ## Call: ## lm(formula = Mercury ~ Station) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3525 -0.3767 -0.1100 0.3592 2.0779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.218509 0.057402 21.228 &lt; 2e-16 *** ## Station.L 0.765487 0.203194 3.767 0.000234 *** ## Station.Q 0.486024 0.220330 2.206 0.028864 * ## Station.C 0.664117 0.218809 3.035 0.002821 ** ## Station^4 0.358959 0.217116 1.653 0.100293 ## Station^5 0.110091 0.238465 0.462 0.644970 ## Station^6 -0.030278 0.190891 -0.159 0.874180 ## Station^7 -0.471084 0.223023 -2.112 0.036266 * ## Station^8 -0.447493 0.215194 -2.079 0.039219 * ## Station^9 -0.485254 0.242256 -2.003 0.046913 * ## Station^10 0.511679 0.236677 2.162 0.032158 * ## Station^11 0.689250 0.234116 2.944 0.003738 ** ## Station^12 -0.359702 0.233627 -1.540 0.125687 ## Station^13 -0.002538 0.218399 -0.012 0.990744 ## Station^14 0.702948 0.282851 2.485 0.014009 * ## Station^15 -0.485066 0.254535 -1.906 0.058541 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6274 on 155 degrees of freedom ## Multiple R-squared: 0.3813, Adjusted R-squared: 0.3214 ## F-statistic: 6.367 on 15 and 155 DF, p-value: 2.1e-10 Let’s plot the contrast polynomials to get a sense for them. temp&lt;-contr.poly(15) plot(temp[,1],lwd=2,xlab=&quot;Group&quot;,ylab=&quot;Coefficient&quot;,typ=&quot;b&quot;) lines(temp[,2],lwd=2,typ=&quot;b&quot;,col=&quot;red&quot;) lines(temp[,3],lwd=2,typ=&quot;b&quot;,col=&quot;blue&quot;) lines(temp[,4],lwd=2,typ=&quot;b&quot;,col=&quot;green&quot;) lines(temp[,5],lwd=2,typ=&quot;b&quot;,col=&quot;orange&quot;) lines(rep(1/sqrt(15),times=15),typ=&quot;b&quot;,lwd=2,col=&quot;purple&quot;) where the last line plot is the intercept (a constant value whose vector magnitude equals 1) which R doesn’t include in the print out of the treatments. We can get R to plot out the model matrix for any of these models as follows: head(model.matrix(~Station,contrasts=list(Station=&quot;contr.treatment&quot;)),n=20) ## (Intercept) Station1 Station2 Station3 Station4 Station5 Station6 Station7 ## 1 1 0 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 0 0 ## 3 1 0 0 0 0 0 0 0 ## 4 1 0 0 0 0 0 0 0 ## 5 1 0 0 0 0 0 0 0 ## 6 1 0 0 0 0 0 0 0 ## 7 1 0 0 0 0 0 0 0 ## 8 1 0 0 0 0 0 0 0 ## 9 1 0 0 0 0 0 0 0 ## 10 1 0 0 0 0 0 0 0 ## 11 1 1 0 0 0 0 0 0 ## 12 1 1 0 0 0 0 0 0 ## 13 1 1 0 0 0 0 0 0 ## 14 1 1 0 0 0 0 0 0 ## 15 1 1 0 0 0 0 0 0 ## 16 1 1 0 0 0 0 0 0 ## 17 1 1 0 0 0 0 0 0 ## 18 1 1 0 0 0 0 0 0 ## 19 1 1 0 0 0 0 0 0 ## 20 1 1 0 0 0 0 0 0 ## Station8 Station9 Station10 Station11 Station12 Station13 Station14 ## 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 ## 15 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 ## 17 0 0 0 0 0 0 0 ## 18 0 0 0 0 0 0 0 ## 19 0 0 0 0 0 0 0 ## 20 0 0 0 0 0 0 0 ## Station15 ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## 11 0 ## 12 0 ## 13 0 ## 14 0 ## 15 0 ## 16 0 ## 17 0 ## 18 0 ## 19 0 ## 20 0 or head(model.matrix(~Station,contrasts=list(Station=&quot;contr.helmert&quot;)),n=20) ## (Intercept) Station1 Station2 Station3 Station4 Station5 Station6 Station7 ## 1 1 -1 -1 -1 -1 -1 -1 -1 ## 2 1 -1 -1 -1 -1 -1 -1 -1 ## 3 1 -1 -1 -1 -1 -1 -1 -1 ## 4 1 -1 -1 -1 -1 -1 -1 -1 ## 5 1 -1 -1 -1 -1 -1 -1 -1 ## 6 1 -1 -1 -1 -1 -1 -1 -1 ## 7 1 -1 -1 -1 -1 -1 -1 -1 ## 8 1 -1 -1 -1 -1 -1 -1 -1 ## 9 1 -1 -1 -1 -1 -1 -1 -1 ## 10 1 -1 -1 -1 -1 -1 -1 -1 ## 11 1 1 -1 -1 -1 -1 -1 -1 ## 12 1 1 -1 -1 -1 -1 -1 -1 ## 13 1 1 -1 -1 -1 -1 -1 -1 ## 14 1 1 -1 -1 -1 -1 -1 -1 ## 15 1 1 -1 -1 -1 -1 -1 -1 ## 16 1 1 -1 -1 -1 -1 -1 -1 ## 17 1 1 -1 -1 -1 -1 -1 -1 ## 18 1 1 -1 -1 -1 -1 -1 -1 ## 19 1 1 -1 -1 -1 -1 -1 -1 ## 20 1 1 -1 -1 -1 -1 -1 -1 ## Station8 Station9 Station10 Station11 Station12 Station13 Station14 ## 1 -1 -1 -1 -1 -1 -1 -1 ## 2 -1 -1 -1 -1 -1 -1 -1 ## 3 -1 -1 -1 -1 -1 -1 -1 ## 4 -1 -1 -1 -1 -1 -1 -1 ## 5 -1 -1 -1 -1 -1 -1 -1 ## 6 -1 -1 -1 -1 -1 -1 -1 ## 7 -1 -1 -1 -1 -1 -1 -1 ## 8 -1 -1 -1 -1 -1 -1 -1 ## 9 -1 -1 -1 -1 -1 -1 -1 ## 10 -1 -1 -1 -1 -1 -1 -1 ## 11 -1 -1 -1 -1 -1 -1 -1 ## 12 -1 -1 -1 -1 -1 -1 -1 ## 13 -1 -1 -1 -1 -1 -1 -1 ## 14 -1 -1 -1 -1 -1 -1 -1 ## 15 -1 -1 -1 -1 -1 -1 -1 ## 16 -1 -1 -1 -1 -1 -1 -1 ## 17 -1 -1 -1 -1 -1 -1 -1 ## 18 -1 -1 -1 -1 -1 -1 -1 ## 19 -1 -1 -1 -1 -1 -1 -1 ## 20 -1 -1 -1 -1 -1 -1 -1 ## Station15 ## 1 -1 ## 2 -1 ## 3 -1 ## 4 -1 ## 5 -1 ## 6 -1 ## 7 -1 ## 8 -1 ## 9 -1 ## 10 -1 ## 11 -1 ## 12 -1 ## 13 -1 ## 14 -1 ## 15 -1 ## 16 -1 ## 17 -1 ## 18 -1 ## 19 -1 ## 20 -1 "],["week-9-lecture.html", "16 Week 9 Lecture 16.1 Week 9 Readings 16.2 Correlation 16.3 Hypothesis testing - Pearson’s r 16.4 Fisher’s \\(z\\) 16.5 Regression 16.6 Estimating the slope and intercept in linear regression 16.7 OK, now the “other” derivation for slope and intercept 16.8 Assumptions of regression 16.9 Confidence vs. Prediction intervals 16.10 How do we know if our model is any good? 16.11 Robust regression 16.12 Type I and Type II Regression 16.13 Week 9 FAQ", " 16 Week 9 Lecture 16.1 Week 9 Readings For this week, I suggest reading Aho Chapters 8 and 9 up to and including Section 9.16, as well as Logan Chapter 8. I also want to share a classic paper on how to model “scaling-type” relationships - I’m looking at you morphometrics folks!, though this is just a recommended reading and not something we will cover in Biometry. 16.2 Correlation Correlation and regression are two ways of looking at the relationship between two continuous variables. Correlation is testing the null hypothesis that the two variables are uncorrelated, and there is complete symmetry between the two variables. On the other hand, regression is concerned with using one variable to PREDICT another, and there is an asymmetry between the predictor and the response variable. A regression model for \\(Y \\sim X\\) is different than \\(X \\sim Y\\). Regression and correlation are not the same. Let’s take two paired samples A and B and introduce a quantity called the correlation coefficient r: \\[ r = \\frac{Cov(A,B)}{\\sqrt{Var(A) \\times Var(B)}} \\] More precisely, this is called “Pearson’s product moment correlation”. We know from before that the sample variance of A (a.k.a. Var(A)) is given by \\[ Var(A) = \\frac{1}{n-1}\\sum_{i=1}^{n}(A_{i}-\\bar{A})(A_{i}-\\bar{A}) \\] Sample variance is the squared deviation of a random variable from its expected value, scaled by the number of data points. When we use squared deviations, our values are always positive. Let’s say \\(A_1 = 12\\) and \\(\\bar{A} = 14\\). The squared deviation is then \\((12 - 14)^2 = 4\\). Now let’s say \\(A_2 = 30\\). The squared deviation is then \\((30 - 14)^2 = 256\\). When using squared deviations, values far away from the mean contribute greatly to the variance (they have a large influence). By extension, the sample covariance of A and B (i.e. Cov(A,B)) is given by \\[ Cov(A,B) = \\frac{1}{n-1}\\sum_{i=1}^{n}(A_{i}-\\bar{A})(B_{i}-\\bar{B}) \\] Why do we only lose 1 degree of freedom when we have to estimate the mean of both the A group and the B group? So as not to distract from the main thread here, I have answered that below in the FAQ. Why does this expression make sense? The covariance measures how much two things go up and down together. A &lt;- c(1, 2, 3, 4) B &lt;- c(5, 6, 7, 8) (1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) ## [1] 1.666667 B &lt;- c(5, 4, 3, 2) (1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) ## [1] -1.666667 However, the covariance isn’t meaningful by itself because it depends on the absolute scale of variability in A and B, so it has to be divided by some aggregate measure of variability in the two samples (i.e. the geometric mean). We can see this by scaling everything by a factor of 10 and recalculating Covariance: (1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) ## [1] -1.666667 A &lt;- A * 10 B &lt;- B * 10 (1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) ## [1] -166.6667 This is why to calculate \\(r\\) we scale the covariance by the geometric mean of the product of the variances for the two samples. \\[ r = \\frac{Cov(A,B)}{\\sqrt{Var(A) \\times Var(B)}} \\] Pearson’s product moment correlation is bounded between -1 and 1. Correlation coefficients of 1 or -1 occur when you set deviations in sample \\(B\\) to be exactly matched in sample \\(A\\). These measures are unitless. Let’s look at a few examples… ## [1] -0.0959171 ## [1] 0.9367331 16.3 Hypothesis testing - Pearson’s r Before getting into hypothesis testing, some terminology: \\(\\rho\\)=true (population) correlation coefficient of A and B \\(r\\)=empirical (sample) correlation coefficient of A and B How can we test hypotheses about correlation? Parametric approach using distribution theory Nonparametric approaches: bootstrap or randomization We’ll start with 1. If \\(\\rho\\) is the population (true) correlation coefficient of \\(A\\) and \\(B\\) and \\(r\\) is the sample (empirical) correlation coefficient of \\(A\\) and \\(B\\), we will test whether \\(\\rho\\) is significantly different from zero. How do we assess whether our estimate of \\(\\rho\\) (which, following our notation from earlier in the semester, we will call \\(\\hat{\\rho}\\)) is significantly different from zero? Note before we begin that calculating Pearson’s correlation coefficient itself does not require that the data (what we’ve been calling A and B) are bivariate normal, but hypothesis testing/constructing CIs based on distribution theory does. If \\(A\\) and \\(B\\) are from a bivariate normal distribution and \\(\\rho = 0\\), the sample correlation coefficient is normally distributed (for large sample sizes): \\[ r | (H_0: \\rho = 0) \\sim \\mathrm{N} {\\left( 0, \\frac{1 - r^2}{n - 2} \\right)} \\] and therefore we use the following test statistic: Then we convert this to the following test statistic: \\[ T^* = r \\sqrt{\\frac{n - 2}{1 - r^2}} | (H_0: \\rho = 0) \\sim \\mathrm{N}(0, 1) \\] Actually, it is more precise to say that under the null hypothesis, \\[ T^* = r \\sqrt{\\frac{n - 2}{1 - r^2}} | (H_0: \\rho = 0) \\sim t_{n-2} \\] Question: How did we generate this test statistic, \\(T^*\\) from the original test statistic, r? Click for Answer By standardizing the distribution of \\(r | H_0\\) so that the normal distribution has a standard deviation/variance of 1 Question: Why did we use this test statistic instead of r? Click for Answer The standard normal distribution and the t-distribution are conventional distributions with associated tables of quantiles that were used before computing was as powerful and widespread. Also, the t-distribution is more appropriate for small sample sizes (it has fatter tails). Question: Where did we lose two degrees of freedom? Click for Answer To estimate r, we calculated the mean of sample A and sample B The Pearson’s product moment correlation assumes the following about the two samples: The joint distribution \\((A, B)\\) is bivariate normal (necessary for hypothesis testing using distribution theory) The relationship between \\(A\\) and \\(B\\) is linear (always necessary) Data are independent samples from the joint distribution. library(ggplot2) A &lt;- rnorm(n = 100, mean = 5, sd = 2) A &lt;- A[order(A)] df &lt;- data.frame(A = A, B = (A - 5)^2 + rnorm(n = 100, mean = 0, sd = 1)) ggplot(data = df, aes(x = A, y = B)) + geom_point() + theme_classic() + theme(text = element_text(size = text.size)) cor(df$A, df$B) ## [1] 0.01481287 Even though A and B clearly have a strong relationship, correlation is only effective if the relationship is linear. Other considerations: What if the data are not bivariate normal? If sample size is large, you’re probably okay. Pearson’s \\(r\\) is also very sensitive to outliers, what can you do in that case? Robust correlation measures (Spearman’s or Kendall’s \\(r\\)) are robust to deviations from bivariate normality and outliers. Confidence intervals on \\(r\\) can also be derived using bootstrap and permutation methods. As these are far more intuitive, require no assumptions, and (with modern computers) are fast, it is often better to use these other methods (which may be slightly less powerful, another topic to be covered later). Question: How could you bootstrap the correlation coefficient given that the samples (A and B) are paired? Click for Answer You need to sample (with replacement) A-B pairs. This will preserve the correlated structure of the data while allowing you to resample a “new” dataset with which to calculate a confidence interval. In other words, sample with replacement from the row indices, preserving the relationship between the two samples iterations &lt;- 1000 A &lt;- rnorm(n = 100, mean = 5, sd = 2) B &lt;- A - rnorm(n = 100, mean = 0, sd = 1) cor.obs &lt;- cor(A, B) dat &lt;- data.frame(A, B) cor.boot &lt;- c() for (i in 1:iterations) { index.boot &lt;- sample(1:nrow(dat), size = 100, replace = TRUE) dat.boot &lt;- dat[index.boot, ] cor.boot[i] &lt;- cor(x = dat.boot$A, y = dat.boot$B) } paste(&quot;The 95% confidence interval for the estimated correlation coefficient, &quot;, round(cor.obs, digits = 3), &quot; is (&quot;, round(quantile(cor.boot, 0.025), digits = 3), &quot;, &quot;, round(quantile(cor.boot, 0.975), digits = 3), &quot;)&quot;, sep = &quot;&quot;) ## [1] &quot;The 95% confidence interval for the estimated correlation coefficient, 0.86 is (0.803, 0.905)&quot; Question: How would we conduct a permutation test for the correlation coefficient (with \\(H_0: \\rho = 0\\))? Click for Answer Shuffle A independently of B and calculate the correlation of the shuffled data to create a null distribution of correlations. Compare the observed correlation to this distribution. 16.4 Fisher’s \\(z\\) If you have large sample sizes, you can test other null hypotheses (i.e. \\(H_{0}: \\rho=\\rho_{0}\\)) and put confidence intervals on \\(\\rho\\) using what is called “Fisher’s transformation”. Using Fisher’s \\(z\\) transformation, we convert a statistic with a bounded distribution to an unbounded (normal) distribution. This is useful for calculating confidence intervals. Also with this method, we can test hypotheses other than \\(\\rho = 0\\). We can transform \\(r\\) to \\(z\\) using the the inverse hyperbolic tangent: \\[ z = 0.5 \\ln \\left( \\frac{1 + r}{1 - r} \\right) = \\tanh^{-1} (r) \\] With this transformation, \\(z\\) is approximately normally distributed for all values of \\(\\rho\\): \\[ z \\sim \\mathrm{N} {\\left( 0.5 \\ln \\left( \\frac{1 + \\rho}{1 - \\rho} \\right), \\frac{1}{n - 3} \\right)} \\] Now, the test statistic that we can use to test the null hypothesis \\(H_0: \\rho = \\rho_0\\) is: \\[ T^* = \\frac{z - 0.5 \\ln \\left( \\frac{1 + \\rho_0}{1 - \\rho_0} \\right)}{\\sqrt{1 / (n - 3)}} \\approx \\mathrm{N}(0, 1) \\] Why did we rewrite a new test statistic (what was wrong with \\(z\\))? The null distribution for \\(z\\) is more difficult to reference (compared to the quantiles of the standard normal, for example). For a bit of evidence showing that both ways of writing the test statistic are fine, see the following code: A &lt;- rnorm(n = 100, mean = 5, sd = 2) B &lt;- A - rnorm(n = 100, mean = 0, sd = 1) r.obs &lt;- cor(A, B) # using z as test statistic z.obs &lt;- 0.5 * log((1 + r.obs) / (1 - r.obs)) # note that z under the null distribution is 0 pnorm(q = z.obs, mean = 0, sd = sqrt(1 / (100 - 3)), lower.tail = FALSE) ## [1] 1.269231e-46 # using transformed test statistic, T* test.stat &lt;- z.obs / sqrt(1 / (100 - 3)) pnorm(q = test.stat, lower.tail = FALSE) ## [1] 1.269231e-46 To simply the notation, let us define \\[ \\zeta_{0} = \\frac{1}{2}ln\\left(\\frac{1+\\rho_{0}}{1-\\rho_{0}}\\right) \\] We can also use this to construct confidence intervals for \\(\\zeta_{0}\\): \\[ P \\left( z - \\frac{t_{[1 - \\alpha / 2](\\infty)}}{\\sqrt{n - 3}} \\leq \\zeta_{0} \\leq z + \\frac{t_{[1 - \\alpha / 2](\\infty)}}{\\sqrt{n - 3}} \\right) = 1 - \\alpha \\] (Note that we need to do some back-transforming to get from here to a confidence interval on \\(\\rho\\).) Implicit in the calculation of the correlation coefficient is that the quantities being compared are either interval or ratio variables. If this is not the case, then you need to use a more general statistical test which uses only the ranks of the data. The underlying principle is that you rank the data in each sample and then compare ranks. There are two common ones: Spearmans rank correlation: Similar to above except instead of using raw values, the data are first transformed into ranks (separately for each sample) before calculating \\(r\\). \\[ r_{s} = \\frac{Cov(ranks_A,ranks_B)}{\\sqrt{Var(ranks_A) \\times Var(ranks_B)}} \\] where \\(ranks_A\\) and \\(ranks_B\\) are the ranks of the two datasets. \\[ r_s | H_0 \\sim \\sqrt{\\frac{1}{n - 1}} \\mathrm{N} (0, 1) \\] Ties are dealt with in a different way (see Aho 8.3.1.1 for more detail). A &lt;- c(2, 3, 5, 4) B &lt;- c(5, 6, 2, 3) cov(rank(A), rank(B)) / sqrt(var(rank(A)) * var(rank(B))) ## [1] -0.8 cor.test(A, B, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: A and B ## S = 18, p-value = 0.3333 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.8 Q: What information is lost when using Spearman’s rather than Pearson’s \\(r\\)? Kendall’s \\(\\tau\\) or “Kendall’s coefficient of rank correlation”: The basic idea is that for sample X, you can compare all possible combinations of \\(X_{i}\\) to see which values are larger. For a second sample Y, you can do the same. Kendall’s tau tallies how many pairs share the same relationship. If (\\(X_{i}\\)&gt; \\(X_{j}\\) and \\(Y_{i} &gt; Y_{j}\\)) OR (\\(X_{i}&lt; X_{j}\\) and \\(Y_{i}&lt; Y_{j}\\)), then X and Y are considered concordant for that combination. If (\\(X_{i}&gt; X_{j}\\) and \\(Y_{i}&lt; Y_{j}\\)) OR (\\(X_{i}&lt; X_{j}\\) and \\(Y_{i}&gt; Y_{j}\\)), then X and Y are considered disconcordant for that combination. This is easier to describe in practice: set.seed(4306) total &lt;- sample(x = 1:15, size = 8) dat &lt;- data.frame(A = total[1:4], B = total[5:8]) dat ## A B ## 1 10 8 ## 2 15 6 ## 3 14 2 ## 4 3 1 \\[ \\tau = \\frac{\\#\\text{concordant pairs} - \\#\\text{discordant pairs}}{0.5 n (n - 1)} \\] where the denominator is simply the number of possible combinations. Once we calculate \\(\\tau\\), how do we know if that value is significant or not? One method would be to do a randomization test. We can also use the following approximation for the test statistic for large (n&gt;10) sample sizes: \\[ \\tau | H_0 \\sim \\mathrm{N} \\left( 0, \\frac{2(2n + 5)}{9n (n - 1)} \\right) \\] In other words, \\[ \\frac{\\tau}{\\sqrt{\\frac{2(2n + 5)}{9n (n - 1)}}} \\sim N(0,1) \\] so we can use the quantiles of the standard normal we are so familiar with. In practice, Pearson’s and Kendall’s rank correlation tests often give very very similar results (if not in the actual value, certainly in the inference derived from them). 16.5 Regression Regression is a linear model (in the vein of those we introduced last week) in which a continuous response is modeled by one or more continuous predictors. (If we have only discrete predictors we call this ANOVA…the names are silly since they are all just linear models.) Linear regression simply finds the straight line that “best fits” a scatterplot of (x,y) data. We will note at the outset that there are two distinct kinds of regression. “Type I” regression assumes that the purpose of the analysis is to use x to predict y. Therefore, we assume x is without uncertainty and we want to minimize the error with which we predict y given x. “Type II” regression, or major axis regression (or standardized major axis regression), assumes a symmetry between X and Y and that the goal is in finding a correlation between them. Type II regression is often used when X (as well as Y) is measured with error, although see Smith (2009) for a full discussion of this interpretation. We will start by talking about “Type I” regression which is much more common, and come back to discussing Type II regression later. I have assigned a reading for this week that goes over all of this is some more detail. Before discussing regression, it is important to note that there is a distinction between regression and correlation. In regression, you are using one or more variables to predict another variable because you believe there is a cause-and-effect relationship (X causes Y). Correlation does not imply the same cause-and-effect type relationship, it just addresses whether two variables are associated with one another (X and Y covary). Remember correlation does not imply causation. To address causation, you usually need to do some kind of a manipulative experiment. As a first start, we will discuss linear regression (We introduced this model briefly when we introduced linear models last week.) \\[ Y_{i} = \\beta_{0}+\\beta_{1}X_{i} + \\epsilon_{i} \\mbox{, where } \\epsilon_{i} \\sim N(0,\\sigma^{2}) \\] where \\(\\beta_{0}\\) is the intercept and \\(\\beta_{1}\\) is the slope. Note that each data point \\(Y_{i}\\) is associated with its own value of the corresponding covariate predictor (\\(X_{i}\\)) and its own sample for the residual \\(\\epsilon_{i}\\). Before we get into some more math, let’s just draw some data and visualize what the slope and intercept represent. Note that we are usually just interested in the line (\\(\\beta_{0}+\\beta_{1}X\\)) that represents the effect of the covariate on the response variable and therefore our focus is on estimating the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\). We are often less interested in the variance associated with \\(\\epsilon\\) but keep in mind that \\(\\sigma\\) is a third parameter of this model and also must be estimated from the data. How do we find the line that best fits the data? Maximum likelihood! 16.6 Estimating the slope and intercept in linear regression The traditional “no calculus” explanation for estimating regression slope and intercept is included below for completeness, but I will bypass this more convoluted approach for now in favor of using maximum likelihood, which we already know how to do. Remember back to the Week 4 lab, the joint likelihood for \\(n\\) i.i.d. Normally distributed data \\(Y \\sim N(\\mu,\\sigma^{2})\\) was given by: \\[ L(\\mu,\\sigma|Y_{1},Y_{2},...,Y_{n}) = \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(Y_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] (Before, we were calling the response variable X, but now we will switch to calling the response variable Y since we will use X for the covariates.) Here we are making one tiny change, we are replacing \\(\\mu\\) with \\(\\beta_{0}+\\beta_{1}X_{i}\\) to fit the model \\(Y \\sim N(\\beta_{0}+\\beta_{1}X_{i},\\sigma^{2})\\). \\[ L(\\beta_{0},\\beta_{1},\\sigma|Y_{1},Y_{2},...,Y_{n}) = \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(Y_{i}-(\\beta_{0}+\\beta_{1}X_{i}))^{2}}{\\sigma^{2}}\\right)} \\] Great! Now we have a joint likelihood for the new linear regression model, and we can find the MLE for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) just like we did before, by setting \\[ \\frac{\\partial NLL}{\\partial \\beta_{0}} =0 \\] and \\[ \\frac{\\partial NLL}{\\partial \\beta_{1}} =0 \\] The values of \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that satisfy this equation are the maximum likelihood estimators for these two parameters. (Note that we will use this approach [writing down the joint likelihood and then minimizing with respect to the parameters] when we come to generalized linear models in another week.) If these conditions are met, then the OLS estimators we just derived are the best linear unbiased estimators (BLUE) and the sampling distributions for the slope and intercept are as follows: \\[ \\hat{\\beta_0} \\sim \\mathrm{N}\\left( \\beta_0, \\frac{\\sigma_\\epsilon^2 \\sum_{i = 1}^n X_i^2}{n \\sum_{i = 1}^n (X_i - \\bar{X})^2}\\right) \\text{, where } \\sigma_\\epsilon^2 = \\mathrm{E}(\\epsilon_i^2) \\] \\[ \\hat{\\beta_1} \\sim \\mathrm{N}\\left( \\beta_1, \\frac{\\sigma_\\epsilon^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2}\\right) \\text{, where } \\sigma_\\epsilon^2 = \\mathrm{E}(\\epsilon_i^2) \\] FYI: these distributions for \\(\\beta\\) can be derived using the second derivatives of the NLL (but you don’t need to be able to do this). These equations may look complex, but there are some features that should make sense. One: The expected value of \\(\\hat{\\beta_{0}}\\) is \\(\\beta_{0}\\), as you would expect for an unbiased estimator. Moreover, if \\(\\sigma_\\epsilon^2\\) goes to zero, the uncertainty in the estimates go to zero. In other words, if there is no variation in the data (all data points lie exactly on the regression line), than there is no uncertainty in the value of the parameters. Also, note that as \\(n \\rightarrow \\infty\\), the uncertainty goes to zero as well (again, as we would expect). Because we do not know the population (true) error variance \\(\\sigma_\\epsilon^2\\), we estimate it from the data using: \\[ s_\\epsilon^2 = \\frac{1}{n - p} \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 \\] We substitute \\(s_\\epsilon^2\\) for \\(\\sigma_\\epsilon^2\\). This is an unbiased and maximally efficient estimator for \\(\\sigma_\\epsilon^2\\). \\(p\\) is the number of parameters required to estimate \\(\\sigma_\\epsilon^2\\). Note that Aho calls this mean squared error (MSE) but most sources reserve that term for the actual mean of the squared errors, whereas here we want an unbiased estimate of the variance from the large pool of residuals our own sample of residuals are drawn from. (In other words, we have \\(n\\) data points and so our model yields \\(n\\) residuals; these \\(n\\) values are just a sample from a larger population of residuals and that’s the variance we want to estimate here, so we have to divide by the degrees of freedom, which is \\(n-p\\). There is a bit more discussion of this terminology in the lab.) Question: How many degrees of freedom do we have for \\(s_\\epsilon^2\\) in a simple linear regression? Click for Answer \\(n - p = n - 2\\). This is because \\(\\hat{Y}\\) involves \\(\\bar{Y}\\) and \\(\\bar{X}\\). This will be different with multiple regression (regression with more than one covariate). Question: What is our null hypothesis for the parameter \\(\\beta_0\\)? What does it mean? Click for Answer \\(\\beta_0 | H_0 = 0\\). When \\(X = 0\\) the estimated value for \\(Y\\) is 0. Question: What about for \\(\\beta_1\\)? Click for Answer \\(\\beta_1 | H_0 = 0\\). The slope of the regression equals 0, \\(X\\) has no linear effect on \\(Y\\). The test statistic and standard error for estimating \\(\\beta_1\\): \\[ T^* = \\frac{\\hat{\\beta_1} - \\beta_{1 | H_0}}{\\text{SE}_{\\hat{\\beta_1}}} \\sim t_{n - p} \\] \\[ \\text{SE}_{\\hat{\\beta_1}} = \\sqrt{\\frac{\\frac{1}{n - p} \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2}} = \\sqrt{\\frac{s_\\epsilon^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2}} \\] Note that usually we are interested in \\(\\beta_{1|H_{0}}=0\\). We construct confidence intervals as we always have \\[ P {\\left( \\hat{\\beta_1 } - t_{(1 - \\frac{\\alpha}{2}) [n - p]} \\text{SE}_{\\hat{\\beta_1}} \\leq \\beta_1 \\leq \\hat{\\beta_1 } + t_{(1 - \\frac{\\alpha}{2}) [n - p]} \\text{SE}_{\\hat{\\beta_1}} \\right)} = 1 - \\alpha \\] You can use similar methods to estimate the CI for \\(\\beta_0\\). We can use bootstrapping to calculate the standard error of the regression slope as well. We demonstrate this with an example on Thursday. Question: I’ve called this value \\(\\text{SE}_{\\hat{\\beta_1}}\\). Aho refers to it as \\({\\hat{\\sigma}_{\\hat{\\beta_1}}}\\). Why are these names interchangeable? Click for Answer The standard deviation of an estimated parameter (here, \\(\\hat{\\beta}_1\\)) is equal to the standard error of the parameter. 16.7 OK, now the “other” derivation for slope and intercept Here I present an alternative method for estimating the slope and intercept values that does not require knowing about maximum likelihood. As you will see, it is a little convoluted, but it does define some key vocabulary along the way, so here goes… It probably makes intuitive sense that the mean value of X should be associated with the mean value of Y and this is in fact true. The best fit line passes through (\\(\\bar{X}\\),\\(\\bar{Y}\\)). First, we will go over the notation for linear models (and regression specifically) we’ll be using for the next couple of weeks. Notation Meaning \\(Y\\) data/response variable/dependent variable \\(Y_i\\) value of \\(i^\\text{th}\\) data point \\(\\bar{Y}\\) mean of data \\(\\hat{Y}_i\\) estimate from the model for point \\(i\\). In regression, this is \\(\\hat{Y}_i = \\beta_0 + \\beta_1 X_i\\) To make any further progress with this question, we need to “partition the variance”. We will not tackle this seriously until we get to ANOVA next week, but the basic idea is fairly straightforward. To determine how to specifically fit slope and intercept parameters, we need to talk about partitioning variance (a goal of modeling in general, and a major part of ANOVA). When modeling, variation is either explained or unexplained. Let’s start from the beginning with this dataset of the relative length of spider webs. Of course, our data are not all exactly the same value, there is some variation: data(webs) ggplot(data = webs, aes(x = 1, y = length)) + geom_point(col = &quot;gray37&quot;) + labs(x = &quot;&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = text.size)) Next, we fit a model with two parameters, one parameter to describe the mean behavior of the system and one parameter to describe the amount of variation around that mean, e.g., \\(Y \\sim \\mathrm{N}(\\mu, \\sigma^2)\\), where we estimate \\(\\mu\\) using \\(\\overline{Y}\\) (there are two parameters, but only \\(\\mu\\) describes the mean). Using this parameter \\(\\mu\\), we have explained some of that variation in our data. y.bar &lt;- mean(webs[, 2]) ggplot(data = webs, aes(x = 0, y = length)) + geom_point(col = &quot;gray37&quot;) + geom_hline(aes(yintercept = mean(length)), col = &quot;dodgerblue1&quot;) + annotate(&#39;text&#39;, x = 0.01, y = y.bar+0.0001, label = &quot;bar(Y)&quot;, parse = TRUE, size = 5, col =&quot;dodgerblue1&quot;) + labs(x = &quot;&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = text.size)) Now, we might be able to explain more of this variation in the data if we add a covariate to the mean, so as to allow the mean to vary. This will “capture” some of the variability in the data and will leave less unexplained variation to be assigned to \\(\\sigma\\). There are three total parameters in this more complicated model, two that describe the mean and one to describe the residual variation. (The parameter \\(\\sigma^2\\) is playing the same role in the model as before, but because we hope that at least some of the variation is explained by the covariate, we now refer to whats left as residual variation.) web.fit &lt;- lm(formula = length ~ temp.C, data = webs) ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = &quot;gray37&quot;) + geom_hline(aes(yintercept = mean(length)), col = &quot;dodgerblue1&quot;) + geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = &quot;palegreen1&quot;) + annotate(&#39;text&#39;, x = 12, y = y.bar, label = &quot;bar(Y)&quot;, parse = TRUE, size = 5, col =&quot;dodgerblue1&quot;) + labs(x = &quot;Temperature (in C)&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(text = element_text(size = text.size)) The total amount of variation to be explained is called the “SST” for “sums of squares total”. \\[ \\text{Sums of squares total} = \\sum_{i = 1}^n (Y_i - \\bar{Y})^2 \\] (\\(n\\) is the total number of data points.) We can partition this total sums of squares into the component explain by the regression model and that which is left over as unexplained variation (which we often refer to as “error”, but it is “error” in the sense that it is not explained in the model and not “error” in the sense that we have done something wrong). Note that different sources/books will use different acronyms for partitioning variation and you should not be too invested in the notation I am presenting here. The important thing is to remember the equations and the idea behind partitioning variation. The sum of squares regression is the amount of variation expained by the regression line, or the squared deviations from the points estimated in the regression \\(\\hat{Y}_i\\), which is described by \\(\\hat{Y}_i = \\hat{\\beta_0} + \\hat{\\beta_1} X_i\\), and the estimated mean \\(\\bar{Y}\\): \\[ \\text{SSR} = \\sum_{i = 1}^n{(\\hat{Y}_i - \\bar{Y})^2} \\] You can think of the sum of squares regression as the amount of variation explained when going from a one parameter model describing the mean behavior (\\(\\mu\\) only) to the regression model (here, two parameters describing the mean behavior of the model, \\(\\beta_0\\) and \\(\\beta_1\\)). x.i &lt;- webs[36, 3] y.i.hat &lt;- web.fit$coefficients[1] + web.fit$coefficients[2] * x.i ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = &quot;gray37&quot;) + geom_point(x = x.i, y = y.i.hat, col = &quot;palegreen3&quot;, size = 3) + annotate(geom = &quot;text&quot;, x = x.i + 1.5, y = y.i.hat, label = &quot;hat(Y[i])&quot;, parse = TRUE, col = &quot;palegreen3&quot;, size = 5) + geom_hline(aes(yintercept = mean(length)), col = &quot;dodgerblue1&quot;) + geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = &quot;palegreen1&quot;) + annotate(&quot;text&quot;, x = 12, y = 0.9987, label = &quot;bar(Y)&quot;, parse = TRUE, size = 5, col =&quot;dodgerblue1&quot;) + labs(x = &quot;Temperature (in C)&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(text = element_text(size = text.size)) The sum of squares error is the amount of variation not expained by the regression line, or the squared deviations from the actual data points themselves, \\(Y_i\\), and the points estimated by the regression \\(\\hat{Y}_i\\): \\[ \\text{SSE} = \\sum_{i = 1}^n{(Y_i - \\hat{Y}_i)^2} \\] The sum of squares error is the remaining (unexplained, residual) error after modeling. Again, \\(\\hat{Y}_i = \\beta_0 + \\beta_1 X_i\\), or the Y value predicted by the regression model. The best fit line minimizes the sum of squares error. All errors (residuals) are the vertical distances from the data points to the points estimated in the regression. This is because all error exists with respect to the response variable (here, \\(Y\\)) only. Note that there all alternatives to minimizing the sum of squares error that we will briefly discuss later. To estimate \\(\\beta_0\\) and \\(\\beta_1\\), we could use multiple approaches. We could use the method we have been discussing, minimizing the sums of squares error, which is called ordinary least squares (OLS). We could also use maximum likelihood. As long as the assumptions of regression are met, these two methods are equivalent. The best fit line will pass through \\((\\bar{X}, \\bar{Y})\\) because this is the “center mass” of the data and because we assumed the expected value of the errors is zero. We use this fact (and some algebra) to show: \\[ \\bar{Y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\bar{X} \\] \\[ \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1} \\bar{X} \\] Then, we solve for \\(\\hat{\\beta_1}\\) by plugging in what we know back into the equation for SSE: \\[ \\text{SSE} = \\sum_{i = 1}^n{(Y_i - \\hat{Y}_i)^2} = \\sum_{i = 1}^n{(Y_i - \\hat{\\beta_0} - \\hat{\\beta_1} X_i)^2} = \\sum_{i = 1}^n{(Y_i - \\bar{Y} + \\hat{\\beta_1} \\bar{X} - \\hat{\\beta_1} X_i)^2} \\] Then we only have one unknown left \\(\\text{SSE} = f(\\hat{\\beta_1})\\), so we can minimize the function by taking the derivative with respect to \\(\\hat{\\beta_1}\\) and set it equal to 0. \\[ \\frac{\\partial \\text{SSE}}{\\partial \\hat{\\beta_1}} = 0 \\] After working this out, \\[ \\hat{\\beta_1} = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i = 1}^n (X_i - \\bar{X})^2} \\] Try this derivation yourself and then check your work here. 16.8 Assumptions of regression \\[ Y_{i} = \\beta_{0}+\\beta_{1}X_{i} + \\epsilon_{i} \\mbox{, where } \\epsilon_{i} \\sim N(0,\\sigma^{2}) \\] A linear model appropriately described the relationship between X and Y. For any given value of X, the sampled Y values are independent with normally distributed errors. We can express this assumption as follows: \\(E[\\epsilon_{i}]=0\\) \\(E[(\\epsilon_{i})^{2}]=\\sigma_{\\epsilon}^{2}\\) \\(E[(\\epsilon_{i})(\\epsilon_{j})]=0 \\mbox{, where } i \\neq j\\) Variances are constant along the regression line. (Non-constant variances are an example of heteroskedacity. More on this in a second.) An implicit assumption is that the regression fit is only valid over the range of X represented in the original data. It is very dangerous to extrapolate outside the range over which the regression line was originally fit. Our model allows us to make a prediction about the response expected for any given value of X, which may be a value of X in the original dataset or it may be a value of X not in the original dataset. (The model is only valid for X values contained within the range of the original data; more on this later.) We will refer to a new X value as \\(X^{*}\\) and the corresponding estimates for Y as \\(\\hat{Y}\\). Don’t forget that the fitted \\(\\hat{Y}\\) values are also estimated quantities, and as such come with uncertainty (i.e. standard errors and confidence intervals). It turns out there are really two ways to express “confidence”: one of which we call a “confidence interval”, the other we call a “prediction interval”. 16.9 Confidence vs. Prediction intervals One way is to ask about variability in our predicted values is: “What are the 1-\\(\\alpha\\) confidence intervals on the mean value of Y given \\(X^{*}\\)”? Equivalently, confidence intervals for \\(\\hat{Y}\\) mean, “there is a 95% chance that my 95th percentile CIs include the true underlying population parameter value, which is the mean value of \\(\\hat{Y}\\) (\\(\\mathrm{E}[Y|X^{*}]\\)).” We calculate the CIs using \\[ \\hat{Y^*} \\pm t_{(1 - \\frac{\\alpha}{2}) [n - p]} \\sqrt{s_\\epsilon^2 {\\left(\\frac{1}{n} + \\frac{(X^* - \\bar{X})^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2}\\right)}} \\] Note the notation here. The above equation represents the confidence intervals on \\(Y\\) associated with the value \\(X^*\\). The “hat” on Y reflects the fact that Y is an estimate based on the data, and the “star” is just meant to remind you that this interval is associated with the specific X value \\(X^*\\). However, there is a second way to think about uncertainty in the response value. In stead of wanting to know our uncertainty in the MEAN predicted value, we may want an estimate for the variation expected for a future value of \\(Y\\) for a given \\(X^{*}\\). Our estimates of uncertainty for the predicted values take into account both the mean of \\(\\hat{Y_h}\\) (uncertainty in the regression line itself) as well as the variance in \\(\\hat{Y_h}\\). These are called prediction intervals (PIs). We calculate PIs using \\[ \\hat{Y^*} \\pm t_{(1 - \\frac{\\alpha}{2}) [n - p]} \\sqrt{s_\\epsilon^2 {\\left( 1 + \\frac{1}{n} + \\frac{(X^* - \\bar{X})^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2}\\right)}} \\] Question: What is the difference between these two formulas? Does it make sense why they are different? Click for Answer The formula for the prediction interval has a 1 in the parenthesis, which is associated with the addition of \\(s^{2}_{\\epsilon}\\). This should make sense because it is the residual uncertianty that is being added to the uncertainty in the best fitting line itself. Prediction intervals are always wider than confidence intervals. webs$CIl &lt;- predict(web.fit, interval = &quot;confidence&quot;)[, 2] webs$CIu &lt;- predict(web.fit, interval = &quot;confidence&quot;)[, 3] webs$PIl &lt;- predict(web.fit, interval = &quot;prediction&quot;)[, 2] webs$PIu &lt;- predict(web.fit, interval = &quot;prediction&quot;)[, 3] ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = &quot;gray37&quot;) + geom_line(aes(x = temp.C, y = CIl), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = CIu), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = PIl), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) + geom_line(aes(x = temp.C, y = PIu), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) + geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = &quot;palegreen1&quot;) + labs(x = &quot;Temperature (in C)&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(text = element_text(size = text.size)) Remember: Confidence intervals represent our uncertainty about where the line should be (the mean behavior). Prediction intervals represent our certainty about the predicted values you could expect (the mean behavior + the variance). We can confirm the interpretation of the confidence interval by bootstrapping our data and refitting a line each time. webs$CIl &lt;- predict(web.fit, interval = &quot;confidence&quot;)[, 2] webs$CIu &lt;- predict(web.fit, interval = &quot;confidence&quot;)[, 3] webs$PIl &lt;- predict(web.fit, interval = &quot;prediction&quot;)[, 2] webs$PIu &lt;- predict(web.fit, interval = &quot;prediction&quot;)[, 3] p&lt;-ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = &quot;gray37&quot;) + geom_line(aes(x = temp.C, y = CIl), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = CIu), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = PIl), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) + geom_line(aes(x = temp.C, y = PIu), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) + labs(x = &quot;Temperature (in C)&quot;, y = &quot;Relative length of spider webs&quot;) + theme_classic() + theme(text = element_text(size = text.size)) for (i in 1:100) { selection&lt;-sample(1:nrow(webs),replace=T) webs.bootstrapped&lt;-webs[selection,] web.fit.new &lt;- lm(formula = length ~ temp.C, data = webs.bootstrapped) p&lt;-p+geom_abline(intercept = web.fit.new$coefficients[1], slope = web.fit.new$coefficients[2], col = &quot;palegreen1&quot;) } p&lt;-p+ geom_line(aes(x = temp.C, y = CIl), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = CIu), linetype = &quot;dashed&quot;, col = &quot;dodgerblue1&quot;) + geom_line(aes(x = temp.C, y = PIl), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) + geom_line(aes(x = temp.C, y = PIu), linetype = &quot;dotted&quot;, col = &quot;slateblue1&quot;) plot(p) Notice that the confidence intervals “capture” the range of possible regression lines created by fitting a linear model to the bootstrapped datasets (each “playing the role”” of a re-do of the original data collection.) Question: Why are confidence and prediction intervals “bow-tie” shaped, i.e. wider further away from the middle of the line? Click for Answer Mathematically, the squared deviation from \\(\\bar{X}\\) is greater as you move away from \\(\\bar{X}\\), which increases \\(\\text{SE}_{\\hat{Y}}\\). Intuitively, we can think about it this way: Because the best fitting line has to go through (\\(\\bar{X},\\bar{Y}\\)), the confidence interval is narrowest at this point. Away from (\\(\\bar{X},\\bar{Y}\\)), we have the additional uncertainty about what the slope should be moving away from (\\(\\bar{X},\\bar{Y}\\)), and so the intervals “grow” as you move away from this point. 16.10 How do we know if our model is any good? Earlier we determined that we fit our regression by minimizing the sum of squares error (SSE). However, SSE depends on the units of the dependent variable, so that doesn’t help us decide if the model fits “well” (e.g., the model could be a best fit line, but still not very good). However, we can use the total variation, partitioned into 1) the explained variation in the regression, SSR, and 2) the unexplained residual variance, SSE, to create a useful metric of model fit. \\[ \\sum_{i = 1}^n (Y_i - \\bar{Y})^2 = \\sum_{i = 1}^n (\\hat{Y_i} - \\bar{Y})^2 + \\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2 \\] On the left side is the total sum of squares (SST), or how much each data point differs from the mean. The first term on the right is the sum of squares regression (SSR), or the amount of variation explained by the regression. The final term on the right is the sum of squares error (SSE), or how much each data point deviates from the best fit line. The coefficient of determination \\(R^2\\) is the proportion of variation in \\(Y\\) explained by the regression model. Think of it like the ratio of variance explained relative to the total amount of variation in the data. \\[ R^2 = \\frac{\\text{SSR}}{\\text{SST}} \\] The coefficient of determination ranges from 0 to 1. It is equivalent to the square of Pearson’s \\(r\\). High \\(R^2\\) means that the model has explanatory power. However, it is not evidence that the relationship is causal. Lets go back to our calculations, and think about how many degrees of freedom we had for each Total sum of squares = SST = \\(\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})(Y_{i}-\\bar{Y})\\). This required one parameter \\(\\bar{Y}\\), be estimated from the data, so there remain n-1 degrees of freedom. Sum of squared error = SSE = \\(\\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})(Y_{i}-\\hat{Y_{i}})\\). This required estimation of \\(\\hat{Y_{i}}\\), which used two estimated parameters (slope and intercept) \\(\\rightarrow\\) n-2 d.o.f. remain. Regression model sum of squares SSR: The regression d.o.f. reflect the number of extra parameters involved in going from the null model (\\(\\bar{Y}\\)) to the regression model, which in this case is 1. ##Robust regression Ordinary Least Squares regression as defined above is fairly sensitive to outliers, which can be seen in the definition of SSE: \\[ SSE = \\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2} \\] Because we square the residuals, points that are far off the regression line have more influence on the estimates of intercept and slope than points that are much closer to the best fit line. There are several methods that are more robust against outliers. Not surprisingly, these fall under the heading of “robust regression”. The most common robust regression approach is called M-estimation. M-estimation minimizes any monotonic function of the error, allowing for methods other than squared error. These functions may associate less of a “cost” with having a large residual, or they may even have a saturation point which defines the maximum influence any point can have. In other words, the metric that is being minimized can be any function f() of the difference between each point and the best-fit line: \\[ \\sum_{i = 1}^n f(Y_i - \\hat{Y_i}) \\] Visualizing M-estimation (these are four functions, \\(\\rho\\), that can be used to model the error, \\(x\\)): Figure 16.1: Possible shapes for the weighting function in robust regression. Source: Wikimedia Commons We won’t get into the mathematics, because they are complex (another benefit of least squares is simpler math) but it’s an avenue you may use in the future and is good to know about. 16.11 Robust regression Sometimes the relationship between X and Y is not linear and cannot be made linear using a transformation of the variables. In this case, you can carry out a non-parametric test for regression. Here we will discuss Kendall’s robust line fitting method. Step 1 – Order the x and y values in order of increasing \\(X\\) value Step 2 – Compute the slope between every pair of \\(X\\) values \\[ \\mbox{Slope between i and j} = \\frac{Y_{j}-Y_{i}}{X_{j}-X_{i}} \\] Step 3 – \\[ \\widehat{\\mbox{Slope}}_{non-parameteric} = \\mbox{median of slopes} \\] \\[ \\widehat{\\mbox{Intercept}}_{non-parameteric} = \\mbox{median of } Y_{i}-\\widehat{\\mbox{Slope}}_{non-parametric}X_{i} \\] Question: How to know if the slope is significant? Click for Answer Use Kendall’s rank correlation test. 16.12 Type I and Type II Regression We have only discussed Type I regression up to this point. With Type I regression, we use \\(X\\) to predict \\(Y\\), and have only minimized the error in \\(Y\\). This means that we have assumed that there is no error in \\(X\\) (\\(X\\) is fixed and not random). However, in many cases there could be error in \\(X\\). Major axis regression and standardized major axis regression (Type II or Model II regressions) model variance in both the explanatory and response variables. Importantly, however, these methods to not strictly predict \\(Y\\) from \\(X\\), but rather calculate the major axis of variation in the bivariate dataset \\(X\\) and \\(Y\\). This means Type II regression models are not suggested for prediction. Major axis regression places equal weight on deviations in the \\(X\\) and \\(Y\\) directions and minimizes the sum of squared perpendicular distances to the best fit line. The major axis regression line is the first principal component from PCA (stay tuned). This requires that \\(X\\) and \\(Y\\) are measured on the same scale, using the same units. Standardized major axis regression standardizes each variable by its own variance and then fits a best fit line, so that deviations in \\(X\\) and \\(Y\\) are weighted accordingly. When is MA and SMA used? (1) In tests of isometry, allometry, stoichiometry (is the relationship between X and Y 1? Is the ratio of C:N 1?), (2) Testing if two methods of measurement agree. 16.13 Week 9 FAQ Question: Why do we divide by n-1 in the denominator of the sample covariance? The sample covariance is given by \\[ Cov(A,B) = \\frac{1}{n-1}\\sum_{i=1}^{n}(A_{i}-\\bar{A})(B_{i}-\\bar{B}) \\] The best way of thinking about this is as follows: We start with 2n degrees of freedom, n for the A data and n for the B data. Where we lose degrees of freedom may be seen by re-writing the product in the sum as follows: \\[ \\sum_{i=1}^{n}(A_{i}-\\bar{A})(B_{i}-\\bar{B})=\\sum_{i=1}^{n}A_{i}(B_{i}-\\bar{B})-\\sum_{i=1}^{n}\\bar{A}(B_{i}-\\bar{B}) \\] We have terms on the left hand side, and lose one degree of freedom for the use of \\(\\bar{B}\\) in each term (for a total of \\(n\\) d.o.f. lost) and we lose one additional d.o.f. for the one use of ̅\\(\\bar{B}\\) in the second term. Therefore we end up with \\(2n-n-1=n-1\\) degrees of freedom for the whole product. "],["week-9-lab.html", "17 Week 9 Lab 17.1 Correlation 17.2 Linear modelling 17.3 Centering the covariates 17.4 Weighted regression 17.5 Robust regression 17.6 Bootstrapping standard errors for robust regression 17.7 Type I vs. Type II regression: The ‘smatr’ package", " 17 Week 9 Lab We will need 4 packages for this week’s lab, so we might as well load them all in now. library(MASS) library(car) ## Loading required package: carData library(boot) ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:car&#39;: ## ## logit library(smatr) 17.1 Correlation We will first go over how to test for correlation between two variables. We will use a dataset of July mean temperatures at an Alaskan weather station (Prudhoe Bay) over a period of 12 years. Temperature&lt;-c(5.1,5.6,5.7,6.6,6.7) Year&lt;-c(1979,1982,1985,1988,1991) First we will plot the data to get a sense for whether the correlation coefficient is likely to be positive or negative. plot(Year,Temperature,pch=16) We can test the correlation between Temperature and Year using the R function ‘cor.test’ ans&lt;-cor.test(Temperature,Year) ans ## ## Pearson&#39;s product-moment correlation ## ## data: Temperature and Year ## t = 6.4299, df = 3, p-value = 0.007626 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5625553 0.9978118 ## sample estimates: ## cor ## 0.965581 ‘cor’ is the correlation coefficient - we see there is a strongly positive (and statistically significant) correlation between year and temperature. Let’s make sure we understand every part of this output. Part 1: This is just spitting back what two variables are being correlated. Part 2: t=6.4299 How do we get this? Remember: \\[ t_{s} = r\\sqrt{\\frac{n-2}{1-r^{2}}} \\] We can calculate this by pulling out various elements of the variable ‘ans’. names(ans) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;estimate&quot; &quot;null.value&quot; ## [6] &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; &quot;conf.int&quot; r&lt;-as.numeric(ans$estimate) #as.numeric supresses labels df&lt;-as.numeric(ans$parameter) r*sqrt(df/(1-(r^2))) ## [1] 6.429911 This gives us the same as ans$statistic ## t ## 6.429911 Part 3: Why is df=3? For a correlation coefficient, you have n-2 degrees of freedom - you lose one degree of freedom for the mean of each variable. Part 4: How do we get p-value = 0.007626? First, lets start with a plot of the t-distribution with three d.o.f. plot(seq(-7,7,0.01),dt(seq(-7,7,0.01),df=3),typ=&quot;l&quot;,col=&quot;purple&quot;,lwd=2) abline(v=6.4299) We can ask P(t&gt;6.4299)=1-P(t&lt;6.4299) by pt(6.4299,df=3,lower.tail=F)*2 ## [1] 0.007625665 Why multiple by 2? Because we want a two-tailed test, so we want to consider correlations larger in magnitude that are either positive or negative. Part 5: 95 percent confidence interval (0.5625553,0.9978118). \\[ z=\\frac{1}{2}ln(\\frac{1+r}{1-r})=arctanh(r) \\] z&lt;-(1/2)*log((1+r)/(1-r)) z ## [1] 2.022468 \\[ P(z-\\frac{t_{[1-\\alpha/2](\\infty)}}{\\sqrt{n-3}}\\leq arctanh(\\rho) \\leq z+\\frac{t_{[1-\\alpha/2](\\infty)}}{\\sqrt{n-3}})=1-\\alpha \\] \\[ P(tanh(z-\\frac{t_{[1-\\alpha/2](\\infty)}}{\\sqrt{n-3}})\\leq \\rho \\leq tanh(z+\\frac{t_{[1-\\alpha/2](\\infty)}}{\\sqrt{n-3}}))=1-\\alpha \\] n&lt;-5 LL.z&lt;-z-(1/sqrt(n-3))*qnorm(0.975) LL.r&lt;-tanh(LL.z) LL.r ## [1] 0.5625553 UL.z&lt;-z+(1/sqrt(n-3))*qnorm(0.975) UL.r&lt;-tanh(UL.z) UL.r ## [1] 0.9978118 Notice that I can extract the LL and the UL by taking the tanh of the limits for the transformed variable. In class we discussed several different ways of testing for a correlation. We can see these by querying the help page for ‘cor’ and ‘cor.test’: ?cor.test Notice that under the method option there are three options. The “pearson” is the first (default). We can change the default by trying cor.test(Temperature,Year, method=&quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: Temperature and Year ## T = 10, p-value = 0.01667 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 1 Does it make sense why Kendall’s tau=1.0? Now that we have some practice with correlations, let play a game! Each member of your group should visit this site and play a few rounds. Click the “Track Performance” check box to track your performance over time. Checkpoint #1: Who in your group is doing the best? 17.2 Linear modelling Linear modeling in R occurs primarily through two functions ‘lm’ and ‘glm’. The first is reserved for linear regression in the form we have been discussing this week. The second function is for generalized linear models; we will discuss these in the next few weeks. Fitting simulated data Before working with real data, let’s play around with a simulated dataset, so you can see how the values used to simulate the data are reflected in the parameter estimates themselves. n&lt;-30 X&lt;-seq(1,n) #a stand in for some covariate value&lt;-c() intercept&lt;-0.15 slope&lt;--2.2 sigma&lt;-10 for (i in 1:length(X)) { value&lt;-c(value,rnorm(1,mean=intercept+slope*X[i],sd=sigma)) } fit&lt;-lm(value~X) summary(fit) ## ## Call: ## lm(formula = value ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.806 -3.261 1.601 5.601 16.075 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.8186 3.8025 -2.056 0.0492 * ## X -1.9549 0.2142 -9.127 6.94e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.15 on 28 degrees of freedom ## Multiple R-squared: 0.7484, Adjusted R-squared: 0.7395 ## F-statistic: 83.31 on 1 and 28 DF, p-value: 6.944e-10 Copy this script into R and r-run it several times. Notice how the estimates for slope and intercept bounce around, but they should be correct on average and also the scale of variation from one run to the next should make sense given the estimate of the standard error. (Their standard deviation should be the standard error.) Notice also that as you increase sigma, the R2 goes down because now you are increasing the variation that is not explained by the covariate. Try changing the number of samples drawn, either by extending the vector of the covariates or by drawing multiple times for each value (you will have to modify the code to make this latter change work). Notice how the standard errors on the intercept and slope coefficients gets smaller as the data set gets larger but the estimate for sigma (which is listed as the residual standard error near the bottom) does not. The parameter sigma is a property of the underlying population, not a property of the sample drawn, so it does not get smaller as you increase the number of samples in the dataset. (If this does not make sense, ask me!) We can use this simple model to define some common (and often confusing) terms used in regression (and later, ANOVA), which will require using the function “residuals” to pull out the difference between each point and the best-fit line. The mean squared error is usually defined as mean(residuals(fit)^2) ## [1] 96.23298 which is the same as sum(residuals(fit)^2)/n ## [1] 96.23298 while the root mean squared error is sqrt(mean(residuals(fit)^2)) ## [1] 9.809841 which is just the square-root of the mean squared error above. Note that some authors (like Aho) will divide by the degrees of freedom to get an unbiased estimate of the population variance \\(\\sigma^{2}_{\\epsilon}\\) and call that the mean squared error (or MSE). Just be careful with these terms to know whether the calculation is a description of the residuals observed (in which case the denominator is \\(n\\)) or whether the calculation is being used as an unbiased estimate of the larger population of residuals (in which case the denominator should be the degrees of freedom). The residual sum of squares is actually better thought of as the “sum of residuals squared”, i.e. sum(residuals(fit)^2) ## [1] 2886.989 and the residual standard error is sqrt(sum(residuals(fit)^2)/(n-2)) ## [1] 10.15415 Note that this last term uses the degrees of freedom in the numerator. The residual standard error is taking the data you have as a sample from the larger population and trying to estimate the standard error from the larger population. So it takes the residual sum of squares, divides that by the degrees of freedom (we have 30 data points, we lost 2 degrees of freedom, so we are left with 28 degrees of freedom for the estimation of the residual standard error) and then takes the square root. We can think of the mean squared error as being the population variance (i.e. the variance of the residuals if the dataset represented the entire population, see our discussion in Week 1) but this underestimates the variance if all we have is a sample from the larger population, so in this case we want to calculate the sample variance (i.e. our estimate of the population variance if all we have is a sample) mean(residuals(fit)^2)*(n/(n-2)) ## [1] 103.1068 This should come close to what we used to generate the data (\\(\\sigma=10\\) so \\(\\sigma^{2}=100\\)). (If you go back and change the code to use a larger number of data points, the estimate will be closer.) There is a function called sigma() is the R package ‘stats’ that may be loaded by default in your workspace. sigma(fit) will grab that residual standard error value, which is helpful if you need to extract an estimate of \\(\\sigma\\) (a.k.a. \\(\\hat{\\sigma}\\), which is really estimated as \\(\\sqrt{\\widehat{\\sigma^2}}\\) ) from the model fit. sigma(fit) ## [1] 10.15415 Notice that this is note quite the same (and is always slightly larger) than this alternative estimate of \\(\\sigma\\). fitdistr(residuals(fit),&quot;normal&quot;)$estimate[2] ## sd ## 9.809841 This takes your residuals, uses fitdistr to fit a Normal distribution to them, and then reports the standard deviation of the residuals. This should give you a measure of the spread of the residuals, which should be the same as the residual standard error extracted from sigma(fit). Question: Why does this method always underestimate \\(\\sigma\\) (which you know here because you used a known value to simulate the data)? Click for Answer Because fitdistr() uses MLE to estimate the parameters of the Normal fit to the residuals, and the MLE for \\(\\sigma^2\\) are biased estimates. Keep in mind that MLE estimates are not guaranteed to be unbiased. Fitting real data Now that we’ve gained some intuition, we can dive into fitting a real dataset. The syntax of ‘lm’ is straightforward. We will run through some examples using a dataset on Old Faithful eruptions. The dataset is built into the MASS library, so we just have to load it. attach(faithful) #A rare exception to the rule of avoiding &#39;attach&#39; head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 plot(waiting, eruptions,pch=16) This dataset lists the times of an Old Faithful eruption as a function of the waiting time prior to the eruption. We can see that as the waiting time increases, so does the length of the eruption. We can fit a linear regression model to this relationship using the R function ‘lm’. eruption.lm&lt;-lm(eruptions~waiting) Model: y~x1 Meaning: y is explained by x1 only (intercept implicit) Model: y~x1-1 Meaning: y is explained by x1 only (no intercept) Model: y~x1+x2 Meaning: y is explained x1 and x2 Model: x1+x2+x1:x2 Meaning: y is explained by x1,x2 and also by the interaction between them Model: y~x1*x2 Meaning: y is explained by x1,x2 and also by the interaction between them (this is an alternative way of writing the above) We print out a summary of the linear regression results as follows: summary(eruption.lm) ## ## Call: ## lm(formula = eruptions ~ waiting) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29917 -0.37689 0.03508 0.34909 1.19329 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.874016 0.160143 -11.70 &lt;2e-16 *** ## waiting 0.075628 0.002219 34.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4965 on 270 degrees of freedom ## Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 ## F-statistic: 1162 on 1 and 270 DF, p-value: &lt; 2.2e-16 Question: In words, how would we interpret the coefficients for this model? Make sure you understand all of this output! We can check the output on the residuals using either the R function ‘residuals’ which takes in the lm object and spits out the residuals of the fit quantile(residuals(eruption.lm),probs=c(0.0,0.25,0.5,0.75,1.0)) ## 0% 25% 50% 75% 100% ## -1.29917268 -0.37689320 0.03508321 0.34909412 1.19329194 or by using the R function ‘predict’ (which calculates the predicted y value for each x value) and calculating the residuals ourselves: residuals&lt;-eruptions-predict(eruption.lm, data.frame(waiting)) # Note that predict wants a dataframe of values quantile(residuals,probs=c(0.0,0.25,0.5,0.75,1.0)) ## 0% 25% 50% 75% 100% ## -1.29917268 -0.37689320 0.03508321 0.34909412 1.19329194 Now we can calculate the slope and its standard error: x&lt;-waiting y&lt;-eruptions SSXY&lt;-sum((x-mean(x))*(y-mean(y))) SSX&lt;-sum((x-mean(x))*(x-mean(x))) slope.est&lt;-SSXY/SSX #Also could have used slope.est&lt;-cov(x,y)/var(x) n&lt;-length(x) residuals&lt;-residuals(eruption.lm) var.slope&lt;-(1/(n-2))*sum((residuals-mean(residuals))*(residuals-mean(residuals)))/SSX s.e.slope&lt;-sqrt(var.slope) slope.est ## [1] 0.07562795 s.e.slope ## [1] 0.002218541 We calculate the t-value as: t.value&lt;-slope.est/s.e.slope p.value&lt;-2*(1-pt(abs(t.value),n-2)) t.value ## [1] 34.08904 p.value ## [1] 0 We can calculate the intercept and its standard error in a similar manner. The residual standard error is: residual.se&lt;-sqrt((1/(n-2))*sum((residuals-mean(residuals))*(residuals-mean(residuals)))) residual.se ## [1] 0.4965129 and the R2 as SST&lt;-sum((y-mean(y))*(y-mean(y))) SSR&lt;-SST-sum(residuals*residuals) R2&lt;-SSR/SST R2 ## [1] 0.8114608 Checkpoint #2: Were you able to reproduce all of the output from cor.test covered so far? R also produces an “adjusted R2”, which attempts to account for the number of parameters being estimated, and provides one way of comparing goodness of fit between models with different numbers of parameters. It is defined as \\[ R^{2}_{adj} = 1-(1-R^{2})\\left(\\frac{n-1}{n-p-1}\\right) \\] but we won’t get into more details here. Notice that the percentage of explained variation \\(R^{2}\\) is just the square of the Pearson’s product moment correlation coefficient. (cor(x,y))^2 ## [1] 0.8114608 We will hold off on a discussion of the F statistic until we do ANOVA next week. In lecture we distinguished between confidence intervals and prediction intervals. The former tells us our uncertainty about the mean of Y at a given X, whereas the latter gives us the interval within which we expect a new value of Y to fall for a given X. We can calculate both of these using the option ‘interval’ in the predict function. newdata&lt;-data.frame(waiting=seq(min(waiting),max(waiting))) confidence.bands&lt;-predict(eruption.lm,newdata,interval=&quot;confidence&quot;) prediction.bands&lt;-predict(eruption.lm,newdata,interval=&quot;predict&quot;) plot(waiting,eruptions,ylim=c(0,7)) lines(newdata[,1],confidence.bands[,1],col=1) lines(newdata[,1],confidence.bands[,2],col=2) lines(newdata[,1],confidence.bands[,3],col=2) lines(newdata[,1],prediction.bands[,2],col=3) lines(newdata[,1],prediction.bands[,3],col=3) Checkpoint #3: Do you understand the difference in interpretation between a confidence interval and a prediction interval? Do you understand why the prediction interval is always wider? What do we do if we want to force the intercept through the origin (i.e., set the intercept to zero)? eruption.lm2&lt;-lm(eruptions~waiting-1) summary(eruption.lm2) ## ## Call: ## lm(formula = eruptions ~ waiting - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.54127 -0.57533 -0.00846 0.42257 1.25718 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## waiting 0.0501292 0.0005111 98.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6084 on 271 degrees of freedom ## Multiple R-squared: 0.9726, Adjusted R-squared: 0.9725 ## F-statistic: 9621 on 1 and 271 DF, p-value: &lt; 2.2e-16 Wait…look closely…what’s strange about the model when we suppress the intercept? Somehow we have gone from a bigger model (intercept and slope) to a smaller model (slope only) and R is telling us that the fit of the model has actually improved. Not possible! So what is going on? Remember the definition of R2: \\[ R^{2} = \\frac{SSR}{SST} = 1-\\frac{SSE}{SST} = 1-\\frac{\\Sigma{(Y_{i}-\\hat{Y_{i}})^{2}}}{\\Sigma(Y_{i}-\\bar{Y})^{2}} \\] When there is no intercept, R (silently!) uses an alternative definition of R2 \\[ R^{2} = 1-\\frac{\\Sigma{(Y_{i}-\\hat{Y_{i}})^{2}}}{\\Sigma{Y_{i}^{2}}} \\] Why does R do that? In the first case, you have a slope and an intercept, and R is comparing the model you have against an alternate model which includes only an intercept. When you have an intercept-only model, that intercept is going to be the mean \\(\\bar{Y}\\). (Does it make sense why that is?) However, when you have supressed the intercept, the original alternate model (intercept only) no longer makes sense. So R chooses a new alternate model which is one of just random noise with \\(\\bar{Y}=0\\). If we look at the expression above, the effect of this is to increase the residuals going into SSE and the total sum of squares SST. However, the increase in SST is generally larger than the increase in SSE, which means that the R2 actually increases. The bottom line is that funny things happy when you suppress the intercept and while the output (effect sizes and standard errors) is still perfectly valid, the metrics of model fit become different and the with-intercept and without-intercept models can no longer be compared sensibly. 17.3 Centering the covariates So far we have taken the covariates as is and have used those in our linear model. That decision has some consequences that we will explore now. Let’s use bootstrap to sample with replacement, and then look at how our estimates of the slope and intercept vary from one bootstrapped sample to the next. intercepts&lt;-c() slopes&lt;-c() for (i in 1:1000) { indices&lt;-sample(seq(1,length(eruptions)),replace=T) bootstrapped.eruptions&lt;-eruptions[indices] bootstrapped.waiting&lt;-waiting[indices] eruption.bs.lm&lt;-lm(bootstrapped.eruptions~bootstrapped.waiting) intercepts&lt;-c(intercepts,eruption.bs.lm$coefficients[1]) slopes&lt;-c(slopes,eruption.bs.lm$coefficients[2]) } plot(slopes,intercepts) The estimates of slope and intercept and highly correlated, because an increase in the slope will tend to decrease the intercept and vice versa. This is a consequence of the fact that the two parameters are estimated from the same data and in this case, an increase in the estimate of the slope has to drive a decrease in the intercept. (You can see this by looking back at the plot of the data. A steeper slope that goes through \\((\\bar{X},\\bar{Y})\\) has to have a smaller intercept.) This is not a problem per se, but it does mean that the estimates of the slope and intercept are not independent of each other. What happens if we redo the same bootstrapping exercise but center the covariate by subtracting the mean; this creates a new covariates that is centered on waiting=0. intercepts&lt;-c() slopes&lt;-c() for (i in 1:1000) { indices&lt;-sample(seq(1,length(eruptions)),replace=T) bootstrapped.eruptions&lt;-eruptions[indices] bootstrapped.waiting&lt;-waiting[indices] - mean(waiting) eruption.bs.lm&lt;-lm(bootstrapped.eruptions~bootstrapped.waiting) intercepts&lt;-c(intercepts,eruption.bs.lm$coefficients[1]) slopes&lt;-c(slopes,eruption.bs.lm$coefficients[2]) } plot(slopes,intercepts) Note that by centering the covariate, we break the dependence of the slope and intercept estimates. If you are using ‘optim’ to fit the model (which lm() is doing under the hood anyways), centering the covariate in this way will make it easier for optim() to find the MLE estimates for slope and intercept, and it will make the estimates returned by optim() less sensitive to the starting values. As a general matter, it is a good idea to center your covariates (this is especially true when we get to regression with multiple covariates) before fitting your model. Just be sure to remember that you have done this as it changes the interpretation of the intercept (now the intercept reflect the value of the response at the mean value of the covariates). 17.4 Weighted regression In lecture, we introduced the idea that ordinary least squares regression involves minimizing the sum-of-squares error \\[ SSE = \\sum^{n}_{i=1}(Y_{i}-\\hat{Y}_{i})^{2} \\] where squared residuals are summed as a measure of model fit. Sometimes, however, you want to weight some residuals more or less than others. Often this is done to account for increased variability in the responses Y over a certain range of Xs. We can do this through weighted linear regression, i.e. by minimizing the weighted residuals \\[ SSE = \\sum^{n}_{i=1}w_{i}(Y_{i}-\\hat{Y}_{i})^{2} \\] We can illustrate doing this by using the ‘weights’ option in lm. In this example, we will see what happens when we weight the short eruptions 2 and 10 as much as the long eruptions. The result of this will be that the best fit model will try and fit the short eruptions better because residuals for short eruptions are two or ten times as influential to SSE than residuals for long eruptions. plot(waiting,eruptions,ylim=c(0,7)) lines(newdata[,1],confidence.bands[,1]) short&lt;-(eruptions&lt;3) points(waiting[short],eruptions[short],pch=16) eruption.lm&lt;-lm(eruptions~waiting,weights=rep(1,times=272)) abline(a=eruption.lm$coef[1],b=eruption.lm$coef[2],col=&quot;black&quot;,lwd=2) eruption.lm.wt&lt;-lm(eruptions~waiting,weights=rep(1,times=272)+as.numeric(short)) abline(a=eruption.lm.wt$coef[1],b=eruption.lm.wt$coef[2],col=&quot;green&quot;,lwd=2) eruption.lm.wt&lt;-lm(eruptions~waiting,weights=rep(1,times=272)+9*as.numeric(short)) abline(a=eruption.lm.wt$coef[1],b=eruption.lm.wt$coef[2],col=&quot;purple&quot;,lwd=2) 17.5 Robust regression Weighted linear regression would be one method that could be used for downweighting the influence of certain data points. Robust regression is a another method for making sure that your linear model fit is not unduly influenced by outliers (points with large residuals). We will use the Duncan occupational dataset we used once before library(car) # for Duncan data and (later) data.ellipse) library(MASS) data(Duncan) Duncan ## type income education prestige ## accountant prof 62 86 82 ## pilot prof 72 76 83 ## architect prof 75 92 90 ## author prof 55 90 76 ## chemist prof 64 86 90 ## minister prof 21 84 87 ## professor prof 64 93 93 ## dentist prof 80 100 90 ## reporter wc 67 87 52 ## engineer prof 72 86 88 ## undertaker prof 42 74 57 ## lawyer prof 76 98 89 ## physician prof 76 97 97 ## welfare.worker prof 41 84 59 ## teacher prof 48 91 73 ## conductor wc 76 34 38 ## contractor prof 53 45 76 ## factory.owner prof 60 56 81 ## store.manager prof 42 44 45 ## banker prof 78 82 92 ## bookkeeper wc 29 72 39 ## mail.carrier wc 48 55 34 ## insurance.agent wc 55 71 41 ## store.clerk wc 29 50 16 ## carpenter bc 21 23 33 ## electrician bc 47 39 53 ## RR.engineer bc 81 28 67 ## machinist bc 36 32 57 ## auto.repairman bc 22 22 26 ## plumber bc 44 25 29 ## gas.stn.attendant bc 15 29 10 ## coal.miner bc 7 7 15 ## streetcar.motorman bc 42 26 19 ## taxi.driver bc 9 19 10 ## truck.driver bc 21 15 13 ## machine.operator bc 21 20 24 ## barber bc 16 26 20 ## bartender bc 16 28 7 ## shoe.shiner bc 9 17 3 ## cook bc 14 22 16 ## soda.clerk bc 12 30 6 ## watchman bc 17 25 11 ## janitor bc 7 20 8 ## policeman bc 34 47 41 ## waiter bc 8 32 10 Let’s identify any data points we think are outliers plot(Duncan$education,Duncan$income,ylim=c(0,100)) temp&lt;-c(which(rownames(Duncan)==&quot;RR.engineer&quot;),which(rownames(Duncan)==&quot;conductor&quot;)) text(x=Duncan$education[temp]-8,y=Duncan$income[temp],labels=rownames(Duncan)[temp],cex=0.5) #identify(x=Duncan$education, y=Duncan$income, labels=rownames(Duncan)) Visually, we may think that conductors and railroad engineers may have a disproportionate influence on the linear regression of income and education. We will explore this by first doing a regular linear regression using ‘lm’ and then doing a robust linear regression using ‘rlm’. Duncan.model.lm&lt;-lm(income~education, data=Duncan) summary(Duncan.model.lm) ## ## Call: ## lm(formula = income ~ education, data = Duncan) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.572 -11.346 -1.501 9.669 53.740 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.6035 5.1983 2.040 0.0475 * ## education 0.5949 0.0863 6.893 1.84e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.04 on 43 degrees of freedom ## Multiple R-squared: 0.5249, Adjusted R-squared: 0.5139 ## F-statistic: 47.51 on 1 and 43 DF, p-value: 1.84e-08 Let’s compare the fit with the one in which we remove the two potential outliers. outliers&lt;-c(which(rownames(Duncan)==&quot;RR.engineer&quot;),which(rownames(Duncan)==&quot;conductor&quot;)) Duncan.model2&lt;-lm(income[-outliers]~education[-outliers],data=Duncan) We see that removing these two professions changes the slope and intercept, as expected. Checkpoint #4: Does the change in slope and intercept when you remove those outliers make sense intuitively? Let’s try doing a robust regression now. First, let’s remind ourselves that robust regression minimizes some function of the errors. \\[ \\sum^{n}_{i=1}f(Y_{i}-\\hat{Y}_{i}) \\] Let’s look at the help file for ‘rlm’: ?rlm The default robust weighting scheme is Huber’s method. Duncan.model.rlm&lt;-rlm(income~education,data=Duncan) summary(Duncan.model.rlm) ## ## Call: rlm(formula = income ~ education, data = Duncan) ## Residuals: ## Min 1Q Median 3Q Max ## -40.8684 -9.8692 0.8085 7.8394 56.1770 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) 6.3002 4.4943 1.4018 ## education 0.6615 0.0746 8.8659 ## ## Residual standard error: 13.06 on 43 degrees of freedom The residuals are much more similar to what we got from ‘lm’ when we excluded the outlying datapoints. Robust methods are generally preferred over removing outliers. 17.6 Bootstrapping standard errors for robust regression The standard errors reported by ‘rlm’ rely on asymptotic approximations that may not be particularly reliable in this case because our sample size is only 45. We will use bootstrapping to construct more appropriate standard errors. There are two ways to do bootstrapping for calculating the standard errors of regression model parameters. We can sample with replacement (X,Y) pairs from the original dataset. We can sample with replacement residuals from the original model and use the same predictor variables, i.e. we use \\[ (x_{1},\\hat{y_{1}}+\\epsilon^{*}_{1}) \\] \\[ (x_{2},\\hat{y_{2}}+\\epsilon^{*}_{2}) \\] \\[ (x_{3},\\hat{y_{3}}+\\epsilon^{*}_{3}) \\] You might use this latter approach if the predictor variables were fixed by the experimentor (they do not reflect a larger population of fixed values), so they should really remain fixed in calculating the standard errors. Today I will only go through the mechanics of the first approach, called “random x resampling”. Although writing the bootstrap script yourself is straightforward, we will go through the functions available in the package ‘boot’. library(boot) boot.huber&lt;-function(data,indices,maxit) { data&lt;-data[indices,] #select observations in bootstrap sample mod&lt;-rlm(income~education,data=data,maxit=maxit) coefficients(mod) #return the coefficient vector } Note that we have to pass the function the data and the indices to be sampled. I’ve added an additional option to increase the number of iterations allowed for the rlm estimator to converge. duncan.boot&lt;-boot(Duncan,boot.huber,1999,maxit=100) duncan.boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Duncan, statistic = boot.huber, R = 1999, maxit = 100) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 6.3002197 0.146046228 4.66295183 ## t2* 0.6615263 -0.003855036 0.07448981 Checkpoint #5: How would we know if the bias is significant (i.e., how would we calculate the standard error of the bias)? 17.7 Type I vs. Type II regression: The ‘smatr’ package The two main functions in the smatr package are ‘sma’ and ‘ma’ regression for doing standardized major axis regression. Look at the help file for sma to see what some of the options are. ?sma Let’s say we wanted to look at the Duncan dataset again, but instead of asking whether we can use income to predict education, we can ask instead simply whether the two are correlated. Duncan.model.sma&lt;-sma(income~education, data=Duncan) Duncan.model.sma ## Call: sma(formula = income ~ education, data = Duncan) ## ## Fit using Standardized Major Axis ## ## ------------------------------------------------------------ ## Coefficients: ## elevation slope ## estimate -1.283968 0.8210480 ## lower limit -11.965282 0.6652483 ## upper limit 9.397345 1.0133356 ## ## H0 : variables uncorrelated ## R-squared : 0.5249182 ## P-value : 1.8399e-08 This gives us a very different result from what we got from ‘lm’. Let’s plot the data, and the best-fit lines to see why this makes sense. plot(Duncan$education,Duncan$income) abline(a=coef(Duncan.model.lm)[1],b=coef(Duncan.model.lm)[2]) abline(a=coef(Duncan.model.sma)[1],b=coef(Duncan.model.sma)[2],col=&quot;red&quot;) The SMA line is closer to what you would probably draw by eye as going through the ‘cloud’ of points, since our instinct is to draw a line through the principle axis of variation and not through the regression line, which has a smaller slope. \\[ \\mbox{SMA slope} = \\frac{\\mbox{OLS slope}}{|\\mbox{correlation r}|} \\] The SMA slope is the OLS slope divided by the absolute value of the Pearson’s product moment correlation coefficient and is always, therefore, steeper than the OLS slope. So, how do we use the ‘smatr’ package? sma(y~x) will fit a SMA for y vs. x, and report confidence intervals for the slope and elevation (a.k.a., the intercept). sma(y~x,robust=T) will fit a robust SMA for y vs. x using Huber’s M estimation, and will report (approximate) confidence intervals for the slope and elevation. ma(y~x*groups-1) will fit MA lines for y vs. x that are forced through the origin (because we explicitly removed the intercept) with a separate MA line fit to each of several samples as specifed by the argument groups. It will also report results from a test of the hypothesis that the true MA slope is equal across all samples. "],["week-10-lecture.html", "18 Week 10 Lecture 18.1 Week 10 Readings 18.2 Week 10 outline 18.3 An example 18.4 Generalized linear models 18.5 Logistic regression 18.6 Fitting a GLM 18.7 Poisson regression 18.8 Deviance 18.9 Other methods – LOESS, splines, GAMs", " 18 Week 10 Lecture 18.1 Week 10 Readings For this week, I suggest reading Aho Section 9.20, as well as Logan Chapters 9 and 16-17. You will also need to read Siddhartha et al. (1989). This paper is another one that is quite important and should be read carefully. We will also discuss it at length in class. There are two other papers some of you will find worthwhile as well, this paper on why not to log-transform count data (please, don’t do it, it you have count data and don’t know what to do, ask me!) and another paper laying out when to use GLM and when to use OLS regression. 18.2 Week 10 outline This week we will cover my favorite topic of all: Generalized Linear Regression and Multiple Regression! An outline for this week’s materials: Basic idea behind GLM Logistic regression Poisson regression Deviance LOESS/spline smoothing Generalized additive models Multiple regression 18.3 An example Let’s start with a model of the presence or absence of a wood-boring beetle as a function of the wood density of decaying aspen trees in Quebec. The data look like this: ## Loading required package: tcltk So, let’s do what we know, and we’ll fit a linear regression to the data using lm() (like we did last week). Then we’ll plot the model fit. beetle.fit &lt;- lm(formula = ANAT ~ Wood.density, data = beetle) new.predictor &lt;- list(Wood.density = seq(min(beetle$Wood.density), max(beetle$Wood.density), length.out = 24)) beetle.vals &lt;- predict(beetle.fit, newdata = new.predictor) beetle.predict &lt;- data.frame(WoodDensity = new.predictor[[1]], Presence = beetle.vals) ggplot(data = beetle, aes(x = Wood.density, y = ANAT)) + geom_point(col = &quot;gray55&quot;) + geom_line(data = beetle.predict, aes(x = WoodDensity, y = Presence)) + labs(x = expression(paste(&quot;Wood density (g cm&quot; ^ &quot;-3&quot;, &quot;)&quot;)), y = &quot;Presence of wood-boring beetle&quot;, parse = TRUE) + theme_classic() + theme(text = element_text(size = 10)) How well does this model fit, and how well does it represent the data? Let’s predict beetle presence for woody density of 0.1 g/cm3 and then 0.4 g/cm3. unname(beetle.fit$coefficients[1] + beetle.fit$coefficients[2] * 0.1) ## [1] 1.622327 unname(beetle.fit$coefficients[1] + beetle.fit$coefficients[2] * 0.4) ## [1] -0.03375074 Last, we’ll look at the residuals for our model fit. beetle.resid &lt;- data.frame(PredictedValues = predict(beetle.fit), Residuals = residuals(beetle.fit)) ggplot(data = beetle.resid, aes(x = PredictedValues, y = Residuals)) + geom_point(col = &quot;gray55&quot;) + labs(x = &quot;Predicted values from linear regression&quot;, y = &quot;Residuals&quot;, parse = TRUE) + theme_classic() + theme(text = element_text(size = text.size)) 18.4 Generalized linear models The regression models that we have been introduced to thus far all assume two things: A linear relationship between the independent variable and the dependent variables Normally distributed errors Linear regression can be thought of as \\[ Y_i \\sim \\mathrm{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2) \\] or, equivalently, \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\text{, where } \\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2) \\] Generalized linear models are extensions of what we have been talking about that free us from these two constraints. We can think of the above case as having two parts, the first is a model for the mean (or the expected value \\(E[Y_{i}]\\)). \\[ \\mathrm{E}[Y_i] = \\mu_i = \\beta_0 + \\beta_1 X_i \\] and the second being a model for the variance \\[ \\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2) \\] The Normal distribution is “special”. The mean (\\(\\mathrm{E}(X)\\)) and the variance (\\(\\mathrm{Var}(X)\\)) of the distribution are directly related to the two parameters of the distribution (\\(\\mu\\) and \\(\\sigma^2\\), respectively). This means the mean and variance are decoupled. For other distributions, the parameters of the distribution often do not map uniquely to the mean and variance. GLMs allow us to model the response as some distribution (no longer has to be a Normal), and to model the parameters of that distribution (or a function of those parameters) using a linear model. In other words, \\[ \\text{Response} \\sim \\text{Distribution}(\\text{parameters}) \\] \\[ g(\\text{parameters}) = \\beta_0 + \\beta_1 X_i \\] \\(g()\\) is a function of the parameters. For most distributions other than the Normal, the mean and variance are linked. \\[ \\epsilon_i \\sim \\text{Variance function}(\\text{usually determined by the distribution for the mean}) \\] The function that relates the parameters of the distribution to the parameters of the linear model is called a link function (here, \\(g()\\)). Not all functions are acceptable, there are a few frequently used functions that serve this role (depends on the distribution of our data). Traditional linear regression may fail to represent our data (predicting nonsensical data, etc.) in many cases in ecology and evolution. Think about the error structure you’d expect with each of these types of data: Count data Binary data Proportion data In this lecture we will go through logistic regression for binomial data and Poisson regression for Poisson (count) data. Keep in mind that other GLMs exist, specifically beta regression and gamma regression, among others. The order of complexity (and flexibility) of the models we will be discussing is: Standard linear regression \\(&lt;\\) Generalized linear models (GLMs) \\(&lt;\\) Generalized Additive Models (GAMs). 18.5 Logistic regression Logistic regression is used to model Bernoulli or binomial response variables. Bernoulli data includes: Survival for individual organisms Presence/absence for individual organisms Allele or phenotype of type 1 or type 2 for an individual The underlying equation is \\[ Y_i \\sim \\mathrm{Bernoulli}(p_i) \\] The response is either 0 or 1, with probability \\(p\\). For Bernoulli data (or binomial), the link function \\(g()\\) is \\[ \\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\log \\left( &#39;&#39;odds&#39;&#39; \\right) = \\mathrm{logit}(p_i) \\] This function is known as the logit function and also represents the log odds of a response. By using the logit function as the link function for a logistic regression, we are saying that the logit function maps the linear model (\\(\\beta_0 + \\beta_1 X_i\\)) to the parameter(s) of the Bernoulli (or binomial). Why use the logit function? We want to map values in the range \\([0, 1]\\) to \\([- \\infty, \\infty]\\). The variance function describes the distribution of the error in the GLM: \\[ \\epsilon_i \\sim \\text{Variance function} \\] Roughly, a variance function relates the mean of the distribution to the variance (see Aho 9.20.7.3 for more details). For example, the variance function of the normal distribution is 1, \\(\\mathrm{V}[\\theta] = 1\\) (describing constant variance, i.e., homoscedasticity). Note that \\(\\mathrm{V}[\\theta]\\) is not equivalent to \\(\\mathrm{Var}[\\theta]\\), though they are related (see Appendix A.6 in Aho for details). The variance of \\(Y_{i}\\) for the Bernoulli, where \\(n_i=1\\)is given by: \\[ \\mathrm{Var}(Y_i) = p_i (1 - p_i) \\] While for a normal linear regression we need errors to be the same (homoscedasctic), with a logistic regression, variation is largest around \\(p = 0.5\\) and shrinks to zero at the end points of \\(p=0\\) and \\(p=1\\). p &lt;- seq(0, 1, 0.1) plot(x = p, y = p * (1 - p), type = &quot;l&quot;) The complete model for a logistic regression with Bernouilli data \\(Y_i\\) and covariate \\(X_i\\) is: \\[ Y_i \\sim \\mathrm{Bernoulli}(p_i) \\text{, where } \\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 X_i \\] Keep in mind that for Bernoulli data, each data point is either a 0 or a 1 (a “success” or a “fail”). The probability \\(p_{i}\\) for each data point \\(Y_{i}\\) is a continuous variable from [0,1]. The value of the covariate \\(X_{i}\\) determines the probability \\(p_{i}\\), which in turn “weights the coin” and controls the probability that the outcome \\(Y_{i}\\) will be 0 or 1. The binomial is closely related to a Bernoulli. Whereas the Bernoulli represents a single “coin flip”, the bimomial represents a collection of coin flips. To model a binomial response variable: \\[ Y_i \\sim \\mathrm{Binomial}(n_i, p_i) \\text{, where } \\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 X_i \\] Now, the variance of \\(Y_i\\) is: \\[ \\mathrm{Var}(Y_i) = n_i p_i (1 - p_i) \\] Some examples of data that are best modelled as either Bernoulli or Binomial are: Evidence for joint damage in a rocket (this is a dataset we will tackle in the lab): Challenger_data &lt;- read.csv(&quot;_data/Challenger_data.csv&quot;) plot(Challenger_data$Temp,Challenger_data$O.ring.failure,pch=16,cex=2,col=rgb(0,0.55,0.4,0.6),xlab=&quot;Temperature&quot;,ylab=&quot;Presence of at least one failed O-ring&quot;) or the presence/absence of a species (in this case, a newt) plotted as a function of habitat suitability (data from this website): Newt_HSI &lt;- read.csv(&quot;_data/Newt HSI.csv&quot;) plot(Newt_HSI$HSI,Newt_HSI$presence,pch=16,cex=2,col=rgb(0,0.55,0.4,0.3),xlab=&quot;Habitat Suitability Index&quot;,ylab=&quot;Species presence (1)/Absence (0)&quot;) By modeling our data as it exists, we have solved four problems The errors now reflect the binomial process The variance reflects the binomial process The response \\(Y_i\\) is now bounded \\([0, 1]\\) We have maintained the information on sample size for each data point (only relevant when data are binomial) An example with the beetle dataset (To be worked out on your own) beetle.glm.fit &lt;- glm(formula = ANAT ~ Wood.density, data = beetle, family = &quot;binomial&quot;) beetle.glm.vals &lt;- predict(beetle.glm.fit, newdata = new.predictor, type = &quot;response&quot;) beetle.glm.predict &lt;- data.frame(WoodDensity = new.predictor[[1]], Presence = beetle.glm.vals) ggplot(data = beetle, aes(x = Wood.density, y = ANAT)) + geom_point(col = &quot;gray55&quot;) + geom_line(data = beetle.glm.predict, aes(x = WoodDensity, y = Presence)) + labs(x = expression(paste(&quot;Wood density (g cm&quot; ^ &quot;-3&quot;, &quot;)&quot;)), y = &quot;Presence of wood-boring beetle&quot;, parse = TRUE) + theme_classic() + theme(text = element_text(size = text.size)) Question: How do we interpret this GLM? Click for Answer With increasing wood density, the probability of finding wood-boring beetles decreases. And let’s see our predictions of the response again: predict(beetle.glm.fit, newdata = list(Wood.density = 0.1), type = &quot;response&quot;) ## 1 ## 0.9999694 predict(beetle.glm.fit, newdata = list(Wood.density = 0.4), type = &quot;response&quot;) ## 1 ## 0.004522846 The predictions are \\(\\hat{Y}_i\\), which is equal to the expected value of \\(Y_i\\) given \\(X_i\\) (\\(\\mathrm{E}(Y_i)\\)). With a normally distributed regression, \\(\\hat{Y}_i = \\mu_i = \\beta_0 + \\beta_1 X_i\\). With GLMs, we have to consider the link function. Now, \\(\\mathrm{E}(Y_i) = p_i = \\hat{Y}_i\\). Thus, \\(\\hat{Y}_i\\) are probabilities in logistic regression. This is why predict() doesn’t give us 0’s and 1’s. predict(beetle.glm.fit, newdata = list(Wood.density = 0.1), type = &quot;response&quot;) ## 1 ## 0.9999694 predict(beetle.glm.fit, newdata = list(Wood.density = 0.4), type = &quot;response&quot;) ## 1 ## 0.004522846 The argument type = \"response\" in predict() back-transforms the predicted response using the inverse logit function. In general, the outputs (\\(\\hat{Y}_i\\)) from logistic regression are in logit units because of the link function we applied, \\(\\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 X_i\\). This also means that the relationship between the link function and the predictor, expressed with the parameters \\(\\beta_0\\) and \\(\\beta_1\\) is linear on the scale of the logit probability. We can transform the logit output to units of odds (using the exponential function) or probability (using the inverse logit function). If the probability of an event is 0.3: The odds of the event occurring: \\(\\text{odds} = \\frac{0.3}{0.7} = 0.43\\) The log odds (logit) of the event occurring: \\(\\text{log odds} = \\ln \\frac{0.3}{0.7} = -0.85\\) The probability of the event (which we already knew) can be obtained using the logit using the inverse logit: \\(\\frac{\\exp(\\text{log odds})}{1 + \\exp(\\text{log odds})}\\) Adapted from this website. summary(beetle.glm.fit) ## ## Call: ## glm(formula = ANAT ~ Wood.density, family = &quot;binomial&quot;, data = beetle) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5141 -0.1748 0.1050 0.4717 2.0057 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.659 6.856 2.284 0.0224 * ## Wood.density -52.632 23.838 -2.208 0.0272 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 31.755 on 23 degrees of freedom ## Residual deviance: 14.338 on 22 degrees of freedom ## AIC: 18.338 ## ## Number of Fisher Scoring iterations: 6 The relationship between the link function and the predictor, \\(\\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 X_i\\), is linear on the scale of logit units (see Aho example 9.28). If the logistic model is fit without an intercept, the best fit line predicts \\(p = 0.5\\) when \\(X = 0\\). The intercept \\(\\hat{\\beta_0}\\) shifts the probability for \\(X = 0\\) up or down relative to 0.5. In logit units, the probability is shifted up (approximating 1) when \\(X = 0\\). beetle.glm.fit$coefficients[1] # logit units ## (Intercept) ## 15.6586 inv.logit &lt;- function(x) { exp(x) / (1 + exp(x)) } inv.logit(beetle.glm.fit$coefficients[1]) # units of probability (but no longer linear) ## (Intercept) ## 0.9999998 Since the parameters are linear on the scale of logit units, the logit (log odds) of beetle presence decreases by 52.6 given a 1 g/cm3 increase in wood density (decreases to almost 0 probability). beetle.glm.fit$coefficients[2] # logit units ## Wood.density ## -52.6317 Be aware that the significance of coefficients fit for GLMs is done using a Wald test, which relies on asymptotic (large-sample) estimates of standard errors of the coefficients. The Wald test uses a test statistic, \\(z\\), that is the parameter estimate divided by the estimated standard error of the parameter estimate, which is asymptotically standard normal under \\(H_0 = 0\\). 18.6 Fitting a GLM To fit GLMs, we need another method besides ordinary least squares (which assumes data are normally distributed). We need to fit the parameters using maximum likelihood. We’ll use binomial data as an example. The likelihood for binomial data, where we have \\(N\\) data points \\(Y = \\{ Y_1, Y_2, ...,Y_N \\}\\), each a draw from \\(\\mathrm{Binomial}(n_i, p_i)\\): \\[ \\mathcal{L}(Y_i | p_i) = \\prod^N_{i = 1} \\frac{n_i !}{Y_i! (n_i - Y_i)!} p_i^{Y_i} (1 - p_i)^{n_i - Y_i} \\] Note that I’ll use \\(\\mathcal{L}\\) to represent likelihood and \\(\\ell\\) to represent log-likelihood (as in Aho). We can express \\(p_i\\) as a function of the model parameters by rearranging the model equation: \\[ \\log \\left( \\frac{p_i}{1 - p_i} \\right) = \\beta_0 + \\beta_1 X_i \\] \\[ p_i = \\frac{\\exp(\\beta_0 + \\beta_1 X_i)}{1 + \\exp (\\beta_0 + \\beta_1 X_i)} \\] So, we can substitute this expression for \\(p_i\\) into the likelihood (after converting to negative log-likelihood), then we get the likelihood as a function of the model parameters \\(\\beta_0\\) and \\(\\beta_1\\), \\(\\text{NLL}(\\beta_0, \\beta_1 | X_i)\\). We can solve for the maximum likelihood estimate (MLE) for \\(\\hat{\\beta_0}\\): \\[ \\frac{\\partial \\text{NLL}}{\\partial \\beta_0} = 0 \\rightarrow \\hat{\\beta_{0}} \\] \\[ \\frac{\\partial \\text{NLL}}{\\partial \\beta_1} = 0 \\rightarrow \\hat{\\beta_{1}} \\] These equations cannot be solved algebraically. They must be solved numerically, for example, using iteratively weighted least squares with optimizing algorithms. When you think about the computing power needed to fit GLMs (which is no big deal today), it makes sense why it was traditional to transform your data to become normally distributed rather than model your data as it exists using GLM. How do we interpret the coefficients? If we fit a logistic model without an intercept, the best fit line must predict p=0.5 when X=0. So the intercept in this case shifts the probability for X=0 up or down relative to 0.5, and it does so by shifting the entire curve left and right along the x axis. This is illustrated in the figure that follows, where I have added “some amount \\(i\\) to the intercept for \\(i=1\\) to \\(i=12\\) and plotted the colors using the rainbow color set. plot(seq(-20,20),exp(-3+1*seq(-20,20))/(1+exp(-3+1*seq(-20,20))),col=&quot;black&quot;,typ=&quot;l&quot;,xlab=&quot;X value&quot;,ylab=&quot;Logistic probability&quot;) for (i in 1:12) { lines(seq(-20,20),exp(-3+i+1*seq(-20,20))/(1+exp(-3+i+1*seq(-20,20))),col=rainbow(15)[i]) } abline(v=0,col=&quot;black&quot;,lty=3) abline(h=exp(-3)/(1+exp(-3)),col=&quot;black&quot;,lty=3) abline(h=exp(-3+5)/(1+exp(-3+5)),col=rainbow(15)[5],lty=3) Note how the change of intercept from \\(\\beta_{0}=-3\\) to \\(\\beta_{0}=5\\) shifts the distribution to the left and as a result increases the value where the curve crosses the y-axis. (If the slope were negative, a shift to the left would have the opposite effect.) The “slope” (\\(\\widehat{\\beta_{1}}\\))̂ can be interpreted as follows: \\[ e^{\\widehat{\\beta_{1}}}=\\mbox{=&quot;change in the odds for a 1 unit change in X&quot;} \\] Another way to think about the slope term in logistic regression is that is controls how steeply the curve shifts between 0 and 1. Consider the following figure, where I reduce the slope from \\(\\beta_{1}=1\\) to \\(\\beta_{1}=0.077\\), Notice how the lines get flatter as the slope decreases (but without changing the intercept). plot(seq(-20,20),exp(-3+1*seq(-20,20))/(1+exp(-3+1*seq(-20,20))),col=&quot;black&quot;,typ=&quot;l&quot;,xlab=&quot;X value&quot;,ylab=&quot;Logistic probability&quot;) for (i in 1:12) { lines(seq(-20,20),exp(-3+(1-(i/13))*seq(-20,20))/(1+exp(-3+(1-(i/13))*seq(-20,20))),col=rainbow(15)[i]) } abline(v=0,col=&quot;black&quot;,lty=3) Important note Note that the sampling variability is fully prescribed by the model used to describe the response \\[ Y_i \\sim Binomial(n_i,p_i) \\] In other words, by using a logistic model, we have implicitly stated that the variability in the response that is due to sampling variation (‘residual error’) is entirely driven by the variation that would be expected by a Binomial distribution. We do not have a separate “error model”. We have one model that contains information both on the expected (mean) value of the response for each value of the covariate, and on the amount of random noise in that response that we would expect from one data point to the next (for the same value of the covariate). 18.7 Poisson regression Poisson regression is used to model response variables that are counts. Poisson data includes: Number of plants is a given area Number of offspring for an individual Number of bacterial colonies in a Petri dish The underlying model of the data is \\[ Y_i \\sim \\mathrm{Poisson}(\\lambda_i) \\] Question: What are the issues with modeling Poisson data as Normal? Click for Answer If you fit a normal linear regression to Poisson data, your regression might predict values that are negative. Also, the variance for Poisson is equal to the mean (the Normal has no such restriction). For Poisson data, the link function \\(g()\\) is \\[ g (\\lambda_i) = \\log (\\lambda_i) \\] This means that we map the linear model \\(\\beta_0 + \\beta_1 X_i\\) to the parameter of the Poisson using a log function. Why use a log function? We want to map values that are non-negative integers to \\([- \\infty, \\infty]\\). The variance of \\(Y_i\\) for a Poisson regression is: \\[ \\mathrm{Var}(Y_i) = \\lambda_i \\] The complete model for a Poisson regression with data \\(Y_i\\) and covariate \\(X_i\\) is: \\[ Y_i \\sim \\mathrm{Poisson}(\\lambda_i) \\text{, where } \\log (\\lambda_i) = \\beta_0 + \\beta_1 X_i \\] To fit this GLM using maximum likelihood, we could plug in \\(\\lambda_i = \\exp(\\beta_0 + \\beta_1 X_i)\\) to the likelihood function for a Poisson distributed variable. \\[ \\mathcal{L}(Y_i | \\lambda_i) = \\prod^N_{i = 1} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!} \\] As before, we solve for the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) by plugging into the likelihood \\(\\lambda_{i} = e^{\\beta_{0}+\\beta_{1}X_{i}}\\), taking the partial derivatives with respect to \\(\\beta_{0}\\) and \\(\\beta_{1}\\), respectively, setting those partial derivatives to zero, and solving for \\(\\hat{\\beta_{0}}\\) and \\(\\hat{\\beta_{1}}\\). By modeling our data as it exists, we have solved three problems: The errors now reflect the Poisson process The variance reflects the Poisson process The response is now restricted to non-negative integers However, there are things you need to understand about your data before fitting a Poisson regression: Data are often overdispersed, or the variance is larger than the mean. To resolve this, you can add an additional term to the model. These models are called quasi-poisson models. Data may have an unusually large number of zeros. To resolve this, you can fit a zero-inflated Poisson model, which models two components, the zero component (as a logistic regression), and the Poisson regression component. These classes of models (with multiple distributions) are more generally called mixture models. These types of Poisson regressions are common in ecology and evolution! 18.8 Deviance When we were fitting linear regression models, we assessed model fit using the coefficient of determination \\(R^{2}\\). To measure fit of GLMs we use the deviance. Conceptually, it is not all that different from other fit metrics, and is analogous to the residual sum of squares for a linear model (actually it is equal to the residual sum of squares, see table 9.3 in Aho). The deviance compares the NLL of the proposed model with the NLL of the “saturated” model. The saturated model is a model in which the predicted mean response matches each data point exactly, with one parameter for each observation. Note that the saturated model is a theoretical construct used to calculate deviance, not a model that you actually fit yourself. Deviance is calculated as: \\[ D = 2 (\\ell_{\\text{saturated model}} - \\ell_{\\text{proposed model}}) \\] If the proposed model fits the data nearly as well as the saturated model, then \\[ D | H_0 \\sim \\chi^2_{n - p} \\] The degrees of freedom for the chi-square distribution is calculated using \\(n\\), the number of parameters in the saturated model (usually the number of data points), and \\(p\\), the number of parameters in the proposed model. The saturated model has a likelihood greater than or equal to your proposed model, so the deviance is positive. We calculate the P-value as \\(P(D | H_0 \\ge D_{\\text{observed}})\\) (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the model fit is poor. In addition to assessing how well the model fits the data, we can use deviance to compare two models. We can use deviance to compare nested models. When one model is a reduced version of another model, the models are nested. When comparing nested models, we no longer need to use the idea of the saturated model. We just compare the difference in deviance between the two nested models. \\[ \\Delta D = 2 (\\ell_{\\text{larger model}} - \\ell_{\\text{smaller model}}) = -2 (\\ell_{\\text{smaller model}} - \\ell_{\\text{larger model}}) \\] Under the null hypothesis that the smaller model is the true model: \\[ \\Delta D | H_0 \\sim \\chi^2_{\\text{additional parameters in larger model}} \\] Models with more parameters have higher likelihood, because any additional parameters allow the model more “freedom” to fit the data. This means that the difference in log likelihoods (\\(\\ell_{\\text{larger model}} - \\ell_{\\text{smaller model}}\\)) will be positive and therefore the deviance difference will be negative. Adding additional parameters always improves model fit. BUT, we want to know whether the fit improves above and beyond what would be expected purely by adding additional parameters to the model? We calculate the P-value as \\(P(\\Delta D | H_0 \\ge \\Delta D_{\\text{observed}})\\) (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the larger model fits the data above and beyond what we would expect. We already used this concept of deviance to construct 95% confidence intervals on a maximum likelihood estimate! From Week 4’s Problem Set, “Using your function for the negative log-likelihood, calculate the MLE for \\(\\lambda\\) and the 95th percentile confidence interval. What would the 99th percentile confidence interval be? (4 pts) [Hint: The 95th percentile cut-off is \\(0.5 \\times \\chi^2_{df = 1}(0.95)\\) or 0.5 * qchisq(0.95, df = 1) = 1.92 in R; in other words, the NLL would have to be &gt;1.92 higher than the minimum to fall outside the 95th percentile confidence interval. Likewise, the 99th percentile cut-off is \\(0.5 \\times \\chi^2_{df = 1}(0.99) = 3.32\\).] So, you used the difference in NLL required to find the 95% CIs, but now you can see where that comes from. An alternative approach to assessing the “importance” of a variable is to look at the test statistic for the model parameters the same way that we did for ordinary linear regression. \\[ z=\\frac{\\mbox{parameter estimate}}{\\mbox{s.e. of parameter estimate}} \\] The test statistic z is approximately normal for large sample sizes. 18.9 Other methods – LOESS, splines, GAMs Traditional regression methods are incredibly useful for predicting or explaining empirical data. However, in some situations, these methods are not flexible enough. For these cases, there are non-parametric approaches to curve fitting. These methods may be useful for: Visualization of patterns Interpolating discrete data Identifying a threshold Prediction (e.g., niche modeling) Keep in mind that LOESS, splines, and GAMS do not produce models that are straightforward and mechanistic. It is difficult/impossible to explain what the coefficients they produce actually mean. LOESS (LOWESS), or locally weighted regression, is a non-parameteric method for fitting a curve through data. The dependent variable is fit in a moving fashion (like a moving window average) \\[ Y_i = g(X_i) + \\epsilon_i \\text{, where } \\epsilon_i \\sim \\mathrm{N} (0, \\sigma^2) \\] The only assumption about the function \\(g()\\) is that it is a smooth function. In LOESS, the regression curve is fit using a moving window and weighted linear regression where each point in the neighborhood is weighted according to its distance from \\(X\\). \\[ Y_i = \\beta_0 + w_1(X_i - X_0) + w_2 (X_i - X_0)^2 + ... + w_p (X_i - X_0)^p + \\epsilon_i \\] bomregions2012&lt;-read.csv(&quot;~/Dropbox/Biometry/Week 10 Multiple regression and GLMs/Week 10 Lecture/bomregions2012.csv&quot;) ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(x = &quot;Year&quot;, y = &quot;Northern rainfall&quot;, parse = TRUE) + theme(text = element_text(size = text.size)) + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; A smoothing spline is a model whose metric of fit is the sum of a measure of residuals, as well as a measure of roughness: \\[\\sum^n_{i = 1} (Y_i - f(X_i))^2 + h \\int^{x_{\\text{max}}}_{x_{\\text{min}}} f&#39;&#39;(X_i)^2 dx\\] The first term is the residual sum of squares, and the second term is a roughness penalty. \\(h\\) is a smoothing parameter. A larger \\(h\\) means a smoother line, because curves are more heavily penalized. A curve with \\(h = 0\\) will just interpolate the data and a curve with a very large \\(h\\) will fit a linear regression. The endpoints of the integral enclose the data. ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() + geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) + labs(x = &quot;Year&quot;, y = &quot;Northern rainfall&quot;, parse = TRUE) + theme(text = element_text(size = text.size)) + theme_classic() Generalized additive models (GAMs) allow us specify both smoothing and conventional parametric terms for models. GAMs are compatible with nonlinearity and nonnormal errors by using link functions (like with GLMs). With one covariate: \\[Y = \\beta_0 + g(X)\\] This function, \\(g()\\), could be smoothing splines, LOESS smoothers, kernel smoothers, etc. ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;)) + labs(x = &quot;Year&quot;, y = &quot;Northern rainfall&quot;, parse = TRUE) + theme(text = element_text(size = text.size)) + theme_classic() Personally, I really dislike GAMs and would discourage their use. GAMs rarely if ever provide information that you couldn’t see with the naked eye, and their use lends an air of statistical rigor to an analysis that is usually unjustified. I often see GAMs used as a crutch to avoid thinking seriously about the statistical model, and it tends to produce features that are artifacts of the data rather than meaningful information about the underlying process. Multiple regression is not fundamentally different from what we have discussed in the past three weeks. The major differences are that we need to be careful about what covariates we include, as well as our interpretations of linear model coefficients. Note that covariates, predictors, and explanatory variables all refer to the same thing, what I’ve referred to in linear regression as \\(X_i\\). Multiple regression is useful to: Build better predictive models Investigate the relative effects of each covariate standardized across the effects of the other covariates. With multiple regression, we use partial regression lines, where the slope of any single partial regression line is the effect of that covariate holding all the other covariates at their mean value. \\[ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_{p - 1} X_{(p - 1)i} + \\epsilon_i \\] Where \\(Y_i\\) is the response for the \\(i^{\\text{th}}\\) data point, \\(X_{(p - 1)i}\\) is the covariate \\(p - 1\\) for the \\(i^{\\text{th}}\\) data point. There are \\(p\\) parameters in the model (\\(p - 1\\) partial regression slopes and one intercept). Note that we could write this is matrix form like we did with the linear models in Week 8: \\[\\mathbf{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} \\mathbf{X} = \\begin{bmatrix} 1 &amp; X_{11} &amp; ... &amp; X_{(p - 1)1} \\\\ 1 &amp; X_{12} &amp; ... &amp; X_{(p - 1)2} \\\\ 1 &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{1n} &amp; ... &amp; X_{(p - 1)n} \\end{bmatrix} \\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{bmatrix} \\mathbf{e} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\\] The null hypothesis for each of the regression coefficients is: \\(\\beta_0 = 0\\) (the intercept is equal to zero), \\(\\beta_1 = 0\\), …, \\(\\beta_{p - 1} = 0\\) (the partial regression slope of \\(X_{p - 1}\\) on \\(Y\\) is zero). Multiple regression is fit the exact same way as simple linear regression (see Aho 9.5.1). The first three assumptions are the same as linear regression with one covariate. The final assumption is new and relevant only with multiple covariates. Linearity (A linear model appropriately describes the relationship between \\(X\\) and \\(Y\\)) Normality (For any given value of \\(X\\), the sampled \\(Y\\) values are independent with normally distributed errors) Homogeneity of variances (Variances are constant along the regression line) The covariates, \\(X_1, X_2, ..., X_{p-1}\\), are uncorrelated. Multicollinearity occurs when covariates are correlated with one another. Multicollinearity does NOT bias parameter estimates! But it still can be a big problem. It causes instability of estimated partial regression slopes, meaning that small changes in the data cause large changes in the parameter estimates. It inflates standard errors and confidence intervals of parameter estimates, increasing the chance of a Type II error. ANother way to say this is that multicollinearity reduces power, or increases the chance that you won’t find a significant result even if the null hypothesis is false. For simple linear regression with a single covariate, the slope parameter had the following sampling distribution: \\[ \\hat{\\beta_1} \\sim \\mathrm{N} {\\left( \\beta_1, \\frac{\\sigma_\\epsilon^2}{\\sum_{i = 1}^n (X_i - \\bar{X})^2} \\right)} \\] Review Question: What is \\(\\sigma_\\epsilon^2\\)? Click for Answer The population error variance (the expected value of squared errors). In practice, we estimate this as \\(s^2_\\epsilon\\) from the data. It’s also called the mean squared error. When we have multiple covariates, the sampling distribution for the partial slope parameter (the slope for the \\(X_1\\) covariate, for example) is: \\[ \\hat{\\beta_1} \\sim \\mathrm{N} {\\left ( \\beta_1, \\frac{\\sigma_\\epsilon^2}{\\sum_{i = 1}^n (X_{1i} - \\bar{X_1})^2} \\frac{1}{1 - R^2_1} \\right)} \\] where \\(R^2_1\\) is the multiple coefficient of determination, or the fraction of variance in the covariate 1 explained by all other covariates, \\(X_1 \\sim X_2 + X_3 + ... + X_{p - 1}\\). The bigger that \\(R^2_1\\) is, the larger the standard error of \\(\\hat{\\beta_1}\\): Var.inflation &lt;- function(R2.1) { 1 / (1 - R2.1) } Var.inflation(0.1) ## [1] 1.111111 Var.inflation(0.6) ## [1] 2.5 Var.inflation(0.9) ## [1] 10 How do we diagnose multicollinearity? Look at the pairwise correlations between all of he predictor variables either by a correlation matrix or by a scatterplot matrix (try the package corrplot) Calculate the Variance Inflation Factor. A large VIF may be &gt; 5 or &gt; 10 (depends who you ask). Two covariates have to be very correlated for this to be an issue. \\[ \\text{VIF} = \\frac{1}{1 - R^2_{p - 1}} \\] 3. Do a principal components analysis (PCA) to see which covariates are highly correlated (we will get into this Week 14). How do we deal with multicollinearity? If the goal of your model is prediction, multicollinearity isn’t necessarily a problem. It may be best to leave all predictors in the model, even if they are collinear. Remove the highly correlated predictors, starting with the least biologically interesting ones Do a PCA, because the principal components are guaranteed to be orthogonal (completely uncorrelated). However, this does make hypothesis testing difficult, because the parameters will be combinations of the original variables. How can we test hypotheses about model parameters in multiple regression? Compare individual parameters using a t-test \\[ \\frac{\\hat{\\beta_1} - \\beta_{1 | H_0}}{\\text{SE}_{\\hat{\\beta_1}}} \\sim t_{df} \\] How many degrees of freedom do we have for each model parameter? Let’s jump back to the distribution for the partial slope of covariate \\(X_1\\) out of \\(p - 1\\) total covariates. In our estimate of the unknown error variance \\(\\sigma^2_\\epsilon\\) (back from Week 9 lecture): \\[ s_\\epsilon^2 = \\frac{1}{n - p} \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 \\] Calculating \\(\\hat{Y_i}\\) requires \\(\\bar{Y}\\) as well as the mean of each covariate (\\(\\bar{X_1}, \\bar{X_2}, ..., \\bar{X}_{p - 1}\\)), so we lose \\(p\\) degrees of freedom. Compare the full model to the reduced model in which that particular term has been removed: \\[ \\text{Full model: } Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\beta_4 X_{4i} + \\epsilon_i \\] \\[ \\text{Reduced model: } Y_i = \\beta_0 + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\beta_4 X_{4i} + \\epsilon_i \\] Question: How can we compare these two models? Click for Answer Since the models are nested, we can use the deviance difference. "],["week-10-lab.html", "19 Week 10 Lab 19.1 Discussion of Challenger analysis 19.2 Weighted linear regression 19.3 Logistic regression practice 19.4 Poisson regression practice 19.5 Getting a feel for Deviance 19.6 Generalized Additive Models", " 19 Week 10 Lab There are 7 parts to this week’s lab: Discussion of the Challenger analysis Linear regression with ‘lm’ Weighted linear regression using ‘lm’ Logistic regression Poisson regression Understanding deviance Generalized Additive Models Practice with Multiple regression will be held off until our discussion of model criticism in Week 13. 19.1 Discussion of Challenger analysis One of the classic datasets for logistic regression is the Challenger dataset, so even though its not related to ecology, I think its worth reading. You have already read the article about the Challenger analysis so you know the background behind the o-ring data. Because the Challenger paper is so well written, we will go over the analysis in detail together. 1- The night before the launch, there was a meeting to discuss the influence of temperature on o-ring failure. The focus was on Figure 1a showing the number of o-ring failures CONDITIONAL ON there being at least one o-ring failure. 2- There are 6 o-rings on each shuttle, what is the appropriate model for o-ring failure? \\[ X\\sim Binom(n=6,p(t,s)) \\] where p(s,t) is the probability of o-ring failure as a function of temperature t and pressure s. 3- Therefore, the appropriate GLM for o-ring failure is \\[ log\\left(\\frac{p(t,s)}{1-p(t,s)}\\right)=\\alpha+\\beta t + \\gamma s \\] 4- They then fit the model using maximum likelihood (Quick review: What exactly does that mean?) 5- They calculate the “goodness of fit” statistic \\(G^{2}\\). This is just another name for the Deviance. \\[ G^{2} = 2*log\\left(\\frac{\\mbox{Likelihood saturated model}}{\\mbox{Likelihood model being considered}}\\right) \\] \\[ \\mbox{Deviance} = -2*(\\mbox{LL(model being considered)}-\\mbox{LL(saturated model)}) \\] \\[ \\mbox{Deviance} = 2*(\\mbox{LL(saturated model)}-\\mbox{LL(model being considered)}) \\] \\[ \\mbox{Deviance} = 2*log\\left(\\frac{\\mbox{Likelihood saturated model}}{\\mbox{Likelihood model being considered}}\\right) = G^{2} \\] 6- Recognizing that devaince is only really meaningful relative to another model, they fit the temperature-only model \\[ log\\left(\\frac{p(t)}{1-p(t)}\\right) = \\alpha + \\beta t \\] The difference in deviances is given by \\[ \\mbox{Deviance difference} \\sim \\chi^{2}_{\\mbox{additional parameters}} \\] How many additional parameters in this case? Just one, so \\[ \\mbox{Deviance difference} \\sim \\chi^{2}_{1} \\] The difference in deviance is not significant, i.e. this model fits about as well as the more complex model, so we can say that pressure has little effect on the probability of o-ring failure and we drop it from the model. 7- They construct 90th percentile confidence intervals for the expected number of incidents. Checkpoint #1: What exactly are they doing here? Do you understand what they have done and why? They are sampling with replacement from the original data and are refitting the model each time. 8- Next they plot the contours of the log-likelihood function and note that the contours are elliptical and therefore the data were not leading to ill-conditioned computation. What do they mean by that? 9- They collapse the binomial data to make a Bernoulli dataset in which “0” means that no o-rings failed, and “1” means that at least one o-ring failed. Checkpoint #2: Why did they do this? Because the o-rings may not be independent 10- They refit the data using the Bernoulli model and find that the fits are quite close. 11- They want to construct confidence intervals for the model parameters. They say “instead of using the aymptotic theory to construct confidence intervals, we use the parametric bootstrap procedure”. Checkpoint #3: Why might they have used a bootstrap approach here? Small size size probably… How do they do a parametric bootstrap? They take the best-fit model, sample with replacement from the logistic model (presumably drawing (x,predicted y) pairs at random with replacement), and refit the bootstrapped data to get new model parameter estimates. 12- Then they look at the sensitivity of the model to each of the data points, by pulling out each data point in turn and refitting the model. Checkpoint #4: What is this called? Jackknife! 13- They next consider a non-linear model of the form \\[ log\\left(\\frac{p(t,s)}{1-p(t,s)}\\right)=\\alpha + \\beta(t-t_{0})+\\gamma(t-t_{0})^{2} \\] 14- They again consider the change in deviance in going from the simpler linear model to the more complex quadratic model, and they find the quadratic term is not significant. 15- They then consider a model in which they use a non-parametric smoothing fit (using a moving window appraoch) in Figure 8. 16- They identify possible outliers in the data (more on model criticism in three weeks). You should have downloaded the data linking O-ring failure to launch temperature. Response: Presence/Absence of erosion or blow-by on at least one o-ring field joint (Y=1 if occured, 0 if not) Predictor Variable: Temperature at lift-off (degrees Fahrenheit) Sample size: 23 shuttle lift-offs prior to Challenger challenger&lt;-read.csv(&quot;_data/Challenger_data.csv&quot;,head=T) challenger ## Flight.number Temp O.ring.failure ## 1 1 66 0 ## 2 2 70 1 ## 3 3 69 0 ## 4 4 68 0 ## 5 5 67 0 ## 6 6 72 0 ## 7 7 73 0 ## 8 8 70 0 ## 9 9 57 1 ## 10 10 63 1 ## 11 11 70 1 ## 12 12 78 0 ## 13 13 67 0 ## 14 14 53 1 ## 15 15 67 0 ## 16 16 75 0 ## 17 17 70 0 ## 18 18 81 0 ## 19 19 76 0 ## 20 20 79 0 ## 21 21 75 1 ## 22 22 76 0 ## 23 23 58 1 attach(challenger) plot(Temp,O.ring.failure,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,main=&quot;O-ring damage vs. Temperature&quot;) challenger.fit1&lt;-lm(O.ring.failure~Temp) summary(challenger.fit1) ## ## Call: ## lm(formula = O.ring.failure ~ Temp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43762 -0.30679 -0.06381 0.17452 0.89881 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.90476 0.84208 3.450 0.00240 ** ## Temp -0.03738 0.01205 -3.103 0.00538 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3987 on 21 degrees of freedom ## Multiple R-squared: 0.3144, Adjusted R-squared: 0.2818 ## F-statistic: 9.63 on 1 and 21 DF, p-value: 0.005383 lines(Temp,fitted(challenger.fit1),col=&quot;red&quot;) What’s wrong with this model? (1) predictions will quickly escape the bounds (0,1) (2) residuals are clearly not normal resid1&lt;-residuals(challenger.fit1) plot(Temp,resid1,xlab=&quot;Temperature&quot;,ylab=&quot;Residuals for lm model&quot;) 19.2 Weighted linear regression One possible solution to these problems is to use weighted linear regression (to account for high variance at intermediate temperatures) and to truncate the predicted values to the range (0,1). Remember that weighted linear regression is just like ordinary linear regression except that we minimize the weighted squared residuals \\[ \\mbox{weighted SS} = \\sum^{n}_{i=1} w_{i} (Y_{i}-\\hat{Y}_{i})^{2} \\] The idea behind weighted linear regression is to give lower weights to observations with high variances so they have less influence over the final model fits. While any weights could be chosen, the inverse variances are most commonly used, \\[ w_{i} = \\frac{1}{Variance} \\] which, for Binomially distributed data is given by \\[ w_{i} = \\frac{1}{\\pi_{i}(1-\\pi_{i})} \\] Since the actual probabilities \\(\\pi_{i}\\) are unknown, we use estimated weights instead constructed from the empirical proportions as follows: \\[ \\hat{w}_{i} = \\frac{1}{\\hat{Y}_{i}(1-\\hat{Y}_{i})} \\] The procedure then goes as follows: Fit ordinary least squares Obtain estimates \\(\\hat{Y}_{i}\\) If an estimate is less than 0 or greater than 1, set it to 0.001 (or something small) and 0.999 (or something close to but less than one), respectively Compute the weights \\(W_{i}\\) Fit weighted least squares We have the estimates already, they are fitted(challenger.fit1) ## 1 2 3 4 5 6 ## 0.43761905 0.28809524 0.32547619 0.36285714 0.40023810 0.21333333 ## 7 8 9 10 11 12 ## 0.17595238 0.28809524 0.77404762 0.54976190 0.28809524 -0.01095238 ## 13 14 15 16 17 18 ## 0.40023810 0.92357143 0.40023810 0.10119048 0.28809524 -0.12309524 ## 19 20 21 22 23 ## 0.06380952 -0.04833333 0.10119048 0.06380952 0.73666667 We have to truncate these to the correct range but notice that we cannot set any values exactly to 0 or 1 because this would make the estimated variance blow up, so we set \\(\\hat{Y}_{i} &lt; 0\\) to 0.001 and \\(\\hat{Y}_{i} &gt; 0\\) to 0.999. We can do this in one step by using the ‘pmin’ and ‘pmax’ functions: Sidenote: Using the ‘pmin’ and ‘pmax’ functions pmin(c(1,2,3),c(0,3,5)) ## [1] 0 2 3 pmin(1,c(0,3,5)) ## [1] 0 1 1 (The same logic works for ‘pmax’.) Back to the problem at hand: new.predictions&lt;-pmin(0.999,pmax(0.001,fitted(challenger.fit1))) vars&lt;-new.predictions*(1-new.predictions) challenger.fit2&lt;-lm(O.ring.failure~Temp,weights=(1/vars)) summary(challenger.fit2) ## ## Call: ## lm(formula = O.ring.failure ~ Temp, weights = (1/vars)) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -1.3136 -0.6779 -0.4313 0.8330 2.8846 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.343825 0.532408 4.402 0.000248 *** ## Temp -0.029517 0.006746 -4.376 0.000265 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.105 on 21 degrees of freedom ## Multiple R-squared: 0.4769, Adjusted R-squared: 0.452 ## F-statistic: 19.15 on 1 and 21 DF, p-value: 0.0002647 Now we can plot the results: plot(Temp,O.ring.failure,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,main=&quot;O-ring Damage vs. Temperature&quot;) lines(Temp,fitted(challenger.fit1),col=&quot;red&quot;) lines(Temp,fitted(challenger.fit2),col=&quot;blue&quot;) The \\(R^{2}\\) of the weighted model is higher, but the fit still has the same problems as the original fit. To properly solve this problem, we need to do logistic regression, which accurately captures the non-linear form of the relationship and the nature of the residuals. 19.3 Logistic regression practice challenger.fit3&lt;-glm(O.ring.failure~Temp, family=&quot;binomial&quot;) plot(Temp,O.ring.failure,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,main=&quot;O-ring Damage vs. Temperature&quot;) # Above line only needed because RMarkdown doesn&#39;t keep previous plot lines(Temp,fitted(challenger.fit1),col=&quot;red&quot;) # Above line only needed because RMarkdown doesn&#39;t keep previous plot lines(Temp,fitted(challenger.fit2),col=&quot;blue&quot;) # Above line only needed because RMarkdown doesn&#39;t keep previous plot lines(sort(Temp), fitted(challenger.fit3)[order(Temp)],col=&quot;green&quot;,lwd=2) summary(challenger.fit3) ## ## Call: ## glm(formula = O.ring.failure ~ Temp, family = &quot;binomial&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0611 -0.7613 -0.3783 0.4524 2.2175 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.0429 7.3786 2.039 0.0415 * ## Temp -0.2322 0.1082 -2.145 0.0320 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 28.267 on 22 degrees of freedom ## Residual deviance: 20.315 on 21 degrees of freedom ## AIC: 24.315 ## ## Number of Fisher Scoring iterations: 5 newdata&lt;-data.frame(Temp=seq(30,85)) confidence.bands&lt;-predict.glm(challenger.fit3,newdata,se.fit=TRUE) Important note: If we were modelling data on the number of O-rings failed as a Binomial distribution rather than the existence of failed O-rings as a Bernoulli, we would have to pass the data to glm() in a slightly different way. In particular, we would have to create a two-column object, where the first column is the number of O-rings failed, and the second column is the number of O-rings that did not fail. The glm() call would look something like: challenger.fit3&lt;-glm(cbind(O.ring.failure, 6-O.ring.failure)~Temp, family=&quot;binomial&quot;) OK, back to the task at hand…The default is for predict.glm to give you the fit and s.e. on the scale of the predictor, so you need to use the inverse logit function to extract the fit and s.e. on the scale of the probabilities. In other words, the model is given by: \\[ Y_{i} \\sim \\text{Binom}(n_{i},p_{i}) \\\\ logit(p_{i}) = \\beta_{0} + \\beta_{1}X_{i} \\] and R’s function predict.glm() provides the confidence intervals on the right hand side of this latter equation, that is \\[ \\text{confidence interval} = [\\text{LL of } (\\beta_{0} + \\beta_{1}X_{i}), \\text{UL of } (\\beta_{0} + \\beta_{1}X_{i})] \\] Therefore, in order to construct confidence intervals on the scale of the probability \\(p\\), you need to back-transform, so the LL for \\(p\\) is given by \\(logit^{-1}(\\text{LL of } (\\beta_{0} + \\beta_{1}X_{i}))\\) and the UL is given by \\(logit^{-1}(\\text{UL of } (\\beta_{0} + \\beta_{1}X_{i}))\\). Operationally, this looks as follows in R: library(boot) plot(Temp,O.ring.failure,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,main=&quot;O-ring Damage vs. Temperature&quot;) # Above line only needed because RMarkdown doesn&#39;t keep previous plot lines(newdata[,1],inv.logit(confidence.bands$fit),col=&quot;purple&quot;,lwd=2) lines(newdata[,1],inv.logit(confidence.bands$fit+1.96*confidence.bands$se.fit),col=&quot;purple&quot;,lwd=2,lty=2) lines(newdata[,1],inv.logit(confidence.bands$fit-1.96*confidence.bands$se.fit),col=&quot;purple&quot;,lwd=2,lty=2) If you look at the help file for predict.glm, you will see that the only option is a confidence interval on the model (a ‘confidence’ interval). There is no way to generate a prediction interval for GLMs generally speaking. Checkpoint #5: How would you construct a prediction interval for a binary value?You cannot, it doesn’t make sense, the value is either 0 or 1. On the day of the Challenger launch, the temperature was 31 degrees - what was the probability of o-ring failure? [Work this out at home!] Now we will calculate the deviances reported by summary(), and discuss the AIC (to be discussed more formally next week). -2*logLik(challenger.fit3) ## &#39;log Lik.&#39; 20.31519 (df=2) challenger.fit4&lt;-glm(O.ring.failure~1,family=binomial) -2*logLik(challenger.fit4) ## &#39;log Lik.&#39; 28.26715 (df=1) The null deviance is that with only an intercept. The statistical significance of the model can be assessed by comparing the deviance with the parameters vs. the model with only an intercept. 19.4 Poisson regression practice Since we have the data loaded already, we will use the challenger o-ring data to illustrate how a Poisson model is fit, even though a Poisson model would be inappropriate for the o-ring data. Checkpoint #6: Why is a Poisson model inappropriate? Note that now the link function is the \\(log()\\) so the inverse link function needed to get a CI of the Poisson intensity \\(\\lambda\\) is now the exponential \\(exp()\\). In other words, to construct confidence intervals on the scale of the intensity \\(\\lambda\\), you need to back-transform, so the LL for \\(p\\) is given by \\(exp^{\\text{LL of } (\\beta_{0} + \\beta_{1}X_{i})}\\) and the UL is given by \\(exp^{\\text{UL of } (\\beta_{0} + \\beta_{1}X_{i})}\\). challenger.fit4&lt;-glm(O.ring.failure~Temp,family=&quot;poisson&quot;) summary(challenger.fit4) ## ## Call: ## glm(formula = O.ring.failure ~ Temp, family = &quot;poisson&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.81159 -0.67620 -0.48134 -0.04632 1.53598 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.78518 3.13068 1.848 0.0646 . ## Temp -0.10448 0.04878 -2.142 0.0322 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 16.654 on 22 degrees of freedom ## Residual deviance: 12.206 on 21 degrees of freedom ## AIC: 30.206 ## ## Number of Fisher Scoring iterations: 6 confidence.bands&lt;-predict.glm(challenger.fit4,newdata,se.fit=TRUE) plot(Temp,O.ring.failure,xlab=&quot;Temperature&quot;,ylab=&quot;Damage&quot;,main=&quot;O-ring Damage vs. Temperature&quot;) # Above line only needed because RMarkdown doesn&#39;t keep previous plot lines(newdata[,1],exp(confidence.bands$fit),col=&quot;orange&quot;,lwd=2) lines(newdata[,1],exp(confidence.bands$fit+1.96*confidence.bands$se.fit),col=&quot;orange&quot;,lwd=2,lty=3) lines(newdata[,1],exp(confidence.bands$fit-1.96*confidence.bands$se.fit),col=&quot;orange&quot;,lwd=2,lty=3) We can see that the Poisson model has no inflection point and therefore its predictions would not be bounded by (0,1). 19.5 Getting a feel for Deviance We can use the challenger dataset to get a feel for deviance, and the idea that while a larger model will always fit the data better (i.e. have a lower deviance), we need to make sure that the improvement in deviance is more than what would be expected by random chance. This is best illsutrated by an example. We’re going to look at the null expectation for the deviance difference of two models that differ by a single randomly generated covariate. In other words, we will add a covariate which is just noise and show that the deviance decrease we get adding a random covariate goes as a chi-squared distribution with 1 d.o.f. # Fit a logistic regression with Temp as the only covariate challenger.smaller.model &lt;- glm(O.ring.failure ~ Temp, data=challenger, family=&quot;binomial&quot;) # Generate a random covariate with same mean and sd as Temp randvar &lt;- rnorm(n=length(challenger$Temp), mean=mean(challenger$Temp), sd=sd(challenger$Temp)) # Add the random covariate to a data frame for model-fitting newdata &lt;- cbind(challenger, randvar) # Fit the logistic regression with Temp and the random covariate challenger.larger.model &lt;- glm(O.ring.failure ~ Temp + randvar, data=newdata, family=&quot;binomial&quot;) # Calculate the deviance difference of the two models dev_diff &lt;- deviance(challenger.smaller.model) - deviance(challenger.larger.model) dev_diff ## [1] 0.07540336 Notice that even though the covariate that we added is just noise, it still decreases the deviance. Now we need to repeat those steps a number of times to generate a distribution of expected deviance differences dev_diff &lt;- c() for (i in 1:1000){ # Generate a random covariate with same mean and sd as Temp randvar &lt;- rnorm(n=length(challenger$Temp), mean=mean(challenger$Temp), sd=sd(challenger$Temp)) # Add the random covariate to a data frame for model-fitting newdata &lt;- cbind(challenger, randvar) # Fit the model challenger.fit.larger.model &lt;- glm(O.ring.failure ~ Temp + randvar, data=newdata, family=&quot;binomial&quot;) # Calculate the deviance difference dev_diff_rand &lt;- deviance(challenger.smaller.model) - deviance(challenger.fit.larger.model) dev_diff &lt;- c(dev_diff, dev_diff_rand) } ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # plot the distribution and add a line for a chi-square with df=1 hist(dev_diff, xlab=&quot;Deviance Difference&quot;, main=&quot;Expected distribution&quot;, freq=FALSE,breaks=30) lines(seq(0,20,0.1), dchisq(seq(0,20,0.1),df=1), col=&quot;red&quot;,lwd=2) Sure enough, as expected, the difference in deviance we get by adding a covariate that has no association with the response is a \\(\\chi^{2}_{1}\\) distributed variable. Therefore, to justify adding a covariate to a model, we want to see that the decrease in deviance is much larger than this. Specifically, we want to see that the decrease in deviance is so unlikely to have arisen from a \\(\\chi^{2}_{1}\\) distribution that we reject the null hypothesis that the two models are equivalent. (In other words, by rejecting the null hypothesis, we say that the larger model is, in fact, the better model and the additional covariate is worth keeping.) Checkpoint #7: Does this make sense? 19.6 Generalized Additive Models There are two libraries that can fit GAMs: ‘gam’ and ‘mcgv’. We will use the ‘gam’ package for now, but keep in mind that the ‘mgcv’ package is more flexible and more powerful (so worth considering if you need to do GAMs for your own research). Stop: Install the GAM package and then load it into the workspace using library(&#39;gam&#39;) Inside the ‘gam’ package is a dataset that we will use on ozone in New York as a function of solar radiation, temperature, and wind. First we histogram the ozone data hist(airquality$Ozone) Clearly the ozone data are not normal. It turns out the log transformation gets the ozone data to something more ‘normal-like’. (How would we compare different transformations of the data? Try and do a ks.test comparing various transformed datasets against the normal.) First we’ll just fit a linear model for comparison. air.lm&lt;-lm(log(Ozone)~Solar.R+Wind+Temp,data=airquality) summary(air.lm) ## ## Call: ## lm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.06193 -0.29970 -0.00231 0.30756 1.23578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2621323 0.5535669 -0.474 0.636798 ## Solar.R 0.0025152 0.0005567 4.518 1.62e-05 *** ## Wind -0.0615625 0.0157130 -3.918 0.000158 *** ## Temp 0.0491711 0.0060875 8.077 1.07e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5086 on 107 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6644, Adjusted R-squared: 0.655 ## F-statistic: 70.62 on 3 and 107 DF, p-value: &lt; 2.2e-16 Now we’ll try fitting this data with a GAM. Note the syntax: s() fits a smoothing spline, lo() would fit a LOESS curve. air.gam&lt;-gam(log(Ozone)~s(Solar.R)+s(Wind)+s(Temp),data=airquality) summary(air.gam) ## ## Call: gam(formula = log(Ozone) ~ s(Solar.R) + s(Wind) + s(Temp), data = airquality) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.84583 -0.24538 -0.04403 0.31419 0.99890 ## ## (Dispersion Parameter for gaussian family taken to be 0.2235) ## ## Null Deviance: 82.47 on 110 degrees of freedom ## Residual Deviance: 21.9077 on 98.0001 degrees of freedom ## AIC: 162.8854 ## 42 observations deleted due to missingness ## ## Number of Local Scoring Iterations: NA ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## s(Solar.R) 1 16.041 16.0408 71.756 2.484e-13 *** ## s(Wind) 1 17.208 17.2083 76.978 5.521e-14 *** ## s(Temp) 1 12.723 12.7227 56.913 2.351e-11 *** ## Residuals 98 21.908 0.2235 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(Solar.R) 3 2.8465 0.04151 * ## s(Wind) 3 3.4736 0.01897 * ## s(Temp) 3 2.9358 0.03713 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mfrow=c(3,1)) plot(air.gam,se=T) The default is that the smoothing splines have df=4, but we can control the amount of smoothing by changing the number of d.o.f. air.gam&lt;-gam(log(Ozone)~s(Solar.R,df=20)+s(Wind)+s(Temp),data=airquality) plot(air.gam,se=T) That makes the curve for Solar.R much more curvy. "],["week-11-lecture.html", "20 Week 11 Lecture 20.1 Week 11 Readings 20.2 Week 11 outline 20.3 Comparing variance components 20.4 Two ways to estimate variance 20.5 Single-factor ANOVA 20.6 Fixed effects vs. random effects 20.7 Post-hoc tests", " 20 Week 11 Lecture 20.1 Week 11 Readings For this week, I suggest reading Aho Sections 10.1-10.7, as well as Logan Chapter 10. 20.2 Week 11 outline Basic idea behind ANOVA Single-factor ANOVA Fixed effects (Model I ANOVA) vs. Random effects (Model II ANOVA) Follow up analyses to ANOVA The model structure of ANOVA is identical to a linear regression of categorical covariate(s). Specifically, ANOVA involves statistics used to estimate group/treatment effects. \\[ Y_{ij} = \\mu + A_i + \\epsilon_{ij} \\text{, where } \\epsilon_{ij} \\sim \\mathrm{N}(0, \\sigma^2) \\] The value of each data point, \\(Y_{ij}\\) is modeled as a function of: 1) the overall mean \\(\\mu\\), 2) the modification of the mean due to membership in group \\(i\\), \\(A_i\\), and 3) the unexplained variability remaining after the group-level effect is \\(\\epsilon_{ij}\\). Variation among groups is represented by \\(A_i\\) and variation within groups is represented by \\(\\epsilon_{ij}\\). The statistical question is whether or not “group” has a statistically significant effect on the overall mean \\(\\mu\\). This is identical to the linear models we have already talked about, except that now we are focusing on models in which the predictor variables are categorical factors (or groups). What is the null hypothesis here? Under the null hypothesis, this linear model can be written \\[ Y_{ij}=\\mu+\\epsilon_{ij} \\] So the null hypothesis being tested is \\(H_{0}\\): The differences among groups is no larger than would be expected based on the differences within groups. Therefore, there are no significant differences among the groups. So, do the groups all come from the same population? Based on what we know already know, we might try using pairwise t-tests. But wait! If we use pairwise t-tests to determine which groups were different, we would have \\(a(a - 1) / 2\\) pairwise comparisons with \\(a\\) group means. Question: Why is this ill advised? Click for Answer Multiple comparisons will inflate the familywise error rate. ANOVA is an omnibus test, meaning that if we reject the null hypothesis, we only know that at least one group is different. We do not know which groups in particular are different. Omnibus tests are very useful to keep familywise error rates down while testing many groups. So, our null and alternative hypothesis are: \\(H_0\\): The groups have the same mean value \\(H_A\\): At least one of the groups comes from a population with a different mean Note that often the null and alternative hypotheses of ANOVA are stated in terms of groups coming from the same or different populations. However, I have carefully worded the null and alternative hypotheses so as to emphasize that ANOVA is a test of means. Because ANOVA assumes that the distributions within each group are Gaussian and have the same variance, if they have the same mean, they must come from the same population (remember: the Gaussian has only two parameters, mean and variance). However, if your data do not exactly meet the assumptions, ANOVA may not flag the groups as coming from different populations if the means are the same. In other words, groups that have the same mean but different variances (a violation of the ANOVA assumptions) may not yield a statistically significant result. An ANOVA with one categorical predictor with two levels is identical to a two sample t-test. Before we discuss the mathematics behind ANOVA, we will start with a conceptual example adapted from the McKillup textbook (the chapter is posted in “Additional Readings”). We are studying the effect of experimental drugs on the growth of brain tumors in humans. We have 12 experimental patients with brain tumors. We assign four to a control group (no drug), we treat four with Tumostat, and four with Inhibin 4. We are measuring (our response variable is) tumor diameter. We want to know whether there is a difference in the mean tumor diamerer among the drug treatments. Imagine our results look like this: How do we test our hypothesis? First let’s relate the figure back to the equation: \\(Y_{ij}\\) are each of the individual data points (e.g., \\(Y_{21}\\) is the first individual in treatment 2, Tumostat). The horizonal line is equal to \\(\\mu\\), and \\(A_i\\) is equal to the difference between \\(\\mu\\) and the treatment means (narrow horizontal lines). Notice that there is variation at two levels: 1) variation among individuals within a given treatment group, and 2) variation among the three treatment group means. We can partition the variance into these two components. 20.2.1 Variation within treatment group Variation among individuals within a treatment group is residual variation. This variation exists because of any number of factors we weren’t able to measure. 20.2.2 Variation among treatment group means Variation among treatment means is variation due to the effect of the treatment (if there is an effect) in addition to the individual variation. 20.2.3 Comparing variance components We can relate these two components of variation to one another, and this is our test statistic. Informally (we will get into the mathematical formulas soon) we can write this as: \\[ \\frac{\\text{Among group variance (group effect + error)}}{\\text{Within group variance (error)}} \\] Question: Approximately what is this ratio equal to under the null hypothesis for ANOVA? Click for Answer Under the null hypothesis, we assume there is no group effect. So, the ratio is about 1. Let’s imagine our data looked slightly different: How much variation is there among groups relative to within groups? Without knowing the details yet, do you think there is a significant difference among groups? 20.3 Comparing variance components Question: Given that we are using null hypothesis significance testing methods, what do you think the next step is after we estimate the test statistic? Click for Answer Compare the value of the test statistic to the distribution of the test statistic under the null hypothesis. Question: The distribution of the test statistic under the null hypothesis describes the ratios of variances. What distribution do you think it is? Click for Answer The F distribution (see Week 5 lecture) Another way we can phrase the statistical question is to ask whether the differences among group means are significantly different from the differences seen within groups. In ANOVA, we compare variances in order to compare means. 20.4 Two ways to estimate variance Variation within groups We can calculate the within-group variance as an average of the within group variances from each group. In other words: \\[ \\sigma^2 = \\frac{1}{a} (\\text{Within-grp var grp }1 +\\text{Within-grp var for grp }2+...+\\text{Within-grp var group }a) \\] This can be re-written as: \\[ \\sigma^2 = \\frac{1}{a} \\sum_{i = 1}^a \\text{Within-grp var group }i \\] There is actually a second way we can calculate the within-group variance: Variation among groups Under \\(H_0\\), the data in each group are an independent sample from the same underlying population. We can calculate a mean for each of the \\(a\\) groups. Here we are assuming that each group has the same number (\\(n\\)) of data points. (This is called a “balanced design”. More on this in a bit.) We can use the Central Limit Theorem to estimate the variance. We calculate the variation in means as: \\(\\sigma^2_{\\bar{Y}} = \\frac{\\sigma^2}{n}\\), where \\(\\sigma^2_{\\bar{Y}}\\) is our estimated variance of the group means and \\(\\frac{\\sigma^2}{n}\\) is our overall uncertainty divided by \\(n\\) (thus, our ability to estimate \\(\\mu\\) with \\(\\bar{Y}\\) improves with increasing sample size). We can rearrange this to get a second formula for the within-group variance: \\[ \\sigma^2 = n \\sigma^2_{\\bar{Y}} \\] Under \\(H_0\\), both ways of calculating variance (among groups using CLT or within groups as an average of group variance) will give equivalent estimates. \\[ \\text{Under H}_o \\rightarrow n \\sigma^2_{\\bar{Y}} = \\frac{1}{a} \\sum_{i = 1}^a \\text{Within-group variance in group }i \\] However, if there are true group differences, the variation among groups will be large compared to the variation within groups. Now we’ll use our example of the tumor diameter data with no true treatment differences and run simulations of the experiment with 100 samples in each treatment in order to estimate and compare variance within groups and variance among groups. set.seed(3467) replicates &lt;- 100 var.among &lt;- c() var.within &lt;- c() for (i in 1:1000) { y &lt;- rnorm(replicates * length(treatment.names)) x &lt;- rep(treatment.names, each = replicates) dat &lt;- data.frame(Treatment = x, TumorDiameter = y) dat$GroupMeans &lt;- rep(c(mean(dat$TumorDiameter[dat$Treatment == &quot;Control&quot;]), mean(dat$TumorDiameter[dat$Treatment == &quot;Tumostat&quot;]), mean(dat$TumorDiameter[dat$Treatment == &quot;Inhibin 4&quot;])), each = replicates) dat$Treatment &lt;- factor(dat$Treatment, levels = treatment.names) var.among[i] &lt;- replicates * var(unique(dat$GroupMeans)) v1 &lt;- var(dat$TumorDiameter[which(dat$Treatment == &quot;Control&quot;)]) v2 &lt;- var(dat$TumorDiameter[which(dat$Treatment == &quot;Tumostat&quot;)]) v3 &lt;- var(dat$TumorDiameter[which(dat$Treatment == &quot;Inhibin 4&quot;)]) var.within[i] &lt;- (1 / 3) * sum(c(v1, v2, v3)) } mean(var.among) ## [1] 0.991662 mean(var.within) ## [1] 0.9948506 20.5 Single-factor ANOVA First we will start will the simplest ANOVA type, where we have a single factor (discrete covariate). We will work through an example involving the heights of plants, \\(Y\\), under three different experimental treatments (\\(i\\)), low nitrogen (N), ambient N, and high N. We have four replicates in each treatment. We index the heights as \\(Y_{ij}\\), where \\(i\\) is treatment \\(i = (1, ..., a)\\) and \\(j\\) is the individual plant \\(j = (1, ..., n)\\). There will be a lot of variation in plant heights for a million reasons we can’t measure. But, we want to partition the variance into 1) the variance due to individual fluctuations (“error”), and 2) the variance due to the treatment (nitrogen level). This is a single-factor (one-way) ANOVA because we have one grouping category, nitrogen treatment, with three mutually exclusive levels (low N, ambient N, high N). Our data is as follows: Height Treatment 10 low N 12 low N 12 low N 13 low N 9 ambient N 11 ambient N 11 ambient N 12 ambient N 12 high N 13 high N 15 high N 16 high N We first need the group means: \\[ \\bar{Y}_{\\text{low N}} = \\frac{10 + 12 + 12 + 13}{4} = 11.75 \\] \\[ \\bar{Y}_{\\text{ambient N}} = \\frac{9 + 11 + 11 + 12}{4} = 10.75 \\] \\[ \\bar{Y}_{\\text{high N}} = \\frac{12 + 13 + 15 + 16}{4} = 14.00 \\] The overall mean is \\(\\bar{Y} = 12.17\\). Both this week and next week, we will be working through different ANOVA tables for each model we review. ANOVAs can be expressed in a table where each row is a category of variation that we are estimating. As we go from the leftmost column to the rightmost column, we estimate the partitioned variation and then conduct our hypothesis test. I suggest you try to fill out the ANOVA tables in your notes during lecture as we go through them, and practice filling out ANOVA tables on your own as well. Source of variation SS DOF MS F P Among groups Within groups Total Then, we will calculate the total amount of variation, or the squared difference between each data point (\\(Y_{ij}\\)) and the overall mean (\\(\\bar{Y}\\)): \\[ \\text{SS}_{\\text{total}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2 = 41.67 \\] You can think of a double sum like a nested for loop in R. Note that different sources express the same formula in different ways (I will try to be consistent in lecture, but let me know if you need a clarification on notation). Next, we will calculate the sum of squares among groups, or how much the mean of group \\(i\\) (\\(\\bar{Y}_i\\)) differs from the overall mean (\\(\\bar{Y}\\)): \\[ \\text{SS}_{\\text{among groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2 = 22.17 \\] We sum up \\((\\bar{Y}_{i} - \\bar{Y})^2\\) for every data point \\(j\\) in group \\(i\\), so in this case we sum up the same number four times for each cell. Question: Because there are the same number of replicates in each treatment for this case, can you think of another way to express \\(\\text{SS}_{\\text{among groups}}\\)? Click for Answer \\(\\text{SS}_{\\text{among groups}} = n \\sum^a_{i = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\). Note that this only works if there are the same number of observations in each group. Always explicitly using the double sum is applicable to more situations (but a lot of textbooks express the formula in this way). Last, we will calculate the sum of squares within groups, or how much each data point in group \\(i\\) (\\(Y_{ij}\\)) differs from the the mean of group \\(i\\) (\\(\\bar{Y}_i\\)): \\[ \\text{SS}_{\\text{within groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2 = 19.50 \\] Does this remind you of linear regression? \\[ \\text{SS}_{\\text{among groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2 ; \\quad \\text{SSR} = \\sum_{i = 1}^n{(\\hat{Y}_i - \\bar{Y})^2} \\\\ \\text{SS}_{\\text{within groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2 ; \\quad \\text{SSE} = \\sum_{i = 1}^n{(Y_i - \\hat{Y}_i)^2} \\] The estimated \\(\\hat{Y}_i\\) from the regression is comparable to the group means ## Loading required package: tcltk Now we have the first column to fill in our ANOVA table: Source of variation SS DOF MS F P Among groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) Within groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2\\) Total \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2\\) Question: How do we calculate degrees of freedom (in general)? Click for Answer Subtract the number of quantities estimated from the data from the total number of data points. Question: How many quantities did we estimate in the ANOVA calculation? First, how many quantities were estimated and how many data points are included in the formula for the total sums of squares? \\[ \\text{SS}_{\\text{total}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2 \\] Click for Answer We estimated the overall mean (“uses up” one degree of freedom). We have \\(a \\times n\\) data points. So, for \\(\\text{SS}_{\\text{total}}\\) we have \\((a \\times n) - 1\\) degrees of freedom. Question: Now, how many quantities were estimated and how many data points are included in the formula for the sums of squares among groups? \\[ \\text{SS}_{\\text{among groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2 \\] Click for Answer We estimated the overall mean (“uses up” one degree of freedom), and we have \\(a\\) data points (one for each group mean). For \\(\\text{SS}_{\\text{among groups}}\\) we have \\(a - 1\\) degrees of freedom. Question: Last, how many quantities were estimated and how many data points are included in the formula for the sums of squares within groups? \\[ \\text{SS}_{\\text{within groups}} = \\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2 \\] Click for Answer We estimated the group means (“uses up” \\(a\\) degrees of freedom). We have \\(a \\times n\\) data points. So, for \\(\\text{SS}_{\\text{within groups}}\\) we have \\((a \\times n) - a\\) degrees of freedom. Importantly, though, note that if the groups have different numbers of data points, you need to add up the numbers of degrees of freedom within each group. In other words, if you have three groups (a=3) that have different numbers in each group, the degrees of freedom “within groups” is \\((n_{1}-1)+(n_{2}-1)+(n_{3}-1)\\), which does not simplify to \\(a*n\\) since the n’s are all different. We will discuss such unbalanced designs in more depth next week. We have the second column to fill in our ANOVA table: Source of variation SS DOF MS F P Among groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) \\(a - 1 = 2\\) Within groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2\\) \\((a \\times n) - a = 9\\) Total \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2\\) \\((a \\times n) - 1 = 11\\) Mean squares are calculated by dividing the sums of squares by the degrees of freedom for each row of the table. Mean squares are estimates of variance for components of the model. Relating this idea back to our first example of tumor diameters: \\[ \\text{MS}_{\\text{among groups}} = \\text{Among group variance (group + error)} \\] \\[ \\text{MS}_{\\text{within groups}} = \\text{Within group variance (error)} \\] Our estimate of the variance among groups, \\[ \\sigma^2 = n \\sigma^2_{\\bar{Y}} = \\text{MS}_{\\text{among groups}} \\] Our estimate of the variance within groups, \\[ \\sigma^2 = \\frac{1}{a} \\sum_{i = 1}^a \\text{Variance in group }i = \\text{MS}_{\\text{within groups}} \\] Mean squares within groups is also called the residual mean squares or mean squares error. Remember that under \\(H_0\\), both ways of estimating variance (among groups using CLT or within groups as an average of group variance) will give equivalent estimates. However, if there are true group differences, the variation among groups will be large compared to the variation within groups. Out test statistic, \\(F\\), is the ratio of these estimates of variance components. \\[ \\frac{\\text{Among group variance (group + error)}}{\\text{Within group variance (error)}} = \\frac{\\text{MS}_{\\text{among groups}}}{\\text{MS}_{\\text{within groups}}} = F \\] And the third and fourth columns of our ANOVA table can be filled in: Source of variation SS DOF MS F P Among groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2 = 22.17\\) \\(a - 1 = 2\\) \\(\\frac{\\text{SS}_{\\text{among groups}}}{\\text{DOF}_{\\text{among groups}}} = 11.08\\) \\(\\frac{\\text{MS}_{\\text{among groups}}}{\\text{MS}_{\\text{within groups}}} = 5.11\\) Within groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2 = 19.50\\) \\((a \\times n) - a = 9\\) \\(\\frac{\\text{SS}_{\\text{within groups}}}{\\text{DOF}_{\\text{within groups}}} = 2.17\\) Total \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2 = 41.67\\) \\(a \\times n - 1 = 11\\) Because doing this in R (especially when we get to multi-way ANOVA next week) can be tricky, I show you here how this can be done in Excel so the underlying logic is clear Figure 20.1: Example of how Excel can be used to work out the sums-of-squares for one way ANOVA We now test whether the F ratio (our test statistic) is significant given its distribution under the null hypothesis. The distribution of test statistic \\(F\\) under the null hypothesis is \\(F | H_0 \\sim F_{\\text{DOF}_{\\text{among}}, \\text{DOF}_{\\text{within}}}\\). x.values &lt;- seq(0, 100, 0.001) y.values &lt;- df(x = x.values, df1 = 2, df2 = 9) dat &lt;- data.frame(FStatistic = x.values, Density = y.values) ggplot(dat, aes(x = FStatistic, y = Density)) + geom_line() + geom_vline(xintercept = 5.11, col = &quot;dodgerblue3&quot;, size = 1.5) + annotate(geom = &quot;text&quot;, col = &quot;dodgerblue3&quot;, label = &quot;F*&quot;, x = 5.6, y = 0.5, size = 8) + coord_cartesian(xlim = c(0, 8)) + labs(y = &quot;Probability&quot;, x = &quot;Quantiles of the F dist.&quot;) + theme_classic() + theme(text = element_text(size = text.size)) Note that the F distribution has a different shape when df1 has higher values. How would you calculate P here? sum(dat$Density[dat$FStatistic &gt;= 5.11] * 0.001) # Riemann sums integration ## [1] 0.03290738 pf(5.11, df1 = 2, df2 = 9, lower.tail = FALSE) ## [1] 0.03290039 Source of variation SS DOF MS F P Among groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2 = 22.17\\) \\(a - 1 = 2\\) \\(\\frac{\\text{SS}_{\\text{among groups}}}{\\text{DOF}_{\\text{among groups}}} = 11.08\\) \\(\\frac{\\text{MS}_{\\text{among groups}}}{\\text{MS}_{\\text{within groups}}} = 5.11\\) \\(0.033\\) Within groups \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y}_i)^2 = 19.50\\) \\((a \\times n) - a = 9\\) \\(\\frac{\\text{SS}_{\\text{within groups}}}{\\text{DOF}_{\\text{within groups}}} = 2.17\\) Total \\(\\sum^a_{i = 1} \\sum^n_{j = 1} (Y_{ij} - \\bar{Y})^2 = 41.67\\) \\((a \\times n) - 1 = 11\\) How do we report our findings in a manuscripts/thesis/report/etc.? We found a significant difference among the three nitrogen treatments given a single-factor ANOVA (\\(F_{2, 9} = 5.11\\), \\(P = 0.033\\)). Special note I strongly prefer to keep all sums explicit, e.g., \\[ \\sum_{i=1}^{a}\\sum_{j=1}^{n}(\\bar{Y_{i}}-\\bar{Y})^{2} \\] instead of \\[ n\\sum_{i=1}^{a}(\\bar{Y_{i}}-\\bar{Y})^{2} \\] because the former can be used even if the ANOVA design is not balanced. (A balanced design is one in which the sample size in each cell is the same (n).) Many other texts use the latter notation, so do not let this confuse you. This figure from Logan (2010) does a nice job of summarizing the steps of one-way ANOVA: Figure 20.2: Source: Logan (2010) Biostatistical Design and Analysis Using R ANOVA comes with a number of assumptions. We assume that the residuals are: Normally distributed. The error terms, \\(\\epsilon_{ij}\\), should be normaly distributed within each group. Note that ANOVA is robust to non-normality if you have a balanced design and similar variances. Independent within and among groups, i.e., each experimental unit is independent of each other experimentl unit. Variance is equal (or similar) across all treatments. Question: What are the residuals in an ANOVA model? Click for Answer The difference between the data points \\(Y_{ij}\\) and the group mean \\(\\mu_i\\), \\(\\epsilon_{ij} = Y_{ij} - \\mu_i \\sim \\mathrm{N} (0, \\sigma^2)\\). 20.6 Fixed effects vs. random effects We have so far assumed that factors are fixed effects. Fixed effects represent all the states in the system you would ever be interested in. Our null hypothesis for a fixed effects factor is \\(H_0: \\text{all treatment effects, } A_i = 0\\) (e.g., the three experimental, pre-planned nitrogen treatments as in our plant example). ANOVA models with only fixed effects are sometimes called Model I ANOVAs. Random effects represent a random sample of some larger population about which you want to make inference. For example, you are studying literacy in a human population, and in your sample you have English, French, Spanish, and Russian speakers. You are not interested in the specific effect of Russian vs. English, you just want to be able to include in your model variability that may come from the fact that different languages are represented. Random effects are also useful to extrapolate your findings to other languages that were not included in your sample. Our null hypothesis for a random effects factor is \\(H_0: \\mathrm{Var}(\\text{All treatment effects, } A_i) = 0\\). ANOVA models with only random effects are sometimes called Model II ANOVAs. Models with both fixed and random effects are called mixed models \\[ Y_{ij} = \\mu + A_i + \\epsilon_{ij} \\] Term Effects Model Description \\(H_0\\) \\(A_i\\) Fixed I effect of the \\(i\\)th group \\(A_i = 0\\) \\(A_i\\) Random II effect of the \\(i\\)th random group selected from larger population of groups \\(\\mathrm{Var}(A_i) = 0\\) The models and statistics (\\(\\text{SS, MS, F, } P\\)) for single-factor ANOVAs are the same with fixed effects or random effects. Some researchers, such as Gelman and Hill, argue that all effects are actually random. The distinction is between whether randomness is modeled or not. Randomness that is modeled is what we call a random effect and randomness that is unmodeled is a fixed effect. This distinction is the basis for hierarchical models (often Bayesian). For example, if we model \\(A_i \\sim \\text{Distribution(parameters)}\\), the ANOVA model is now hierarchical. PS: Here’s my first plug for Gelman and Hill’s excellent book: Data analysis using regression and multilevel/hierarchical models. If you plan to use regression or ANOVA in your research, particularly if you plan on fitting mixed/hierarchical models, I highly recommend getting a copy of this book. Figure 10.5 from Aho: A random effects model, where the random effects factor is soil fertilizer brand and the response variable is wheat yield \\(Y_{ij}\\). The top panel describes the underlying distribution of the random factor \\(\\mathrm{N} (0, \\sigma_A^2)\\). The bottom two panels describe the distribution of the data for two randomly selected fertilizer brands \\(Y_{1j} \\sim\\mathrm{N} (\\mu_1, \\sigma^2)\\) and \\(Y_{2j} \\sim\\mathrm{N} (\\mu_2, \\sigma^2)\\). Let’s practice a bit on reasoning whether a factor is best modelled as a fixed effect or a random effect. Survey of student satisfaction at U.S. colleges and universities. (What if the universities all belonged to a category, e.g., Ivy League? Would this change your answer?) Survey of public health in Massachusetts (post Romney-care) vs. New Hampshire, with five cities sampled in each state. Is state a fixed or random effect? City? Growth rates of plants as a function of nutrient level (What if this was a study in the field, sampling across a gradient of nutrient levels?) Again, ANOVA is an omnibus test, so if we get a significant result, we only know that at least one group is different. What do we do to find out which group(s) is/are different? This brings us to the subject of “post hoc tests”. 20.7 Post-hoc tests To follow up on a significant ANOVA result, we can conduct post hoc (or a posteriori) tests. These only make sense for fixed effects, because with random effects, the factors belong to the same population, so differences between pairs has no interpretation. In general, these are similar to pairwise t-tests assuming equal variance while accounting for familywise error rate and using a pooled estimate of standard error. * Remember that we assume equal variance as one of the assumptions of ANOVA* There are a huge number of post hoc tests: Fishers Least Significant Difference (LSD), Tukey’s Honest Significant Difference (HSD), Dunnett’s test, Bonferroni’s test, Holm’s test, Scheffe’s test (see Day and Quinn 1989 in “Additional readings” for descriptions). 20.7.1 Tukey’s HSD Tukey’s HSD is one of the most common post hoc tests. It is very similar to an unpaired two-sample t-test assuming equal variances. With the t-test, we use a null distribution for the difference between two means, \\(T | H_0 \\sim t_{n_A + n_B - 2}\\). With Tukey’s HSD, we use the null distribution for the range of an arbitrary number of means, or the distribution for the largest pairwise differences to be expected under the null model for a given number of comparisons, \\(q^* | H_0 \\sim q_{n_{\\text{groups}}, \\text{DOF}}\\). This is a new distribution called the studentized range distribution. We will simulate draws from the studentized range distribution on Thursday. For now, its fine to think of this like any other univariate distribution. The q distribution has two parameters, the first representing the number of groups being compared and the second representing the degrees of freedom left over after calculating all the group means. We will get into more details on Tukey’s HSD and the q distribution on Thursday. For now, imagine that you have four groups for your ANOVA and the p-value of the ANOVA is sufficiently low that you reject the null hypothesis. We now want to do an additional (post-hoc) analysis to determine which pairs of groups are actually significantly different. In our hypothestical example of having four groups in our analysis (a=4), we can rank hypothetical group means A through D from largest value to smallest: \\(B &gt; D &gt; A &gt; C\\). We then calculate our test statistic as \\[ q^* = \\frac{\\bar{X}_B - \\bar{X}_C}{\\sqrt{\\text{MS}_{\\text{within}} \\frac{\\frac{1}{n_B} + \\frac{1}{n_C}}{2}}} \\sim q_{a, \\text{DOF_within}} \\] where \\(MS_{within}\\) is the mean squares within groups and is directly analogous to the pooled variance when we were doing two-sample t-tests. (Note that in comparing this equation to the expression for a pooled t test there is a factor of 2 difference; this is absorbed into the definition the q distribution.) Note that \\(n_{B}\\) and \\(n_{C}\\) are the number of data points in each group, and because we are assuming a balanced design, we can simplify this a bit to \\[ q^* = \\frac{\\bar{X}_B - \\bar{X}_C}{\\sqrt{\\frac{\\text{MS}_{\\text{within}}}{n}}} \\sim q_{a, \\text{DOF_within=an-a}} \\] We can re-arrange this to define the Minimum Significant Range (MSR) \\[ \\bar{X}_B - \\bar{X}_C = MSR = \\sqrt{\\frac{\\text{MS}_{\\text{within}}}{n}} q_{(0.05)[a, \\text{DOF_within=an-a}]} \\] where now \\(q_{(0.05)[a, \\text{DOF_within=an-a}]}\\) is the quantile of the q-distribution. After comparing the largest mean to the smallest, you compare the second largest mean to the smallest, and so on, until you find nonsignificant differences. "],["week-11-lab.html", "21 Week 11 Lab 21.1 R’s ANOVA functions 21.2 Single-factor ANOVA in R 21.3 Follow up analyses to ANOVA 21.4 More practice: Model I ANOVA 21.5 More practice: Brief intro to doing Model II ANOVA in R", " 21 Week 11 Lab There are 5 parts to this week’s lab: R’s ANOVA functions Single-factor ANOVA Follow-up analyses to ANOVA More ANOVA practice: Fixed effects (Model I ANOVA) Brief intro to doing Model II ANOVA in R 21.1 R’s ANOVA functions R has two different functions that can do ANOVA Option #1: The ‘aov’ only fits an ANOVA model Option #2: The ‘anova’ command takes as input the result of the ‘lm’ function, and extracts the ANOVA table from the fitted linear model Since I prefer to think of ANOVA as just another linear model, I will show you how to use the ‘anova’ command following model fit by ‘lm’. There is no reason to think ‘lm’ is only used for regression and ‘aov’ for ANOVA. Both are linear models and should be fit using ‘lm’. The ‘anova’ command simply outputs the results of the model fit in the format of an ANOVA table. 21.2 Single-factor ANOVA in R Let’s review the model referred to as a single-factor ANOVA \\[ Y_{ij}=\\mu + A_{i} + \\epsilon_{ij} \\] Question: What is \\(H_{0}\\)? Answer: \\[ H_{0}: \\mbox{all } A_{i}=0 \\] What is \\(H_{A}\\)? \\[ H_{A}: \\mbox{at least one } A_{i}\\neq 0 \\] It is important to keep in mind that the alternative hypothesis does not require that all of the \\(A_{i}\\) are different, only that at least one \\(A_{i}\\) is different from the others. To review, the main components needed to construct an ANOVA table are (remember that n is the number PER CELL) \\[ SS_{total} = \\sum^{a}_{i=1}\\sum^{n}_{j=1}(Y_{ij}-\\bar{Y})^{2} \\] \\[ SS_{\\text{among groups}} = \\sum^{a}_{i=1}\\sum^{n}_{j=1}(\\bar{Y}_{i}-\\bar{Y})^{2} \\] \\[ SS_{\\text{within groups}} = \\sum^{a}_{i=1}\\sum^{n}_{j=1}(Y_{ij}-\\bar{Y}_{i})^{2} \\] We will start by doing a simple one-way ANOVA. You should already have downloaded some data from the web relating “Salary” to three different kinds of “Education” and “Gender”. These data relate to starting salaries for those leaving college with an English degree. salaries&lt;-read.csv(&quot;_data/TwoWayANOVAdata_balanced.csv&quot;, header=T) head(salaries) ## Salary Gender Education ## 1 24 Female Masters ## 2 26 Female Masters ## 3 25 Female Masters ## 4 24 Female Masters ## 5 27 Female Masters ## 6 24 Female Masters par(mfrow=c(2,1)) boxplot(salaries$Salary~salaries$Gender,ylab=&quot;Salary&quot;) boxplot(salaries$Salary~salaries$Education,ylab=&quot;Salary&quot;) Question: What determines the order of plotting for the boxplots? Answer: The alphabet! Next we will fit a linear model for Education: lm.fit&lt;-lm(Salary~Education,data=salaries) summary(lm.fit) ## ## Call: ## lm(formula = Salary ~ Education, data = salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1667 -2.0833 -0.3333 1.8333 5.8333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.083 0.724 36.026 &lt; 2e-16 *** ## EducationNo degree -7.583 1.024 -7.406 1.65e-08 *** ## EducationPhD 2.083 1.024 2.035 0.05 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.508 on 33 degrees of freedom ## Multiple R-squared: 0.7495, Adjusted R-squared: 0.7343 ## F-statistic: 49.37 on 2 and 33 DF, p-value: 1.201e-10 How do we interpret the model fit? Notice that ‘lm’ has fit an intercept which is by default the “Masters” group, and it is comparing groups “No degree” and “PhD” AGAINST the “Masters” group. It has done this because it has ordered the factors alphabetically, which may not be what you want. In this case, we probably want to order the factors like No degree, Masters, PhD salaries$Education&lt;-factor(salaries$Education,levels=c(&quot;No degree&quot;,&quot;Masters&quot;,&quot;PhD&quot;),ordered=F) If you plot salaries$Education now you’ll see that the data have not changed, but we have forced R to list the degrees in some meaningful order. Note that we have said “ordered=F” because we do not want to treat them as No degree\\(\\leq\\)Masters\\(\\leq\\)PhD in the modelling, we just want the various levels to appear in a way that we can interpret them easily. lm.fit&lt;-lm(Salary~Education,data=salaries) summary(lm.fit) ## ## Call: ## lm(formula = Salary ~ Education, data = salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1667 -2.0833 -0.3333 1.8333 5.8333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.500 0.724 25.552 &lt; 2e-16 *** ## EducationMasters 7.583 1.024 7.406 1.65e-08 *** ## EducationPhD 9.667 1.024 9.441 6.70e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.508 on 33 degrees of freedom ## Multiple R-squared: 0.7495, Adjusted R-squared: 0.7343 ## F-statistic: 49.37 on 2 and 33 DF, p-value: 1.201e-10 This is better, but the intercept is still a little mysterious. Question: How do we interpret the intercept and the other estimates? Answer: Intercept = No degree mean vs. Other estimates = DIFFERENCE between the “No degree” and the other levels Notice that the standard error of the intercept is different from the standard error of the other estimates. The estimate of the standard error for each group’s mean is calculated as the estimated standard deviation (\\(\\sigma\\)) divided by the square root of the sample size. This is the same formula as we learned much earlier in the course. However, here, we can get a better estimate of the standard deviation by pooling the data across all groups. So instead of just using the data in the No degree category to calculate the uncertainty of the No degree mean, we are using all the data to help us estimate our uncertainty about the No degree mean. What does this look like in practice? We just average the variances across all groups as follows: var.masters&lt;-var(salaries[salaries$Education==&quot;Masters&quot;,]$Salary) var.PhD&lt;-var(salaries[salaries$Education==&quot;PhD&quot;,]$Salary) var.NoDegree&lt;-var(salaries[salaries$Education==&quot;No degree&quot;,]$Salary) sigma.ave&lt;-sqrt(mean(c(var.masters,var.PhD,var.NoDegree))) #take the mean variance, and then apply the square root sigma.ave ## [1] 2.508068 No.degree.uncertainty&lt;-sigma.ave/sqrt(length(salaries[salaries$Education==&quot;No degree&quot;,]$Salary)) No.degree.uncertainty ## [1] 0.7240168 OK, so now we know how they calculated the standard error for the intercept. Why are the other standard errors larger? The reason is that when you are comparing the intercept against the number 0, then the only source of uncertainty is from the estimate of the intercept. But the other quantities represent differences, so the uncertainty in the difference arises from both quantities. Remember that variances for independent quantities add, Figure 7.1: Variances of independent variables add. Source: Hays, W. (1994) Statistics so the variance of the difference between the mean of “Masters” and the mean of “No degree” is the sum of the variances. No.degree.uncertainty&lt;-sigma.ave/sqrt(length(salaries[salaries$Education==&quot;No degree&quot;,]$Salary)) No.degree.variance&lt;-No.degree.uncertainty^2 Masters.uncertainty&lt;-sigma.ave/sqrt(length(salaries[salaries$Education==&quot;Masters&quot;,]$Salary)) Masters.variance&lt;-Masters.uncertainty^2 variance.of.difference&lt;-No.degree.variance+Masters.variance sd.of.difference&lt;-sqrt(variance.of.difference) sd.of.difference ## [1] 1.023914 So this is the same as what the summary of the regression fit has provided for the uncertainty of the difference between the two groups. Checkpoint #1: Does this make sense? What do the t-statistics and p-values tell you? \\[ \\mbox{t statistic} = \\frac{\\mbox{estimate}-\\mbox{estimate}|H_{0}}{\\mbox{s.e. of the estimate}} \\] It is always assumed that \\[ \\mbox{estimate}|H_{0} = 0 \\] Therefore, the t-statistic is just the estimate / s.e., and the p-values derived from that reflect the significance of difference in “Masters” vs. “No degree” and “PhD” vs. “No degree”. We can eliminate the intercept to make the coefficients more obviously analogous to ANOVA. lm.fit2&lt;-lm(Salary~Education-1,data=salaries) summary(lm.fit2) ## ## Call: ## lm(formula = Salary ~ Education - 1, data = salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1667 -2.0833 -0.3333 1.8333 5.8333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## EducationNo degree 18.500 0.724 25.55 &lt;2e-16 *** ## EducationMasters 26.083 0.724 36.03 &lt;2e-16 *** ## EducationPhD 28.167 0.724 38.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.508 on 33 degrees of freedom ## Multiple R-squared: 0.9906, Adjusted R-squared: 0.9897 ## F-statistic: 1155 on 3 and 33 DF, p-value: &lt; 2.2e-16 The main results (the group means and their differences) haven’t fundamentally changed, but they are now displayed in a way that makes the most sense. Note that the metrics of model fit (R2, F-statistic, and p-value for the model) have changed, and in the opposite direction as we might expect. We ran into this earlier in the semester when comparing the fit of a regression model with an intercept and the same model without an intercept. Now the estimates represent the group means, which is easier to interpret. How do we calculate the standard errors, and why are all the standard errors the same? Recalling our discussion from Week #8, we remember that we want to use the residual variation (which we can extract by using the sigma() function) divided by the square-root of the sample size: sigma(lm.fit2)/sqrt(length(salaries$Salary[salaries$Education==&quot;Masters&quot;])) ## [1] 0.7240168 Since we have a balanced design, there are the same number of samples from each educational level, and so each educational coefficient estimate has the same standard error. In other words, when calculating the estimate standard errors, we used the mean squared residuals, which is equivalent to a pooled variance estimator, where we have pooled with within group variance from all the groups. Checkpoint #2: In words, how do we interpret the p-values for the Masters and PhD group? What hypothesis are they addressing? Answer: Now the p-values are meaningless, because they test the uninteresting null hypothesis that the group means are zero. Before working out all the numbers here, lets print out the ANOVA table for this model anova(lm.fit) ## Analysis of Variance Table ## ## Response: Salary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Education 2 621.17 310.58 49.374 1.201e-10 *** ## Residuals 33 207.58 6.29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the Mean squared error in the ANOVA table is 6.29, which is 2.508\\(^2\\). In other words, when we printed the output of this model framed as a linear regression, R spit out the “residual standard error”. Re-framed as an analysis of variance, we think in terms of the mean squared error, which is just the square of the residual standard error. \\[ \\mbox{SE of the coef} = \\sqrt{\\frac{MS_{within}}{n}} \\] (The main take home message is that regression with discrete covariates is the SAME as analysis of variance. Using a regression approach, we focus more on coefficients and hypothesis tests on those coefficients. Using an ANOVA approach, we focus more on the partitioning of variance, and the null hypothesis being addressed is an omnibus hypothesis which addresses whether the covariate in question can explain more variation than would be expected by random chance alone.) Compare results of ‘summary(lm.fit)’ with a t-test looking at groups “No degree” and “Masters”. Checkpoint #3: Why would these be different? (The anova case pools the errors from all three cases, so the results will be slightly different.) t.test(salaries$Salary[salaries$Education==&quot;Masters&quot;],salaries$Salary[salaries$Education==&quot;No degree&quot;]) ## ## Welch Two Sample t-test ## ## data: salaries$Salary[salaries$Education == &quot;Masters&quot;] and salaries$Salary[salaries$Education == &quot;No degree&quot;] ## t = 8.2357, df = 21.657, p-value = 4.09e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 5.671978 9.494689 ## sample estimates: ## mean of x mean of y ## 26.08333 18.50000 Question: Why the funny d.o.f. in the two-sample case? Answer: Remember the d.o.f. for the two sample case when we did not assume equal variance? \\[ d.o.f.^{*} = \\frac{\\left[\\frac{s^{2}_{A}}{n_{A}}+\\frac{s_{B}^{2}}{n_{B}}\\right]^{2}}{\\frac{\\left(\\frac{s^{2}_{A}}{n_{A}}\\right)^{2}}{n_{A}-1}+\\frac{\\left(\\frac{s^{2}_{B}}{n_{B}}\\right)^{2}}{n_{B}-1}} \\] As an aside, what were to happen if we were to print out the ANOVA table for the model with no intercept (lm.fit2)? anova(lm.fit2) ## Analysis of Variance Table ## ## Response: Salary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Education 3 21791.4 7263.8 1154.7 &lt; 2.2e-16 *** ## Residuals 33 207.6 6.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The ANOVA table is now very different from what we got before, for the exact same reason that comparing regression models with and without intercepts is confusing. In essence, the total sum of squares being calculated by R is not relative to the overall grand mean, but relative to zero. We can see this easily by calculating \\(SS_{total}\\) sum(salaries$Salary^2) ## [1] 21999 which is the new sum-of-squares total, quite a bit larger than the previous, and more sensible calculation of sum((salaries$Salary-mean(salaries$Salary))^2) ## [1] 828.75 R uses the \\(SS_{total}\\) to work back to what \\(SS_{among}\\) should be. By mis-calculating \\(SS_{total}\\), it inflates \\(SS_{among}\\). There are ways to fix this (insert the \\(SS_{total}\\) from the linear model with an intercept and reconstruct the correct \\(SS_{among}\\)), but I won’t get into much detail about that now. 21.3 Follow up analyses to ANOVA On Tuesday, we introduced the studentized range distribution q. Unfortunately, R does not have a function to draw from this distribution (i.e. there is no ‘rtukey’). However, we can easy simulate draws from this distribution. In the case of the Educational data, we have three groups, each with 12 data points. Let’s start by randomly drawing 36 datapoints from a normal distribution with mean 0 and with a standard deviation that equals the empirical standard deviation. Under the null hypothesis, the three groups are arbitrary subsets of the full data and so we randomly divide them up into three groups of 12. Let’s put this in a loop and histogram it to get a sense for the full distribution (each iteration through the loop is one draw from the studentized range distribution). Note that because this is a balanced design, \\(n_{A}=n_{B}=n_{C}=12\\). attach(salaries) q&lt;-c() for (i in 1:1000) { data&lt;-data.frame(type=c(rep(&quot;A&quot;,times=12),rep(&quot;B&quot;,times=12),rep(&quot;C&quot;,times=12)),response=rnorm(36,0,sd(Salary))) temp&lt;-aov(data$response~data$type) MSwithin&lt;-sum((temp$residuals^2))/33 gr.means&lt;-aggregate(data$response, list(data$type), mean)$x q&lt;-c(q,(max(gr.means)-min(gr.means))/sqrt(MSwithin*(1/12))) } hist(q) This is a histogram of the studentized range distribution. While R does not provide a function to sample randomly from this distribution, R does provide functions for calculating the cumulative distribution, and so we can check that our simulation works by plotting the empirical distribution against that which is output by ‘ptukey’. plot(ecdf(q)) lines(seq(0,5,0.01),ptukey(seq(0,5,0.01),3,33),col=&quot;red&quot;) Yeah! Our simulation correctly samples from the studentized range distribution. Now we can use this to reconstruct what R’s function TukeyHSD produces. Before employing Tukey’s HSD test, rank the group means (A,B,C) from largest to smallest. If \\[ A&gt;B&gt;C \\] then Tukey’s HSD test is \\[ \\frac{\\bar{X}_{A} - \\bar{X}_{C}}{\\sqrt{MS_{within}\\frac{\\left(\\frac{1}{n_{A}}+\\frac{1}{n_{C}}\\right)}{2}}} \\sim q_{a,DOF_{\\text{within}}} \\] a = # of treatment groups = 3 (in this case) \\(DOF_{\\text{within}}\\) = # of degrees of freedom used in the calculation of \\(MS_{within}\\) = 33 (in this case) \\(n_{A}=n_{C}\\) = number of samples within groups A and C = 12 TukeyHSD(aov(Salary~Education,data=salaries)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Salary ~ Education, data = salaries) ## ## $Education ## diff lwr upr p adj ## Masters-No degree 7.583333 5.0708578 10.095809 0.000000 ## PhD-No degree 9.666667 7.1541912 12.179142 0.000000 ## PhD-Masters 2.083333 -0.4291422 4.595809 0.119761 Checkpoint #4: Where are the confidence intervals coming from? Hint: We use the same basic procedure as in Week #4, we use the quantiles of the distribution to create upper and lower confidence intervals: \\[ q_{a,DOF_{\\text{within}}} \\sim \\frac{max(results)-min(results)}{\\sqrt{MS_{within}*\\frac{1}{12}}} \\] then \\[ \\mbox{LL for max(results)-min(results)} = max(results)-min(results) - q_{(a,DOF)[0.95]}\\sqrt{MS_{\\text{within}}*\\frac{1}{12}} \\] and \\[ \\mbox{UL for max(results)-min(results)} = max(results)-min(results) + q_{(a,DOF)[0.95]}\\sqrt{MS_{\\text{within}}*\\frac{1}{12}} \\] where \\(q_{(a,DOF)[0.95]}\\) is the 95th quantile of the q-distribution. First, let’s make sure the estimates for the group differences make sense. We’ll do the PhD-No degree as an example, since these groups make up the largest and smallest responses, respectively. mean(Salary[Education==&quot;PhD&quot;])-mean(Salary[Education==&quot;No degree&quot;]) ## [1] 9.666667 This matches the output of TukeyHSD. So far, so good. What would we calculate for the lower limit? temp&lt;-aov(Salary~Education,data=salaries) MSwithin&lt;-sum((temp$residuals^2))/33 # These lines just extract MSwithin LL&lt;-mean(Salary[Education==&quot;PhD&quot;])-mean(Salary[Education==&quot;No degree&quot;]) - qtukey(0.95,3,33)*sqrt(MSwithin*(1/12)) UL&lt;-mean(Salary[Education==&quot;PhD&quot;])-mean(Salary[Education==&quot;No degree&quot;]) + qtukey(0.95,3,33)*sqrt(MSwithin*(1/12)) LL ## [1] 7.154191 UL ## [1] 12.17914 We have recreated the output from the TukeyHSD function. 21.4 More practice: Model I ANOVA Example 10A from Logan: Medley and Clements (1998) investigated the impact of zinc contamination (and other heavy metals) on the diversity of diatom species in the USA Rocky Mountains (see Box 8.1 of Quinn and Keough) The diversity of diatoms (number of species) and degree of zinc contamination (categorized as either high, medium, low, or natural background level) were recorded from between four and six sampling within each of six streams known to be polluted. These data were used to test the null hypothesis that there were no differences the diversity of diatoms between different zinc levels. \\[ H_{0}:\\mu_{H} = \\mu_{M} = \\mu_{L} = \\mu_{B} = \\mu; \\alpha_{i}=0 \\] The linear model would be written as \\[ Y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij} \\] \\[ \\mbox{Diatom spp diversity} = \\mbox{overall mean} + \\mbox{effect of zinc level} + \\mbox{error} \\] Step 1: Import the data medley&lt;-read.table(&quot;_data/medley.csv&quot;,header=T,sep=&quot;,&quot;) Step 2: Reorganize the levels of the categorical variable into a more logical order medley$ZINC&lt;-factor(medley$ZINC,levels=c(&quot;HIGH&quot;,&quot;MED&quot;,&quot;LOW&quot;,&quot;BACK&quot;),ordered=F) Now we will work through the steps of Logan’s “Key for Single-factor classification”. Step 3: Assess normality/homogeneity of variance using a boxplot of species diversity against zinc group. boxplot(DIVERSITY~ZINC, medley) Conclusions? No obvious violations of normality or homogeneity of variance (boxplots are not asymmetrical and do not vary greatly in size). Step 4: Assess homogeneity of variance assumption with a table and/or plot of mean vs variance plot(tapply(medley$DIVERSITY, medley$ZINC, mean), tapply(medley$DIVERSITY, medley$ZINC, var),pch=16,cex=2) Conclusions? No obvious relationship between mean and variance. Step 5: Test \\(H_{0}\\) that population group means are all equal - perform analysis of variance (fit the linear model) of species diversity versus zinc level group and examine the diagnostics (residual plot) medley.aov&lt;-aov(DIVERSITY~ZINC, medley) plot(medley.aov) Conclusions? We won’t discuss this much until we get to model diagnostics, but there are no obvious violations of normality of homogeneity among the residuals (no obvious wedge shape in the residuals, Q-Q plot against a normal is approximately linear). Note that Cook’s D values are meaningless in ANOVA. Step 6: Examine the ANOVA table anova(medley.aov) ## Analysis of Variance Table ## ## Response: DIVERSITY ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ZINC 3 2.5666 0.85554 3.9387 0.01756 * ## Residuals 30 6.5164 0.21721 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Conclusions? Reject \\(H_{0}\\) that population group means are equal. ZINC was found to have a significant impact on the DIVERSITY of diatoms (\\(F_{3,30}=3.939, P=0.018\\)). Step 7: Perform post-hoc Tukey’s test to investigate pairwise mean differences between all groups. First, we will do this manually, using the equations introducted in lecture. \\[ \\frac{\\bar{X}_{A}-\\bar{X}_{D}}{\\sqrt{MS_{within}\\frac{\\left(\\frac{1}{n_{A}}+\\frac{1}{n_{D}}\\right)}{2}}} \\sim q_{num.groups,dof} \\] MS.within&lt;-0.217 #from anova table output before n.A&lt;-sum(as.numeric(medley$ZINC==&quot;LOW&quot;)) n.D&lt;-sum(as.numeric(medley$ZINC==&quot;HIGH&quot;)) numerator&lt;-mean(medley$DIVERSITY[medley$ZINC==&quot;LOW&quot;])-mean(medley$DIVERSITY[medley$ZINC==&quot;HIGH&quot;]) denominator&lt;-sqrt(MS.within*(((1/n.A)+(1/n.D))/2)) test.statistic&lt;-numerator/denominator 1-ptukey(test.statistic,nmeans=4,df=30) ## [1] 0.01160652 Checkpoint #5: Why is this always a one-tailed test? Note that we did a one tailed test because we always take the largest minus the smallest group mean. We can also use the built in function we used before TukeyHSD(medley.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = DIVERSITY ~ ZINC, data = medley) ## ## $ZINC ## diff lwr upr p adj ## MED-HIGH 0.44000000 -0.15739837 1.0373984 0.2095597 ## LOW-HIGH 0.75472222 0.13893808 1.3705064 0.0116543 ## BACK-HIGH 0.51972222 -0.09606192 1.1355064 0.1218677 ## LOW-MED 0.31472222 -0.30106192 0.9305064 0.5153456 ## BACK-MED 0.07972222 -0.53606192 0.6955064 0.9847376 ## BACK-LOW -0.23500000 -0.86863665 0.3986367 0.7457444 Conclusion? We see that diatom species diversity is significantly higher in low zinc sites than high zinc sites. (We could check the others manually as well, but we see from the output of ‘TukeyHSD’ that no other \\(H_{0}\\) is rejected.) (Note that Logan uses a function called ‘glht’ in the package ‘multcomp’. This function uses a normal approximation and a randomization test to assess significance, which is more complicated and hard to reproduce, so I am sticking to ‘TukeyHSD’ which implements what we discussed in lecture.) 21.5 More practice: Brief intro to doing Model II ANOVA in R Now we will look at the influence of STREAM, which can be considered a random variable (Model II ANOVA). Step 1: The data is already loaded, but we need to assess normality/homogeneity of variances using a boxplot of species diversity against stream. boxplot(DIVERSITY ~ STREAM, medley) Step 2: We fit the ANOVA just like we already know how to do for fixed effects; this is a Model I ANOVA medley.aov&lt;-aov(DIVERSITY~STREAM,medley) anova(medley.aov) ## Analysis of Variance Table ## ## Response: DIVERSITY ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## STREAM 5 1.8278 0.36557 1.4108 0.2508 ## Residuals 28 7.2552 0.25911 Conclusions? Do not reject the null hypothesis because there is no evidence to suggest that stream identity is influencing diatom diversity. Step 3: I won’t go into more detail, but we can fit models with only random effects using ‘aov’ or fit models with random-only models or mixed-models (both random and fixed effects included) using the package ‘lme4’. Just to get it out of the way, let’s load the ‘lme4’ package: library(lme4,quietly=TRUE,verbose=FALSE,warn.conflicts=FALSE) Using the ‘aov’ function we already know how to use for random effects is fairly straightforward: aov(DIVERSITY~Error(STREAM),medley) ## ## Call: ## aov(formula = DIVERSITY ~ Error(STREAM), data = medley) ## ## Grand Mean: 1.694118 ## ## Stratum 1: STREAM ## ## Terms: ## Residuals ## Sum of Squares 1.827846 ## Deg. of Freedom 5 ## ## Residual standard error: 0.6046231 ## ## Stratum 2: Within ## ## Terms: ## Residuals ## Sum of Squares 7.255178 ## Deg. of Freedom 28 ## ## Residual standard error: 0.5090319 Using the lmer function, the syntax gets slightly messier: lmer(DIVERSITY~1|STREAM,medley) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: DIVERSITY ~ 1 | STREAM ## Data: medley ## REML criterion at convergence: 54.2562 ## Random effects: ## Groups Name Std.Dev. ## STREAM (Intercept) 0.1433 ## Residual 0.5075 ## Number of obs: 34, groups: STREAM, 6 ## Fixed Effects: ## (Intercept) ## 1.693 In either case, the notation here says that we want to model DIVERSITY with an intercept (a group mean) that is “grouped” by STREAM. STREAM describes the nature of the random variation, but each stream is considered a random sample from a larger population of streams, and so we do not interpret the means grouped by stream in the way we would if they were fixed factors. Note that ‘aov’ and ‘lmer’ do not yield the same answer in this case because the design is unbalanced. In these cases, ‘lmer’ is to be used, but I have shown ‘aov’ just to illustrate the syntax. Understanding the output of ‘lmer’ (or understanding mixed models in general) is well beyond the scope of this course, but if you end up needing to use mixed models for your research, I highly suggest reading Chapter 12 of Gelman and Hill’s excellent book “Data Analysis Using Regression and Multilevel/Hierarchical Models”. "],["week-12-lecture.html", "22 Week 12 Lecture 22.1 Week 12 Readings 22.2 Week 12 outline 22.3 Review: ANOVA with one factor 22.4 ANOVA with more than one factor 22.5 Two-way ANOVA factorial designs 22.6 Why bother with random effects? 22.7 Mixed model 22.8 Unbalanced designs 22.9 Unbalanced design – Missing cell 22.10 Two factor nested ANOVA 22.11 Experimental design", " 22 Week 12 Lecture 22.1 Week 12 Readings For this week, I suggest reading Aho Sections 7.65 and 10.8-10.14, as well as Logan Chapters 11-12. You will also need to read Hurlbert (1984). Note that I have long considered eliminating this paper because Hurlbert has made statements in the public sphere that many find offensive (regarding politics, not statistics), I continue to assign this paper because the paper itself is so well known that it would be a disservice not to familiarize everyone with the topics and terminology introduced here. We will discuss this paper at length in class and while it is a bit long, I would encourage you to read it carefully. Here is a nice simple overview on two factor designs worth reading as well. There are several other (very short) papers that may help clarify some of the more nuanced issues of this week, on ‘blocking’ in ANOVA, split plot designs, and nested designs. 22.2 Week 12 outline ANOVA for two-way factorial design Dealing with unbalanced design – unequal sample sizes: Type I, II, and III SS Dealing with unbalanced design- missing cells ANOVA for two-way nested design Some more experimental design 22.3 Review: ANOVA with one factor With single factor ANOVA, we compared multiple levels of a factor. Another way to say this is that we only had one categorical covariate in our linear model (e.g., the effect of zinc concentration on diatom diversity). 22.4 ANOVA with more than one factor With two-way (and higher) ANOVA, we look at more than one factor at a time (e.g., the effect of density and season on limpet egg production). There are different ways in which multiple factors can be modeled, and this depends on the design of your study. We will focus our discussion primarily on two ways in which multiple factors can be applied in an experimental design: Nested vs Factorial. Note that in some complicated experiments, there may be elements that are nested and others that are factorial. This will become clear as we work through some examples. In this example below, the factors are crossed (two-way factorial design). The two factors are fully crossed, or all combinations of factors are included in the design and every level of every factor occurs in combination with every level of the other factors. We will start with factorial designs because these are the most logical extension of the one-way analyses we discussed last week. 22.5 Two-way ANOVA factorial designs In the previous example, the number of egg masses per limpet was measured at different densities (factor A) and in different seasons (factor B). Two-way factorial designs can be represented in a table, where each cell is a combination of factor A and factor B. Each cell has multiple replicates. Factor level \\(B_1\\) \\(B_2\\) \\(A_1\\) 8 animals/enclosure, spring 8 animals/enclosure, summer \\(A_2\\) 15 animals/enclosure, spring 15 animals/enclosure, summer \\(A_3\\) 30 animals/enclosure, spring 30 animals/enclosure, summer \\(A_4\\) 45 animals/enclosure, spring 45 animals/enclosure, summer In a two-way factorial design, adding in an additional factor gives us a new issue to contend with. We are trying to measure the effect of factor A, marginalizing across factor B, or the effect of factor B, marginalizing across factor A. The marginal means for density (averaged across both seasons) and for season (averaged across all densities) are shown: But, what if season has an effect on density, or, the two factors influence one another? With two-way designs we need to account for this. We call this potential \\(\\text{factor} \\times \\text{factor}\\) influence an interaction, which we include in our model as a parameter that allows for the effect of factor A to depend on factor B. For a hypothetical example, let’s say we are measuring plant biomass in different fertilizer treatments (factor A, levels 1-3: low N, ambient N, and high N) and watering treatments (factor B, levels 1-4: no water, low water, ambient water, excess water), where \\(Y_{ijk}\\) is the \\(k^{\\text{th}}\\) individual with fertilizer treatment \\(i\\) and watering treatment \\(j\\). All possible combinations of treatments were measured. Interaction term \\({AB}_{ij}\\) allows for the effect of watering treatment to depend on fertilizer treatment (and vice versa). \\[ Y_{ijk} = \\mu + A_i + B_j + {AB}_{ij} + \\epsilon_{ijk} \\text{, where } \\epsilon_{ijk} \\sim \\mathrm{N} ( 0, \\sigma^2 ) \\] Question: What is \\(A_i\\)? Click for Answer The effect of fertilizer treatment \\(i\\), \\(\\mu_i - \\mu\\). The effect of treatment \\(i\\) is the mean for factor A, level \\(i\\), pooling across all levels for factor B. Question: How do we write the model for the 2nd individual plant in the fertilizer treatment “low N” and the watering treatment “excess water”? Click for Answer \\(Y_{142} = \\mu + A_1 + B_4 + {AB}_{14} + \\epsilon_{142} \\text{, where } \\epsilon_{142} \\sim \\mathrm{N} ( 0, \\sigma^2 )\\) Each level of one factor is applied to all levels of the other factor, and all combinations are replicated. Factor A, fertilizer treatment, has 3 levels and factor B (watering treatment) has 4 levels, so we have 12 possible combinations of factor A and B that must be represented. In a balanced design, you would have the same number of samples in each of the 12 combinations of factors A and B. Factor level \\(B_1\\) \\(B_2\\) \\(B_3\\) \\(B_4\\) \\(A_1\\) \\(A_1 , B_1 = Y_{111}\\) \\(A_1 , B_1 = Y_{112}\\) \\(A_1 , B_1 = Y_{113}\\) \\(A_1 , B_2 = Y_{121}\\) \\(A_1 , B_2 = Y_{122}\\) \\(A_1 , B_2 = Y_{123}\\) \\(A_1 , B_3 = Y_{131}\\) \\(A_1 , B_3 = Y_{132}\\) \\(A_1 , B_3 = Y_{133}\\) \\(A_1 , B_4 = Y_{141}\\) \\(A_1 , B_4 = Y_{142}\\) \\(A_1 , B_4 = Y_{143}\\) \\(A_2\\) \\(A_2 , B_1 = Y_{211}\\) \\(A_2 , B_1 = Y_{212}\\) \\(A_2 , B_1 = Y_{213}\\) \\(A_2 , B_2 = Y_{221}\\) \\(A_2 , B_2 = Y_{222}\\) \\(A_2 , B_2 = Y_{223}\\) \\(A_2 , B_3 = Y_{231}\\) \\(A_2 , B_3 = Y_{232}\\) \\(A_2 , B_3 = Y_{233}\\) \\(A_2 , B_4 = Y_{241}\\) \\(A_2 , B_4 = Y_{242}\\) \\(A_2 , B_4 = Y_{243}\\) \\(A_3\\) \\(A_3 , B_1 = Y_{311}\\) \\(A_3 , B_1 = Y_{312}\\) \\(A_3 , B_1 = Y_{313}\\) \\(A_3 , B_2 = Y_{321}\\) \\(A_3 , B_2 = Y_{322}\\) \\(A_3 , B_2 = Y_{323}\\) \\(A_3 , B_3 = Y_{331}\\) \\(A_3 , B_3 = Y_{332}\\) \\(A_3 , B_3 = Y_{333}\\) \\(A_3 , B_4 = Y_{341}\\) \\(A_3 , B_4 = Y_{342}\\) \\(A_3 , B_4 = Y_{343}\\) Question: How would you find the mean biomass for factor B, level 1 (no water)? Click for Answer \\(\\bar{Y}_j = \\bar{Y}_1\\) = mean of all biomass values in column 1 Question: What about the mean for factor A, level 3 (high N)? Click for Answer \\(\\bar{Y}_i = \\bar{Y}_3\\) = mean of all biomass values in row 3 Question: What about the mean for factor A, level 2 AND factor B, level 4 (ambient N, excess water)? Click for Answer \\(\\bar{Y}_{ij} = \\bar{Y}_{24}\\) = mean of all biomass values in the cell in row 2 and column 4 Interactions occur when: \\[ \\text{Effect of factor A alone} + \\text{Effect of factor B alone} \\neq \\text{Effect of A and B together} \\] Let’s go through multiple examples of hypothetical two-way factorial ANOVA results to interpret the main effects and interactions in each outcome. Example #1 A large positive value for factor A is associated with an increase in the response variable, a large positive value for factor B is associated with an increase in the response variable, and there is an interaction of A \\(\\times\\) B, showing that B has little effect at low A, but a large effect at high A. We can see this more clearly in the diagram below. For evaluating the main effect of factor B (left panel), we average across (‘marginalize out’) all the A levels and see the impact of changing the B level. Likewise, evaluating the main effect of factor A (right panel) requires us to average across all the B levels. In both cases, when we average across the levels of the other factor, we see that both A and B are associated with a change in the mean response. (There is also an interaction here as well.) Figure 7.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level. Example #2 Once again, we can see the main effects by averaging across the levels of the other factor. Here both A and B have a main effect on the response, but there is no interaction because the effect of factor A is the same for both levels of factor B (and vice versa). Figure 20.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level. Example #3 Below we can see that there is no main effect for Factor A because when we average over the levels of B, A has no effect on the response (right panel). There is, however, a main effect of B (left panel). There is no interaction of A \\(\\times\\) B. Figure 22.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level. Example #4 Below we see that Factor A has no main effect (right panel) and Factor B also has no main effect (left panel). There is, however, an interaction of A \\(\\times\\) B. In fact, the effect of A is negative for B=Low and positive for B=High. Figure 14.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level. Let’s continue interpreting two-way ANOVA plots of main effects and interactions with Aho Fig. 10.9. The response variable is biomass. Factor A, water level, is shown on the x-axis, and factor B, nutrient level, is shown with the line type, where a solid line represents added N, and a dashed line represents the control. ## Loading required package: tcltk Question: How would you interpret these results for each scenario (a,b,c, and d)? Click for Answer a) Water has a positive effect on biomass, Fertilization has a positive effect on biomass, and there is no significant interaction. b) There is no effect of water on biomass, Fertilization has a positive effect on biomass and there is no significant interaction. c) A significant interaction obscures the main effects. In wet conditions, Fertilizer has a negative effect on biomass. In dry conditions, Fertilizer has a positive effect on biomass. d) There is still an interaction, but it doesn’t change (or obscure) the main effects. Ferilizer always has a positive effect on biomass. In wet conditions, this effect is more extreme. Let’s come back to the equation for a two-factor factorial design: \\[ Y_{ijk} = \\mu + A_i + B_j + {AB}_{ij} + \\epsilon_{ijk} \\text{, where } \\epsilon_{ijk} \\sim \\mathrm{N} ( 0, \\sigma^2 ) \\] In the two-way factorial ANOVA model, there are separate null hypotheses for the two main effects and for the interaction term. The null hypotheses for the main effects (factors A and B) are: \\[ H_0 (A): \\mu_1 = \\mu_2 = ... = \\mu_i \\text{ or } A_1 = A_2 = \\dots = A_i = 0 \\] \\[ H_0 (B): \\mu_1 = \\mu_2 = ... = \\mu_j \\text{ or } B_1 = B_2 = \\dots = B_j = 0 \\] Question: What’s the difference between writing the null hypothesis as all \\(\\mu_i\\) equal vs. all \\(A_i = 0\\)? Click for Answer First way: all population group means are equal, second way: all treatment effects (differences between group means and overall mean) are zero. Null hypothesis for interaction (factors A and B) \\[ H_0 (AB): {AB}_{11} = {AB}_{12} = {AB}_{13} = \\dots = {AB}_{ij} = 0 \\] Under the null hypothesis for the interaction term, the effects of factors A and B are additive. Another way to say this is, the effect of having factor A level 1 (low N treatment) and factor B level 3 (ambient water treatment) is equal to the sum of the effect of factor A and the effect of factor B. Source of variation SS DOF MS Among groups (factor A) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) \\(a - 1\\) \\(\\frac{\\text{SS}_{A}}{\\text{DOF}_{A}}\\) Among groups (factor B) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{j} - \\bar{Y})^2\\) \\(b - 1\\) \\(\\frac{\\text{SS}_{B}}{\\text{DOF}_{B}}\\) Interaction \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{ij} - \\bar{Y}_{i} - \\bar{Y}_{j} + \\bar{Y})^2\\) \\((a - 1) (b - 1)\\) \\(\\frac{\\text{SS}_{AB}}{\\text{DOF}_{AB}}\\) Within groups (residual) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (Y_{ijk} - \\bar{Y}_{ij})^2\\) \\(ab (n - 1)\\) \\(\\frac{\\text{SS}_{\\text{within}}}{\\text{DOF}_{\\text{within}}}\\) Total \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (Y_{ijk} - \\bar{Y})^2\\) \\(abn - 1\\) Given our example of plant biomass (\\(Y\\)) under different fertilizer (factor A) and watering treatments (factor B). Question: What is \\(\\bar{Y}\\)? Click for Answer The overall mean biomass, including all factors and levels. Question: What is \\(\\bar{Y}_i\\)? Click for Answer The mean biomass in fertilizer (factor A) \\(i\\), averaged across all watering treatments (factor B). Question: What is \\(\\bar{Y}_j\\)? Click for Answer The mean biomass in watering treatment (factor B) \\(j\\), marginalized across fertilizer treatments (factor A). Question: What is \\(\\bar{Y}_{ij}\\)? Click for Answer The mean biomass in treatment combination fertilizer (factor A) \\(i\\) and water level (factor B) \\(j\\). We will refer to this as the cell mean. Question: What is going on with the sums of squares for factor A? There are no \\(k\\) or \\(j\\) subscripts in the formula \\((\\bar{Y}_{i} - \\bar{Y})^2\\), yet the summations \\(\\sum^b_{j = 1}\\) and \\(\\sum^n_{k = 1}\\) are included in the equation. Click for Answer You end up multiplying the same value, \\((\\bar{Y}_{i} - \\bar{Y})^2\\) multiple times. The sums of squares for factor A is equal to \\(n b \\sum^a_{i = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) when you have a balanced design (equal number of replicates in each cell). Question: Why is \\(\\text{DOF}_{A} = a - 1\\) and \\(\\text{DOF}_{B} = b - 1\\)? Click for Answer We estimate \\(a\\) or \\(b\\) group means, minus one for the overall mean. Question: Why is \\(\\text{DOF}_{AB} = (a - 1) (b - 1)\\)? Click for Answer Multiply this out. \\(ab\\) = number of combinations, we estimate \\(ab\\) cell means, minus \\(a\\), minus \\(b\\) for each of the factor means, and minus one for the overall mean. Question: Why is \\(\\text{DOF}_{\\text{within}} = a b (n - 1)\\)? Click for Answer Multiply this out. \\(abn\\) = total number of data points. Subtract \\(ab\\) for the cell means. The confusing sums of squares for the interaction can be rearranged to make more sense: \\[ \\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{ij} - \\bar{Y}_{i} - \\bar{Y}_{j} + \\bar{Y})^2 = \\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} ((\\bar{Y}_{ij} - \\bar{Y}) - (\\bar{Y}_{i} - \\bar{Y}) - (\\bar{Y}_{j} - \\bar{Y}))^2 \\] The interaction term represents the difference between the cell mean and overall mean relative to the difference between the main effects mean and the overall mean. This tells us how “special” this cell is relative to being in factor A alone or factor B alone. To test the null hypothesis that factor A has no effect (\\(H_0(A): A_i = 0\\)), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: \\(P ( X \\ge F^*), X \\sim F_{[\\text{DOF}_{A}, \\text{DOF}_{\\text{within}}]}\\). To test the null hypothesis that factor B has no effect (\\(H_0(B): B_j = 0\\)), \\(P ( X \\ge F^*), X \\sim F_{[\\text{DOF}_{B}, \\text{DOF}_{\\text{within}}]}\\). Last, to test the null hypothesis that there is no interaction (\\(H_0(AB): AB_{ij} = 0\\)), \\(P ( X \\ge F^*), X \\sim F_{[\\text{DOF}_{AB}, \\text{DOF}_{\\text{within}}]}\\). In our data, there is some total amount of variation: \\[ \\text{SS}_{\\text{total}} = \\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (Y_{ijk} - \\bar{Y})^2 \\] We have partitioned the variance into the variance explained by factor A (\\(\\text{SS}_{A}\\)), the variance explained by factor B (\\(\\text{SS}_{B}\\)), the variance explained by the interaction between factor A and factor B (\\(\\text{SS}_{AB}\\)), and the unexplained (residual) variance (\\(\\text{SS}_{\\text{within}}\\)). So, if our ANOVA is balanced: \\[ \\textbf{SS}_{\\text{total}} = \\textbf{SS}_{A} + \\textbf{SS}_{B} + \\textbf{SS}_{AB} + \\textbf{SS}_{\\text{within}} \\] What if factors A and B are both random effects? Our null hypotheses would be: \\[ H_0: \\sigma^2_A = 0 \\text{ and } \\sigma^2_B = 0 \\] These null hypotheses mean that there is no added variance due to the levels within factors A or B, respectively. What about the interaction in this case? \\[ H_0: \\sigma^2_{AB} = 0 \\] This means that there is no added variance due to the combination of A and B. If both effects are random, most of the ANOVA table is exactly the same, however, the F-ratio is calculated differently. Source of variation F ratio for fixed effects only F ratio for random effects only F ratio for A random / B fixed Factor A \\(\\frac{\\text{MS}_{A}}{\\text{MS}_{\\text{within}}}\\) \\(\\frac{\\text{MS}_{A}}{\\text{MS}_{AB}}\\) \\(\\frac{\\text{MS}_{A}}{\\text{MS}_{\\text{within}}}\\) Factor B \\(\\frac{\\text{MS}_{B}}{\\text{MS}_{\\text{within}}}\\) \\(\\frac{\\text{MS}_{B}}{\\text{MS}_{AB}}\\) \\(\\frac{\\text{MS}_{B}}{\\text{MS}_{AB}}\\) Interaction \\(\\frac{\\text{MS}_{AB}}{\\text{MS}_{\\text{within}}}\\) \\(\\frac{\\text{MS}_{AB}}{\\text{MS}_{\\text{within}}}\\) \\(\\frac{\\text{MS}_{AB}}{\\text{MS}_{\\text{within}}}\\) The F ratio is the mean squared error (MS) of the factor of interest divided by the mean squared error for the term that has everything but the factor of interest. When you have factor A and are considering interactions with a random effects variable B, that adds a new component to the expected variance of A (remember the new variance term, \\(\\sigma_{AB}^2\\) with random effects). Therefore, the appropriate comparison for the F ratio test is the mean squared error for the interaction term, which includes both the within group error and this additional variance component associated with the random factor. Remember from last week: \\[ \\text{Among group variance (group effect + error)} \\] When factors A and B are random, the estimate of \\(\\text{MS}_{A}\\) can be described as: \\[ \\text{Among group variance component}_A \\text{(group A variance + interaction variance + residual variance)} \\] And when factors A and B are random, the estimate of \\(\\text{MS}_{AB}\\) can be described as: \\[ \\text{Interaction variance component}_{AB} \\text{(interaction variance + residual variance)} \\] So, the F ratio is the mean squares of the factor of interest divided by the MS for the term that has everything but the factor of interest. Review question: How did we estimate the F ratio with a single factor ANOVA with a random effect? Click for Answer The F ratio was exactly the same as for fixed effects (denominator was mean square within groups). Only your interpretation of the test changes with a single factor random effects ANOVA. 22.6 Why bother with random effects? You have a study measuring the effect of different levels of zinc contamination on diatom diversity, measured in multiple streams. You could model the data with or without stream as a random effect. The model without a random effect (pooling all data from different streams), where \\(A_i\\) is the effect of zinc level \\(i\\), would be: \\[ Y_{ij} = \\mu + A_i + \\epsilon_{ij} \\text{, where } \\epsilon_{ijk} \\sim \\mathrm{N} ( 0, \\sigma_{\\epsilon1}^2 ) \\] The model with stream as a random effect, where \\(B_j\\) is the random effect of stream, and \\(AB_{ij}\\) is also a random effect describing whether the effect of zinc is consistent across streams, would be: \\[ Y_{ijk} = \\mu + A_i + B_j + {AB}_{ij} + \\epsilon_{ijk} \\text{, where } \\epsilon_{ijk} \\sim \\mathrm{N} ( 0, \\sigma_{\\epsilon2}^2 ) \\\\ B_j \\sim \\mathrm{N}(0, \\sigma^2_B) \\\\ {AB}_{ij} \\sim \\mathrm{N}(0, \\sigma^2_{AB}) \\] Your data is exactly the same in both cases, but in the model with stream as a random effect, you have decided to let the random effect of stream absorb some of the residual/unexplained variation in the model. Therefore, \\(\\sigma_{\\epsilon2} \\leq \\sigma_{\\epsilon1}\\). In the case where you ignored the effect of stream, the unexplained variation included some variation that could have been explained by stream. By lumping these together, you lose statistical power to test the null hypothesis on \\(A_{i}\\). 22.7 Mixed model What if we have one fixed effect and one random effect. Models with both fixed and random effects are called mixed models. Our null hypothesis in this case is: Factor A (fixed effect) \\(H_0: A_1 = A_2 = \\dots = 0\\) Question: How can we interpret this null hypothesis? Click for Answer The means of each population of factor A, pooled over all levels of random factor B, are equal. Factor B (random effect) \\(H_0: \\sigma^2_B = 0\\) Question: How can we interpret this null hypothesis? Click for Answer There is no added variance due to the levels within factor B. Interaction \\(H_0: \\sigma^2_{AB} = 0\\) Important: The interaction between a fixed factor and a random factor is a random factor. Question: How can we interpret this null hypothesis? Click for Answer There is no added variance due to the combinations of factor A or B. 22.8 Unbalanced designs Unbalanced designs have a surprising influence on our estimates in ANOVA. Earlier, we discussed partitioning the total variance in our data into many components: \\[ \\text{SS}_{\\text{total}} = \\text{SS}_{A} + \\text{SS}_{B} + \\text{SS}_{AB} + \\text{SS}_{\\text{within}} \\] When we have an unbalanced design, the assignment of the variance to different components becomes ambiguous, and depends on the order that we assign variance. There is no longer a simple way to partition the variance into components of \\(\\text{SS}_{\\text{total}}\\). The formulae in the two-way factorial ANOVA table are no longer applicable. Were we to calculate these variance components: \\[ \\text{SS}_{\\text{total}} \\ne \\text{SS}_{A} + \\text{SS}_{B} + \\text{SS}_{AB} + \\text{SS}_{\\text{within}} \\] There are multiple ways that a design can be unbalanced, for example, having different sample sizes in different treatments: Factor level \\(B_1\\) \\(B_2\\) \\(B_3\\) \\(B_4\\) \\(A_1\\) \\(A_1 , B_1 = Y_{111}\\) \\(A_1 , B_1 = Y_{112}\\) \\(A_1 , B_1 = Y_{113}\\) \\(A_1 , B_2 = Y_{121}\\) \\(A_1 , B_2 = Y_{122}\\) \\(A_1 , B_2 = Y_{123}\\) \\(A_1 , B_3 = Y_{131}\\) \\(A_1 , B_3 = Y_{132}\\) \\(A_1 , B_3 = Y_{133}\\) \\(A_1 , B_4 = Y_{141}\\) \\(A_1 , B_4 = Y_{142}\\) \\(A_1 , B_4 = Y_{143}\\) \\(A_2\\) \\(A_2 , B_1 = Y_{211}\\) \\(A_2 , B_1 = Y_{212}\\) \\(A_2 , B_2 = Y_{221}\\) \\(A_2 , B_2 = Y_{222}\\) \\(A_2 , B_2 = Y_{223}\\) \\(A_2 , B_3 = Y_{231}\\) \\(A_2 , B_3 = Y_{232}\\) \\(A_2 , B_3 = Y_{233}\\) \\(A_2 , B_4 = Y_{241}\\) \\(A_2 , B_4 = Y_{242}\\) \\(A_2 , B_4 = Y_{243}\\) \\(A_3\\) \\(A_3 , B_1 = Y_{311}\\) \\(A_3 , B_1 = Y_{312}\\) \\(A_3 , B_1 = Y_{313}\\) \\(A_3 , B_2 = Y_{321}\\) \\(A_3 , B_2 = Y_{322}\\) \\(A_3 , B_2 = Y_{323}\\) \\(A_3 , B_3 = Y_{331}\\) \\(A_3 , B_3 = Y_{332}\\) \\(A_3 , B_4 = Y_{341}\\) \\(A_3 , B_4 = Y_{342}\\) \\(A_3 , B_4 = Y_{343}\\) Or, one or more cells (factor A B combination) may be missing entirely: Factor level \\(B_1\\) \\(B_2\\) \\(B_3\\) \\(B_4\\) \\(A_1\\) \\(A_1 , B_1 = Y_{111}\\) \\(A_1 , B_1 = Y_{112}\\) \\(A_1 , B_1 = Y_{113}\\) \\(A_1 , B_2 = Y_{121}\\) \\(A_1 , B_2 = Y_{122}\\) \\(A_1 , B_2 = Y_{123}\\) \\(A_1 , B_3 = Y_{131}\\) \\(A_1 , B_3 = Y_{132}\\) \\(A_1 , B_3 = Y_{133}\\) \\(A_1 , B_4 = Y_{141}\\) \\(A_1 , B_4 = Y_{142}\\) \\(A_1 , B_4 = Y_{143}\\) \\(A_2\\) \\(A_2 , B_2 = Y_{221}\\) \\(A_2 , B_2 = Y_{222}\\) \\(A_2 , B_2 = Y_{223}\\) \\(A_2 , B_3 = Y_{231}\\) \\(A_2 , B_3 = Y_{232}\\) \\(A_2 , B_3 = Y_{233}\\) \\(A_2 , B_4 = Y_{241}\\) \\(A_2 , B_4 = Y_{242}\\) \\(A_2 , B_4 = Y_{243}\\) \\(A_3\\) \\(A_3 , B_1 = Y_{311}\\) \\(A_3 , B_1 = Y_{312}\\) \\(A_3 , B_1 = Y_{313}\\) \\(A_3 , B_2 = Y_{321}\\) \\(A_3 , B_2 = Y_{322}\\) \\(A_3 , B_2 = Y_{323}\\) \\(A_3 , B_3 = Y_{331}\\) \\(A_3 , B_3 = Y_{332}\\) \\(A_3 , B_3 = Y_{333}\\) \\(A_3 , B_4 = Y_{341}\\) \\(A_3 , B_4 = Y_{342}\\) \\(A_3 , B_4 = Y_{343}\\) 22.8.1 Unbalanced design – Different sample sizes We’ll look into the first situation first. Again, when you have an unbalanced design, not only is your ANOVA more sensitive to deviations from the assumptions of ANOVA (i.e., cells with different sample sizes might have different variances), but the sum of squares can no longer be neatly partitioned as we have assumed in the past: \\[ \\text{SS}_{\\text{total}} \\neq \\text{SS}_{A} + \\text{SS}_{B} + \\text{SS}_{AB} + \\text{SS}_{\\text{within}} \\] Because the partitioning of variance is ambiguous and depends on the order in which we estimate the components, there are three different ways to calculate the sums of squares for the main effects terms. These are called Type I, Type II, and Type III sums of squares. Why does unbalanced design change estimates of SS? In a two-way ANOVA, there are two ways to interpret the main effects: What is the effect of factor A on \\(Y\\), IGNORING factor B? What is the effect of factor A on \\(Y\\), CONTROLLING for factor B? It turns out that if you have equal numbers of observations in each cell, then these are the same question, but if you have an unbalanced design, then these are actually different questions. Why? Let’s say that we have unbalanced data. In our sample, men are more likely to have PhDs than women. Also, PhDs make more money than non-PhDs. In this case, if you just considered the influence of gender on salary, you might conclude that men make more money than women even if there is actually no influence of gender. In other words, the factors gender and education are correlated, and there is some amount of overlap in the variance explained by each predictor. Figure 22.2: Source: Logan (2010) In our previous example, “ignoring” education would mean that you let gender “take credit” for all of the variance it explains, even the variance that it shares with education. Note that this is implicitly the case with all variables not included in your study (hidden explanatory variables). “Controlling” for education would mean that you are testing the effect of gender only after the effect of education had already been taken into account. Another way of looking at it: These two approaches actually address different hypotheses. “Ignoring” education tests whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample. “Controlling for” education tests whether men make more money than women in a population in which all educational levels are equally likely. Usually, we are interested in inference where we are controlling for the other variables. When we have unbalanced designs, this is the only approach that makes much sense. But a word of warning (that will be often repeated), this is not the default in some major ANOVA functions in R! We will now define each of the types of sums of squares. Note that each definition assumes the two-way factorial design: \\(Y \\sim A + B + A \\times B\\). 22.8.2 Type I (sequential) sums of squares \\(\\text{SS}(A)\\) for factor A \\(\\text{SS}(B | A)\\) for factor B With sequential sums of squares, we first test the main effect A. Then, we estimate the main effect of B AFTER the main effect of A has “taken up” the shared variation. Bringing this back to our earlier example, with Type I SS, we would be “ignoring” education by testing whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample. The order in which you add factors in a model has a huge influence. This is the default in R’s function anova(), despite often not being what you are interested in! Figure 22.3: Source: Logan (2010) The sums of squares for factors A and B in Type I models are estimated using the differences in the sums of squares error for a model with just the overall mean, to a model with just factor A, to a model with factor A and factor B (but no interaction). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with just the overall mean to the model with just factor A: \\[ Y_{ijk} = \\mu + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + \\epsilon_{ijk} \\] Notice that the variance associated with \\(\\epsilon_{ijk}\\) on the left hand side gets divided up: some will be ‘assigned’ to factor \\(A\\) and some will still be left over in \\(\\epsilon_{ijk}\\) on the right hand side. In other words, \\[ \\epsilon_{ijk} \\sim N(0,\\sigma^2_{\\epsilon1}) \\longrightarrow \\epsilon_{ijk} \\sim N(0,\\sigma^2_{\\epsilon2}) \\] where \\(\\sigma^2_{\\epsilon_2} &lt; \\sigma^2_{\\epsilon_1}\\) because some of that variation is now explained by factor \\(A\\). We use that difference in residual variation as a measure of how much variation is ‘taken up’ or ‘explained’ by the factor \\(A\\). \\[ \\text{SS}_{A} = \\text{SSE}(\\mu) - \\text{SSE}(A) \\] In the above equation, \\(SS_{A}\\) is the sum of squares associated with the factor A. In other words, it is the sum of squares error that factor A “takes credit for”. SSE is the sum-of-squares error, or the residual sum-of-squares variation left over after the model. The model with no factors is comparing each data point to the grand mean \\(\\mu\\), so here \\(SSE(\\mu)\\) is just the total sum-of-squares variation. (In other words, with no covariates, all variation is residual.) \\(SSE(A)\\) is the residual sum-of-squares variation with \\(A\\) in the model. To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor A to the model with factor A and factor B (but no interaction). In other words, we add the factor \\(B\\) \\[ Y_{ijk} = \\mu + A_i + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + B_i + \\epsilon_{ijk} \\] and calculate the decrease in the residual variation in going from an A-only model to an (A+B) model. \\[ \\text{SS}_{B} = \\text{SSE}(A) - \\text{SSE}(A + B) \\] 22.8.3 Type II (hierarchical) sums of squares \\(\\text{SS}(A | B)\\) for factor A \\(\\text{SS}(B | A)\\) for factor B With hierarchical sums of squares, we assume no significant interaction. This does not depend on the order that factors are input. This can be done using the Anova() function in the package car. Figure 22.4: Source: Logan (2010) The sums of squares for each main effect are calculated by comparing the sums of squares error in a model with the factor of interest to a model without it (including all other terms at the same or lower level). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with factor A to the model without it (notice the missing interaction): \\[ Y_{ijk} = \\mu + B_i + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + B_i + \\epsilon_{ijk} \\] \\[ \\text{SS}_{A} = \\text{SSE}(B) - \\text{SSE}(A + B) \\] To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor B to the model without it: \\[ Y_{ijk} = \\mu + A_i + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + B_i + \\epsilon_{ijk} \\] \\[ \\text{SS}_{B} = \\text{SSE}(A) - \\text{SSE}(A + B) \\] 22.8.4 Type III (marginal) sums of squares \\(\\text{SS}(A | B, AB)\\) for factor A \\(\\text{SS}(B | A, AB)\\) for factor B With marginal sums of squares, we are estimating the marginal effect of a factor after the effect of the other factors (and interactions) have been taken into account. Back to our earlier example, we would be “controlling for” education, by testing whether men make more money than women in a population in which all educational levels are equally likely, or “controlling” for education by estimating the effect of gender only after the effect of education had already been taken into account. This can be done using the Anova() function in the package car. Figure 22.5: Source: Logan (2010) The sums of squares are estimated in Type III models by comparing the difference in the sums of squares between the full model and the model without the main effect being measured. To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the full model and the model missing factor A: \\[ Y_{ijk} = \\mu + B_i + (AB)_{ij} + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + B_i + (AB)_{ij} + \\epsilon_{ijk} \\] \\[ \\text{SS}_{A} = \\text{SSE}(B + A:B) - \\text{SSE}(A + B + A:B) \\] To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the full model and the model missing factor B: \\[ Y_{ijk} = \\mu + A_i + (AB)_{ij} + \\epsilon_{ijk} \\longrightarrow Y_{ijk} = \\mu + A_i + B_i + (AB)_{ij} + \\epsilon_{ijk} \\] \\[ \\text{SS}_{B} = \\text{SSE}(A + A:B) - \\text{SSE}(A + B + A:B) \\] 22.8.5 Comparing type I, II, and III SS When the design is balanced (equal sample sizes in each category), the factors are “orthogonal,” and Types I,II, and III all give equivalent results. When the interaction AB is not significant, then Type II and III SS estimates are equivalent. Aho 10.14 contains more detail. Note that \\(\\text{SS}_{AB}\\) and \\(\\text{SS}_{\\text{within}}\\) are the same for all three ways of calculating SS. \\(\\text{SS}_{\\text{within}}\\) is the sum of squared deviations between each fitted data point \\(\\hat{Y}_{ijk} = \\bar{Y}_{ij}\\) and the overall mean for the full model \\(Y_{ijk} = \\mu + A_i + B_i + (AB)_{ij} + \\epsilon_{ijk}\\) and \\(\\text{SS}_{AB}\\) is calculated as the difference in sums of squares between the full model and the model without the interaction term. There are only differences in estimates of SS for the main effects terms for the three different SS methods because it depends on the way you calculate marginal means. 22.9 Unbalanced design – Missing cell When an entire cell (a combination of factors) is missing, it is not possible to test all the main effects and interactions. One solution is to fit a large single factor ANOVA with as many levels as there are cells, and then compare combinations using specific contrasts to tests hypotheses of interest. This is like treating each factor combination like a dummy coded variate. This is called a “cell means model.” This approach is worked out in Logan 12.6. 22.10 Two factor nested ANOVA In nested designs, the categories of the nested factor within each level of the main factor are unique. Usually this happens because 1) you have unique organisms within each treatment, or 2) you have unique plots within each treatment. The nested factors are usually random effects (but not always). Nested designs refer to any design in which there is subsampling within the replicates. Nested designs, or hierarchical designs, can have many levels. For example, if you were interested in barnacle diversity, you could have subsamples within replicates, replicates nested within intertidal zones, intertidal zones nested with shores, shores nested within regions, etc. Let’s imagine we are measuring the amount of glycogen in rat livers. We have three treatments that we gave rats. We included two rats in each treatment. We took six liver samples from each rat and measured each one of those liver samples once. (We will tackle a slightly more complex nested version in the lab.) There are 36 total measurements. Figure 16.1: Nested design of the rat experiment. What would our model equation look like? \\[ Y_{ijk} = \\mu + A_i + B_{j(i)} + \\epsilon_{ijk} \\text{, where } \\epsilon_{ijk} \\sim \\mathrm{N} ( 0, \\sigma^2) \\] \\(A_i\\) is the effect of treatment, and \\(B_{j(i)}\\) is the nesting factor (rats within treatment). \\(Y_{ijk}\\) is the \\(k^{th}\\) liver sample from rat \\(j\\) nested in treatment \\(i\\). It would be impossible to have a crossed design for this experiment, for example, rat 1 cannot be in treatment 1, 2, and 3. Why use a nested structure? Nested designs are actually quite powerful. Single units in an experimental design may not adequately represent the populations. By only measuring the response in single units in an experiment, this can actually increases the unexplained variation in the system, which can mask the true differences. By subsampling within each sample, we get a more precise estimate of the mean response within each sampling unit (think: Central Limit Theorem). If we estimate the response in three subsamples, we can get a more precise estimate of the treatment mean than if we only had one sample per replication. This way, we reduce unexplained variability. Now we know why subsampling is helpful, but why not just average among the subsamples? A nested analysis allows you to partition the variance into the amount of variation at each level of the hierarchy. Why not just ignore the nestedness of the data? Because by ignoring the nestedness of the data, you would treat all the data as being independent, but because of the nested samping design, this is inappropriate. Source of variation SS DOF MS F Among groups (factor A) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) \\(a - 1\\) \\(\\frac{\\text{SS}_{\\text{among grp.}}}{\\text{DOF}_{\\text{among grp.}}}\\) \\(\\frac{\\text{MS}_{\\text{among grp.}}}{\\text{MS}_{\\text{among rep.}}}\\) Among replicates within groups (nesting factor B) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (\\bar{Y}_{j(i)} - \\bar{Y}_{i})^2\\) \\(a (b - 1)\\) \\(\\frac{\\text{SS}_{\\text{among rep.}}}{\\text{DOF}_{\\text{among rep.}}}\\) \\(\\frac{\\text{MS}_{\\text{among rep.}}}{\\text{MS}_{\\text{residual}}}\\) Subsamples within replicates (residual) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (Y_{ijk} - \\bar{Y}_{j(i)})^2\\) \\(ab (n - 1)\\) \\(\\frac{\\text{SS}_{\\text{residual}}}{\\text{DOF}_{\\text{residual}}}\\) Total \\(\\sum^a_{i = 1} \\sum^b_{j = 1} \\sum^n_{k = 1} (Y_{ijk} - \\bar{Y})^2\\) \\(abn - 1\\) In the above table, the notation is as follows: \\(\\bar{Y}\\) is the grand mean (i.e. the average across all 36 measurements) \\(\\bar{Y}_{i}\\) is the mean within each group. In the rat example, this is the mean for each treatment. \\(\\bar{Y}_{j(i)}\\) = is the mean within of the subsets within that group. In the rat example, this would be the mean within each rat. Notice that there are 6 measurements for each rat and there is variation within these measurements. Given this experimental set-up, this is the residual variation. The mean squared error at this bottom level of the model is \\(\\text{MS}_{residual}\\). Large residual variation is going to make it difficult to say that variation between groups higher in the hierarchy are actually statistically significant. To test the null hypothesis that factor A has no effect (\\(H_0(A): A_i = 0\\)), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: \\(P ( X \\ge F^*), X \\sim F_{[\\text{DOF}_{\\text{among grp.}}, \\text{DOF}_{\\text{among rep.}}]}\\). To test the null hypothesis that nesting factor B has no effect (\\(H_0(B): B_j = 0\\)), \\(P ( X \\ge F^*), X \\sim F_{[\\text{DOF}_{\\text{among rep.}}, \\text{DOF}_{\\text{residual}}]}\\). Again, there is no interaction term with nested designs. Finally, nested design arise frequently in a spatial context, where you might have multiple plots within fields receiving different treatments. The most common scenario would look like this: Figure 22.6: Nested ANOVA with sub-sampling 22.10.1 Potential issues with nested designs Nested designs are often mis-analyzed, because people often incorrectly treat subsamples as independent replicates. This ignores correlation among subsamples and artificially boosts sample size. As a result, the chance of Type I error increases dramatically. Nested designs are difficult if not impossible to analyze correctly if sample sizes are not the same in each group. The power of ANOVA designs is more sensitive to the number of independent replicates than it is to the precision with which you can estimate the mean of each replicate, which means that you should never divert resources away from more independent replicates in favor of subsampling the replicates you have. All of these potential issues can be resolved with careful experimental design. 22.11 Experimental design Experimental design is a very important part of statistics that deals with the structure of an experiment or sampling scheme (experimental design is still important even with observational studies). Proper experimental design is essential to correctly be able to make inferences. More on this topic in Aho Ch. 7 and the Hurlbert (1989) paper. 22.11.1 Completely randomized design We didn’t say so explicitly, but when we discussed single-factor ANOVA before we assumed a completely randomized design. In a completely randomized design, treatments are assigned at random to experimental units (plots, organisms, patients). A negative of completely randomized design is that if there is an environmental gradient, or “noise,” then the completely randomized design will have low power. This is because differences among treatments will be swamped by background “noise” not accounted for in the model. Figure 22.7: Environmental gradient 22.11.2 Randomized block design A “block” is a unit of space or time within which conditions are considered to be relatively homogeneous. Blocks may be placed randomly or systematically, but they should be arranged so that environmental conditions are more are more similar within blocks than between them.Within each block, treatments are assigned randomly. Each treatment is assigned once per block. Blocks should be placed far enough apart that blocks can be considered independent. Figure 22.8: Randomized block design Randomized block designs can account for some of the background heterogeneity that completely randomized designs miss. When environmental heterogeneity is present, the randomized block design is more efficient than a completely randomized layout and will need fewer replicates for the same statistical power. A negative of randomized block design is that we assume no interaction between the block and the treatments (in other words, it assumes that the ranking of treatment responses will be the same in each block). Randomized block design models are very similar to two-way factorial designs, but without an interaction term. \\(Y_{ijk}\\) is the \\(k^{\\text{th}}\\) observation from the \\(j^{\\text{th}}\\) level in blocking factor B and the \\(i^{\\text{th}}\\) level in factor A (a fixed effect). \\[ Y_{ijk} = \\mu + A_i + B_j +\\epsilon_{ijk}, \\text{where } \\epsilon_{ijk} \\sim \\mathrm{N} (0, \\sigma^2) \\] Our null hypotheses are the effect of treatment factor A is zero (\\(H_0(A): \\text{all } A_i = 0\\)), and the effect of blocking factor B is zero (\\(H_0(B): \\text{all } B_i = 0\\)). If we reject the null hypothesis that block effects are significant, we know that using blocks as replicates was effective in addressing the homogeneity among replicates. We fully expect that the block effect will be significant, since this was our rationale for using a randomized block design! Source of variation SS DOF MS F Among treatments (factor A) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} (\\bar{Y}_{i} - \\bar{Y})^2\\) \\(a - 1\\) \\(\\frac{\\text{SS}_{A}}{\\text{DOF}_{A}}\\) \\(\\frac{\\text{MS}_{A}}{\\text{MS}_{\\text{within}}}\\) Among blocks (factor B) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} (\\bar{Y}_{j} - \\bar{Y})^2\\) \\(b - 1\\) \\(\\frac{\\text{SS}_{B}}{\\text{DOF}_{B}}\\) \\(\\frac{\\text{MS}_{B}}{\\text{MS}_{\\text{within}}}\\) Within groups (residual) \\(\\sum^a_{i = 1} \\sum^b_{j = 1} (Y_{ij} - \\bar{Y}_{j} - \\bar{Y}_{i} + \\bar{Y})^2\\) \\((b - 1)(a - 1)\\) \\(\\frac{\\text{SS}_{\\text{within}}}{\\text{DOF}_{\\text{within}}}\\) There is no \\(\\sum^n_{k = 1}\\) because there is only one replicate of each treatment in each block. Also notice the similarity of the residual SS to the interaction mean squares in other designs. 22.11.3 Latin square design A Latin square design is a special case of a randomized block design for cases where environmental heterogeneity may occur along two dimensions (east-west and north-south for example). In this case you want each treatment to occur exactly once in each row and in each column (like Sudoku). Figure 22.9: Latin square design. Source: Wikipedia 22.11.4 Split plot design With a split plot design, we have an experimental design that exists at two scales: a large unit (which has a whole plot treatment assigned), and smaller units within the large unit (which have split plot treatments). This is a hierarchical design in that the large unit treatments are pseudoreplicates, but the split plots have informative factor levels that mean exactly the same thing in other plots (unlike rat individuals in nested designs). For example, the large unit could be “pond,” to which you apply different nutrient addition treatments, and the smaller unit could be “predation treatment” where you multiple different cage setups within each pond. The predation treatments mean the same thing in the other ponds. Often one factor is applied at the block level because of logistical constraints. One example might be an experiment to test insecticide application vs. grass seed type. If insecticides are being applied by airplane, then it is easier to apply one insecticide treatment to a whole field instead of just to a portion of field, in which case insecticide might be applied at the field level but grass variety might vary among subplots within the larger field (“block”). "],["week-12-lab.html", "23 Week 12 Lab 23.1 Example #1: Two-way factorial ANOVA in R 23.2 Example #2: Nested design 23.3 Example #3: Nested design 23.4 Example #4: Randomized Block Design 23.5 Example #5: Nested design", " 23 Week 12 Lab Today we will discuss Hurlbert’s famous paper on pseudoreplication. You can also find an interview with Hurlbert here. There are 5 parts to this week’s lab: Example #1: Two-way factorial ANOVA in R Example #2: Nested deisgn Example #3: Nested design Example #4: Randomized block design Example #5: Nested design 23.1 Example #1: Two-way factorial ANOVA in R Two-way ANOVA in R is a lot like one-way ANOVA in R, except now we have a second covariate in the model equation. We will go through a two-way example using the same data on income and education that we used last week. salaries&lt;-read.csv(&quot;_data/TwoWayANOVAdata.csv&quot;) par(mfrow=c(2,1)) boxplot(salaries$Salary~salaries$Gender,ylab=&quot;Salary&quot;) boxplot(salaries$Salary~salaries$Education,ylab=&quot;Salary&quot;) NB: The data we are using this week are the original (unbalanced) version of the data we used last week. These data are unbalanced, which will have important consequences for the ANOVA as we will see. Let’s stop for a second and figure out how we can look at the data: summary(salaries) ## Salary Gender Education ## Min. :15.00 Length:31 Length:31 ## 1st Qu.:20.50 Class :character Class :character ## Median :25.00 Mode :character Mode :character ## Mean :23.97 ## 3rd Qu.:27.00 ## Max. :32.00 str(salaries) ## &#39;data.frame&#39;: 31 obs. of 3 variables: ## $ Salary : int 24 26 25 24 27 24 27 23 30 27 ... ## $ Gender : chr &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ Education: chr &quot;Masters&quot; &quot;Masters&quot; &quot;Masters&quot; &quot;Masters&quot; ... head(salaries) ## Salary Gender Education ## 1 24 Female Masters ## 2 26 Female Masters ## 3 25 Female Masters ## 4 24 Female Masters ## 5 27 Female Masters ## 6 24 Female Masters I’m going to show you another nice way of looking at ANOVA data which defines a new function which calculates the mean and its standard error and prints it out in a nice format. Don’t worry too much about the details, but I include the script here so you have it in case it is helpful for you later… meansd&lt;-function(x) { tmp.mean&lt;-format(mean(x),digits=3) tmp.sd&lt;-format(apply(x,2,sd),digits=3) mean.sd&lt;-paste(tmp.mean, &quot; (&quot;,tmp.sd,&quot;)&quot;, sep=&quot;&quot;) mean.sd&lt;-as.matrix(mean.sd) names(mean.sd)&lt;- &quot;Mean (SD)&quot; return(mean.sd) } library(rms) #the function &quot;summary&quot; uses the rms package ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## Warning in .recacheSubclasses(def@className, def, env): undefined subclass ## &quot;numericVector&quot; of class &quot;Mnumeric&quot;; definition not updated summary(Salary~Education+Gender,data=salaries,method=&quot;cross&quot;,fun=meansd) ## ## meansd by Education, Gender ## ## +------+ ## | N| ## |Salary| ## +------+ ## +---------+-----------+-----------+-----------+ ## |Education| Female | Male | ALL | ## +---------+-----------+-----------+-----------+ ## | Masters| 8 | 3 | 11| ## | |25 (1.51) |27 (2) |25.5 (1.81)| ## +---------+-----------+-----------+-----------+ ## |No degree| 4 | 7 | 11| ## | |17 (2.16) |20 (1.41) |18.9 (2.21)| ## +---------+-----------+-----------+-----------+ ## | PhD| 4 | 5 | 9 | ## | |29.2 (2.22)|27.4 (1.67)|28.2 (2.05)| ## +---------+-----------+-----------+-----------+ ## | ALL| 16| 15| 31| ## | |24.1 (4.89)|23.9 (4.03)|24 (4.42) | ## +---------+-----------+-----------+-----------+ This is a quick and dirty way to look at the raw data to make sense of the model output to come. Note that you could stick this inside ‘latex()’ and it would make a latex ready formatted table for you. Back to the modelling… Note that Salary~Education+Gender+Education:Gender is the same as Salary~Education*Gender since the * says “consider Education and Gender and all interactions”. For reasons that will become clear in a second, we will use the first notation for now. lm.fit.2way&lt;-lm(Salary~Education+Gender+Education:Gender,data=salaries) summary(lm.fit.2way) ## ## Call: ## lm(formula = Salary ~ Education + Gender + Education:Gender, ## data = salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.250 -1.125 0.000 1.000 3.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.0000 0.6162 40.569 &lt; 2e-16 *** ## EducationNo degree -8.0000 1.0674 -7.495 7.55e-08 *** ## EducationPhD 4.2500 1.0674 3.982 0.000519 *** ## GenderMale 2.0000 1.1800 1.695 0.102516 ## EducationNo degree:GenderMale 1.0000 1.6081 0.622 0.539664 ## EducationPhD:GenderMale -3.8500 1.6612 -2.318 0.028945 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.743 on 25 degrees of freedom ## Multiple R-squared: 0.8706, Adjusted R-squared: 0.8447 ## F-statistic: 33.64 on 5 and 25 DF, p-value: 2.509e-10 anova(lm.fit.2way) ## Analysis of Variance Table ## ## Response: Salary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Education 2 471.78 235.888 77.6458 1.882e-11 *** ## Gender 1 8.96 8.955 2.9478 0.09836 . ## Education:Gender 2 30.29 15.143 4.9846 0.01507 * ## Residuals 25 75.95 3.038 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Question: How do we interpret the intercept?) Note that these effect sizes should make sense if we keep track of the correct usage of the interaction terms. The intercept is the mean salary of a Female with a Masters degree (25k). We can re-create the other cells in the table above by adding or subtracting from the baseline the effects of swapping categories of gender or education. For example, what is the mean salary of a man with a PhD. Going from the baseline condition of the intercept to a Man with a PhD we have to make two swaps: Female-&gt;Male and Masters-&gt;PhD. This involves both the main effects of Gender and Education but also the interaction between Gender and Education. Let’s do this stepwise: \\[ \\mbox{Female&amp;Masters}=25 \\\\ \\mbox{Change for a woman getting a PhD}=4.25 \\\\ ------- \\rightarrow\\mbox{Women&amp;PhD}=25+4.25=29.25 \\\\ \\mbox{Change for a Woman with PhD becoming a Man} = 2-3.85 = -1.85\\\\ ------- \\rightarrow \\mbox{Man&amp;PhD} = 29.25-1.85=27.4 \\] This is exactly what the table shows as well. Note that because of the interaction, a woman with a PhD is better off staying a woman because the benefit of getting a PhD is much bigger for a woman than for a man and this more than compensates for the negative effect being female has on average salary. Another way of thinking about it would be to do the gender swap first: \\[ \\mbox{Female&amp;Masters}=25 \\\\ \\mbox{Change for a Woman with Masters becoming a Man} = 2 \\\\ ------- \\rightarrow \\mbox{Man&amp;Masters} = 25+2 = 27 \\mbox{ [Magic, no additional skills required! :) ]}\\\\ \\mbox{Change for a Man getting a PhD}=4.25-3.85 =0.4 \\\\ ------- \\rightarrow \\mbox{Man&amp;PhD}=27+0.4=27.4 \\\\ \\] Now try doing the “same” analysis but switching the order of inputs lm.fit.2way&lt;-lm(Salary~Gender+Education+Gender:Education,data=salaries) summary(lm.fit.2way) ## ## Call: ## lm(formula = Salary ~ Gender + Education + Gender:Education, ## data = salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.250 -1.125 0.000 1.000 3.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.0000 0.6162 40.569 &lt; 2e-16 *** ## GenderMale 2.0000 1.1800 1.695 0.102516 ## EducationNo degree -8.0000 1.0674 -7.495 7.55e-08 *** ## EducationPhD 4.2500 1.0674 3.982 0.000519 *** ## GenderMale:EducationNo degree 1.0000 1.6081 0.622 0.539664 ## GenderMale:EducationPhD -3.8500 1.6612 -2.318 0.028945 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.743 on 25 degrees of freedom ## Multiple R-squared: 0.8706, Adjusted R-squared: 0.8447 ## F-statistic: 33.64 on 5 and 25 DF, p-value: 2.509e-10 anova(lm.fit.2way) ## Analysis of Variance Table ## ## Response: Salary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender 1 0.30 0.297 0.0977 0.75716 ## Education 2 480.43 240.217 79.0708 1.547e-11 *** ## Gender:Education 2 30.29 15.143 4.9846 0.01507 * ## Residuals 25 75.95 3.038 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Did you get the same answer? (No. In fact, Gender is no longer significant the second way.) Notice that while the ANOVA tables are different, the parameter estimates and their standard errors/p-values are the same in both cases. Why? Remember when we were discussing multiple regression, we said that the coefficients in a multiple regression represented the slope of the “partial regression line” which is to be interpreted as the effect of that covariate holding all other predictor variables at their mean value. In other words, the multiple regression implicitly uses a Type III approach because that is typically what is of interest. Likewise, we are usually interested in the Type III sum of squares ANOVA table. The only problem is that this is not R’s default. To get Type II and Type III sum of squares ANOVA tables, you have to use the ‘Anova’ function in the ‘car’ package. Before moving on from this example, its worth thinking about a slightly simpler analysis in which you model income as a function of gender (men v. women) and education (Masters v. PhD). (Same as above but considering only two of the original three education levels.) Considering the 2-way ANOVA analysis for this, we may find ourselves asking the question “If you have a factorial design and you are going to include an interaction term, why bother doing a two-way ANOVA - why not just do two separate t-tests?”. In other words, the interaction allows for the effect of Education to differ for men and for women, so why not just do a t-test for men and a separate t-test for women? One issue is the concern over multiple comparisons. The other difference involves how error is estimated. Let’s do the ANOVA first: salaries.v2&lt;-salaries[-which(salaries$Education==&quot;No degree&quot;),] summary(lm(Salary~Gender+Education+Gender:Education,data=salaries.v2)) ## ## Call: ## lm(formula = Salary ~ Gender + Education + Gender:Education, ## data = salaries.v2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.250 -1.288 -0.200 1.250 2.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.0000 0.6247 40.020 &lt;2e-16 *** ## GenderMale 2.0000 1.1962 1.672 0.1140 ## EducationPhD 4.2500 1.0820 3.928 0.0012 ** ## GenderMale:EducationPhD -3.8500 1.6840 -2.286 0.0362 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.767 on 16 degrees of freedom ## Multiple R-squared: 0.5091, Adjusted R-squared: 0.417 ## F-statistic: 5.531 on 3 and 16 DF, p-value: 0.008446 Now lets do the analogous t-test, looking at the difference in salary between women with a Masters degree and women with a PhD: t.test(salaries.v2$Salary[salaries.v2$Gender==&quot;Female&quot;&amp;salaries.v2$Education==&quot;Masters&quot;],salaries.v2$Salary[salaries.v2$Gender==&quot;Female&quot;&amp;salaries.v2$Education==&quot;PhD&quot;]) ## ## Welch Two Sample t-test ## ## data: salaries.v2$Salary[salaries.v2$Gender == &quot;Female&quot; &amp; salaries.v2$Education == &quot;Masters&quot;] and salaries.v2$Salary[salaries.v2$Gender == &quot;Female&quot; &amp; salaries.v2$Education == &quot;PhD&quot;] ## t = -3.453, df = 4.4536, p-value = 0.02188 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.5342704 -0.9657296 ## sample estimates: ## mean of x mean of y ## 25.00 29.25 Notice that the difference in means is exactly what you would expect from the summary table from the lm() function (4.25) but the t-statistic (and hence the p-value) are now different. Why? In the lm(), data on both males and females are used to estimate the within-group error (in other words, the data are pooled to estimate MSwithin) whereas the t-test uses data for women only and therefore ends up with a slightly different estimate for the test statistic and the p-value. Which is correct? It depends on whether you think that there is value in pooling the errors. It also depends on how concerned you are about multiple comparisons with the separate t-tests. 23.2 Example #2: Nested design We will work through one of the classic examples from Sokal &amp; Rohlf in which we are looking at the effect of a treatment on rat livers. We have three treatments, two rats per treatment, three liver samples per rat, and two measurements of each liver sample. The experimental design is sketched out here: Figure 23.1: Nested design of the rat experiment. rats&lt;-read.csv(&quot;_data/rats.txt&quot;,header=T) attach(rats) Treatment&lt;-factor(TREAT) Rat&lt;-factor(RAT) Liver&lt;-factor(PREP) anova(lm(GLYCO~TREAT)) ## Analysis of Variance Table ## ## Response: GLYCO ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TREAT 2 1557.6 778.78 14.498 3.031e-05 *** ## Residuals 33 1772.7 53.72 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like Treatment is highly significant! But…we made a big mistake here because we have assumed that all 36 liver measurements are independent samples when we know that they are not because they only come from 6 rats (2 for each treatment). What happens if we simply average the Glycogen context for each rat and redo the ANOVA? tapply(GLYCO,list(TREAT,RAT),mean) ## Rat1 Rat2 Rat3 Rat4 Rat5 Rat6 ## Compound217 NA NA 149.6667 152.3333 NA NA ## Compound217Sugar NA NA NA NA 134.3333 136 ## Control 132.5 148.5 NA NA NA NA anova(lm(c(tapply(GLYCO,list(TREAT,RAT),mean))[!is.na(c(tapply(GLYCO,list(TREAT,RAT),mean)))==TRUE]~factor(c(1,2,3,1,2,3)))) ## Analysis of Variance Table ## ## Response: c(tapply(GLYCO, list(TREAT, RAT), mean))[!is.na(c(tapply(GLYCO, ## Response: list(TREAT, RAT), mean))) == TRUE] ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(c(1, 2, 3, 1, 2, 3)) 2 2.12 1.06 0.0081 0.9919 ## Residuals 3 390.42 130.14 Analyzed this way we see that Treatment is not in fact significant, but averaging over all the liver samples precludes the possibility of looking at variation at the other scales of study (within rat, between measurements). Nested designs allow us to do just that. To analyze nested models, we use the ‘/’ operator to show how things are nested, and we add a new error term to tell R how the errors are nested as well. model&lt;-aov(GLYCO~TREAT+Error(RAT/PREP)) ## Warning in aov(GLYCO ~ TREAT + Error(RAT/PREP)): Error() model is singular summary(model) ## ## Error: RAT ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TREAT 2 1557.6 778.8 2.929 0.197 ## Residuals 3 797.7 265.9 ## ## Error: RAT:PREP ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 12 594 49.5 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 18 381 21.17 Note that if you leave the error term off, the sum of squares calculated are correct, but the F ratios are wrong and, as a result, the p-values are way too small. 23.3 Example #3: Nested design Case study taken from Logan (see Figure 2): “In an unusually detailed preparation for an Environmental Effects Statement for a proposed discharge of dairy wastes in the Curdies River, in western Victoria, a team of stream ecologists wanted to describe the basic patterns of variation in a stream invertebrate thought to be sensitive to nutrient enrichment. As an indicator species, they focused on a small flatworm, Dugesia, and started by sampling populations of this worm at a range of scales. They sampled in two season, they sampled three randomly chosen (well, haphazardly, because sites are nearly always chosen to be close to road access) sites. A total of six sites in all were visited, 3 in each season. At each site, they sampled six stones, and counted the number of flatworms on each stone.” Why is this a nested design and not a factorial (crossed) design? Because the sites chosen in each season are not the same sites, they are randomly selected in each season depending on logistical constraints. worms&lt;-read.csv(&quot;_data/flatworms.csv&quot;,header=T) worms$Site&lt;-factor(worms$Site) Question: What are the main hypotheses being tested? \\(H_{0}\\) for Effect #1: There is no effect of season on flatworm density \\(H_{0}\\) for Effect #2: There is no added variance due to the random variable Sites First, let’s check the assumption of normality by looking at boxplots of the data boxplot(Density~Season,data=worms) We see that Winter has significantly more variance than Summer. What do we do? We try and transform Density so as to stabilize variance between two seasons. The original authors use a fourth-root transformation, so we will try that… boxplot(Density^(1/4)~Season,data=worms) A log transformation would also have worked.There are strategies for finding the best transformation but we don’t have time to get into that. Question: What are the correct F-ratios and the appropriate degrees of freedom? (See Table 1) Let’s fit the model in R: model2&lt;-aov(Density^(1/4)~Season+Error(Site),data=worms) summary(model2) ## ## Error: Site ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Season 1 5.571 5.571 34.5 0.0042 ** ## Residuals 4 0.646 0.161 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 30 4.556 0.1519 summary(lm(model2)) ## ## Call: ## lm(formula = model2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3811 -0.2618 -0.1381 0.1652 0.9023 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3811 0.1591 2.396 0.02303 * ## SeasonWINTER 0.7518 0.2250 3.342 0.00224 ** ## Site2 0.1389 0.2250 0.618 0.54156 ## Site3 -0.2651 0.2250 -1.178 0.24798 ## Site4 -0.0303 0.2250 -0.135 0.89376 ## Site5 -0.2007 0.2250 -0.892 0.37955 ## Site6 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3897 on 30 degrees of freedom ## Multiple R-squared: 0.5771, Adjusted R-squared: 0.5066 ## F-statistic: 8.188 on 5 and 30 DF, p-value: 5.718e-05 Question: How do we interpret the model? The F-ratio for Season is significant - what does this mean biologically? If we were to go back and redesign the study to maximize power, what would we do? Click for Answer If a nested design was to be used, then since SITES are the replicates for the effect of SEASON, then power is maximized by having more sites. Whilst having more stones may increase the precision of the measure of Dugusia within a site, it will not improve the power of the test of SEASON. Let’s fit the model using the untransformed data just to compare: model3&lt;-aov(Density~Season+Error(Site),data=worms) summary(model3) ## ## Error: Site ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Season 1 36.81 36.81 3.918 0.119 ## Residuals 4 37.58 9.40 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 30 203.9 6.798 Now we get a grossly inflated level of significance for Season driven by the larger variance of the Winter densities. 23.4 Example #4: Randomized Block Design (From Quinn, Keough, and Carey; see Figure 3) A plant pathologist wanted to examine the effects of two different strengths of tobacco virus on the number of lesions on tobacco leaves. She knew from pilot studies that leaves were inherently very variable in response to the virus. In an attempt to account for this leaf to leaf variability, both treatments were applied to each leaf. Eight individual leaves were divided in half, with half of each leaf incoculated with weak strength virus and the other half inoculated with strong virus. Question: What kind of analysis is this? Click for Answer A randomized block design, with Leaf as the “blocking factor”. So in this case, the leaves are considered the “blocks” and each treatment is represented once in each block. A completely randomized design would have had 16 leaves, with 8 whole leaves randomly allocated to each treatment. tobacco&lt;-read.csv(&quot;_data/tobacco.csv&quot;) head(tobacco) ## Leaf Treatment Number ## 1 L1 Strong 35.89776 ## 2 L1 Weak 25.01984 ## 3 L2 Strong 34.11786 ## 4 L2 Weak 23.16740 ## 5 L3 Strong 35.70215 ## 6 L3 Weak 24.12191 Question: What are the main hypotheses being tested? Click for Answer \\(H_{0}\\) for Effect #1: There is no effect of treatment on lesion number within each leaf block. \\(H_{0}\\) for Effect #2: There is no added variance due to the random variable Leaf. (In other words, there is no effect of the blocking factor leaf.) We can’t actually test for an interaction between Block and Treatment in this design, but we can look at the data using an “Interaction plot”. boxplot(Number~interaction(Leaf,Treatment),data=tobacco) The interaction plot suggests that there is some evidence of an interaction. Although the number of lesions appear to be greater in strongly innoculated leaves than the weakly innoculated leaves in most of the leaf pairs (blocks), this trend is either absent or reversed in two of the eight (1/4) of the leaf pairs. As a result, the test of block may not be reliable, and the power of the main test of treatment is reduced. Question: What is the right R command to fit this model? Click for Answer model4&lt;-aov(Number~Treatment+Error(Leaf/Treatment),data=tobacco) summary(model4) ## ## Error: Leaf ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 7 292.1 41.73 ## ## Error: Leaf:Treatment ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 248.3 248.34 17.17 0.00433 ** ## Residuals 7 101.2 14.46 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that Treatment is WITHIN Leaf, not the other way around. Question: What are the relevant F-ratios here? Click for Answer Going back to our table for Blocked design, we see that the F-ratio is MS/MS_resid. Question: What is the biological interpretation here? Click for Answer Strongly innoculated tobacco leaves were found to have significantly higher mean numbers of lesions than weakly innoculated leaves. Leaf pairs (blocks) explained substantial amounts of the variation and therefore probably contributed to the sensitivity of the main test of treatment - thereby justifying the blocking design over a completely randomized design. How would we fit this with a mixed model? library(lme4) ## Loading required package: Matrix model5&lt;-lmer(Number~Treatment+(1|Leaf),tobacco) summary(model5) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Number ~ Treatment + (1 | Leaf) ## Data: tobacco ## ## REML criterion at convergence: 88.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.61850 -0.48453 0.01133 0.40900 1.42778 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Leaf (Intercept) 13.63 3.692 ## Residual 14.46 3.803 ## Number of obs: 16, groups: Leaf, 8 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 34.940 1.874 18.645 ## TreatmentWeak -7.879 1.901 -4.144 ## ## Correlation of Fixed Effects: ## (Intr) ## TreatmentWk -0.507 23.5 Example #5: Nested design In an experiment on eye color, each male fly is mated with four different female flies. Two offspring are born from each mating, and the intensity of eye color among these offspring are measured. The question is, how much variation in eye color is due to differences between females and how much is due to differences between males? flies&lt;-read.table(&quot;_data/flies.txt&quot;,header=T) flies$male&lt;-factor(flies$male) flies$female&lt;-factor(flies$female) model6&lt;-aov(eye~male/female+Error(male/female),data=flies) summary(model6) ## ## Error: male ## Df Sum Sq Mean Sq ## male 2 665.7 332.8 ## ## Error: male:female ## Df Sum Sq Mean Sq ## male:female 9 1721 191.2 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 12 15.62 1.302 Question: What is the interpretation of this result? Click for Answer Females within male are much more important! "],["week-13-lecture.html", "24 Week 13 Lecture 24.1 Week 13 Readings 24.2 Model criticism 24.3 Residuals 24.4 Leverage 24.5 Influence 24.6 Comparing residuals, leverage, and influence 24.7 Residuals for GLMs 24.8 Model selection vs. model criticism 24.9 Comparing two models 24.10 Model weighting 24.11 Stepwise regression 24.12 Week 13 FAQ", " 24 Week 13 Lecture 24.1 Week 13 Readings The topic covered this week is huge, and here I provide a number of readings that might be helpful to your understanding: Aho et al. (2014), Burnham et al. (2011), Fox (2002), Grueber et al. (2011), Hutto (2012), Johnson and Omland (2004), Nakagawa and Cuthill (2007), Tong (2019), and Zuur et al. (2010). 24.2 Model criticism Before we can decide on what the “best” model is, we need to decide what we mean by the best model. There are two reasonable definitions of the best model based on the two separate and distinct goals of testing a mechanism and making predictions. \\[ Y = f(X) + \\epsilon \\] Models are always imperfect representation of reality (“All models are wrong but some are useful”). However, when you fit a model, you need to show that the model is a good representation of the data. This is important because if your data do not fit the assumptions of the model, the model can misrepresent the data. Model criticism and selection involves three parts: Assumptions/Robustness: Are any of the assumptions of the model violated? Are any model assumptions having an undue impact on the results? Assessment: Does the model provide adequate fit to the data? Selection: Which model (or models) should we choose for final inference or prediction? The difference between the predicted value (based on the regression equation) and the actual, observed value. Remember that a lot of the assumptions for linear regression have to do with the residuals, (e.g., we often assumed \\(\\epsilon_i \\sim \\mathrm{N} (0, \\sigma^2)\\)). Plots of residuals should not change with the fitted values. Sometimes, systematic features in residual plots can suggest specific failures of model assumptions. We will look at a past Biometry exam question to both practice for the exam and link what we are talking about today to past lectures. Below are four plots depicting the residuals of a linear model plotted as a function of \\(\\hat{Y}\\). For each panel, state whether the model violates any of the assumptions of linear regression and, if yes, which assumption(s) of linear regression are violated. No apparent violations Variance is heteroscedastic (not constant). Larger fitted values have larger variance. Depending on your data, what type of regression may be more appropriate here? Poisson regression (why? mean = variance). You could also consider transforming the data or fitting a weighted regression to resolve this pattern. Linear pattern in the residuals, the errors are not independent Nonlinear pattern in the residuals (consider transforming data or fitting the predictor quadratically). 24.3 Residuals In linear regression, an outlier is an observation with a large residual. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity, a data entry error, or other problem. 24.4 Leverage An observation with an extreme value for a predictor variable/independent variable/covariate (often \\(X\\)) is a point with high leverage. Leverage for a point is high when \\((X - \\bar{X})^2\\) is large. X-values further from \\(\\bar{X}\\) influence \\(\\hat{Y}_i\\) more than those close to \\(\\bar{X}\\). High leverage points may have a large effect on the estimate of regression coefficients. The leverage of the \\(i^{\\text{th}}\\) point is called \\(h_{ii}\\) (the \\(i^{\\text{th}}\\) diagonal element in the hat matrix). The hat matrix relates the response \\(Y\\) to the predicted response \\(\\hat{Y}\\). This hat value, \\(h_{ii}\\) thus is a measure of the contribution of data point \\(Y_i\\) to all fitted values \\(\\hat{Y}\\). Average leverage is considered to be equal to \\(p / n\\), or the number of parameters divided by the sample size. Twice the average leverage is considered high. 24.5 Influence An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. Influence can be thought of as the product of leverage and outlierness. We assess influence by looking at the effect of deleting that data point. We compare the least squares estimate of coefficient \\(\\beta\\) for the full dataset to the least squares estimate of coefficient \\(\\beta\\) when the \\(i^{\\text{th}}\\) data point is removed. \\[ \\text{Influence} \\propto \\hat{\\beta} - \\hat{\\beta}_{-i} \\] 24.6 Comparing residuals, leverage, and influence The red and the blue points have large residuals. The red and the green points have high leverage (large X values). Only the red point here has high influence. Note that in your own work you would support these claims with statistics (which we will go over next). Important: Outliers might not be influential. (Depends on the \\(X\\) value) High leverage values may not be influential. (May fit right in with predicted line) Influential observations may not be outliers. (May just have high leverage) We will use our two linear regressions of spider web length as a function of temperature, web.fit, and percent of males earning $3500 or more as a function of high school graduation rate, Duncan.fit, to go through the process of diagnosing a model. Do these models meet the assumptions of linear regression? Do they fit the data well? Remember that certain analyses have certain assumptions. With a Poisson regression, you need to check for overdispersion or an abundance of zeros, and use the appropriate model. If you have more than one covariate, you need to assess collinearity, which inflates Type II error and may mask covariate significance. We will go through this process in more detail in lab. Are the residuals normally distributed? Histogram the residuals. They should be centered on zero and approximately normal (assuming a basic linear regression). Remember that with some analyses, like ANOVA, you assume normality within each group, so you would not look at all residuals together in that case. Also note that some analyses are fairly robust to non-normality. hist(web.fit$residuals) Are the residuals normally distributed? Look at a Q-Q plot comparing the quantiles of the residuals against the quantiles of the distribution we expect them to match (usually normal). plot(web.fit, which = 2) Are the residuals independent and homoscedasctic? Plot the residuals vs. the fitted values. The residuals should be uncorrelated with the fitted values and the variance of the residuals should not change as a function of the fitted values. plot(web.fit, which = 1) Are the residuals normally distributed? We can formally check the residuals are normally distributed using the Kolmogorov-Smirnov test (null hypothesis is that the two vectors come from the same distribution). In this case, we would check against a normal distribution with mean of 0 and standard deviation of \\(\\sigma\\) from the regression. ks.test(web.fit$residuals, y = &quot;pnorm&quot;, mean = 0, sd = summary(web.fit)$sigma) ## ## One-sample Kolmogorov-Smirnov test ## ## data: web.fit$residuals ## D = 0.075099, p-value = 0.9205 ## alternative hypothesis: two-sided Question: How do we interpret the results of this hypothesis test? Click for Answer We have insufficient evidence to reject the null hypothesis that the residuals are normally distributed (both samples come from the same distribution). Are the residuals independent? Use the Durbin-Watson test, where the null hypothesis is that there is no correlation among residuals. If errors are correlated, we cannot assume that our errors are independent (an assumption of regression). These kind of correlations may be temporal or spatial, and can be controlled for with different modeling approaches. See Zuur et al. (2010) “Step 8” for details on how to deal with correlated errors. \\[d = \\frac{\\sum_{i = 2}^n (\\epsilon_i - \\epsilon_{i - 1})^2}{\\sum_{i = 1}^n (\\epsilon_i)^2}\\] library(car) durbinWatsonTest(web.fit) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.03369874 2.059546 0.982 ## Alternative hypothesis: rho != 0 Question: How do we interpret the results of this hypothesis test? Click for Answer We have insufficient evidence to reject the null hypothesis that the residuals are independent. Also be sure to look for outliers and points with high influence. There are a couple of ways you can determine if a point really is an outlier. We will use the Duncan regression to assess outliers, Duncan.fit. What do you do with an outlier? Well… Figure 22.1: One possible solution to outliers. (No!! Just kidding!! Do not do this.) Source: John Fox, Regression Diagnostics (1991) Don’t remove it unless you can verifiably say it was recorded or measured in error. Present the model with and without outliers, providing an explanation for the outliers (is there a potential unmeasured covariate that could explain the outlier?) Use a robust method (tests based on rank, weighted models, etc.). Calculate the leave-one-out residuals for each data point and scale them by the standard deviation of the leave-one-out residuals. Data points with \\(t_i &gt; 2\\) are potential outliers, \\(t_i &gt; 3\\) are more serious outliers. \\[ t_i = \\frac{Y_i - \\hat{Y}_{i, (-i)}}{\\sqrt{\\mathrm{Var} (Y_i - \\hat{Y}_{i, (-i)})}} \\] library(MASS) Duncan[outliers, ] ## type income education prestige ## conductor wc 76 34 38 ## RR.engineer bc 81 28 67 studres(Duncan.fit)[which(studres(Duncan.fit) &gt; 2)] ## conductor RR.engineer ## 2.919721 3.646444 Cook’s distances are related to the studentized residuals in that both use leave-one-out residuals: \\[ D_i = \\frac{\\sum_{j = 1}^n (\\hat{Y}_j - \\hat{Y}_{j, (-i)})^2}{p \\times \\mathrm{MSE}} \\] where p is number of parameters in the model, and MSE is the mean squared error. There are different cut-offs for what might be considered significant, but D &gt; 1 would be a basic guideline. Another guideline from Chatterjee and Hadi is to adjust D for sample size, \\(4 / (n - p - 1)\\). Cook’s distance describes the influence of a point (a function of both leverage and outlierness). cutoff &lt;- 4 / (nrow(Duncan) - 3 - 1) cooks.distance(Duncan.fit)[which(cooks.distance(Duncan.fit) &gt; cutoff)] # in car package ## minister conductor RR.engineer ## 0.1415350 0.1162735 0.2025096 24.7 Residuals for GLMs Note that the definition of residuals for GLMs is a bit different. Residuals can be defined as \\(\\hat{Y}_i - Y_i\\). For normally distributed linear models, this is equal to the statistical error \\(\\epsilon_i = \\mathrm{E}(Y_i) - Y_i\\). Remember that \\(\\mathrm{E}(Y_i)\\) or \\(\\hat{Y}_i\\) for a binomial or Poisson GLM is \\(\\hat{p_i}\\) or \\(\\hat{\\lambda_i}\\), respectively, so not on the same scale as the data (\\(Y_i\\) for binomial or Poisson is an integer 0 through n). Also, nonconstant variance is an inherent part of GLMs. We can use different types of residuals for model criticism of GLMs. We can look at the residuals on the scale of the fitted values or response (using the inverse link function): residuals.glm(crab.glm, type = \"response\"), or residuals that are scaled by the variance inherent to the GLM: deviance residuals type = \"deviance\" or Pearson residuals type = \"pearson\" (see 9.20.7.1 in Aho for details). R provides these in diagnostic plots. crabs &lt;- read.csv(file = &quot;_data/crabs.csv&quot;) crab.glm &lt;- glm(satell ~ width, family = &quot;poisson&quot;, data = crabs) plot(crab.glm, which = 1) crab.glm &lt;- glm(satell ~ width, family = &quot;poisson&quot;, data = crabs) plot(crab.glm, which = 2) crab.glm &lt;- glm(satell ~ width, family = &quot;poisson&quot;, data = crabs) plot(crab.glm, which = 5) 24.8 Model selection vs. model criticism Before we move on to model selection, let’s review the difference between model selection and model criticism: Model criticism involves determining whether the model is appropriate, meets assumptions, and accurately represents the data. Model selection gives us the best model, the best suite of models, or the best averaged model. Your analyses should always involve model criticism, and may also additionally involve model selection. To decide what model is best, we first need to determine what our goal was in modeling. If our goal was to make predictions, or definition of what makes a model the best is different from if our goal was to test a mechanism. We will revisit some ideas from the Shmueli (2010) paper that was assigned Week 8. If our goal is to explain, then we are interested in the amount of variation in the response variable that is explained by each of the covariates. With mechanism-based, explanatory models, we use Occam’s razor—the law of parsimony. By applying this principle, we identify models that are simple but effective, and we do not consider models that are needlessly complex. In other words, given two models, we prefer the smaller model unless the larger model is significantly better. Additionally, all parameters should be biologically important, as identified by the modeler. If our goal is to predict, then we want to minimize the expected prediction error, or our expectation of the difference between a new predicted value and the fitted value. For a model: \\[ Y = f(X) + \\epsilon, \\text{ where } \\epsilon \\sim \\mathrm{N} (0, \\sigma^2 ) \\] our expected prediction error of new data point \\((X^\\ast, Y^\\ast)\\) is: \\[ \\text{EPE} = \\mathrm{E} [ (Y^\\ast - \\hat{Y})^2] = \\mathrm{E} [ (Y^\\ast - f(X^\\ast))^2] \\] The expected prediction error boils down to three components of variance: \\[ \\sigma^2 + (\\text{bias}_{\\text{prediction}})^2 + \\mathrm{Var} (\\text{prediction}) \\] \\(\\sigma^2\\) represents the variation inherent to the process. This error is irreducible. Bias, \\((\\text{bias}_{\\text{prediction}})^2\\), represents the difference between the expected prediction of our model (if we could repeat the process of collecting data and building the model many times) and the underlying true value we are trying to predict. Variance, \\(\\mathrm{Var} (\\text{prediction})\\), represents the variability of model predictions for a given data point, if we could repeat the data-gathering and model building process many times. If we had both: 1) the true model (an exact picture of the true mean behavior of the mechanism), and 2) infinite data (no random fluctuations due to finite sampling), we could theoretically minimize both bias and variance. However, that’s not the case in reality, and we tend to find a trade-off between bias and variance. \\[ \\sigma^2 + (\\text{bias}_{\\text{prediction}})^2 + \\mathrm{Var} (\\text{prediction}) \\] 24.9 Comparing two models If we are generating models to test mechanisms, we can use the following three criteria to decide which is best. All three use the likelihood of the models. Likelihood ratio test (LRT) Akaike’s Information Criterion (AIC) Bayesian Information Criterion (BIC) The likelihood ratio test is one way of comparing two nested models. Before we get into the details of the LRT, let’s review for a second what we mean by nested. 24.9.1 Nested or not? Question: Are these models nested? \\[ Y \\sim X_1 + X_2 + X_3 \\\\ Y \\sim X_1 + X_2 + X_4 \\] Click for Answer No, neither model is a subset of the other Question: What about these models? \\[ Y \\sim X_1 + X_2 + X_3 \\\\ Y \\sim X_1 + X_2 \\] Click for Answer Yes, the second model is a subset of the first model Note that this usage of “nested” is completely different from the way we used it with nested ANOVAs. Question: Nested? \\[ Y = a \\\\ Y = a + b X \\] Click for Answer Nested. The second model is equal to the first model with \\(b = 0\\). Question: Nested? \\[ Y = a + b X \\\\ Y = a + b X + c X^2 \\] Click for Answer Nested. The second model is equal to the first model with \\(c = 0\\). Question: Nested? \\[ Y = r \\left ( 1 - \\frac{N_t}{K} \\right ) \\\\ Y = r \\left ( 1 - \\left ( \\frac{N_t}{K} \\right ) ^\\gamma \\right ) \\] Click for Answer Nested. The second model is equal to the first model with \\(\\gamma = 1\\). This model is called the Ricker model (top model is the usual form and the bottom model is the generalized version). It’s used in ecology to model the abundance of animal populations with density dependence. 24.9.2 Likelihood Ratio Test (LRT) Likelihoods are crucial for model selection, they can be used generally for a wide variety of models. We’ll start with a brief review of likelihood. When we were fitting statistical distributions to data, we assumed that each data point was independent. Therefore, for independent events \\(X_1\\), \\(X_2\\), and \\(X_3\\): \\[ \\mathrm{P}(X_1 \\cap X_2 \\cap X_3) = \\mathrm{P}(X_1) \\times \\mathrm{P}(X_2) \\times \\mathrm{P}(X_3) \\] We then calculate the joint density of the data as a function of the unknown parameters (multiplying the probability of each data point given the PDF and as a function of the unknown parameters): \\[ \\mathcal{L}(\\text{parameters} | \\text{data}) = \\prod^N_{i = 1} \\text{PDF} (X_i | \\text{parameters}) \\] Last we find the value(s) of the parameter(s) that maximize the likelihood (in practice, by minimizing the log-likelihood). Now we are using the same idea to fit models to data: \\[ \\mathcal{L}(\\text{model parameters} | \\text{data}) = \\prod^N_{i = 1} \\text{PDF} (X_i | \\text{model parameters}) \\] Question: What are the parameters in a linear regression with one continuous predictor? Click for Answer \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) Question: What are the parameters in a linear model with one categorical predictor (which has four levels)? Click for Answer \\(\\mu\\), \\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\alpha_3\\) (depends on how you code the model, but there will be four parameters describing the mean behavior if the model is not overparameterized), and \\(\\sigma\\) Question: What are the parameters in a Poisson generalized linear model with one continuous predictor? Hint: \\(Y_i \\sim \\mathrm{Pois} (\\lambda_i), \\quad log(\\lambda_i) = \\beta_0 + \\beta_1 X_i\\) Click for Answer \\(\\beta_0\\) and \\(\\beta_1\\) Remember the deviance difference from Week 10? Under the null hypothesis that the larger model fits no better than the smaller model, the deviance difference goes as a chi-square distribution. \\[D = 2 * \\log \\frac{\\mathcal{L}_{\\text{larger model}}}{\\mathcal{L}_{\\text{smaller model}}} = 2 (\\ell_{\\text{larger model}} - \\ell_{\\text{smaller model}}) \\\\ D | H_0 \\sim \\chi^2_{\\text{additional parameters in larger model}}\\] This is called a likelihood ratio test. A large value for \\(D\\) (likelihood ratio), unexpected under the null distribution, would indicate a significant improvement in using the larger model. In this case, we would prefer the larger model over the smaller model. Note that we can only do this for nested models. Question: Will the likelihood be larger for the larger model or for the smaller model? Click for Answer The likelihood will always be larger for the larger model (more parameters = more ability to fit the data). Question: Will the deviance difference, D, be positive or negative? Click for Answer Because the likelihood will always be larger for the larger model, D will always be positive. This makes sense because the \\(\\chi^2\\) distribution is always positive. Duncan.smaller &lt;- lm(formula = prestige ~ income, data = Duncan) Duncan.larger &lt;- lm(formula = prestige ~ income + education, data = Duncan) logLik(Duncan.larger) ## &#39;log Lik.&#39; -178.9822 (df=4) logLik(Duncan.smaller) ## &#39;log Lik.&#39; -191.3776 (df=3) Duncan.D &lt;- 2 * (logLik(Duncan.larger) - logLik(Duncan.smaller)) Duncan.D ## &#39;log Lik.&#39; 24.79077 (df=4) pchisq(q = Duncan.D, df = 1, lower.tail = FALSE) ## &#39;log Lik.&#39; 6.390288e-07 (df=4) xvals &lt;- seq(0, 40, by = 0.1) par(mar = c(2, 2, 2, 2)) plot(x = xvals, y = dchisq(x = xvals, df = 1), type = &quot;l&quot;, xlab = &quot;Quantiles of chi-square[DOF = 1]&quot;, ylab = &quot;Probability&quot;) abline(v = qchisq(0.95, df = 1), col = &quot;indianred2&quot;) text(x = qchisq(0.95, df = 1) + 4, y = 0.5, col = &quot;indianred2&quot;, labels = &quot;Critical value&quot;) abline(v = Duncan.D, col = &quot;dodgerblue2&quot;) text(x = Duncan.D + 6, y = 0.5, col = &quot;dodgerblue2&quot;, labels = &quot;Observed test stat.&quot;) Question: How do we interpret the results of our likelihood ratio test? Click for Answer The larger model explains significantly more variation in the data than we would expect. 24.9.3 Akaike’s Information Criterion (AIC) AIC is just one of the many information theoretic alternatives that have been developed to compare the “divergence” between our particular model (which is an approximation of the true mechanism) and the “true” model (the mechanism that’s actually going on in the real world). We always lose some information when approximating the truth using a model. If you have a set of models that you could consider different alternative hypotheses, information theoretic methods allow you to rank them. In practice, all information theoretic methods reduce to finding the model that minimizes some criterion that includes the sum of two terms: one is based on the likelihood and the other is a penalty term which penalizes for increasing model complexity. For AIC: \\[ \\mathrm{AIC} = -2 \\ell + 2 k \\] where \\(\\ell\\) is the log-likelihood and \\(k\\) is the total number of parameters in the model (including variance as a parameter when appropriate, \\(\\sigma^2\\)). Smaller AIC means better model fit. -2 * logLik(Duncan.smaller) + 2 * 3 ## &#39;log Lik.&#39; 388.7552 (df=3) -2 * logLik(Duncan.larger) + 2 * 4 ## &#39;log Lik.&#39; 365.9645 (df=4) Question: Which model fits best? Click for Answer The larger model. \\[ \\mathrm{AIC} = -2 \\ell + 2 k \\] The penalty term in AIC enforces parsimony, because adding an additional parameter will increase AIC by 2 unless the more complex model considerably increases the log-likelihood. When you have small sample size, or \\(n / k &lt; 40\\), use the small sample corrected AIC (AICc). This converges to AIC when sample size is large. \\[ \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2 k (k + 1)}{n - k - 1} \\] AIC(Duncan.smaller) + (2 * 3 * (3 + 1)) / (nrow(Duncan) - 3 - 1) ## [1] 389.3406 AIC(Duncan.larger) + (2 * 4 * (4 + 1)) / (nrow(Duncan) - 4 - 1) ## [1] 366.9645 Question: Which model fits best? Click for Answer The larger model. We can calculate the AIC of all possible candidate models and compare by calculating the difference in AIC between model i and the AIC of the best candidate model: \\[ \\Delta \\mathrm{AIC}_i = \\mathrm{AIC}_i - \\mathrm{AIC}_{\\min} \\] Information Criterion methods do not allow us say that one model is significantly better. We do not have a sampling distribution for differences in AIC, and we cannot use AIC to calculate a P-value for the significance of a model term. That said, there are some rules of thumb: \\(\\Delta \\mathrm{AIC} &lt; 2\\) are considered equivalent \\(4 &lt; \\Delta \\mathrm{AIC} &lt; 7\\) are considered clearly different \\(\\Delta \\mathrm{AIC} &gt; 10\\) are definitely different Question: Given this definition, is the larger model definitely still the better model? Click for Answer Yes, \\(\\Delta \\mathrm{AIC} &gt; 10\\). 24.9.4 Bayesian Information Criterion (BIC) BIC uses the same idea as AIC, but with a different penalty for model complexity. This is sometimes called the Schwarz criterion. Smaller BIC means better model fit. \\[ \\mathrm{BIC} = -2 \\ell + 2 k \\times \\ln(n) \\] -2 * logLik(Duncan.smaller) + 2 * 3 * log(nrow(Duncan)) ## &#39;log Lik.&#39; 405.5952 (df=3) -2 * logLik(Duncan.larger) + 2 * 4 * log(nrow(Duncan)) ## &#39;log Lik.&#39; 388.4178 (df=4) Question: Which model fits best? Click for Answer The larger model. The penalty for BIC is different than for AIC. For AIC, the probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on \\(k\\) but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as sample size, \\(n\\), gets large. Proponents of BIC like that it tends to select simpler models (fewer parameters) relative to AIC, because it is a consistent estimator of model fit. However, Johnson and Omland (2004) include a strong critique of BIC. They state that it is not based in KL information theory, and it relies on the assumption that there is one true model and it is contained in the set of candidate models. 24.9.5 Comparing LRT and AIC/BIC LRT allows you to put a p-value on model terms. Because you have a sampling distribution for the difference in deviances, you can compare the difference in deviance to the quantiles on the chi-squared distribution to get a p-value. AIC/BIC values do not allow you to refer to one model as being significantly different than another. No p-values. LRT require nested models and Information Theoretic Criterion like AIC do not. This is a major advantage for AIC/BIC! Information theoretic critera (AIC, AICc, BIC) allows you to compare all candidate models at once. We’ll go through the process of model seelction and model averaging using AIC as an example, though you could use the same approach with any criterion. Decide on your candidate set of models Compute the likelihood and AIC for each of the candidate models Rank the models in order of increasing AIC After that, you have three options: Option 1: Choose the model with the lowest AIC. This is not recommended! You may have several models that are very close in AIC, so it is arbitrary to select the lowest AIC model as the unambiguously best model. Option 2: Report all the models with \\(\\Delta \\mathrm{AIC} = 2\\) of the best model. This is better, because you are reporting all models that are basically indistinguishable from the best model, but there is no way to say how much better one model is from another. Option 3: Model weighting. You consider the parameter estimates from all candidate models, where the better models are weighted more than the worse models. Burnham and Anderson define model weights from the AIC values, where the weight is the proportion of times under repeated sampling that the \\(i^{\\text{th}}\\) model would be the best model. \\[ \\Delta_i = \\mathrm{AIC}_i - \\min (\\mathrm{AIC}) \\\\ w_i = \\frac{\\mathrm{e}^{-\\Delta_i / 2}}{\\sum_{i = 1}^M \\mathrm{e}^{-\\Delta_i / 2}} \\] The denominator is summed over all candidate models. Burnham et al. (2011) describe the model weight as the strength of evidence, or the probability of model \\(i\\) given the data and all \\(M\\) models. 24.10 Model weighting Model weights provide a number of benefits: They explicitly address the fact that you don’t know the best model, so it is a straightforward way of presenting the relative strength of evidence for each of the models in the set of candidate models. The model weights present the possibility of calculating weighted parameter estimates. This is the idea behind AIC model averaging. With AIC model averaging, we use the Akaike weights to weight the parameter estimates and variances (i.e., standard errors) from each model and combine those. Thus, we incorporate model selection uncertainty directly into the parameter estimates via the Akaike weights. Model weighting and model averaging account for two types of uncertainty: the uncertainty about the parameters themselves, and the uncertainty about the model. Let’s work through an example. Let’s say we have four possible candidate models – note that they are linear but not nested. Weighted parameters only work with linear models because the parameters have the same interpretation across the models. \\[ \\text{Model 1}: Y = a, \\quad \\hat{a} = 1 \\\\ \\text{Model 2}: Y = a + b X_1, \\quad \\hat{a} = 2, \\hat{b} = 3 \\\\ \\text{Model 3}: Y = a + b X_1 + c X_2, \\quad \\hat{a} = 1.5, \\hat{b} = 2, \\hat{c} = 7 \\\\ \\text{Model 4}: Y = a + b X_1 + d X_3, \\quad \\hat{a} = 1.2, \\hat{b} = 4, \\hat{d} = 4 \\] Let’s say we calculated the following model weights: \\[ \\Delta_i = \\mathrm{AIC}_i - \\min (\\mathrm{AIC}) \\\\ w_i = \\frac{\\mathrm{e}^{-\\Delta_i / 2}}{\\sum_{i = 1}^M \\mathrm{e}^{-\\Delta_i / 2}} \\] Model 1: w = 0.6, Model 2: w = 0.1, Model 3: w = 0.15, Model 4: w = 0.15, The model averaged estimate of parameter \\(a\\) would be calculated by \\[ 0.6(1) + 0.1(2) + 0.15(1.5) + 0.15(1.2) = 1.205 \\] How do we calculate the model averaged estimate of \\(b\\)? This is a little trickier, because \\(b\\) only appears in models 2, 3, 4. One method we can use is to calculate new weights for the only set of models in which \\(b\\) appears, so our new model weights are: \\[ \\text{Model 2}: w = 0.1 / (0.1 + 0.15 + 0.15) = 0.25 \\\\ \\text{Models 3 and 4}:w = 0.15 / (0.1 + 0.15 + 0.15) = 0.375 \\] and our model averaged estimate for \\(b\\) is: \\[ 0.25(3) + 0.375(2) + 0.375(4) = 3 \\] However, this doesn’t take into account that \\(b\\) may not appear in a model because it doesn’t explain much variation in the response. We could also set \\(b\\) to zero in models where it isn’t included. This essentially “shrinks” the parameter estimate for \\(b\\) if it doesn’t appear in many models, bringing it closer to zero. \\[ 0.6(0) + 0.1(3) + 0.15(2) + 0.15(4) = 1.2 \\] Be sure to explicitly state which of these two methods you used to weight parameter estimates in a report/thesis/manuscript. What happens if all of your models are pretty bad, and you use model selection methods like the ones we’ve been discussing? You’ll still get a best/most parsimonious model! You need to also provide evidence of model explanatory power using metrics like the coefficient of determination \\(R^2\\) (or adjusted \\(R^2\\), or marginal/conditional \\(R^2\\) for mixed/hierarchical models, or goodness-of-fit tests) in addition to model fit diagnostics like AIC. Also don’t forget about your coefficient hypothesis tests! See Mac Nally et al. (2017) for more on comparing model fit and model explanatory power. \\[ R^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = \\frac{\\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i = 1}^n (Y_i - \\bar{Y})^2} \\] Which models to include in model selection? In many situations, you might have a large number of potential covariates to include in a model, all of which could have biological significance to your system. You have many choices to make as the investigator. You could: Generate a subset of biologically reasonable models to test. In this case you would support your selections by whatever theory is reasonable for your system. Test all possible subsets of models. In this case, you would calculate AIC/BIC for models with all possible combinations of covariates. This becomes computationally expensive as the list of predictors increases in length, especially if you are testing for interactions as well as main effects. Use stepwise regression. Here, you would add in or remove variables one at a time to find the best model based on some criterion. 24.11 Stepwise regression With stepwise regression, you add in or remove parameters one at a time, then a test is done to check whether some variables can be deleted without appreciably increasing the residual sum of squares (RSS) or some other criterion (for example, AIC can be used). The procedure stops when the available improvement falls below some critical value. Using this approach, you do not test all possible models, because the algorithm only compares models to the optimal model from the previous step. The main approaches are: Forward selection, which involves starting with no variables in the model, trying out the variables one by one and including them if they are ‘statistically significant’. Backward elimination, which involves starting with all candidate variables and testing them one by one for statistical significance, deleting any that are not significant. Methods that are a combination of the above, testing at each stage for variables to be included or excluded. 24.11.1 Criticism of stepwise regression A sequence of F-tests is often used to control the inclusion or exclusion of variables, but these are carried out on the same data and so there will be problems of multiple comparisons for which many correction criteria have been developed. It is therefore difficult to interpret the P-values associated with these tests, since each is conditional on the previous tests of inclusion and exclusion (see “dependent tests” in false discovery rate). When estimating the degrees of freedom, the number of the candidate independent variables from the best fit selected is smaller than the total number of final model variables, causing the fit to appear better than it is when adjusting the \\(R^2\\) value for the number of degrees of freedom. It is important to consider how many degrees of freedom have been used in the entire model, not just count the number of independent variables in the resulting fit. 24.11.2 Criticism of data dredging Automated model selection can be considered data dredging (methods 2 and 3). Critics regard data dredging as substituting intense computation for subject area expertise. You may have many predictors even in a carefully thought out model because biological systems are complex and we may want to know which predictors, from a set of reasonable predictors, best explain the variation in the data. However, we should be careful to recognize that we don’t really understand the Type I error rate for the entire model selection process and our findings should be considered exploratory; where automated model selection has been used to find the best set of predictors, you may need to use that information to propose new hypotheses to be tested more rigorously with a new dataset. 24.11.3 Final thoughts on model selection If the goal is hypothesis testing, think carefully about the models ahead of time and consider a smaller set of candidate models, all of which make biological sense. If the goal is prediction, you can start with a wider set of candidate models, and you are less worried that the correlations stem from true causation. Don’t let the computation drive the biology. It is OK and often appropriate to leave covariates in the model even if they are not statistically significant if you believe them to be biologically significant. 24.12 Week 13 FAQ Question: How to we calculate explanatory power? \\[ r^{2} = \\frac{SSR}{SST} = \\frac{1-SSE}{SST} = \\frac{1-\\mbox{sum-of-squares error}}{\\mbox{sum-of-squares total}} \\] Going back to Week 9, we note that the sum-of-squares total is given by \\[ SST = \\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2} \\] and the sum-of-squares error \\[ SSE = \\sum_{i=1}^{n}(Y_{i}-\\hat{Y_{i}})^{2} \\] Question: How exactly does Mallow’s Cp work? Mallows Cp is used when you are comparing models with different numbers of parameters. If the model with p parameters is correct than Cp will tend to be close to or smaller than p. Mallows Cp will be close to p if the model is unbiased and so Mallows Cp is used if unbiasedness is a criteria of particular interest. Question: What is the rational behind the penalty factor in BIC and why does the penalty grow with n? If you are comparing two nested models, AIC is equivalent to using a cutoff for the difference in -2LogLik between the two models equal to 2*k, where k is the difference in the number of parameters between the two models. (Note that AIC does not require nested models, but the case of nested models allows us to understand the penalty factor for BIC.) The probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on “k” but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as n gets large. Question: How do we do model averaging when the parameter in question is not included in all models? In these cases, we have to restrict attention to the models that include the parameter of question, and recalculate the AIC weights within that subset. For example, in the example presented in lecture, “b” only appears in Models 2,3, and 4. Step 1 therefore is to recalculate AIC weights as follows: \\[ w_{AIC, model2} = \\frac{0.1}{0.1+0.15+0.15} = 0.25 \\] \\[ w_{AIC, model3} = \\frac{0.15}{0.1+0.15+0.15} = 0.375 \\] \\[ w_{AIC, model4} = \\frac{0.15}{0.1+0.15+0.15} = 0.375 \\] The weighted parameter estimate for “b” would then be \\[ \\hat{b} = (0.25 \\times 3) + (0.375 \\times 2) + (0.375 \\times 4) = 3 \\] "],["week-13-lab.html", "25 Week 13 Lab 25.1 Part 1: Model selection / model comparison 25.2 Model selection via step-wise regression 25.3 Part 2: Model criticism", " 25 Week 13 Lab In lab we’ll go through Model selection / model comparison Model criticism We will need a series of packages for today’s lab, some of which we have not used before: MASS, lmtest, MuMIn, car, and gvlma. 25.1 Part 1: Model selection / model comparison There is a frog dataset on the distribution of the Southern Corroboree frog that we are going to attach. More information on the definition of the covariates in this dataset can be found here. frogs&lt;-read.csv(&quot;_data/frogs.csv&quot;,header=T) head(frogs) ## X pres.abs northing easting altitude distance NoOfPools NoOfSites avrain ## 1 2 1 115 1047 1500 500 232 3 155.0000 ## 2 3 1 110 1042 1520 250 66 5 157.6667 ## 3 4 1 112 1040 1540 250 32 5 159.6667 ## 4 5 1 109 1033 1590 250 9 5 165.0000 ## 5 6 1 109 1032 1590 250 67 5 165.0000 ## 6 7 1 106 1018 1600 500 12 4 167.3333 ## meanmin meanmax ## 1 3.566667 14.00000 ## 2 3.466667 13.80000 ## 3 3.400000 13.60000 ## 4 3.200000 13.16667 ## 5 3.200000 13.16667 ## 6 3.133333 13.06667 attach(frogs) plot(northing ~ easting, pch=c(1,16)[frogs$pres.abs+1],xlab=&quot;Meters east&quot;, ylab=&quot;Meters north&quot;) Make sure you understand how the pch command is working in the above plot command. pairs(cbind(altitude,distance,NoOfPools,NoOfSites,avrain,meanmin,meanmax)) Looking at the data in this way, are there any covariates that might benefit from transformation? Question: Why bother transforming a covariate? Click for Answer As you can see from the scatterplots, some covariates have points with large leverage, and these point may (but don’t always) have high influence on model fit. Often we want all the points to have roughly equal influence on the model fit so we will apply a transformation like the log() to spread out small values all bunched together and to pull in large values that are much larger than the others. These transformations (e.hg., log, square root) are often referred to as “variance stabilizing” transformations. Let’s try log-transforming “distance” and “Number of Pools”. pairs(cbind(altitude,log(distance),log(NoOfPools),NoOfSites,avrain,meanmin,meanmax)) Let’s fit a GLM with all the variables. Checkpoint #1: Why a GLM? Because the response variable is binary 0/1. frogs.glm0&lt;-glm(pres.abs~altitude+log(distance)+log(NoOfPools)+NoOfSites+avrain+meanmin+meanmax,family=binomial,data=frogs,na.action=na.fail) summary(frogs.glm0) ## ## Call: ## glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + ## NoOfSites + avrain + meanmin + meanmax, family = binomial, ## data = frogs, na.action = na.fail) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9795 -0.7193 -0.2785 0.7964 2.5658 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.090e+01 1.327e+02 0.308 0.757845 ## altitude -6.648e-03 3.866e-02 -0.172 0.863466 ## log(distance) -7.593e-01 2.554e-01 -2.973 0.002945 ** ## log(NoOfPools) 5.727e-01 2.162e-01 2.649 0.008083 ** ## NoOfSites -8.979e-04 1.074e-01 -0.008 0.993330 ## avrain -6.793e-03 5.999e-02 -0.113 0.909848 ## meanmin 5.305e+00 1.543e+00 3.439 0.000584 *** ## meanmax -3.173e+00 4.839e+00 -0.656 0.512048 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 279.99 on 211 degrees of freedom ## Residual deviance: 197.62 on 204 degrees of freedom ## AIC: 213.62 ## ## Number of Fisher Scoring iterations: 5 (The na.action flag is so MuMIn won’t complain further down.) Anything look funny? Well, for one, meanmin is highly significant but meanmax and altitude are not - but we would expect these three variables to be highly correlated. Use the vif() function to explore this further vif(frogs.glm0) ## altitude log(distance) log(NoOfPools) NoOfSites avrain ## 850.879518 1.396014 1.306058 1.401175 16.664582 ## meanmin meanmax ## 29.319704 996.949239 It appears that the variances for altitude and meanmax are inflated. Checkpoint #2: Why? Plot the data: par(mfrow=c(2,1)) plot(altitude,meanmax) plot(altitude,meanmin) cor(altitude,meanmax) ## [1] -0.996557 cor(altitude,meanmin) ## [1] -0.953661 Question: So…what do we do? Answer: Let’s try removing the least biologically significant variable first. I suggest we remove altitude. frogs.glm1&lt;-glm(pres.abs~log(distance)+log(NoOfPools)+NoOfSites+avrain+meanmin+meanmax,family=binomial,data=frogs) summary(frogs.glm1) ## ## Call: ## glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax, family = binomial, data = frogs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9763 -0.7189 -0.2786 0.7970 2.5745 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 18.2689000 16.1381912 1.132 0.257622 ## log(distance) -0.7583198 0.2558117 -2.964 0.003033 ** ## log(NoOfPools) 0.5708953 0.2153335 2.651 0.008020 ** ## NoOfSites -0.0036201 0.1061469 -0.034 0.972794 ## avrain 0.0007003 0.0411710 0.017 0.986429 ## meanmin 5.3540724 1.5254665 3.510 0.000448 *** ## meanmax -2.3624614 1.0678821 -2.212 0.026947 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 279.99 on 211 degrees of freedom ## Residual deviance: 197.65 on 205 degrees of freedom ## AIC: 211.65 ## ## Number of Fisher Scoring iterations: 5 Better, but we still have a lot of multicollinearity between meanmin and meanmax. We could choose one or the other but at this point, let’s leave them both in and try and find the best model we can. First, let’s do a little review of the three comparison criteria we discussed on Tuesday: Likelihood (specifically, likelihood ratio) Akaike’s Information Criteria (AIC) Bayesian Information Criterion (BIC) We get the log-likelihood of a model in R using the logLik command logLik(frogs.glm0) ## &#39;log Lik.&#39; -98.81244 (df=8) logLik(frogs.glm1) ## &#39;log Lik.&#39; -98.82714 (df=7) How do we do the likelihood ratio test? Remember that \\[ -2*(LL(smaller)-LL(bigger)) \\sim \\chi^{2}_{\\mbox{difference in parameters}} \\] In R we can do this with test.stat&lt;--2*(logLik(frogs.glm1)-logLik(frogs.glm0)) as.numeric(test.stat) # as.numeric is just to suppress the labels which can be misleading ## [1] 0.02941079 as.numeric(1-pchisq(test.stat,df=1)) ## [1] 0.863834 Checkpoint #3: How do we interpret that p-value? Answer: This p-value is the probability that the larger model fits the data better only by the amount expected by its additional degree of freedom. In this case, we can not reject the null hypothesis that the two models are equivalent, and so we would prefer the smaller model on the basis of parsimony. We can actually get R to do the LRT automatically using the ‘lrtest’ function in the ‘lmtest’ package. library(&#39;lmtest&#39;) lrtest(frogs.glm0,frogs.glm1) ## Likelihood ratio test ## ## Model 1: pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## Model 2: pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + avrain + ## meanmin + meanmax ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 8 -98.812 ## 2 7 -98.827 -1 0.0294 0.8638 This gives us exactly the same result we got before. We could also have also compared these models by looking at the ANOVA table for comparison anova(frogs.glm0,frogs.glm1) ## Analysis of Deviance Table ## ## Model 1: pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## Model 2: pres.abs ~ log(distance) + log(NoOfPools) + NoOfSites + avrain + ## meanmin + meanmax ## Resid. Df Resid. Dev Df Deviance ## 1 204 197.62 ## 2 205 197.65 -1 -0.029411 How do we calculate AIC for these two models? logLik(frogs.glm0) ## &#39;log Lik.&#39; -98.81244 (df=8) k&lt;-8 AIC.glm0&lt;-as.numeric(-2*logLik(frogs.glm0)+2*k) AIC.glm0 ## [1] 213.6249 AIC(frogs.glm0) ## [1] 213.6249 AIC(frogs.glm1) ## [1] 211.6543 Question: Are these significantly different? Answer: This is a trick question. In an Information Theoretic context, we avoid the term “significant” because it is implied that we mean “statistically significant” in the context of hypothesis testing, and no hypothesis test is being performed. AIC simply gives us information on the weight of evidence for one model over another. There are no theoretically justified guidelines (although Burnham and Anderson suggest some as we discussed in lecture). It is better (in my opinion) to convert AICs into model weights and, if it makes sense in the context, to do model averaging. How about BIC? USe the AIC function with a different flag for the penalty. AIC(frogs.glm0,k=log(nrow(frogs))) ## [1] 240.4776 AIC(frogs.glm1,k=log(nrow(frogs))) ## [1] 235.1504 We could also do this with the BIC() function BIC(frogs.glm0) ## [1] 240.4776 BIC(frogs.glm1) ## [1] 235.1504 As a group, find a small set (3-5) of candidate models, calculate the AIC for each of these models, and calculate model weights. Checkpoint #4: What covariates were in the best performing model (among the ones you tried as a group) and what was its model weight? 25.2 Model selection via step-wise regression Ideally, we can narrow down the set of candidate covariates based on biology alone. Another approach that is common in the literature, but which has been criticized, is stepwise regression. The default of the step() function is to use both forward and backward steps. If the function step is given the full model, it will begin with the full model, no matter which direction it is working. step(frogs.glm0) #same as step(frogs.glm0,direction=&quot;both&quot;) ## Start: AIC=213.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## ## Df Deviance AIC ## - NoOfSites 1 197.62 211.62 ## - avrain 1 197.64 211.64 ## - altitude 1 197.65 211.65 ## - meanmax 1 198.05 212.05 ## &lt;none&gt; 197.62 213.62 ## - log(NoOfPools) 1 205.31 219.31 ## - log(distance) 1 207.22 221.22 ## - meanmin 1 211.27 225.27 ## ## Step: AIC=211.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + ## meanmin + meanmax ## ## Df Deviance AIC ## - avrain 1 197.64 209.64 ## - altitude 1 197.66 209.66 ## - meanmax 1 198.06 210.06 ## &lt;none&gt; 197.62 211.62 ## - log(NoOfPools) 1 205.31 217.31 ## - log(distance) 1 209.80 221.80 ## - meanmin 1 211.31 223.31 ## ## Step: AIC=209.64 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + ## meanmax ## ## Df Deviance AIC ## - altitude 1 197.66 207.66 ## - meanmax 1 198.74 208.74 ## &lt;none&gt; 197.64 209.64 ## - log(NoOfPools) 1 205.31 215.31 ## - log(distance) 1 209.88 219.88 ## - meanmin 1 213.32 223.32 ## ## Step: AIC=207.66 ## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax ## ## Df Deviance AIC ## &lt;none&gt; 197.66 207.66 ## - log(NoOfPools) 1 205.34 213.34 ## - log(distance) 1 209.91 217.91 ## - meanmax 1 214.18 222.18 ## - meanmin 1 222.40 230.40 ## ## Call: glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + ## meanmax, family = binomial, data = frogs, na.action = na.fail) ## ## Coefficients: ## (Intercept) log(distance) log(NoOfPools) meanmin meanmax ## 18.5268 -0.7547 0.5707 5.3791 -2.3821 ## ## Degrees of Freedom: 211 Total (i.e. Null); 207 Residual ## Null Deviance: 280 ## Residual Deviance: 197.7 AIC: 207.7 Now compare this to what you would get from step(frogs.glm0,direction=&quot;backward&quot;) ## Start: AIC=213.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## ## Df Deviance AIC ## - NoOfSites 1 197.62 211.62 ## - avrain 1 197.64 211.64 ## - altitude 1 197.65 211.65 ## - meanmax 1 198.05 212.05 ## &lt;none&gt; 197.62 213.62 ## - log(NoOfPools) 1 205.31 219.31 ## - log(distance) 1 207.22 221.22 ## - meanmin 1 211.27 225.27 ## ## Step: AIC=211.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + ## meanmin + meanmax ## ## Df Deviance AIC ## - avrain 1 197.64 209.64 ## - altitude 1 197.66 209.66 ## - meanmax 1 198.06 210.06 ## &lt;none&gt; 197.62 211.62 ## - log(NoOfPools) 1 205.31 217.31 ## - log(distance) 1 209.80 221.80 ## - meanmin 1 211.31 223.31 ## ## Step: AIC=209.64 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + ## meanmax ## ## Df Deviance AIC ## - altitude 1 197.66 207.66 ## - meanmax 1 198.74 208.74 ## &lt;none&gt; 197.64 209.64 ## - log(NoOfPools) 1 205.31 215.31 ## - log(distance) 1 209.88 219.88 ## - meanmin 1 213.32 223.32 ## ## Step: AIC=207.66 ## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax ## ## Df Deviance AIC ## &lt;none&gt; 197.66 207.66 ## - log(NoOfPools) 1 205.34 213.34 ## - log(distance) 1 209.91 217.91 ## - meanmax 1 214.18 222.18 ## - meanmin 1 222.40 230.40 ## ## Call: glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + ## meanmax, family = binomial, data = frogs, na.action = na.fail) ## ## Coefficients: ## (Intercept) log(distance) log(NoOfPools) meanmin meanmax ## 18.5268 -0.7547 0.5707 5.3791 -2.3821 ## ## Degrees of Freedom: 211 Total (i.e. Null); 207 Residual ## Null Deviance: 280 ## Residual Deviance: 197.7 AIC: 207.7 The output can be interpreted as follows: the full model has an AIC of 213.62, but if NoOfSites were removed the subsequent model would have an AIC of 211.62, if avrain were removed 211.64, etc, including \\(&lt;none&gt;\\), which represents removing no variables. The covariates are listed in order of lowest AIC if they were to be removed, to highest AIC. Since removing NoOfSites would result in the lowest AIC, including being lower than the current model, \\(&lt;none&gt;\\), NoOfSites is removed for the next step and the process is repeated over again. This continues until \\(&lt;none&gt;\\) has the lowest AIC, meaning no variables can be removed to decrease the AIC from the current model. In this case, this occurred when log(NoOfPools), log(distance), meanmax, and meanmin were in the model. At this point, the process ends and the model from that step is reported as the output. However, if we attempt to use forward stepwise selection starting with the full model, the process does not work. The function begins with the full model, sees that there are no variables it could add to decrease the AIC because we haven’t given it any more variables, then reports the full model as the outcome. step(frogs.glm0,direction=&quot;forward&quot;) ## Start: AIC=213.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## ## Call: glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + ## NoOfSites + avrain + meanmin + meanmax, family = binomial, ## data = frogs, na.action = na.fail) ## ## Coefficients: ## (Intercept) altitude log(distance) log(NoOfPools) NoOfSites ## 40.8988354 -0.0066477 -0.7593044 0.5727269 -0.0008979 ## avrain meanmin meanmax ## -0.0067930 5.3047879 -3.1729589 ## ## Degrees of Freedom: 211 Total (i.e. Null); 204 Residual ## Null Deviance: 280 ## Residual Deviance: 197.6 AIC: 213.6 This is clearly not the result that we want, because there are many insignificant variables in the model. summary(frogs.glm0) ## ## Call: ## glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + ## NoOfSites + avrain + meanmin + meanmax, family = binomial, ## data = frogs, na.action = na.fail) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9795 -0.7193 -0.2785 0.7964 2.5658 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.090e+01 1.327e+02 0.308 0.757845 ## altitude -6.648e-03 3.866e-02 -0.172 0.863466 ## log(distance) -7.593e-01 2.554e-01 -2.973 0.002945 ** ## log(NoOfPools) 5.727e-01 2.162e-01 2.649 0.008083 ** ## NoOfSites -8.979e-04 1.074e-01 -0.008 0.993330 ## avrain -6.793e-03 5.999e-02 -0.113 0.909848 ## meanmin 5.305e+00 1.543e+00 3.439 0.000584 *** ## meanmax -3.173e+00 4.839e+00 -0.656 0.512048 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 279.99 on 211 degrees of freedom ## Residual deviance: 197.62 on 204 degrees of freedom ## AIC: 213.62 ## ## Number of Fisher Scoring iterations: 5 We need to give the function the model that we want it to start with, in this case, the “empty model.” The empty model predicts the number of species at a site using no covariates, only an intercept, and is denoted as pres.abs ~ 1 in R. frogs.glm.empty &lt;- glm(pres.abs~1, frogs, family=&quot;binomial&quot;) summary(frogs.glm.empty) ## ## Call: ## glm(formula = pres.abs ~ 1, family = &quot;binomial&quot;, data = frogs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9657 -0.9657 -0.9657 1.4051 1.4051 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5209 0.1420 -3.667 0.000245 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 279.99 on 211 degrees of freedom ## Residual deviance: 279.99 on 211 degrees of freedom ## AIC: 281.99 ## ## Number of Fisher Scoring iterations: 4 We also need to give the step function all of the possible covariates that it can add to find the best model. This is done using the scope= ~ command in step. step(frogs.glm.empty, scope= ~log(distance)+log(NoOfPools)+NoOfSites+avrain+meanmin+meanmax, data=frogs, direction=&quot;forward&quot;) ## Start: AIC=281.99 ## pres.abs ~ 1 ## ## Df Deviance AIC ## + log(distance) 1 229.15 233.15 ## + meanmin 1 254.40 258.40 ## + meanmax 1 269.52 273.52 ## + log(NoOfPools) 1 273.59 277.59 ## + NoOfSites 1 274.51 278.51 ## &lt;none&gt; 279.99 281.99 ## + avrain 1 279.95 283.95 ## ## Step: AIC=233.15 ## pres.abs ~ log(distance) ## ## Df Deviance AIC ## + meanmin 1 220.90 226.90 ## + meanmax 1 226.55 232.55 ## &lt;none&gt; 229.15 233.15 ## + log(NoOfPools) 1 227.44 233.44 ## + NoOfSites 1 228.28 234.28 ## + avrain 1 229.07 235.07 ## ## Step: AIC=226.9 ## pres.abs ~ log(distance) + meanmin ## ## Df Deviance AIC ## + meanmax 1 205.34 213.34 ## + avrain 1 210.12 218.12 ## + log(NoOfPools) 1 214.18 222.18 ## &lt;none&gt; 220.90 226.90 ## + NoOfSites 1 220.26 228.26 ## ## Step: AIC=213.34 ## pres.abs ~ log(distance) + meanmin + meanmax ## ## Df Deviance AIC ## + log(NoOfPools) 1 197.66 207.66 ## &lt;none&gt; 205.34 213.34 ## + avrain 1 205.33 215.33 ## + NoOfSites 1 205.34 215.34 ## ## Step: AIC=207.66 ## pres.abs ~ log(distance) + meanmin + meanmax + log(NoOfPools) ## ## Df Deviance AIC ## &lt;none&gt; 197.66 207.66 ## + NoOfSites 1 197.66 209.66 ## + avrain 1 197.66 209.66 ## ## Call: glm(formula = pres.abs ~ log(distance) + meanmin + meanmax + ## log(NoOfPools), family = &quot;binomial&quot;, data = frogs) ## ## Coefficients: ## (Intercept) log(distance) meanmin meanmax log(NoOfPools) ## 18.5268 -0.7547 5.3791 -2.3821 0.5707 ## ## Degrees of Freedom: 211 Total (i.e. Null); 207 Residual ## Null Deviance: 280 ## Residual Deviance: 197.7 AIC: 207.7 The function now does what we want it to do! It begins with the empty model and reports an AIC of 281.99, and lists the AIC for each variable if it were to be added to the empty model (log(distance) = 229.15, meanmin = 258.40, etc.). Since adding log(distance) results in the lowest AIC, it adds log(distance), then repeats the process. It continues this process until \\(&lt;none&gt;\\) is at the top of the list, meaning there are no variables that can be added that will decrease the AIC. It then reports that model, in this case pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax, as the best model. Starting with the full model works fine for direction = “both”. It takes the full model, takes away the least useful covariate, then has the option to add that covariate back in, or take away another. It repeats this until neither adding nor subtracting variables decreases the AIC of the model. step(frogs.glm0, direction=&quot;both&quot;) ## Start: AIC=213.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + NoOfSites + ## avrain + meanmin + meanmax ## ## Df Deviance AIC ## - NoOfSites 1 197.62 211.62 ## - avrain 1 197.64 211.64 ## - altitude 1 197.65 211.65 ## - meanmax 1 198.05 212.05 ## &lt;none&gt; 197.62 213.62 ## - log(NoOfPools) 1 205.31 219.31 ## - log(distance) 1 207.22 221.22 ## - meanmin 1 211.27 225.27 ## ## Step: AIC=211.62 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + avrain + ## meanmin + meanmax ## ## Df Deviance AIC ## - avrain 1 197.64 209.64 ## - altitude 1 197.66 209.66 ## - meanmax 1 198.06 210.06 ## &lt;none&gt; 197.62 211.62 ## + NoOfSites 1 197.62 213.62 ## - log(NoOfPools) 1 205.31 217.31 ## - log(distance) 1 209.80 221.80 ## - meanmin 1 211.31 223.31 ## ## Step: AIC=209.64 ## pres.abs ~ altitude + log(distance) + log(NoOfPools) + meanmin + ## meanmax ## ## Df Deviance AIC ## - altitude 1 197.66 207.66 ## - meanmax 1 198.74 208.74 ## &lt;none&gt; 197.64 209.64 ## + avrain 1 197.62 211.62 ## + NoOfSites 1 197.64 211.64 ## - log(NoOfPools) 1 205.31 215.31 ## - log(distance) 1 209.88 219.88 ## - meanmin 1 213.32 223.32 ## ## Step: AIC=207.66 ## pres.abs ~ log(distance) + log(NoOfPools) + meanmin + meanmax ## ## Df Deviance AIC ## &lt;none&gt; 197.66 207.66 ## + altitude 1 197.64 209.64 ## + NoOfSites 1 197.66 209.66 ## + avrain 1 197.66 209.66 ## - log(NoOfPools) 1 205.34 213.34 ## - log(distance) 1 209.91 217.91 ## - meanmax 1 214.18 222.18 ## - meanmin 1 222.40 230.40 ## ## Call: glm(formula = pres.abs ~ log(distance) + log(NoOfPools) + meanmin + ## meanmax, family = binomial, data = frogs, na.action = na.fail) ## ## Coefficients: ## (Intercept) log(distance) log(NoOfPools) meanmin meanmax ## 18.5268 -0.7547 0.5707 5.3791 -2.3821 ## ## Degrees of Freedom: 211 Total (i.e. Null); 207 Residual ## Null Deviance: 280 ## Residual Deviance: 197.7 AIC: 207.7 However, it can also be done starting with the empty model, where its first step is to add a covariate to the empty model, then proceed from there. step(frogs.glm.empty, scope= ~log(distance)+log(NoOfPools)+NoOfSites+avrain+meanmin+meanmax, direction=&quot;both&quot;) ## Start: AIC=281.99 ## pres.abs ~ 1 ## ## Df Deviance AIC ## + log(distance) 1 229.15 233.15 ## + meanmin 1 254.40 258.40 ## + meanmax 1 269.52 273.52 ## + log(NoOfPools) 1 273.59 277.59 ## + NoOfSites 1 274.51 278.51 ## &lt;none&gt; 279.99 281.99 ## + avrain 1 279.95 283.95 ## ## Step: AIC=233.15 ## pres.abs ~ log(distance) ## ## Df Deviance AIC ## + meanmin 1 220.90 226.90 ## + meanmax 1 226.55 232.55 ## &lt;none&gt; 229.15 233.15 ## + log(NoOfPools) 1 227.44 233.44 ## + NoOfSites 1 228.28 234.28 ## + avrain 1 229.07 235.07 ## - log(distance) 1 279.99 281.99 ## ## Step: AIC=226.9 ## pres.abs ~ log(distance) + meanmin ## ## Df Deviance AIC ## + meanmax 1 205.34 213.34 ## + avrain 1 210.12 218.12 ## + log(NoOfPools) 1 214.18 222.18 ## &lt;none&gt; 220.90 226.90 ## + NoOfSites 1 220.26 228.26 ## - meanmin 1 229.15 233.15 ## - log(distance) 1 254.40 258.40 ## ## Step: AIC=213.34 ## pres.abs ~ log(distance) + meanmin + meanmax ## ## Df Deviance AIC ## + log(NoOfPools) 1 197.66 207.66 ## &lt;none&gt; 205.34 213.34 ## + avrain 1 205.33 215.33 ## + NoOfSites 1 205.34 215.34 ## - meanmax 1 220.90 226.90 ## - log(distance) 1 225.85 231.85 ## - meanmin 1 226.55 232.55 ## ## Step: AIC=207.66 ## pres.abs ~ log(distance) + meanmin + meanmax + log(NoOfPools) ## ## Df Deviance AIC ## &lt;none&gt; 197.66 207.66 ## + NoOfSites 1 197.66 209.66 ## + avrain 1 197.66 209.66 ## - log(NoOfPools) 1 205.34 213.34 ## - log(distance) 1 209.91 217.91 ## - meanmax 1 214.18 222.18 ## - meanmin 1 222.40 230.40 ## ## Call: glm(formula = pres.abs ~ log(distance) + meanmin + meanmax + ## log(NoOfPools), family = &quot;binomial&quot;, data = frogs) ## ## Coefficients: ## (Intercept) log(distance) meanmin meanmax log(NoOfPools) ## 18.5268 -0.7547 5.3791 -2.3821 0.5707 ## ## Degrees of Freedom: 211 Total (i.e. Null); 207 Residual ## Null Deviance: 280 ## Residual Deviance: 197.7 AIC: 207.7 The real magic comes when we use a package like ‘MuMIn’ (Multimodel Inference) dredge(frogs.glm0) ## Fixed term is &quot;(Intercept)&quot; ## Global model call: glm(formula = pres.abs ~ altitude + log(distance) + log(NoOfPools) + ## NoOfSites + avrain + meanmin + meanmax, family = binomial, ## data = frogs, na.action = na.fail) ## --- ## Model selection table ## (Int) alt avr log(dst) log(NOP) mnmx mnmn ## 61 18.53000 -0.7547 0.5707 -2.38200 5.3790 ## 46 -51.58000 0.0231400 -0.7619 0.5598 5.9520 ## 62 28.93000 -0.0034840 -0.7553 0.5716 -2.72100 5.2570 ## 125 18.53000 -0.7590 0.5710 -2.37700 5.3700 ## 63 18.16000 0.0009902 -0.7543 0.5707 -2.36100 5.3560 ## 48 -45.93000 0.0181700 0.0266100 -0.7437 0.5631 5.3080 ## 110 -50.64000 0.0228600 -0.7953 0.5623 5.8910 ## 64 41.04000 -0.0066960 -0.0067780 -0.7583 0.5727 -3.17800 5.3050 ## 126 28.84000 -0.0034520 -0.7558 0.5716 -2.71700 5.2570 ## 127 18.27000 0.0007003 -0.7583 0.5709 -2.36200 5.3540 ## 112 -45.80000 0.0182300 0.0257700 -0.7551 0.5638 5.3080 ## 47 -14.29000 0.0721300 -0.7924 0.5455 2.1740 ## 53 21.47000 -0.9221 -2.25600 4.7810 ## 128 40.90000 -0.0066480 -0.0067930 -0.7593 0.5727 -3.17300 5.3050 ## 38 -44.64000 0.0217700 -0.9299 5.2960 ## 111 -14.33000 0.0722500 -0.7903 0.5454 2.1760 ## 54 33.40000 -0.0039860 -0.9222 -2.64600 4.6470 ## 55 20.22000 0.0034080 -0.9202 -2.18500 4.7050 ## 117 21.47000 -0.9193 -2.25900 4.7870 ## 40 -39.23000 0.0168400 0.0272300 -0.9080 4.6710 ## 102 -43.92000 0.0215600 -0.9548 5.2490 ## 121 15.48000 0.7078 -2.86800 6.5640 ## 118 34.67000 -0.0044110 -0.9147 -2.69700 4.6490 ## 56 38.63000 -0.0053740 -0.0029350 -0.9239 -2.84300 4.6660 ## 119 20.08000 0.0037900 -0.9150 -2.18300 4.7070 ## 104 -39.22000 0.0168500 0.0271500 -0.9091 4.6700 ## 57 14.78000 0.7515 -2.76600 6.4440 ## 39 -10.29000 0.0704800 -0.9389 1.7990 ## 42 -67.27000 0.0271400 0.7407 7.1610 ## 123 10.13000 0.0145800 0.7037 -2.55800 6.2240 ## 106 -68.17000 0.0275000 0.7057 7.1910 ## 122 36.11000 -0.0068950 0.7084 -3.54600 6.3380 ## 120 39.59000 -0.0057160 -0.0027840 -0.9164 -2.88200 4.6670 ## 108 -59.48000 0.0198800 0.0415200 0.6935 6.1980 ## 44 -60.67000 0.0214900 0.0300900 0.7388 6.4230 ## 58 4.26100 0.0035230 0.7501 -2.42300 6.5640 ## 59 14.15000 0.0016700 0.7514 -2.72900 6.4030 ## 103 -10.54000 0.0711800 -0.9270 1.8100 ## 16 7.78400 -0.0095890 0.0772300 -0.9211 0.4622 ## 124 7.02900 0.0009015 0.0156800 0.7033 -2.44600 6.2280 ## 31 -25.39000 0.0889800 -0.9126 0.4583 1.21000 ## 60 -25.77000 0.0115700 0.0161800 0.7449 -1.28600 6.4470 ## 45 0.68640 -0.9050 0.5027 1.1150 ## 30 171.20000 -0.0549900 -0.9926 0.4544 -5.93600 ## 109 1.52600 -0.9867 0.5102 1.1030 ## 80 8.03300 -0.0094940 0.0759000 -0.9417 0.4640 ## 32 28.01000 -0.0153900 0.0696800 -0.9275 0.4632 -0.73800 ## 95 -24.73000 0.0873300 -0.9365 0.4605 1.19700 ## 8 7.89600 -0.0079910 0.0738300 -1.0310 ## 23 -19.78000 0.0836700 -1.0230 1.01000 ## 107 -24.84000 0.0878800 0.6831 2.7640 ## 94 167.30000 -0.0536800 -1.0180 0.4564 -5.78400 ## 96 26.31000 -0.0147500 0.0691700 -0.9460 0.4648 -0.66760 ## 22 163.80000 -0.0513600 -1.0970 -5.66500 ## 37 4.04100 -1.0450 0.8095 ## 24 27.44000 -0.0135900 0.0665100 -1.0370 -0.71300 ## 72 8.03500 -0.0079340 0.0730600 -1.0420 ## 43 -22.87000 0.0784300 0.7370 2.7040 ## 87 -19.37000 0.0826300 -1.0380 1.00100 ## 101 4.82900 -1.1200 0.7954 ## 86 161.10000 -0.0504200 -1.1140 -5.55700 ## 113 18.49000 -2.84900 6.1290 ## 88 26.58000 -0.0132600 0.0662000 -1.0470 -0.67720 ## 14 10.82000 -0.0035650 -1.0220 0.4011 ## 78 11.71000 -0.0035710 -1.1130 0.4129 ## 29 0.38330 -1.0400 0.3807 0.37270 ## 115 9.95400 0.0233400 -2.36100 5.6050 ## 100 -54.64000 0.0185100 0.0480700 5.6130 ## 114 46.40000 -0.0093210 -3.77000 5.8350 ## 93 1.22500 -1.1330 0.3946 0.37800 ## 98 -63.97000 0.0270400 6.6860 ## 6 10.90000 -0.0024360 -1.1230 ## 49 17.74000 -2.70600 5.9250 ## 21 3.78600 -1.1340 0.25010 ## 116 -14.39000 0.0070520 0.0321900 -1.48000 5.6320 ## 34 -62.19000 0.0264000 6.5890 ## 70 11.71000 -0.0024230 -1.2060 ## 5 7.68400 -1.2040 ## 13 6.80900 -1.1680 0.2237 ## 36 -55.32000 0.0203300 0.0332800 5.8200 ## 85 4.61900 -1.2190 0.25220 ## 51 15.12000 0.0070460 -2.55200 5.7580 ## 50 7.34400 0.0034820 -2.36600 6.0410 ## 69 8.51000 -1.2860 ## 77 7.65500 -1.2540 0.2315 ## 7 7.11800 0.0040160 -1.2080 ## 15 6.95000 -0.0011020 -1.1670 0.2273 ## 52 -57.05000 0.0208200 0.0339800 0.06385 5.8190 ## 99 -22.42000 0.0909300 2.4310 ## 71 8.25500 0.0016800 -1.2860 ## 79 8.23400 -0.0041500 -1.2530 0.2459 ## 35 -19.64000 0.0789800 2.3260 ## 91 -41.16000 0.1113000 0.6039 1.59300 ## 76 2.41000 -0.0125800 0.0953200 0.6084 ## 41 -7.61300 0.7316 1.5830 ## 105 -7.72800 0.7033 1.5490 ## 92 -68.98000 0.0080870 0.1209000 0.5997 2.61100 ## 27 -38.57000 0.1006000 0.6573 1.54900 ## 12 3.82200 -0.0121600 0.0843300 0.6619 ## 28 -108.30000 0.0202200 0.1254000 0.6439 4.09600 ## 90 174.80000 -0.0596700 0.6255 -6.25300 ## 83 -37.08000 0.1114000 1.41100 ## 68 1.51100 -0.0111000 0.0969900 ## 84 -83.69000 0.0135300 0.1277000 3.11500 ## 26 142.50000 -0.0493600 0.6721 -5.02400 ## 19 -33.37000 0.0978600 1.33700 ## 4 3.21500 -0.0104300 0.0832300 ## 20 -130.00000 0.0279800 0.1324000 4.86500 ## 97 -4.69200 1.1780 ## 33 -4.34600 1.2070 ## 10 6.32200 -0.0056080 0.6344 ## 74 5.74400 -0.0053960 0.6019 ## 82 170.70000 -0.0570500 -6.13000 ## 25 -10.59000 0.6152 0.60690 ## 89 -10.44000 0.5822 0.57870 ## 18 132.00000 -0.0445500 -4.65700 ## 66 4.88500 -0.0038050 ## 81 -6.45900 0.40020 ## 2 5.63300 -0.0039940 ## 17 -6.38200 0.42720 ## 73 -1.97500 0.3479 ## 75 -1.28400 -0.0048460 0.3651 ## 9 -1.58400 0.3765 ## 65 -1.04300 ## 11 -0.02344 -0.0112600 0.4140 ## 67 -1.64300 0.0039450 ## 1 -0.52090 ## 3 -0.17310 -0.0023490 ## NOS df logLik AICc delta weight ## 61 5 -98.828 207.9 0.00 0.250 ## 46 5 -99.368 209.0 1.08 0.146 ## 62 6 -98.819 210.0 2.10 0.088 ## 125 -0.0039920 6 -98.827 210.1 2.12 0.087 ## 63 6 -98.828 210.1 2.12 0.087 ## 48 6 -99.030 210.5 2.52 0.071 ## 110 -0.0313600 6 -99.320 211.1 3.10 0.053 ## 64 7 -98.812 212.2 4.23 0.030 ## 126 -0.0005225 7 -98.819 212.2 4.24 0.030 ## 127 -0.0036200 7 -98.827 212.2 4.26 0.030 ## 112 -0.0100800 7 -99.025 212.6 4.65 0.024 ## 47 5 -101.473 213.2 5.29 0.018 ## 53 4 -102.670 213.5 5.59 0.015 ## 128 -0.0008979 8 -98.812 214.3 6.39 0.010 ## 38 4 -103.185 214.6 6.62 0.009 ## 111 0.0019890 6 -101.473 215.4 7.41 0.006 ## 54 5 -102.658 215.6 7.66 0.005 ## 55 5 -102.666 215.6 7.68 0.005 ## 117 0.0025550 5 -102.670 215.6 7.68 0.005 ## 40 5 -102.831 216.0 8.01 0.005 ## 102 -0.0229600 5 -103.158 216.6 8.66 0.003 ## 121 0.1417000 5 -103.690 217.7 9.72 0.002 ## 118 0.0069150 6 -102.655 217.7 9.77 0.002 ## 56 6 -102.656 217.7 9.78 0.002 ## 119 0.0045760 6 -102.665 217.7 9.79 0.002 ## 104 -0.0009286 6 -102.831 218.1 10.12 0.002 ## 57 4 -104.954 218.1 10.15 0.002 ## 39 4 -105.058 218.3 10.36 0.001 ## 42 4 -105.494 219.2 11.23 0.001 ## 123 0.1495000 6 -103.612 219.6 11.69 0.001 ## 106 0.1109000 5 -104.704 219.7 11.75 0.001 ## 122 0.1494000 6 -103.648 219.7 11.76 0.001 ## 120 0.0067250 7 -102.654 219.9 11.91 0.001 ## 108 0.1414000 6 -103.743 219.9 11.95 0.001 ## 44 5 -104.940 220.2 12.22 0.001 ## 58 5 -104.941 220.2 12.23 0.001 ## 59 5 -104.953 220.2 12.25 0.001 ## 103 0.0109500 5 -105.052 220.4 12.45 0.000 ## 16 5 -105.670 221.6 13.68 0.000 ## 124 0.1491000 7 -103.611 221.8 13.82 0.000 ## 31 5 -105.761 221.8 13.87 0.000 ## 60 6 -104.902 222.2 14.27 0.000 ## 45 4 -107.089 222.4 14.42 0.000 ## 30 5 -106.658 223.6 15.66 0.000 ## 109 -0.0868600 5 -106.703 223.7 15.75 0.000 ## 80 -0.0208000 6 -105.649 223.7 15.76 0.000 ## 32 6 -105.654 223.7 15.77 0.000 ## 95 -0.0240100 6 -105.733 223.9 15.93 0.000 ## 8 4 -108.393 225.0 17.03 0.000 ## 23 4 -108.460 225.1 17.17 0.000 ## 107 0.1784000 5 -107.491 225.3 17.33 0.000 ## 94 -0.0277700 6 -106.622 225.7 17.71 0.000 ## 96 -0.0193800 7 -105.637 225.8 17.88 0.000 ## 22 4 -109.274 226.7 18.79 0.000 ## 37 3 -110.450 227.0 19.07 0.000 ## 24 5 -108.379 227.0 19.10 0.000 ## 72 -0.0112700 5 -108.387 227.1 19.12 0.000 ## 43 4 -109.478 227.1 19.20 0.000 ## 87 -0.0139100 5 -108.451 227.2 19.25 0.000 ## 101 -0.0774900 4 -110.128 228.4 20.50 0.000 ## 86 -0.0188800 5 -109.256 228.8 20.86 0.000 ## 113 0.1874000 4 -110.480 229.2 21.21 0.000 ## 88 -0.0098920 6 -108.374 229.2 21.21 0.000 ## 14 4 -110.658 229.5 21.56 0.000 ## 78 -0.0998600 5 -110.119 230.5 22.58 0.000 ## 29 4 -111.202 230.6 22.65 0.000 ## 115 0.1996000 5 -110.252 230.8 22.85 0.000 ## 100 0.1917000 5 -110.283 230.9 22.91 0.000 ## 114 0.1977000 5 -110.391 231.1 23.13 0.000 ## 93 -0.1042000 5 -110.610 231.5 23.56 0.000 ## 98 0.1580000 4 -111.733 231.7 23.71 0.000 ## 6 3 -112.923 232.0 24.01 0.000 ## 49 3 -112.924 232.0 24.02 0.000 ## 21 3 -113.273 232.7 24.71 0.000 ## 116 0.1964000 6 -110.234 232.9 24.93 0.000 ## 34 3 -113.514 233.1 25.20 0.000 ## 70 -0.0886600 4 -112.483 233.2 25.21 0.000 ## 5 2 -114.577 233.2 25.26 0.000 ## 13 3 -113.719 233.6 25.61 0.000 ## 36 4 -112.729 233.7 25.70 0.000 ## 85 -0.0917800 4 -112.799 233.8 25.84 0.000 ## 51 4 -112.900 234.0 26.05 0.000 ## 50 4 -112.910 234.0 26.07 0.000 ## 69 -0.0861500 3 -114.142 234.4 26.45 0.000 ## 77 -0.0911900 4 -113.233 234.7 26.71 0.000 ## 7 3 -114.537 235.2 27.24 0.000 ## 15 4 -113.716 235.6 27.68 0.000 ## 52 5 -112.729 235.7 27.80 0.000 ## 99 0.2238000 4 -113.959 236.1 28.16 0.000 ## 71 -0.0841400 4 -114.135 236.5 28.52 0.000 ## 79 -0.0966200 5 -113.194 236.7 28.73 0.000 ## 35 3 -117.391 240.9 32.95 0.000 ## 91 0.1876000 5 -115.662 241.6 33.67 0.000 ## 76 0.1952000 5 -115.875 242.0 34.09 0.000 ## 41 3 -118.231 242.6 34.63 0.000 ## 105 0.0995700 4 -117.532 243.3 35.31 0.000 ## 92 0.1825000 6 -115.625 243.7 35.71 0.000 ## 27 4 -118.048 244.3 36.34 0.000 ## 12 4 -118.452 245.1 37.15 0.000 ## 28 5 -117.797 245.9 37.94 0.000 ## 90 0.1989000 5 -119.943 250.2 42.23 0.000 ## 83 0.2269000 4 -121.222 250.6 42.69 0.000 ## 68 0.2335000 4 -121.485 251.2 43.22 0.000 ## 84 0.2183000 5 -121.111 252.5 44.57 0.000 ## 26 4 -122.592 253.4 45.43 0.000 ## 19 3 -124.993 256.1 48.15 0.000 ## 4 3 -125.468 257.1 49.10 0.000 ## 20 4 -124.476 257.1 49.20 0.000 ## 97 0.1438000 3 -125.583 257.3 49.33 0.000 ## 33 2 -127.199 258.5 50.51 0.000 ## 10 3 -126.706 259.5 51.58 0.000 ## 74 0.1141000 4 -125.693 259.6 51.63 0.000 ## 82 0.2354000 4 -126.085 260.4 52.42 0.000 ## 25 3 -127.803 261.7 53.77 0.000 ## 89 0.1093000 4 -126.867 261.9 53.98 0.000 ## 18 3 -130.106 266.3 58.38 0.000 ## 66 0.1516000 3 -132.034 270.2 62.24 0.000 ## 81 0.1481000 3 -132.892 271.9 63.95 0.000 ## 2 2 -133.978 272.0 64.07 0.000 ## 17 2 -134.759 273.6 65.63 0.000 ## 73 0.1564000 3 -134.617 275.4 67.40 0.000 ## 75 0.1491000 4 -134.550 277.3 69.35 0.000 ## 9 2 -136.793 277.6 69.70 0.000 ## 65 0.1733000 2 -137.255 278.6 70.62 0.000 ## 11 3 -136.394 278.9 70.96 0.000 ## 67 0.1786000 3 -137.204 280.5 72.58 0.000 ## 1 1 -139.993 282.0 74.06 0.000 ## 3 2 -139.974 284.0 76.06 0.000 ## Models ranked by AICc(x) We can make a table to compare a hand-selected set of models. model.sel(frogs.glm0,frogs.glm1) ## Model selection table ## (Int) alt avr log(dst) log(NOP) mnmx mnmn NOS ## frogs.glm1 18.27 0.0007003 -0.7583 0.5709 -2.362 5.354 -0.0036200 ## frogs.glm0 40.90 -0.006648 -0.0067930 -0.7593 0.5727 -3.173 5.305 -0.0008979 ## family na.action df logLik AICc delta weight ## frogs.glm1 binomial(logit) 7 -98.827 212.2 0.00 0.744 ## frogs.glm0 binomial(logit) na.fail 8 -98.812 214.3 2.13 0.256 ## Models ranked by AICc(x) We can do model averaging summary(model.avg(frogs.glm0,frogs.glm1)) ## ## Call: ## model.avg(object = frogs.glm0, frogs.glm1) ## ## Component model call: ## glm(formula = &lt;2 unique values&gt;, family = binomial, data = frogs, ## na.action = &lt;2 unique values&gt;) ## ## Component models: ## df logLik AICc delta weight ## 234567 7 -98.83 212.20 0.00 0.74 ## 1234567 8 -98.81 214.33 2.13 0.26 ## ## Term codes: ## altitude avrain log(distance) log(NoOfPools) meanmax ## 1 2 3 4 5 ## meanmin NoOfSites ## 6 7 ## ## Model-averaged coefficients: ## (full average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) 24.068195 69.288185 69.693226 0.345 0.72984 ## log(distance) -0.758572 0.255698 0.257218 2.949 0.00319 ** ## log(NoOfPools) 0.571365 0.215568 0.216850 2.635 0.00842 ** ## NoOfSites -0.002923 0.106481 0.107115 0.027 0.97823 ## avrain -0.001220 0.046837 0.047114 0.026 0.97934 ## meanmin 5.341442 1.530021 1.539117 3.470 0.00052 *** ## meanmax -2.570165 2.641027 2.656497 0.968 0.33329 ## altitude -0.001704 0.019784 0.019899 0.086 0.93178 ## ## (conditional average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) 24.068195 69.288185 69.693226 0.345 0.72984 ## log(distance) -0.758572 0.255698 0.257218 2.949 0.00319 ** ## log(NoOfPools) 0.571365 0.215568 0.216850 2.635 0.00842 ** ## NoOfSites -0.002923 0.106481 0.107115 0.027 0.97823 ## avrain -0.001220 0.046837 0.047114 0.026 0.97934 ## meanmin 5.341442 1.530021 1.539117 3.470 0.00052 *** ## meanmax -2.570165 2.641027 2.656497 0.968 0.33329 ## altitude -0.006648 0.038658 0.038888 0.171 0.86427 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does the output mean “with shrinkage”? These estimates include a zero value when a parameter does not actually appear in a model. In this case ‘altitude’ appears in only one of the two models and so the estimate “with shrinkage” is signicantly smaller than the parameter value estimated only from a weighting of the models including the covariate ‘altitude’. 25.3 Part 2: Model criticism We are going to use a dataset that comes from the journal Ecology, which is available here. STOP: Let’s read the abstract so we know what we are modelling. The goal is to create the best possible, most parsimonious model for maximum longevity in non-volant mammals. The covariates we have in this dataset to consider are: Order Family Genus Species Mass (g) Gestation (mo) Newborn weight (g) Weaning age (mo) Weaning mass (g) Age of first reproduction (mo) Litter size Litters/year First, we will load the data and the ‘car’ package, and use ‘names’ to see what the columns are named data&lt;-read.csv(&quot;_data/MammalLifeHistory.csv&quot;) attach(data) names(data) ## [1] &quot;Order&quot; &quot;Family&quot; &quot;Genus&quot; &quot;Species&quot; ## [5] &quot;Mass&quot; &quot;Gestation&quot; &quot;Newborn&quot; &quot;Weaning&quot; ## [9] &quot;WeanMass&quot; &quot;AFR&quot; &quot;MaxLifespan&quot; &quot;LitterSize&quot; ## [13] &quot;LittersPerYear&quot; Before fitting any models, let’s just look at each potential covariate vs. maximum longevity to get a sense for which variables need to be transformed. I list get the first few here… boxplot(MaxLifespan~Order) boxplot(MaxLifespan~Family) boxplot(MaxLifespan~Genus) boxplot(MaxLifespan~Species) plot(MaxLifespan~Mass) plot(MaxLifespan~Gestation) Note that most of the covariates need to be transformed to linearize the relationship. The easiest transformation to try is the log() - the covariates that should probably be transformed relate to mass and time periods: Mass, Gestation, Newborn, Weaning, WeanMass, AFR, LittersPerYear (possible, not clear what the best transmation is for this). In other words, look at: plot(MaxLifespan~log10(Mass)) plot(MaxLifespan~log10(Gestation)) etc. For now, let’s ignore the taxonomic covariates and focus on just three covariates: Mass, AFR, and LitterSize. Fitting the model using ‘lm’ Let’s fit the model with Mass,AFR, and LitterSize. We will consider this the full model. fit&lt;-lm(MaxLifespan~log10(Mass)+log10(AFR)+LitterSize) summary(fit) ## ## Call: ## lm(formula = MaxLifespan ~ log10(Mass) + log10(AFR) + LitterSize) ## ## Residuals: ## Min 1Q Median 3Q Max ## -263.66 -78.11 -19.31 53.52 763.75 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -168.4019 26.8904 -6.263 8.35e-10 *** ## log10(Mass) 60.7061 5.6008 10.839 &lt; 2e-16 *** ## log10(AFR) 152.5190 17.5461 8.692 &lt; 2e-16 *** ## LitterSize -0.1925 3.8823 -0.050 0.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 118.7 on 485 degrees of freedom ## (951 observations deleted due to missingness) ## Multiple R-squared: 0.6282, Adjusted R-squared: 0.6259 ## F-statistic: 273.1 on 3 and 485 DF, p-value: &lt; 2.2e-16 Model diagnostics Let’s look at the residuals as a function of the fitted values: plot(fitted(fit),residuals(fit)) Let’s look at the residuals using the ‘car’ package function ‘residualPlots’. This command produces scatterplots of the residuals versus each of the predictors and versus the final fitted value. Note that what we did manually above is reproduced as the final panel here. residualPlots(fit) ## Test stat Pr(&gt;|Test stat|) ## log10(Mass) 12.8413 &lt;2e-16 *** ## log10(AFR) 11.6289 &lt;2e-16 *** ## LitterSize 0.2717 0.786 ## Tukey test 15.0728 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can assess the normality of the residuals by histogramming the studentized residuals. We can pull these out from the fitted model using the ‘studres’ function from the ‘MASS’ package. sresid&lt;-studres(fit) hist(sresid,freq=FALSE) xfit&lt;-seq(min(sresid),max(sresid),length=40) yfit&lt;-dnorm(xfit) lines(xfit,yfit) A variation on the basic residual plot is the marginal model plot. marginalModelPlots(fit) Note that loess smoothers have been added showing the non-parametric regression between the actual data (solid line) and the model prediction (dashed line) against each of the predictor variables. If these two lines are close together, that is an indication of good model fit. Note that the marginal plots display the relationship between the response and each covariates IGNORING the other covariates. We can also look at the relationship between the response and each covariates CONTROLLING for the other covariates. We do this through “added variable plots”. Let’s say we have the following model \\[ Y \\sim X_{1} + X_{2} + X_{3} \\] There are two steps in building the added-variable plot for \\(X_{1}\\): Regress \\[ Y \\sim X_{2} + X_{3} \\] The residuals from this plot reflect all the variation that is not otherwise explained in the model (i.e. by all the covariates except \\(X_{1}\\)). Regress \\[ X_{1} \\sim X_{2} + X_{3} \\] The residuals from this plot represent the part of \\(X_{1}\\) not explained by the other covariates. The added variable plot is simply a plot of the residuals from #1 on the y-axis and the residuals from #2 on the x-axis. We can do this in R using the function ‘avPlots’ from the ‘car’ package avPlots(fit,id.n=2) ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter The id.n option will cause the plot to identify the two points that are furthest from the mean on the x axis and the two with the largest absolute residuals. The added variable plot allows us to visualize the effect of each covariate after adjusting for all the other covariates in the model. We can also look at leverage by using the command ‘leveragePlots’ leveragePlots(fit) For covariates with only a single degree of freedom (i.e. not different levels of a factor), this will simply be a rescaled version of the added-variable plots. Outliers Let’s look for outliers using the ‘car’ function ‘outlierTest’ which reports the Bonferroni p-values for Studentized residuals: outlierTest(fit) ## rstudent unadjusted p-value Bonferroni p ## 1440 6.797206 3.1481e-11 1.5394e-08 ## 1439 6.284883 7.3267e-10 3.5828e-07 ## 1438 4.650225 4.2834e-06 2.0946e-03 ## 1437 4.488601 8.9697e-06 4.3862e-03 Plot the qqplot for the studentized residuals using the ‘car’ package function qqPlot. WARNING: This is not the same as qqplot (lower case p); this is a specialized function in the ‘car’ package that will plot the appropriate qqplot for the studentized residuals given a fitted model as input. qqPlot(fit) ## [1] 1439 1440 To identify points with high leverage, we want to calculate the hat values for each of the points. We can do this using the ‘hatvalues” command in the ’car’ package. hatvalues(fit) ## 842 846 847 848 849 850 ## 0.015068582 0.017116554 0.013839481 0.021939881 0.017150953 0.018853062 ## 853 854 856 857 858 859 ## 0.009549529 0.005599577 0.014340191 0.019328979 0.012823175 0.013772585 ## 861 862 863 865 866 867 ## 0.009440959 0.016442917 0.017600641 0.007572179 0.013613866 0.007801192 ## 870 873 874 875 877 879 ## 0.011338383 0.008197299 0.014927487 0.006055714 0.011443251 0.016563954 ## 880 882 884 885 886 887 ## 0.013955050 0.010053036 0.015267217 0.010993510 0.012341340 0.014343983 ## 888 889 891 893 894 895 ## 0.008865008 0.005970050 0.011063709 0.029705488 0.011271224 0.003770331 ## 896 897 899 900 901 902 ## 0.004983555 0.009589510 0.011642918 0.010190113 0.008382325 0.007342826 ## 905 906 907 908 909 910 ## 0.014709335 0.015055631 0.006571569 0.006224857 0.015789476 0.012402864 ## 911 912 913 914 915 916 ## 0.006796631 0.015641460 0.009947711 0.011609740 0.013271976 0.008664947 ## 919 920 921 922 923 924 ## 0.011320700 0.009584098 0.008836642 0.023302778 0.013285879 0.013900936 ## 925 926 928 929 931 932 ## 0.006221926 0.009090168 0.004859531 0.004634291 0.010114932 0.003656926 ## 933 937 938 939 940 941 ## 0.011388887 0.010593856 0.007535007 0.007456337 0.007670776 0.030754607 ## 942 944 945 946 947 949 ## 0.011129173 0.008955209 0.010397163 0.014035279 0.011948458 0.012339818 ## 950 951 952 954 955 956 ## 0.006449339 0.011225454 0.008363365 0.008531905 0.016430642 0.010567908 ## 959 960 961 962 963 966 ## 0.009671992 0.015610676 0.007195425 0.006295996 0.007158921 0.013672739 ## 968 969 970 971 972 973 ## 0.034809065 0.005230397 0.005791008 0.017812912 0.005496217 0.008931287 ## 974 975 976 977 978 979 ## 0.010714088 0.007259116 0.013047745 0.012098943 0.003733398 0.003745609 ## 980 981 983 985 986 987 ## 0.007505282 0.122249610 0.011234272 0.003202089 0.003567088 0.013845063 ## 988 989 990 992 993 996 ## 0.005235163 0.009494211 0.008892926 0.012990091 0.008085905 0.005684921 ## 998 1001 1002 1003 1004 1005 ## 0.003768405 0.007223905 0.005180949 0.004814076 0.007704883 0.004911695 ## 1006 1008 1009 1011 1012 1013 ## 0.008829438 0.008971676 0.008165306 0.007724797 0.006735167 0.011439088 ## 1016 1017 1018 1020 1021 1024 ## 0.011792716 0.003381829 0.009339714 0.003349616 0.004859820 0.005352438 ## 1025 1027 1028 1029 1030 1032 ## 0.006487226 0.006408716 0.005061128 0.003532664 0.008939130 0.004258913 ## 1033 1034 1035 1036 1037 1038 ## 0.002395187 0.004871201 0.005370386 0.007518471 0.010234137 0.003800835 ## 1039 1040 1041 1043 1046 1047 ## 0.013633921 0.002976951 0.005133274 0.007015061 0.003943074 0.051406682 ## 1049 1051 1052 1053 1054 1056 ## 0.004092141 0.006286774 0.002611721 0.013539610 0.008068323 0.004325113 ## 1057 1059 1060 1061 1062 1063 ## 0.005555973 0.004945531 0.002924891 0.004345880 0.005991305 0.007685537 ## 1064 1065 1066 1067 1068 1069 ## 0.004908637 0.004400779 0.003253593 0.011575449 0.007305610 0.005860831 ## 1070 1071 1072 1074 1075 1076 ## 0.002402003 0.002792913 0.005557776 0.007675980 0.010483247 0.006799054 ## 1077 1082 1083 1084 1085 1086 ## 0.006660530 0.006995133 0.006078065 0.002901274 0.007114720 0.004327091 ## 1087 1089 1090 1091 1092 1094 ## 0.005282506 0.003614222 0.006269742 0.006495829 0.008953724 0.004307162 ## 1095 1096 1097 1099 1100 1101 ## 0.007351841 0.005469209 0.003710382 0.010650969 0.002978093 0.003473076 ## 1102 1103 1105 1106 1108 1110 ## 0.006171337 0.007069918 0.005347415 0.003187388 0.024443728 0.011503286 ## 1111 1112 1113 1114 1115 1116 ## 0.011335444 0.006680094 0.007107539 0.007312845 0.002519741 0.007070413 ## 1117 1118 1119 1120 1121 1122 ## 0.002830881 0.004408249 0.003807998 0.003297593 0.003592997 0.002743881 ## 1123 1124 1126 1127 1128 1129 ## 0.002194139 0.003298492 0.009266549 0.006794262 0.007208532 0.008079244 ## 1130 1131 1132 1133 1134 1135 ## 0.006286228 0.008345838 0.003273337 0.003828792 0.005575621 0.006207749 ## 1136 1137 1138 1139 1142 1143 ## 0.007097858 0.013811813 0.006330278 0.011706480 0.003789064 0.002575829 ## 1144 1145 1146 1147 1149 1150 ## 0.006992516 0.003514526 0.005379992 0.034096666 0.005852205 0.033293641 ## 1151 1152 1153 1154 1155 1156 ## 0.003453114 0.009386505 0.007006645 0.030158122 0.004003816 0.005932827 ## 1157 1158 1159 1160 1161 1163 ## 0.006238935 0.007099396 0.003294258 0.004460579 0.002382034 0.004959417 ## 1164 1165 1166 1167 1168 1169 ## 0.005815032 0.005636536 0.007810206 0.011412521 0.016350024 0.003145305 ## 1172 1173 1174 1175 1176 1177 ## 0.006659253 0.004164675 0.010321781 0.008759469 0.008968261 0.005112558 ## 1178 1179 1180 1181 1182 1183 ## 0.007413931 0.004092970 0.004144759 0.005136702 0.004079677 0.007101366 ## 1184 1185 1187 1188 1189 1191 ## 0.007859365 0.004565706 0.007752359 0.004891404 0.004984572 0.010947371 ## 1192 1193 1194 1195 1196 1197 ## 0.005974901 0.003571493 0.006637753 0.005020810 0.002784743 0.008507969 ## 1198 1199 1200 1202 1203 1204 ## 0.005564695 0.004808824 0.005100021 0.011504907 0.002532735 0.005352594 ## 1205 1206 1207 1208 1209 1210 ## 0.005481630 0.004135045 0.008106898 0.010955028 0.005124791 0.003077065 ## 1211 1212 1213 1215 1216 1217 ## 0.003610983 0.004251044 0.004395735 0.003557946 0.006615751 0.003897994 ## 1218 1222 1223 1226 1227 1228 ## 0.007260722 0.004722739 0.008573003 0.006186477 0.007481516 0.006778917 ## 1230 1231 1232 1233 1234 1235 ## 0.004511308 0.007878819 0.002950337 0.004743704 0.007035720 0.005283641 ## 1236 1237 1238 1240 1241 1242 ## 0.011306445 0.022893409 0.003614331 0.003258847 0.003970306 0.004500544 ## 1243 1244 1245 1246 1247 1248 ## 0.008833069 0.011832830 0.004962626 0.006739456 0.002961121 0.004095768 ## 1249 1250 1251 1252 1253 1254 ## 0.002697826 0.007713930 0.008888300 0.006103942 0.004462294 0.004126934 ## 1255 1256 1257 1258 1259 1260 ## 0.003575584 0.012908842 0.007919903 0.003450691 0.005238098 0.002709156 ## 1261 1262 1263 1264 1265 1266 ## 0.004487147 0.004141025 0.005462472 0.005616879 0.007023565 0.004157081 ## 1267 1269 1270 1271 1272 1274 ## 0.005405134 0.004237417 0.008758783 0.003659095 0.003641221 0.006924737 ## 1275 1277 1278 1279 1280 1282 ## 0.006556800 0.006777154 0.004657738 0.007400513 0.006358517 0.006840930 ## 1283 1284 1285 1286 1287 1288 ## 0.003604012 0.004650281 0.002931686 0.004993995 0.004907787 0.004930544 ## 1289 1290 1291 1292 1293 1294 ## 0.005290486 0.008136328 0.006958753 0.006647525 0.003779911 0.007934545 ## 1295 1296 1297 1298 1300 1301 ## 0.004946197 0.004302767 0.006799393 0.006732886 0.004519527 0.005707493 ## 1302 1303 1304 1305 1306 1307 ## 0.005550477 0.002595298 0.007779227 0.009609406 0.004636675 0.011250687 ## 1308 1309 1310 1311 1312 1313 ## 0.004781732 0.005060416 0.003886592 0.003321002 0.011590056 0.004771912 ## 1314 1315 1316 1317 1318 1319 ## 0.004385500 0.005503135 0.010437770 0.006328062 0.009054948 0.005575354 ## 1320 1321 1323 1325 1326 1327 ## 0.006215209 0.005548372 0.008165573 0.002922401 0.006738098 0.006856105 ## 1328 1329 1331 1332 1334 1335 ## 0.004278941 0.005994052 0.008439600 0.005687190 0.012655364 0.004428018 ## 1336 1337 1338 1339 1340 1341 ## 0.002716818 0.006707334 0.006023086 0.007232496 0.003974231 0.005413279 ## 1342 1343 1344 1346 1347 1348 ## 0.010988882 0.003898428 0.005552927 0.005835286 0.005067843 0.006460036 ## 1349 1350 1351 1353 1355 1356 ## 0.003628967 0.005276712 0.005131207 0.006341430 0.005387209 0.005671682 ## 1357 1358 1359 1360 1361 1362 ## 0.007056530 0.005369556 0.005524772 0.007859248 0.015081516 0.005429746 ## 1364 1365 1366 1367 1368 1369 ## 0.004934686 0.002370536 0.008380701 0.007978252 0.006482352 0.005136907 ## 1370 1371 1372 1373 1374 1375 ## 0.002962098 0.004225617 0.006654086 0.010193078 0.004704468 0.003955831 ## 1376 1377 1378 1379 1380 1382 ## 0.005892978 0.006761250 0.009081694 0.004618119 0.003413752 0.005023688 ## 1384 1385 1386 1388 1389 1390 ## 0.006697765 0.011954991 0.007447664 0.004846690 0.007081667 0.007402309 ## 1391 1392 1394 1395 1396 1397 ## 0.005589098 0.006310048 0.005901080 0.008585187 0.016351942 0.003439244 ## 1398 1399 1401 1402 1403 1404 ## 0.006193983 0.004744828 0.005438690 0.005680335 0.010608833 0.006236326 ## 1405 1406 1407 1408 1409 1411 ## 0.006949090 0.004709298 0.010467387 0.008132087 0.014792229 0.006401322 ## 1412 1413 1414 1415 1416 1417 ## 0.008215980 0.010237253 0.007768586 0.005145764 0.007202106 0.008845023 ## 1418 1419 1420 1421 1422 1423 ## 0.008912327 0.004916235 0.010635773 0.008120296 0.007774509 0.012684103 ## 1424 1426 1427 1428 1429 1430 ## 0.008368418 0.008586505 0.016052450 0.016559061 0.012372053 0.015524328 ## 1431 1432 1433 1434 1435 1437 ## 0.008787016 0.014546150 0.017984508 0.014618941 0.011304205 0.012288729 ## 1438 1439 1440 ## 0.022172504 0.025746100 0.020671173 Note that the ‘stats’ package has a function called ‘lm.influence’ that provides much the same information. We can make a plot that highlights highly influential values. We will use John Fox’s suggested cut-off of 4/(n-p-1) where n=number of data points and p=number of estimated parameters. cutoff&lt;-4/((nrow(data)-length(fit$coefficients)-2)) plot(fit,which=4,cook.levels=cutoff) Another useful plot is created by ‘influencePlot’ which creates a “bubble” plot of studentized residuals by hat values, with the areas of the circles representing the observations proportional to Cook’s distances. Vertical reference lines are drawn at twice and three times the average hat value, horizontal reference lines at -2,0,2 on the studentized-residual scale. influencePlot(fit,id.method=&quot;identify&quot;) ## Warning in plot.window(...): &quot;id.method&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.method&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.method&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.method&quot; is not ## a graphical parameter ## Warning in box(...): &quot;id.method&quot; is not a graphical parameter ## Warning in title(...): &quot;id.method&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.method&quot; is not a ## graphical parameter ## StudRes Hat CookD ## 981 -0.4853275 0.12224961 0.008214314 ## 1047 -0.7584248 0.05140668 0.007799829 ## 1439 6.2848828 0.02574610 0.241768060 ## 1440 6.7972059 0.02067117 0.223016923 Heteroskedacity Although we can often pick up heteroskedacity graphically, we can test for it formally by using the ‘ncvTest’ function in the ‘car’ package ncvTest(fit) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 297.9263, Df = 1, p = &lt; 2.22e-16 We can also use the function ‘spreadLevelPlot’ from the ‘car’ package to create plots for examining potential heteroskedacity in the data. spreadLevelPlot(fit) ## Warning in spreadLevelPlot.lm(fit): ## 47 negative fitted values removed ## ## Suggested power transformation: 0.4508167 Multicollinearity We can calculate the variance inflation factors using the function ‘vif’ vif(fit) ## log10(Mass) log10(AFR) LitterSize ## 2.466580 2.434861 1.759614 Serially correlated residuals Remember, linear modelling assumes that the residuals independent and identically distributed. You can test whether the residuals are serially autocorrelated using the Durban-Watson statistic \\[ d=\\frac{\\sum^{T}_{t=2}(e_{t}-e_{t-1})^{2}}{\\sum^{T}_{t=1}e^{2}_{t}} \\] where T is the number of data points. We can calculate the Durban-Watson test as follows: durbinWatsonTest(fit) ## lag Autocorrelation D-W Statistic p-value ## 1 0.6286888 0.6560464 0 ## Alternative hypothesis: rho != 0 More help with model diagnostics There is another package that can help with model diagnostics called the ‘gvlma’ or “Global Validation of Linear Model Assumptions”. Install ‘gvlma’ library(gvlma) gvmodel&lt;-gvlma(fit) summary(gvmodel) ## ## Call: ## lm(formula = MaxLifespan ~ log10(Mass) + log10(AFR) + LitterSize) ## ## Residuals: ## Min 1Q Median 3Q Max ## -263.66 -78.11 -19.31 53.52 763.75 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -168.4019 26.8904 -6.263 8.35e-10 *** ## log10(Mass) 60.7061 5.6008 10.839 &lt; 2e-16 *** ## log10(AFR) 152.5190 17.5461 8.692 &lt; 2e-16 *** ## LitterSize -0.1925 3.8823 -0.050 0.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 118.7 on 485 degrees of freedom ## (951 observations deleted due to missingness) ## Multiple R-squared: 0.6282, Adjusted R-squared: 0.6259 ## F-statistic: 273.1 on 3 and 485 DF, p-value: &lt; 2.2e-16 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fit) ## ## Value p-value Decision ## Global Stat 1752.4 0 Assumptions NOT satisfied! ## Skewness 285.2 0 Assumptions NOT satisfied! ## Kurtosis 1121.3 0 Assumptions NOT satisfied! ## Link Function 156.2 0 Assumptions NOT satisfied! ## Heteroscedasticity 189.6 0 Assumptions NOT satisfied! Yet another package we won’t discuss but may prove helpful is the ‘lmtest’ package. "],["week-14-lecture.html", "26 Week 14 Lecture 26.1 Week 14 Readings 26.2 What does ‘multivariate’ mean? 26.3 Multivariate associations 26.4 Model criticism for multivariate analyses 26.5 Standardizing your data 26.6 Multivariate outliers 26.7 Brief overview of multivariate analyses 26.8 MANOVA and DFA 26.9 Scaling or ordination techniques 26.10 Principal components analysis (PCA) 26.11 Principal components analysis (PCA) 26.12 PCA in R 26.13 Missing data 26.14 Imputing missing data", " 26 Week 14 Lecture 26.1 Week 14 Readings For this week, I suggest reading Aho Sections 9.17. There are also a suite of papers we will discuss as part of our end-of-semester review for the final exam. I suggest reading them all but I will assign you one to focus on (and present) before class: Anderson et al. (2001), Beninger et al. (2012), Borer et al. (2009), Gelman (2011), Guthery et al. (2001), Makin and Xivry (2019), and Murtaugh (2007). 26.2 What does ‘multivariate’ mean? Multivariate data includes more than one variable recorded from each experimental unit (replicate). For example, Lovett et al. (2000) measured many water chemistry variables (e.g., concentration of nitrate, ammonia, organic N) in each of 39 streams in the Catskill Mountains. We’ve already worked with multivariate data when we fit multiple regressions or ANOVA models with more than one factors. We will be looking into some other commonly used multivariate analyses. 26.3 Multivariate associations We are familiar with the process of measuring variation within a variable in linear regression using sums of squares and variances (which are equivalent to scaled sums of squares). For covariate 1, \\(X_1\\): \\[ \\text{SS} (X_1) = \\sum_{i = 1}^n ( X_{1i} - \\bar{X}_{1} ) ^2 \\] \\[ \\mathrm{Var} (X_1) = \\frac{1}{n - 1} \\sum_{i = 1}^n ( X_{1i} - \\bar{X}_{1} ) ^2 \\] With multivariate data, we also look at the covariation between variables, as the sums of cross products or the covariances. For covariates 1 and 2, \\(X_1, X_2\\): \\[ \\text{SCP} (X_1, X_2) = \\sum_{i = 1}^n ( X_{1i} - \\bar{X}_{1} ) ( X_{2i} - \\bar{X}_{2} ) \\] \\[ \\mathrm{Cov} (X_1, X_2) = \\frac{1}{n - 1} \\sum_{i = 1}^n ( X_{1i} - \\bar{X}_{1} ) ( X_{2i} - \\bar{X}_{2} ) \\] We can do this for all of our covariates and get a matrix of the sums of squares and sums of cross products or a matrix of the variances and covariances. We could also standardize the variances and covariances by their standard deviations and get a matrix of correlations (\\(r\\)). For multivariate analyses, we use the matrix of associations to find linear combinations of the covariates in the original dataset. We do this in order to summarize the variation in the original data set using new derived covariates (each new covariate involves all of the original covariates). Only the first couple of the new derived covariates may explain a lot of the variation in the data, which is useful when we have a lot of covariates. When we have \\(n\\) observations and \\(p\\) covariates, our new covariate \\(z_{j}\\) for observation \\(j\\) is: \\[z_j = c_1 X_{1j} + c_2 X_{2j} + \\dots + c_p X_{pj}\\] \\(c_1\\) through \\(c_p\\) are coefficients that describe how much each original covariate contributes to the new covariate, \\(z\\). There will be as many new covariates \\(z\\) as there were original ones (\\(z_{1j}, z_{2j}, \\dots, z_{pj}\\)). Note that you could also calculate \\(z\\) and \\(c\\) using the original matrix of data rather than the matrix of associations (check your R package information to see which method is used). What do we estimate in a multivariate analysis? Each \\(z_j\\) (remember, there are \\(p\\) of these for each observation \\(j\\)) is called a score. The entire equation above (again, there are \\(p\\) of these), is called a component. The set of coefficients for each \\(z\\), \\(c\\), are called eigenvectors. So, there are \\(p\\) eigenvectors, each of length \\(p\\). They are often scaled, depending on your multivariate method. The total amount of variation explained by each eigenvector is called the eigenvalue, \\(\\lambda\\). There are \\(p\\) eigenvalues. The eigenvectors and eigenvalues of the \\(p\\) components is what we use in a principal components analysis, canonical correlaion analysis, or a correspondence analysis. If we have data that can be grouped, we can estimate the components in order to maximize between-group differences relative to within-group differences. This is what we get out of multivariate ANOVA (MANOVA) or discriminant function analysis. Note that if you are working with species abundances, species presence-absences, or genetic data, we might calculate the dissimilarity between observations using a dissimilarity matrix. For example, we might have the species abundances of aquatic invertebrates sampled from 30 ponds. Each pond has different species and abundances of species. We can calculate the dissimilarity of each pond to every other pond based on the species composition of each (for example, using Bray-Curtis distance, or another metric of distance). Then, we can use a multivariate analysis to summarize the variation among all ponds. Ponds that share species and have similar abundances will be closer together in multivariate space. library(vegan) library(MASS) data(varespec) # % cover of lichen spp. in 24 sites vare.dis &lt;- vegdist(varespec) vare.dis ## 18 15 24 27 23 19 22 ## 15 0.5310021 ## 24 0.6680661 0.3597783 ## 27 0.5621247 0.4055610 0.4934947 ## 23 0.3747078 0.3652097 0.5020306 0.4286111 ## 19 0.5094738 0.4560757 0.5092318 0.4878190 0.3606242 ## 22 0.6234419 0.3579517 0.5010050 0.4655224 0.4812706 0.4726483 ## 16 0.5337610 0.3976674 0.5907623 0.5683930 0.4094312 0.4496731 0.2678031 ## 28 0.8418209 0.5225414 0.5736665 0.3027802 0.6979519 0.6431734 0.5985666 ## 13 0.3453347 0.6063846 0.7576747 0.7543736 0.6221471 0.5739244 0.6948736 ## 14 0.5449810 0.4803756 0.6533606 0.7467915 0.5645808 0.6331942 0.5357609 ## 20 0.3879069 0.3784188 0.4346892 0.4957833 0.2877014 0.3953776 0.4627020 ## 25 0.6318891 0.3376115 0.3369098 0.5001593 0.4258617 0.4311299 0.3822981 ## 7 0.3603697 0.6717391 0.7931069 0.7792917 0.6390838 0.6958570 0.7459886 ## 5 0.4955699 0.7178612 0.8561753 0.8732190 0.7295255 0.7898205 0.8611451 ## 6 0.3382309 0.6355122 0.7441373 0.7496935 0.6252483 0.5684030 0.7249162 ## 3 0.5277480 0.7578503 0.8382119 0.8090236 0.7128798 0.5302756 0.8026152 ## 4 0.4694018 0.6843974 0.8309875 0.8413800 0.7117919 0.5177604 0.8015314 ## 2 0.5724092 0.8206269 0.8372551 0.7581924 0.7249869 0.5389222 0.8321464 ## 9 0.6583569 0.7761039 0.7590517 0.7415898 0.6693889 0.5393143 0.7725082 ## 12 0.4688038 0.6794199 0.6894538 0.6253616 0.5384762 0.4288556 0.7051751 ## 10 0.6248996 0.7644564 0.7842829 0.7096540 0.6625476 0.5059910 0.7875328 ## 11 0.4458523 0.4716274 0.5677373 0.6322919 0.4710280 0.3293493 0.5812219 ## 21 0.5560864 0.7607281 0.7272727 0.5456001 0.4951221 0.5315894 0.6771167 ## 16 28 13 14 20 25 7 ## 15 ## 24 ## 27 ## 23 ## 19 ## 22 ## 16 ## 28 0.7015360 ## 13 0.5514941 0.8600122 ## 14 0.4826350 0.8239667 0.5547565 ## 20 0.3737797 0.6963560 0.5785542 0.5115258 ## 25 0.4306058 0.6086150 0.7412605 0.5541517 0.4518556 ## 7 0.6596144 0.8960202 0.4533054 0.6550830 0.5959162 0.7556726 ## 5 0.7184789 0.9539592 0.5148988 0.7257681 0.7153827 0.8600858 0.3237446 ## 6 0.6509879 0.9014440 0.3515673 0.6227473 0.5439118 0.7343872 0.1754713 ## 3 0.6837953 0.9234485 0.4965478 0.7836661 0.6690479 0.8168684 0.5154487 ## 4 0.6462648 0.9381169 0.3881748 0.6734743 0.6771854 0.8400134 0.5601721 ## 2 0.7354202 0.9053213 0.5968691 0.8592489 0.6951539 0.8179089 0.6465777 ## 9 0.8185866 0.8686670 0.7292530 0.8282497 0.6982486 0.7884243 0.8318435 ## 12 0.6342166 0.8543167 0.5902386 0.7507074 0.5182426 0.7062564 0.6991666 ## 10 0.7656598 0.9016604 0.7160439 0.8304088 0.6706349 0.7845955 0.7697453 ## 11 0.5172825 0.7544064 0.4272808 0.6743277 0.4461712 0.6175930 0.5262233 ## 21 0.7474559 0.7248773 0.7212772 0.8096450 0.6320431 0.7466232 0.7933350 ## 5 6 3 4 2 9 12 ## 15 ## 24 ## 27 ## 23 ## 19 ## 22 ## 16 ## 28 ## 13 ## 14 ## 20 ## 25 ## 7 ## 5 ## 6 0.3984538 ## 3 0.5634432 0.4517627 ## 4 0.5377506 0.4665100 0.3592689 ## 2 0.7257597 0.5552754 0.2099203 0.4841145 ## 9 0.9014583 0.7223126 0.3885811 0.6222340 0.2330286 ## 12 0.7808641 0.5762462 0.2641851 0.4870742 0.1846147 0.2277228 ## 10 0.8504191 0.6567926 0.3413378 0.5776062 0.1456729 0.1117280 0.1793368 ## 11 0.5563798 0.4077948 0.3002597 0.3215966 0.4209596 0.5145260 0.3688102 ## 21 0.8888316 0.6720141 0.7507773 0.7641304 0.6779661 0.5952563 0.5602137 ## 10 11 ## 15 ## 24 ## 27 ## 23 ## 19 ## 22 ## 16 ## 28 ## 13 ## 14 ## 20 ## 25 ## 7 ## 5 ## 6 ## 3 ## 4 ## 2 ## 9 ## 12 ## 10 ## 11 0.5043578 ## 21 0.6147874 0.6713363 26.4 Model criticism for multivariate analyses 26.4.1 Transforming your data Just like last week, we are still looking at the linear relationships. The only difference is that now we are looking at the relationships among covariates (last week, we looked for a linear relationship between the response and the covariate). This means that we may need to transform covariates that do not have linear relationships with one another. 26.5 Standardizing your data You may want to standardize your data (divide by the standard deviation) if the covariates are on different scales from one another, or have very different variances. However, if the scales are comparable, leaving the data unstandardized can be informative. You may want to conduct your multivariate analyses on both standardized and unstandardized data. The default behavior in R packages is not obvious, be sure to check the help files. 26.6 Multivariate outliers Outliers can have a lot of influence in multivariate analyses too. You can look for outliers in multivariate space (values far away from most of the density of the dataset) by calculating the Mahalanobis distances of each observation and comparing it to the null distribution (chi-square distribution with p degrees of freedom). Other than using a different criterion to determine what points are outliers, the procedures for dealing with outliers are the same. 26.7 Brief overview of multivariate analyses If you plan on using multivariate analyses in your work, I recommend chapters 15 through 18 in Quinn and Keough 2002 (the previous textbook for this course, available for free online through the SBU library). Multivariate analyses are much easier to implement in R than they are to understand, and Quinn and Keough give a detailed but readable overview of these methods. The goal in lecture today is to make sure that you are aware of these methods. 26.8 MANOVA and DFA If you have two or more response variables and one or more categorical covariates, you may be tempted to fit multiple ANOVAs. Though a single ANOVA is an omnibus test that controls for Type I error, fitting multiple ANOVAs risks inflating Type I error. You could account for this using a correction for familywise error rate. However, this reduces power. Instead, you could fit a multivariate analysis of variance (MANOVA), which fits a linear combination of the response variables in a way that maximizes the between-group to within-group variances. Discriminant function analysis uses the same methods, but the goal is instead to predict group membership. 26.9 Scaling or ordination techniques The goals of these types of analyses (principal components analysis, correspondence analysis, canonical correspondence analysis, multidimensional scaling) are (1) to reduce a large set of covariates to a smaller number of derived covariates, and (2) to reveal patterns in the data in multidimensional space. We will only be able to go over PCA today, but the other methods are covered in Quinn and Keough (2002). # example of MDS using lichen data by site vare.mds0 &lt;- isoMDS(vare.dis, trace = FALSE) ordiplot(vare.mds0, type = &quot;t&quot;) 26.10 Principal components analysis (PCA) The goal of PCA is to reduce a set of variables to a smaller set that captures most of the variability in your data. Mathematically, PCA represents a transformation of your original axes to a new set of orthogonal axes which are ordered in terms of how much variation they explain (the \\(z\\) components from before). Each axis is called a principal component (PC). You will have as many PCs as you have covariates. The first principal component (PC1) is the axis which explains the most variation in the data. PC2 is the axis which is orthogonal (perpendicular) to PC1 and explains the next greatest amount of remaining variation. PC3 the axis orthogonal to both PC1 and PC2 and explains the next greatest amount of remaining variation, etc. 26.11 Principal components analysis (PCA) Using the example I described at the beginning of the lecture, Lovett et al. (2000) measured water chemistry variables in each of 39 streams in the Catskill Mountains. Lovett and colleagues looked at each covariate and decided to log transform dissolved organic C, Cl, and H. streams.init &lt;- read.csv(&quot;_data/lovett2.csv&quot;) streams &lt;- streams.init[, c(6:8, 16, 9, 11, 17, 13, 14, 18)] row.names(streams) &lt;- streams.init[, 1] str(streams) ## &#39;data.frame&#39;: 38 obs. of 10 variables: ## $ NO3 : num 24.2 25.4 29.7 22.1 13.1 27.5 28.1 31.2 22.6 35.9 ... ## $ TON : num 5.6 4.9 4.4 6.1 5.7 3 4.7 5.4 3.1 4.9 ... ## $ TN : num 29.9 30.3 33 28.3 17.6 30.8 32.8 37.1 26 39.8 ... ## $ LDOC: num 2.26 2.04 2.02 1.93 1.92 ... ## $ NH4 : num 0.8 1.4 0.8 1.4 0.6 1.1 1.4 2.5 3.1 1.4 ... ## $ SO4 : num 50.6 55.4 56.5 57.5 58.3 63 66.5 64.5 63.4 58.4 ... ## $ LCL : num 1.19 1.21 1.23 1.23 1.26 ... ## $ CA : num 54.7 58.4 65.9 59.5 54.6 68.5 84.6 73.1 71.1 91.2 ... ## $ MG : num 14.4 17 19.6 19.5 21.9 22.4 26.2 25.4 21.8 22.2 ... ## $ LH : num -0.319 -0.62 -0.328 -0.638 -0.432 ... Some of the covariates are correlated, and would be collinear in a regression. The components from a PCA are orthogonal, removing multicollinearity. pairs(streams) 26.12 PCA in R We can fit a PCA using prcomp(). There are a lot of functions for PCAs, carefully read the help files to understand how the function is fitting the PCA before using in your own analyses. The variances of the water chemistry covariates are very variable. It is probably best if we scale our data with the argument scale = TRUE (note this is not the default in R!). apply(X = streams, MARGIN = 2, var) # variance of each covariate ## NO3 TON TN LDOC NH4 SO4 ## 74.18850640 1.63670697 65.60399004 0.02144044 0.53607397 27.24332859 ## LCL CA MG LH ## 0.02407754 194.74177098 26.24953058 0.08566604 pca.streams &lt;- prcomp(streams, scale = TRUE) We can look at our eigenvalues in R using summary(). The square root of the eigenvalue is labeled Standard deviation. We can also see the proportion of variation explained in each component (labeled Proportion of Variance). summary(pca.streams) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.8504 1.5725 1.0822 0.96516 0.86381 0.80397 0.61930 ## Proportion of Variance 0.3424 0.2473 0.1171 0.09315 0.07462 0.06464 0.03835 ## Cumulative Proportion 0.3424 0.5897 0.7068 0.79995 0.87456 0.93920 0.97755 ## PC8 PC9 PC10 ## Standard deviation 0.36161 0.30237 0.04784 ## Proportion of Variance 0.01308 0.00914 0.00023 ## Cumulative Proportion 0.99063 0.99977 1.00000 We have as many eigenvalues as covariates (10). PC1 explains 34% of the variation in the data, PC2 explains 25%, and PC3 explains 12%. So, we can use just the first three components to describe over 70% of the total variance in the data. We can look at PC1 more closely by looking at the eigenvectors, or the vector of the coefficients of the component equation. The further away each coefficient is from 0, the greater contribution that variable makes to the component. \\[z_j = c_1 (\\text{NO}_3)_j + c_2 (\\text{total organic N})_j + c_3 (\\text{total N})_j + \\\\ c_4 (\\text{NH}_4)_j + c_5 (\\text{log dis. organic C})_j + c_6 (\\text{SO}_4)_j \\\\ c_7 (\\text{log Cl})_j + c_8 (\\text{Ca})_j + c_9 (\\text{Mg})_j + c_{10} (\\text{log H})_j\\] round(pca.streams$rotation[, 1], digits = 2) ## NO3 TON TN LDOC NH4 SO4 LCL CA MG LH ## -0.26 0.15 -0.23 -0.29 0.23 0.37 0.36 0.28 0.47 -0.40 Large values of PC1 are associated with having higher Mg, higher log Cl, higher S04, and lower log H. We can visualize our PCA using either the biplot() function in base R or fviz_pca_biplot() in the package factoextra. library(factoextra) fviz_pca_biplot(pca.streams, axes = c(1, 2), repel = TRUE, col.var = &quot;dodgerblue3&quot;, col.ind = &quot;gray60&quot;, title = NULL) The coordinates of each stream \\(j\\) are equal to the scores \\(z_j\\) for PC1 and PC2 (\\(z_{1j}, z_{2j}\\)). # PCA scores for PC1 and PC2 of the first five streams pca.streams$x[1:5, 1:2] ## PC1 PC2 ## Santa Cruz -3.7157577 -1.3260985 ## Colgate -2.1068761 -0.3092199 ## Halsey -2.4108484 0.3108805 ## Batavia Hill -1.1467251 -0.6644800 ## Windham Ridg -0.8618198 -2.0450124 The vectors are proportional to the eigenvectors, or the contribution of each covariate to the component. # Eigenvectors for PC1 and PC2 pca.streams$rotation[, 1:2] ## PC1 PC2 ## NO3 -0.2608012 0.51882617 ## TON 0.1470633 -0.29927264 ## TN -0.2284551 0.51019471 ## LDOC -0.2882753 -0.14662403 ## NH4 0.2279503 -0.07531484 ## SO4 0.3684647 0.22497994 ## LCL 0.3578632 -0.15795720 ## CA 0.2811469 0.44581193 ## MG 0.4716302 0.01538673 ## LH -0.3972261 -0.28148575 It is possible to fit linear regression in which the covariates are principal components from a PCA. This way, the covariates are not collinear. The problem is that the regression coefficients are particularly difficult to interpret. However, if you have a PCA with easily interpretable eigenvectors, this might be a useful method. Additionally, be aware that the components that explain most of the variance in the covariates may not explain the variation in the response variable. 26.13 Missing data In large, multivariate data sets, you may have observations that are missing values for some of the covariates. We saw this in lab last week. mammal &lt;- read.csv(&quot;_data/MammalLifeHistory.csv&quot;,header=T) carnivore.init &lt;- mammal[which(mammal$Order == &quot;Carnivora&quot;), ] carnivore.init$AFR[1:12] ## [1] 24.00 NA 12.00 10.50 12.00 10.50 NA 12.00 NA NA NA 16.81 Last week, we removed the entire row for observations with one or more missing values for covariates. This is the default behavior in R, for example, when fitting multiple regressions with lm(). However, this is throwing away all of the information we do have for the observation. 26.14 Imputing missing data In problem set 13, you were asked to impute missing data by sampling with replacement from the nonmissing data. There are other ways of dealing with missing data too. You could replace the missing data with the mean value for that covariate. This means that the missing point won’t contribute to the estimate of the partial slope for the missing covariate. However, it will lead to an underestimate of the variance for the estimate (doesn’t add to SSE). For the regression: \\[ \\text{Maximum lifespan} \\sim \\text{Intercept} + \\text{AFR} \\] We could impute missing values of AFR with the mean value of AFR. carnivore &lt;- carnivore.init missing.AFR &lt;- which(is.na(carnivore.init$AFR)) mean.AFR &lt;- mean(carnivore.init$AFR, na.rm = TRUE) carnivore$AFR[missing.AFR] &lt;- mean.AFR You could also use a regression to predict the missing value for one covariate given the other covariates in the data. Using the same example, we could impute missing values of AFR using the coefficients estimated from a regression where AFR is the response variable with one or more other covariates as the predictors. For example: \\[ \\text{AFR} \\sim \\text{Intercept} + \\text{Litter size} \\] imputation.lm &lt;- lm(formula = AFR ~ LitterSize, data = carnivore.init) predicted.AFR &lt;- imputation.lm$coefficients[1] + imputation.lm$coefficients[2] * carnivore.init$LitterSize[missing.AFR] carnivore$AFR[missing.AFR] &lt;- predicted.AFR "],["week-14-lab.html", "27 Week 14 Lab 27.1 Missing at random - practice with GLMs 27.2 Finally, a word about grades", " 27 Week 14 Lab In lab we’ll go through Some practice with PCA using the semester survey results Some practice with GLMs using the semester survey results There are a number of functions you could use in R to do principal components analysis. We will use the ‘prcomp’ function, but there is a very closely related function called ‘princomp’ as well as a function called ‘principal’ which is in the ‘psych’ package. readings&lt;-read.csv(&quot;~/Dropbox/Biometry/Week 14 Multivariate analyses and Review/Week 14 Lab/Readings 2023.csv&quot;,header=T) missing&lt;-is.na(readings$Useful)|is.na(readings$Difficult)|is.na(readings$Interesting) Useful&lt;-aggregate(readings$Useful[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x Difficult&lt;-aggregate(readings$Difficult[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x Interesting&lt;-aggregate(readings$Interesting[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x #Length.means.readings&lt;-aggregate(readings$Length[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x pca.result&lt;-prcomp(~Useful+Interesting+Difficult,retx=T) Before printing out the result, let’s make sure everyone understands what I was doing with the aggregate commands, and how the ‘prcomp’ function input works. To print out a summary of the PCA, we use summary(pca.result) ## Importance of components: ## PC1 PC2 PC3 ## Standard deviation 0.9046 0.4566 0.29753 ## Proportion of Variance 0.7337 0.1869 0.07937 ## Cumulative Proportion 0.7337 0.9206 1.00000 We see that PCA1 is associated with over 73% of the variation in responses. So, what is PCA1? pca.result$rotation ## PC1 PC2 PC3 ## Useful 0.2565179 0.7697893 0.5844853 ## Interesting 0.5521851 0.3796006 -0.7422904 ## Difficult -0.7932781 0.5131548 -0.3276918 PCA1 is an axis which describes papers that are more Interesting and less Difficult, with a very small weight towards papers that are Useful. In other words, a large positive PCA1 score would be associated with an interesting paper that was easy to read. Note that the principal components denote an axis, but the direction is arbitrary. Since no direction is implied by the sign, we do not interpret this as saying that most papers were interesting and easy. Instead we would say that the papers largely fall along a common axis in which Interesting/Easy to read papers are at one end, and Boring/Difficult to read papers are at the other end. (For now I am ignoring the smaller influence of Useful on PCA1.) We can visualize this using the function ‘biplot’ biplot(pca.result) Biplots take some getting used to, and when they have many more dimensions, they become increasingly difficult to interpret. However, papers high on PC1 are generally Interesting and Easy to read and papers low on PC1 are generally Boring and more Difficult to read. Papers high on PC2 are generally more Useful and slightly more Difficult and papers low on PC2 are generally less Useful but less Difficult to read. It’s worth noting that there is a nice spread of papers in this bi-plot, so at least along these two axes, there were no real outliers. Also, keep in mind that since the PCA is picking up relative differences, it is not possible for all papers to end up in one corner of the plot. While of course we would want all papers to be highly Interesting and highly Useful, even if all papers ranked high on both measures overall, the PCA will identify axes of differences so there will always be some that show up as “relatively less Interesting” - does this make sense? Which papers were highly positive on PC2 and also positive on PC1? These are papers that were Useful and also more Interesting than the average. readings[readings$Index==15,1][1] ## [1] &quot;Siddhartha, R. D., E. B. Fowlkes, and B. Hoadley. 1989. Risk analysis of the space shuttle: Pre-challenger prediction of failure. Journal of the American Statistical Association 84(408): 945-957.&quot; readings[readings$Index==21,1][1] ## [1] &quot;Aho, K., D. DeWayne, and T. Peterson. 2014. Model selection for ecologists: the worldviews of AIC and BIC. Ecology 95(3): 631-636.&quot; readings[readings$Index==4,1][1] ## [1] &quot;Johnson, D.H. 2002. The role of hypothesis testing in wildlife science. The Journal of Wildlife Management 66(2): 272-276.&quot; readings[readings$Index==25,1][1] ## [1] &quot;Johnson, J.B., and K.S. Omland. 2004. Model selection in ecology and evolution. TRENDS in Ecology and Evolution 19(2): 101-108.&quot; I totally agree on the Siddhartha paper! Johnson’s paper’s also fall in this qudrant as well. You can play around with this yourself and see why I added the [1] at the end. When I pull out the rows with the Index identified by the PCA, I get the list of all entries (since we had &gt;1 team rating the papers) and so I only print the first one. Which paper fell out along the Difficult axis? readings[readings$Index==22,1][1] ## [1] &quot;Burnham et al. 2011. AIC model selection and multimodel inference in behavioral ecology: some background, observations, and comparisons. Behavior, Ecology, and Sociobiology 65: 23-35.&quot; Burnham’s AIC model selection paper. Also, readings[readings$Index==6,1][1] ## [1] &quot;Bender, R., and S. Lange. 2001. Adjusting for multiple testing – when and how? Journal of Clinical Epidemiology 54: 343-349.&quot; Usually Bolker holds this honor! Bolker is often rated as difficult. I keep this chapter around because his thinking is so “spot on” and the material in his book will serve you well if you continue on doing quantitative modelling. I’m a little surprised to see Bender and Lange here, but every year is different. The real quadrant that I investigate closely are papers that are considered Difficult but not all that Useful. Through this PCA exeercise, I have eliminated papers consistently falling into this corner. Let’s see what 16 is… readings[readings$Index==16,1][1] ## [1] &quot;Altman, N., and M. Krzywinski. 2015. Sources of variation. Nature Methods 12(1): 5-6. (optional)&quot; I’m surprised to see Altman here, as the Points of Significance papers are so short and I think quite well written. I think this probably means we just need to spend a bit of time going over this one next year. One thing to keep in mind is that a PCA identifies variation in the dataset. It’s worth putting these numbers in context of the overall means. mean(Useful) ## [1] 3.874055 mean(Difficult) ## [1] 2.08303 mean(Interesting) ## [1] 3.372958 So the average reading scored pretty high for being Useful and Interesting and was rated below average for Difficulty, so on the whole, I’m fairly pleased with these ratings. You might be interested in how these ratings have changed over time (I was!). Let’s start with the readings. library(readxl) Biometry_change &lt;- read_excel(&quot;_data/Biometry_change.xlsx&quot;) fitR_U&lt;-summary(lm(Biometry_change$R_U_mean~Biometry_change$Year)) fitR_U ## ## Call: ## lm(formula = Biometry_change$R_U_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.36439 -0.26399 0.06366 0.16202 0.38224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -85.55852 51.82955 -1.651 0.150 ## Biometry_change$Year 0.04416 0.02569 1.719 0.136 ## ## Residual standard error: 0.2946 on 6 degrees of freedom ## Multiple R-squared: 0.33, Adjusted R-squared: 0.2183 ## F-statistic: 2.955 on 1 and 6 DF, p-value: 0.1364 plot(Biometry_change$Year,Biometry_change$R_U_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Response (out of 5)&quot;,cex=2,ylim=c(1.5,4.5),main=&quot;Useful (black), Difficult (red), Interesting (green)&quot;) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$R_U_mean[i]-2*Biometry_change$R_U_se[i],y1=Biometry_change$R_U_mean[i]+2*Biometry_change$R_U_se[i]) } fitR_D&lt;-summary(lm(Biometry_change$R_D_mean~Biometry_change$Year)) fitR_D ## ## Call: ## lm(formula = Biometry_change$R_D_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29893 -0.12886 0.03503 0.12404 0.31271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -39.63356 39.88751 -0.994 0.359 ## Biometry_change$Year 0.02076 0.01977 1.050 0.334 ## ## Residual standard error: 0.2267 on 6 degrees of freedom ## Multiple R-squared: 0.1553, Adjusted R-squared: 0.01454 ## F-statistic: 1.103 on 1 and 6 DF, p-value: 0.334 points(Biometry_change$Year,Biometry_change$R_D_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Difficult&quot;,cex=2,col=2) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$R_D_mean[i]-2*Biometry_change$R_D_se[i],y1=Biometry_change$R_D_mean[i]+2*Biometry_change$R_D_se[i],col=2) } fitR_I&lt;-summary(lm(Biometry_change$R_I_mean~Biometry_change$Year)) fitR_I ## ## Call: ## lm(formula = Biometry_change$R_I_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19171 -0.11413 -0.03823 0.12481 0.21891 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -72.68056 29.30218 -2.480 0.0478 * ## Biometry_change$Year 0.03766 0.01452 2.593 0.0410 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1665 on 6 degrees of freedom ## Multiple R-squared: 0.5284, Adjusted R-squared: 0.4498 ## F-statistic: 6.724 on 1 and 6 DF, p-value: 0.04105 points(Biometry_change$Year,Biometry_change$R_I_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Difficult&quot;,cex=2,col=3) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$R_I_mean[i]-2*Biometry_change$R_I_se[i],y1=Biometry_change$R_I_mean[i]+2*Biometry_change$R_I_se[i],col=3) } So it looks like while the overall trend is towards readings that are more Interesting, more Difficult, and more Useful, the only one of these that is statistically significant is the trend towards more Interesting. The problem sets show exactly the same trends, with no significant changes in Useful or Difficult but a significant trend towards ore Interesting problem sets. PS&lt;-read.csv(&quot;~/Dropbox/Biometry/Week 14 Multivariate analyses and Review/Week 14 Lab/ProblemSets 2022.csv&quot;,header=T) missing&lt;-is.na(PS$Useful)|is.na(PS$Difficult)|is.na(PS$Interesting) Useful.means.PS&lt;-aggregate(PS$Useful[!missing], by=list(Index=PS$Week[!missing]),FUN=mean)$x Difficult.means.PS&lt;-aggregate(PS$Difficult[!missing], by=list(Week=PS$Week[!missing]),FUN=mean)$x Interesting.means.PS&lt;-aggregate(PS$Interesting[!missing], by=list(Week=PS$Week[!missing]),FUN=mean)$x pca.result&lt;-prcomp(~Useful.means.PS+Interesting.means.PS+Difficult.means.PS,data=PS,retx=T) Notice that it has simply labeled them in order, so 7=Week #9 PS, 8=Week #10 PS, 9=Week #11 PS, 10=Week #12 PS, and 11=Week #13 PS. To print out a summary of the PCA, we use summary(pca.result) ## Importance of components: ## PC1 PC2 PC3 ## Standard deviation 0.8474 0.6995 0.2560 ## Proportion of Variance 0.5641 0.3844 0.0515 ## Cumulative Proportion 0.5641 0.9485 1.0000 We see that for the problem sets, PC1 is less dominant (56% of the variation). So, what is PCA1? pca.result$rotation ## PC1 PC2 PC3 ## Useful.means.PS -0.7894404 -0.1122903 -0.6034689 ## Interesting.means.PS -0.4937346 -0.4679620 0.7329650 ## Difficult.means.PS 0.3647054 -0.8765857 -0.3139865 PC1 combines all three factors with the largest component being focused on “Useful”, and the axis divides problem sets judged Useless/Boring/Difficult and those that are Useful/Interesting/Easy. (Reminder: the signs of the PCs is arbitrary, so the signs on the rotation could have all be flipped.) Looking across all the PC axes, we want papers that are low (negative) on PC1 and low (negative) on PC2 (though these are also slightly less Useful). PC3 is a toss up, because that axis represents a trade-off between Useful and Interesting. biplot(pca.result) We can see that problem set 6 is the one that is really driving variation here! (As always) If we were to eliminate week 6, the others are all varying primarily on PC2. Again, looking at the means: mean(Useful.means.PS) ## [1] 4.348485 mean(Difficult.means.PS) ## [1] 3.643939 mean(Interesting.means.PS) ## [1] 4.155303 The problem sets overall rated as being very Useful and Interesting but also sort of Difficult. How have things changed over time? fitPS_U&lt;-summary(lm(Biometry_change$PS_U_mean~Biometry_change$Year)) fitPS_U ## ## Call: ## lm(formula = Biometry_change$PS_U_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35934 -0.10749 -0.06325 0.05777 0.54458 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -73.58452 49.66576 -1.482 0.189 ## Biometry_change$Year 0.03848 0.02461 1.563 0.169 ## ## Residual standard error: 0.2823 on 6 degrees of freedom ## Multiple R-squared: 0.2894, Adjusted R-squared: 0.171 ## F-statistic: 2.444 on 1 and 6 DF, p-value: 0.169 plot(Biometry_change$Year,Biometry_change$PS_U_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Response (out of 5)&quot;,cex=2,ylim=c(2,5),main=&quot;Useful (black), Difficult (red), Interesting (green)&quot;) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$PS_U_mean[i]-2*Biometry_change$PS_U_se[i],y1=Biometry_change$PS_U_mean[i]+2*Biometry_change$PS_U_se[i]) } fitPS_D&lt;-summary(lm(Biometry_change$PS_D_mean~Biometry_change$Year)) fitPS_D ## ## Call: ## lm(formula = Biometry_change$PS_D_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40591 -0.18510 0.03417 0.16826 0.38526 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.20979 50.25999 -0.601 0.570 ## Biometry_change$Year 0.01655 0.02491 0.664 0.531 ## ## Residual standard error: 0.2856 on 6 degrees of freedom ## Multiple R-squared: 0.0685, Adjusted R-squared: -0.08675 ## F-statistic: 0.4412 on 1 and 6 DF, p-value: 0.5312 points(Biometry_change$Year,Biometry_change$PS_D_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Difficult&quot;,cex=2,col=2) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$PS_D_mean[i]-2*Biometry_change$PS_D_se[i],y1=Biometry_change$PS_D_mean[i]+2*Biometry_change$PS_D_se[i],col=2) } fitPS_I&lt;-summary(lm(Biometry_change$PS_I_mean~Biometry_change$Year)) fitPS_I ## ## Call: ## lm(formula = Biometry_change$PS_I_mean ~ Biometry_change$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.44739 -0.17128 -0.01723 0.15788 0.44258 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -137.90978 55.40599 -2.489 0.0472 * ## Biometry_change$Year 0.07004 0.02746 2.551 0.0435 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3149 on 6 degrees of freedom ## Multiple R-squared: 0.5202, Adjusted R-squared: 0.4403 ## F-statistic: 6.506 on 1 and 6 DF, p-value: 0.04345 points(Biometry_change$Year,Biometry_change$PS_I_mean,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean Difficult&quot;,cex=2,col=3) for (i in 1:8) { segments(x0=Biometry_change$Year[i], x1=Biometry_change$Year[i], y0=Biometry_change$PS_I_mean[i]-2*Biometry_change$PS_I_se[i],y1=Biometry_change$PS_I_mean[i]+2*Biometry_change$PS_I_se[i],col=3) } Interestingly, this is the same pattern seen for the Readings, with everything getting slightly (but not significantly) more Useful and Difficult over time but statistically significantly more Interesting. 27.1 Missing at random - practice with GLMs One of the things we can do with this dataset is to ask whether data were missing at random, since not all readings were given ratings. missing&lt;-is.na(readings$Useful)|is.na(readings$Difficult)|is.na(readings$Interesting) Useful&lt;-aggregate(readings$Useful[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x Difficult&lt;-aggregate(readings$Difficult[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x Interesting&lt;-aggregate(readings$Interesting[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x Length.means.readings&lt;-aggregate(readings$Length[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x One could ask the question, are these data missing at random? In the problem set for Week #13, we completed the dataset using random imputation. In other words, we assumed that data were missing at random and we drew with replacement from the other values to replace missing datapoints. However, in this case, it seems likely that data are not missing at random. I suspect that papers were not evaluated because no one read them, and that something about the papers may predict whether the papers were read or not. We can answer this question by constructing a model for “missingness” which assumes that the probability of being evaluated is distributed as Binom(n,p) where p is the probability of being evaluated (and presumably, of having been read in the first place). First, I need to go through the data and figure out how many times a paper was evaluated. num.missing&lt;-vector(length=max(readings$Index)) for (i in 1:max(readings$Index)) { num.missing.useful&lt;-sum(as.numeric(is.na(readings$Useful[readings$Index==i]))) num.missing.difficult&lt;-sum(as.numeric(is.na(readings$Difficult[readings$Index==i]))) num.missing.interesting&lt;-sum(as.numeric(is.na(readings$Interesting[readings$Index==i]))) max.missing&lt;-max(num.missing.useful,num.missing.difficult,num.missing.interesting) num.missing[i]&lt;-max.missing } For simplicity, I am considering “evaluated” as evaluated for all three categories (Useful, Difficult, and Interesting). Now I use a Binomial GLM to model the probability of being evaluated as a function of Useful, Interesting, and Difficult (as rated by the other groups). Note that there were 11 groups total, so n=11. fit&lt;-glm(cbind(11-num.missing,num.missing)~Useful+Difficult+Interesting,family=&quot;binomial&quot;) summary(fit) ## ## Call: ## glm(formula = cbind(11 - num.missing, num.missing) ~ Useful + ## Difficult + Interesting, family = &quot;binomial&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8889 0.2226 0.3517 0.5553 1.0689 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.9095 4.7986 -1.857 0.0634 . ## Useful 1.8824 1.0529 1.788 0.0738 . ## Difficult 0.6784 0.8084 0.839 0.4013 ## Interesting 1.3888 1.2207 1.138 0.2553 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 33.997 on 34 degrees of freedom ## Residual deviance: 20.274 on 31 degrees of freedom ## AIC: 41.242 ## ## Number of Fisher Scoring iterations: 6 None of the covariates are significant. We might suspect a high degree of multicollinearity among the predictors. We can use PCA to create new orthogonal covariates which (more efficiently) capture the variability in the survey results. I will rerun the PCA for the readings. pca.result&lt;-prcomp(~Useful+Interesting+Difficult,retx=T) summary(pca.result) ## Importance of components: ## PC1 PC2 PC3 ## Standard deviation 0.9046 0.4566 0.29753 ## Proportion of Variance 0.7337 0.1869 0.07937 ## Cumulative Proportion 0.7337 0.9206 1.00000 pca.result$rotation ## PC1 PC2 PC3 ## Useful 0.2565179 0.7697893 0.5844853 ## Interesting 0.5521851 0.3796006 -0.7422904 ## Difficult -0.7932781 0.5131548 -0.3276918 PCA1 captures about 73% of the variability, so we try using just PCA1 in our GLM. fit&lt;-glm(cbind(11-num.missing,num.missing)~pca.result$x[,1],family=&quot;binomial&quot;) summary(fit) ## ## Call: ## glm(formula = cbind(11 - num.missing, num.missing) ~ pca.result$x[, ## 1], family = &quot;binomial&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0307 0.3584 0.5512 0.6803 1.2636 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.0506 0.4411 9.182 &lt;2e-16 *** ## pca.result$x[, 1] 0.9334 0.4302 2.170 0.03 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 33.997 on 34 degrees of freedom ## Residual deviance: 28.604 on 33 degrees of freedom ## AIC: 45.572 ## ## Number of Fisher Scoring iterations: 6 Aha! So none of the individual covariates are statistically significant, but in part this is because they are co-linear and so the variances are inflated. When we use the first PCA axis, the model does show a statistically significant trend between papers that are Useful, Interesting, and Easy and the probability that the paper was read. Is there an effect of paper length to consider? After all, that’s probably the most salient feature of a paper when you first download it. fit&lt;-glm(cbind(11-num.missing,num.missing)~Length.means.readings,family=&quot;binomial&quot;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(fit) ## ## Call: ## glm(formula = cbind(11 - num.missing, num.missing) ~ Length.means.readings, ## family = &quot;binomial&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.58226 0.00104 0.00981 0.10240 0.95209 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7617 1.3958 -0.546 0.5853 ## Length.means.readings 1.3101 0.6014 2.178 0.0294 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 33.9971 on 34 degrees of freedom ## Residual deviance: 6.4514 on 33 degrees of freedom ## AIC: 23.419 ## ## Number of Fisher Scoring iterations: 10 Yes, but not in the direction we would anticipate. Longer papers are more likely to be read than shorter papers. Digging into the data a bit, the papers that were not rated and presumably not read were the very short Points of Significance papers, so this is certainly driving this unanticipated response. We have assumed throughout that these readings are all independent data points, but can you see why that might not be the case here? If students decide not to read the Points of Significance papers, that will impact all of those papers, and it might be that independent decisions are not being made for each one individually. (They were also assigned in the same week, so perhaps that plays a role as well.) 27.2 Finally, a word about grades The question came up as to how grades have changed over time. Behold, the grade distribution over time (A=4, A-=3.67, B+=3.33, B=3, etc.). year&lt;-c(2012,2013,2014,2015,2016,2017,2018,2020,2021,2022) mean.grade&lt;-c(3.37,2.88,2.62,3.45,3.11,3.22,3.00,3.25,2.45,2.47) sem.grade&lt;-c(0.15,0.21,0.26,0.13,0.18,0.19,0.23,0.20,0.33,0.32) plot(year,mean.grade,pch=15,xlab=&quot;Year&quot;,ylab=&quot;Mean grade&quot;,cex=2,col=1,ylim=c(1.5,4)) points(year[c(8,9)],mean.grade[c(8,9)],pch=15,cex=2,col=2,ylim=c(1.5,4)) for (i in 1:length(mean.grade)) { segments(x0=year[i], x1=year[i], y0=mean.grade[i]-2*sem.grade[i],y1=mean.grade[i]+2*sem.grade[i],col=1) } segments(x0=year[8], x1=year[8], y0=mean.grade[8]-2*sem.grade[8],y1=mean.grade[8]+2*sem.grade[8],col=2) segments(x0=year[9], x1=year[9], y0=mean.grade[9]-2*sem.grade[9],y1=mean.grade[9]+2*sem.grade[9],col=2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
