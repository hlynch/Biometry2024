<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 Week 8 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="14 Week 8 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="14 Week 8 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 Week 8 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-7-lecturelab.html"/>
<link rel="next" href="week-8-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-8-lecture" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">14</span> Week 8 Lecture<a href="week-8-lecture.html#week-8-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-8-readings" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Week 8 Readings<a href="week-8-lecture.html#week-8-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Logan Chapter 7. I have also assigned <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Shmueli_2010.pdf">Shmueli (2010)</a>. This is quite an important paper and I encourage you to read it quite carefully.</p>
</div>
<div id="warm-up" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Warm-up<a href="week-8-lecture.html#warm-up" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Which of these is the best model? Why?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="ModelComplexity.png" alt="Models of three different complexities. Figure adapted from [Grunwald (2007) The Minimum Description Length principle](https://mitpress.mit.edu/books/minimum-description-length-principle)" width="100%" />
<p class="caption">
Figure 0.1: Models of three different complexities. Figure adapted from <a href="https://mitpress.mit.edu/books/minimum-description-length-principle">Grunwald (2007) The Minimum Description Length principle</a>
</p>
</div>
</div>
<div id="the-aims-of-modelling-a-discussion-of-shmueli-2010" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> The aims of modelling – A discussion of Shmueli (2010)<a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we launch into the technical details of building linear models, let’s discuss the Shmueli (2010) paper. Shmueli makes the distinction between three kinds of models</p>
<ol style="list-style-type: decimal">
<li>Explanatory models</li>
<li>Predictive models</li>
<li>Descriptive models</li>
</ol>
<p><strong>Explanatory models</strong> fit data with the goal of testing causal hypotheses about theoretical constructs. Explanatory models can therefore never be confirmed and are harder to contradict than predictive models. These are models used for <em>inference</em> (inferential models).</p>
<p><strong>Predictive models</strong> fit data solely to predict new outcomes. Models either work for prediction or don’t. Predictive models may involve parameters that are uninterpretable (e.g., Generalized Additive Models, spline fits, etc.). These models are used for <em>prediction.</em></p>
<p><strong>Descriptive models</strong> summarize data, and that’s the only goal. These models are used to <em>describe</em> data.</p>
<p><strong>Question: What are the differences among these three types of model?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<p><span style="color: blueviolet;">
“Explanatory modeling refers…to the application of statistical models to data for testing causal hypotheses about theoretical constructs.” (Shmueli 2010) Using this definition, explanatory modeling is testing a hypothesis or a theory and therefore can never be confirmed and are also harder (than predictive models) to contradict. We have been referring to these models as “inferential models” or models used for “inference”.</p>
<p>Predictive models are concerned only with the ability to predict new (measurable) outcomes. Either the models works for prediction or it doesn’t. Predictive models are therefore more utilitarian than explanatory models (use whatever works!). Predictive models may involve predictors that are uninterpretable (more on this later when we discuss Generalized Additive Models and other curve fitting techniques).</p>
Example: We can think back to our discussion of population statistics and sample statistics. If we are measuring every individual of a population and we want to summarize the data, we are doing <em>descriptive</em> statistics. Descriptive modeling is aimed at succinctly summarizing a body of data. If we want to use that data to make some inference about a larger population, we are doing <em>inferential</em> statistics. If we want to use our sample to predict a new sample from that larger population, we are doing <em>predictive</em> statistics.
</span>
</details>
<p>As noted by Shmueli, prediction and explanation are conflated in the hypothetical-deductive paradigm of null hypothesis significance testing, because we are saying in essence “If it predicts the data, then it explains the data”. Does everyone see why this is the case?</p>
<p>The critical distinction between explanation and prediction can be seen by looking at the Expected Prediction Error (Shmueli 2010; page 293), which we will discuss in more detail below.</p>
<p>This distinction is at the core of model selection because, when faced with a number of competing models, we must decide whether we prefer the most accurate model (in terms of capturing the underlying mechanism) or the one that will yield the best predictions. It is also important to note that predictive models are restricted to variables which can actually be measured ahead of time. For example, a model to predict wine quality at the time of harvest cannot depend on the type of cask because the type of cask hasn’t been measured at the time the model is to be run. Likewise, the cost of running an aircraft may depend on the cost of fuel on the day of the flight, but fuel costs on the day of flight cannot be used in a predictive model because it is not possible to know what the cost of fuel will be on that day.</p>
<p><strong>Question: If you have to choose one or the other, do you want the most accurate model that captures the underlying mechanism, or the model that will give the best predictions?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<p><span style="color: blueviolet;">
If we describe “real life” as, <span class="math inline">\(\mathscr{Y} = \mathscr{F}(\mathscr{X})\)</span>, where <span class="math inline">\(\mathscr{F}\)</span> is the <em>mechanism</em>, then in model space, we write, <span class="math inline">\(Y = F(X)\)</span>, where <span class="math inline">\(F\)</span> is the <em>model</em>. With explanatory modeling, the goal is <span class="math inline">\(F = \mathscr{F}\)</span>, while with predictive modeling, the goal is <span class="math inline">\(Y = \mathscr{Y}\)</span>.</p>
<p>The expected prediction error (<span class="math inline">\(EPE\)</span>)</p>
<p><span class="math display">\[\text{EPE} = \mathrm{E} [ (Y - \hat{f} (X))^2 ]\]</span></p>
<p>can be broken down into three components:</p>
<p><span class="math display">\[
\text{EPE} = \mathrm{Var}(Y) + \text{bias}^2 + \mathrm{Var} (\hat{f} (X))
\]</span></p>
<p>1.The variance in the response, or the underlying stochasticity in the process (<span class="math inline">\(\mathrm{Var} (Y)\)</span>), 2. the bias, which describes to what degree the model is misspecified (<span class="math inline">\(\text{bias}^2\)</span>), and
3. the estimation variance, which arises because the model is developed from a sample of the data (<span class="math inline">\(\mathrm{Var} (\hat{f} (X))\)</span>).</p>
While the goal of predictive modeling is to minimize EPE, the goal of explanatory modeling is to minimize bias.
</span>
</details>
<p><strong>Question: Shmueli makes the case that predictive modeling requires a larger sample size than explanatory modeling. Why?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
In explanatory modeling, you only need enough data to “see whats going on” – in essence, you just need to understand the mean behavior. On the other hand, in predictive modeling, you actually want to be able to make good predictions, which means you need to understand the mean behavior AND the variability around the mean. Understanding the variability requires more data. Also, if the method for testing predictive models is to withhold some of the data at the model building stage, then you need more data.
</span>
</details>
<p><strong>Question: What is data partitioning? When/why is it used?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Data partitioning is when you partition the data into a “training” dataset and a “validation” dataset. The model is fit on the “training dataset” and its predictive accuracy tested on the “validation dataset”. Data partitioning is less used in explanatory modeling because by reducing the sample size, it reduces the power of the test.
</span>
</details>
<p><strong>Question: How do explanatory and predictive models differ in their use of “dimension reduction”. (First, what is meant by “dimension reduction”.)</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Sometimes we want to combine a number of related predictors into a smaller set of predictors that are uncorrelated. The classic example of this would be Principal Components Analysis (PCA). Dimension reduction is often used for predictive modeling, because we can reduce the variance of our predictions by reducing the number of predictors used in the model. However, we do so at the expense of interpretability, since the new predictors are linear combinations of the original variables and can be difficult to interpret. (More on this in the last week of the semester.) The important point here is that in predictive modeling, we don’t care what goes into the model, only that what comes out of the model is optimal is terms of the accuracy of its predictions for new data.
</span>
</details>
<p><strong>Question: Shmueli discusses “algorithmic modeling”. What is it?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
There are many data mining techniques which can produce predictive models that are useless for explanatory models. One example would be “neural networks”.
</span>
</details>
<p><strong>Question: What are ensemble methods?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<p><span style="color: blueviolet;">
Ensemble models exploit multiple models simultaneously to achieve predictive accuracy. These models might be completely incompatible (using different predictors for example) but their predictions can be merged into a distribution of predictions, the ensemble of which is better than any of the individual models. Examples include climate change models in which predictions from multiple climate models are combined, and hurricane models where you can look at multiple predicted storm tracks to get an idea of where the storm is most likely to go.</p>
<p>Shmueli distinguishes between three types of model checking:</p>
<p>“Model validation”: How well does the model fit the underlying causal relationships?</p>
<p>“Model fit”: How well does the model fit the data you have?</p>
“Generalization”: How well does the model fit new data?
</span>
</details>
<p><strong>Question: This brings us to the problem of overfitting – what is overfitting?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Overfitting is when models are fit to the noise in the data as well as the underlying “signal”. This can happen when too many predictor variables are used and is the basis for the idea behind model “parsimony” (use the simplest model that works).
</span>
</details>
<p>Model selection depends on a clear idea of purpose – i.e. whether the model is explanatory or predictive. In explanatory models, predictors that have a theoretical justification may be left in the model even if they are not statistically significant because they are thought to be part of the true underlying model F. On the other hand, predictive models may drop predictors even if they are statistically significant, if their effect size is so small than leaving them out of the model actually increases predictive accuracy.</p>
<p>Shmueli illustrates these ideas with two examples:</p>
<p>Example #1: The Netflix prize</p>
<p><em>Question: What was the Netflix prize about?</em></p>
<p><em>Question: What were some of the main conclusions?</em></p>
<ol style="list-style-type: decimal">
<li>Missingness is predictively informative. In other words, it is important to know what makes a person decide to rate a movie at all.</li>
<li>Data reduction was key, and many variables you would think would be useful in the model were not (i.e. which actors were actually in the movie).</li>
<li>You can get better predictions by combining the predictions of several smaller models than by building a single more complex model (i.e. ensemble methods work)</li>
</ol>
<p>Example #2: Online auctions.</p>
<p><em>Question: What was the online auction example about?</em></p>
<p><em>Question: What were some of the main conclusions?</em></p>
<ol style="list-style-type: decimal">
<li><p>In this example, the goal was explanatory model, so covariates such as the number of bids were excluded because they arose from the bidding process and did not contribute to the bidding process (and therefore could not have a causal effect).</p></li>
<li><p>R2 as a measure of explanatory power is used to compare models (this is the common metric of explanatory power, more on this next week)</p></li>
<li><p>The authors retained insignificant variables because the model was built from theory and not from the data itself.</p></li>
</ol>
<p><em>Major conclusions:</em></p>
<ol style="list-style-type: decimal">
<li>Prediction, explanation, and description are three very different goals for a statistical analysis.</li>
<li>R2 is a measure of explanatory power but does not tell you anything about predictive power because you may have fit the model to noise in the original sample. Also, explanatory models tend to be overly optimistic with regards to predictive power.</li>
<li>“Checking the model” can include three component: 1) Model validation: how well does the model fit the underlying causal mechanism?; 2) Model fit: how well does the model fit the data you have? (e.g., <span class="math inline">\(R^2\)</span>); 3) Generalization: how well does the model fit new data? (not <span class="math inline">\(R^2\)</span>).</li>
</ol>
<p><em>Final discussion questions:</em></p>
<ol style="list-style-type: decimal">
<li>Must an explanatory model have some level of predictive power to be considered scientifically useful?</li>
<li>Must a predictive model have sufficient explanatory power to be scientifically useful?</li>
</ol>
</div>
<div id="introduction-to-linear-models" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Introduction to linear models<a href="week-8-lecture.html#introduction-to-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the first half of the semester, we were discussing how to identify distributions, and how to fit distributions to data (i.e. parameter estimation). This is a very simple application of fitting a “statistical model”. For example, we would write</p>
<p><span class="math display">\[
Y \sim N(\mu,\sigma^{2})
\]</span>
and then we would use MLE (or other methods) to estimate the two parameters of this distribution <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. We can also re-write this model as</p>
<p><span class="math display">\[
Y = \mu + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2})
\]</span>
Note that the equation on the left has an equal sign, since Y is strictly equal to the sum of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\epsilon\)</span> and it is <span class="math inline">\(\epsilon\)</span> that is drawn from a statistical distribution. As the semester proceeds, we will spend a lot of time thinking about the <span class="math inline">\(\epsilon\)</span>, which is the <em>residual</em> representing the difference between the actual response value and what is predicted by the model (<span class="math inline">\(\mu\)</span> in this case). In essence, we have decomposed Y into a component that is fixed (<span class="math inline">\(\mu\)</span>) and a component that is stochastic (<span class="math inline">\(\epsilon\)</span>). We read this equation as “Y is modeled as having a mean <span class="math inline">\(\mu\)</span> and a random error that is Normally distributed”.</p>
<p>This illustrates nicely the general format of a linear model:</p>
<p>Response = Deterministic function + Stochastic function</p>
<p>Some authors will write this as:</p>
<p>Response = Model + Error</p>
<p>but I dislike this for two reasons. 1) Your “model” is not just the deterministic function. A model for your data includes the deterministic component and the stochastic component. 2) I don’t like referring to the stochastic component as “error” because that implies that your model is faulty in some way. This stochasticity may be built into the system that you are trying to model, it may be irreducible variability that is not your “fault” and therefore should be considered a legitimate part of what you are trying to model rather than a mistake or an error.</p>
<p>Going back for a moment to our original model <span class="math inline">\(Y \sim N(\mu,\sigma^{2})\)</span>, we can either leave <span class="math inline">\(\mu\)</span> as a constant to be estimated or we can try and improve our model by adding complexity to <span class="math inline">\(\mu\)</span>. In other words, if Y represents the size of individuals in a collection of gentoo penguins, we might model mean size as being a function of a continuous variable like age. In that case, our model looks like:</p>
<p><span class="math display">\[
Y = \beta_{0}+\beta_{1}Age + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2})
\]</span></p>
<p>or, equivalently,</p>
<p><span class="math display">\[
Y \sim N(\beta_{0}+\beta_{1}Age,\sigma^{2})
\]</span>
where I’ve used <span class="math inline">\(\beta_{0}\)</span> to represent the intercept of this linear model and <span class="math inline">\(\beta_{1}\)</span> to represent the slope (the amount by which Y increases for each year of Age).</p>
<p>We may choose to model <span class="math inline">\(\mu\)</span> as a function of a discrete variable like Colony. In this case, we would write the model as</p>
<p><span class="math display">\[
Y = \mu_{i} + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2})
\]</span>
where each group has its own <span class="math inline">\(\mu\)</span>:<span class="math inline">\(\mu_{1}\)</span>,<span class="math inline">\(\mu_{2}\)</span>,…,<span class="math inline">\(\mu_{n}\)</span>.</p>
<p>When the covariates are continuous (the first case), we call this regression. When the covariates represent discrete groups or categories (the second case), we call this ANOVA. But both of these are simply different examples of linear modeling, which is it itself just a special case of fitting distributions, which is what we have been doing all along.</p>
<p>In this class, we are not defining linear models as models that can be described by a line. Instead, we define linear models as models in which the parameters can be arranged in a linear combination, and no parameter is a multiplier, divisor, or exponent to any other parameter. For example, <span class="math inline">\(Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon \text{, } \epsilon \sim N(0, \sigma^2)\)</span>, is a linear model because we can create a new variable <span class="math inline">\(Z=X^{2}\)</span>, which makes it more obvious that Y is a linear function of X and this new variable Z. <span class="math inline">\(Y = \beta_0 + \beta_1 X + \beta_1^2 X\)</span> is also a linear model. An example of a nonlinearizable model is <span class="math inline">\(Y = \frac{\beta_1 + \beta_2 X}{\beta_3 + \beta_4 X}\)</span>.</p>
<p>In this class, we will cover a suite of different linear models. Here I introduce them all briefly as a roadmap for the rest of the semester.</p>
<p>Variables that are best described as coming from a Gaussian distribution can be modelled by a model of the type introduced just above. Here I just re-use the simple example above where the mean <span class="math inline">\(\mu\)</span> is a function of a covariate called <span class="math inline">\(Age\)</span>.</p>
<p><span class="math display">\[
Y \sim N(\mu,\sigma^{2}) \\
\mu = \beta_{0}+\beta_{1}Age
\]</span></p>
<p>However, let’s say <span class="math inline">\(Y\)</span> follows a different distribution, such as a Binomial. In that case the model would look like</p>
<p><span class="math display">\[
Y \sim Binom(n,p) \\
logit(p) = \beta_{0}+\beta_{1}Age
\]</span>
In this case, the parameter influenced by <span class="math inline">\(Age\)</span> is the probability <span class="math inline">\(p\)</span> and here we have to transform the parameter using a function called the logit() function that we will be introduced to formally in Week 10. But mathematically, all we care about now is that some function of the parameter has a linear relationship to <span class="math inline">\(Age\)</span>, just like before.</p>
<p>What if <span class="math inline">\(Y\)</span> is best modelled as a Poisson distributed variable? In that case, we have</p>
<p><span class="math display">\[
Y \sim Pois(\lambda) \\
log(\lambda) = \beta_{0}+\beta_{1}Age
\]</span></p>
<p>where it is the parameter <span class="math inline">\(\lambda\)</span> (actually, the log of <span class="math inline">\(\lambda\)</span>, stay tuned for more details in Week 10) that is linearly related to the covariate.</p>
<p>We can, in fact, write down any number of models like this, for example maybe the data are Beta distributed and we want <span class="math inline">\(Age\)</span> to control the <span class="math inline">\(\alpha\)</span> parameter.</p>
<p><span class="math display">\[
Y \sim Beta(\alpha,\beta) \\
f(\alpha) = \beta_{0}+\beta_{1}Age
\]</span>
where f() is some mathematical function. Maybe we think the <span class="math inline">\(\beta\)</span> parameter is control by some other covariate, say Weight</p>
<p><span class="math display">\[
Y \sim Beta(\alpha,\beta) \\
f(\alpha) = \beta_{0}+\beta_{1}Age \\
g(\beta) = \beta_{2}+\beta_{3}Weight
\]</span>
where f() and g() are some functions that transform the parameter so it is linearly related to the covariates.</p>
<p>The point is that all linear modelling takes this general form. You decide on the best distribution to describe the data, and then you build detail into the model by finding covariates that you think describe variation in the parameters. These covariates allow each response to be modelled by a distribution that is tailored for it, accounting for whatever covariates you think describe why some data points <span class="math inline">\(Y\)</span> are larger than other data points.</p>
<p>Linear regression and all flavors of ANOVA deal with the model of the basic type introduced first:</p>
<p><span class="math display">\[
Y \sim N(\mu,\sigma^{2}) \\
\mu = \mbox{covariates added together linearly}
\]</span>
All the other models get lumped into “Generalized Linear Models” and we will discuss that in Week 10.</p>
</div>
<div id="linear-models-example-with-continuous-covariate" class="section level2 hasAnchor" number="14.5">
<h2><span class="header-section-number">14.5</span> Linear models | example with continuous covariate<a href="week-8-lecture.html#linear-models-example-with-continuous-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear models can be written in matrix form (with vectors and matrices), which is more clear when you look at an example. For the model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>We can expand this model for every data point (<span class="math inline">\(i\)</span>), for a total of five data points:</p>
<p><span class="math display">\[
Y_1 = \beta_0 + \beta_1 X_1 + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_2 = \beta_0 + \beta_1 X_2 + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_3 = \beta_0 + \beta_1 X_3 + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
\]</span>
<span class="math display">\[
Y_4 = \beta_0 + \beta_1 X_4 + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_5 = \beta_0 + \beta_1 X_5 + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
\]</span></p>
<p>Note that <span class="math inline">\(\epsilon_{1}\)</span>, <span class="math inline">\(\epsilon_{2}\)</span>,…,<span class="math inline">\(\epsilon_{5}\)</span> are i.i.d. draws from the same error distribution.</p>
<p>In matrix form the model can be written as:</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; X_1 \\
  1 &amp; X_2 \\
  1 &amp; X_3 \\
  1 &amp; X_4 \\
  1 &amp; X_5
\end{bmatrix}
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the response vector, <span class="math inline">\(X\)</span> is the design matrix, <span class="math inline">\(\beta\)</span> is the vector of parameters, and <span class="math inline">\(\epsilon\)</span> is the error vector. The design matrix tells us which combination of parameters are used to predict each data point. We can simply this notation even further by writing it as</p>
<p><span class="math display">\[
\overrightarrow{Y} = \overleftrightarrow{X}\overrightarrow{\alpha} + \overrightarrow{\epsilon}
\]</span></p>
<p>where <span class="math inline">\(\overrightarrow{Y}\)</span> is the response vector, <span class="math inline">\(\overleftrightarrow{X}\)</span> is design matrix, <span class="math inline">\(\overrightarrow{\alpha}\)</span> is the vector of parameters (coefficients), <span class="math inline">\(\overrightarrow{\epsilon}\)</span> is the vector of residuals (i.e. the remaining stochastic component). (Note: Keeping with traditional mathematical notation, vectors have a single headed arrow whereas as matrices have a double headed arrow. Also, here I use <span class="math inline">\(\overrightarrow{\alpha}\)</span> to represent the vector of model coefficients. More traditional would be to use the Greek letter <span class="math inline">\(\overrightarrow{\beta}\)</span>, but in keeping with my notation above I will stick with <span class="math inline">\(\overrightarrow{\alpha}\)</span>.)</p>
<p>The design matrix is the crucial element here. It tells us what combination of parameters are used to predict each data point (the Ys).</p>
<p>In the above example we had one continuous predictor, and the interpretation in this case is fairly straightforward. The interpretation and construction of the design matrix gets more complicated when we have discrete or categorical variables because there are many ways in which to parametrize the design matrix.</p>
<p>Rewriting what we had above for a categorical predictor</p>
<p><span class="math display">\[
Y_i = \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
\]</span>
where <span class="math inline">\(\alpha_{j(i)}\)</span> is the mean of the group <span class="math inline">\(j\)</span> to which data point <span class="math inline">\(i\)</span> belongs (before we had used the variable <span class="math inline">\(\mu\)</span> to represent the mean but here we will use <span class="math inline">\(\alpha\)</span> as the mean for each of the <span class="math inline">\(j\)</span> discrete groups). We can expand this model for every data point (let’s say there are 5 data points and 3 groups):</p>
<p><span class="math display">\[
Y_1 = \alpha_{1(1)} + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_2 = \alpha_{3(2)} + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_3 = \alpha_{1(3)} + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_4 = \alpha_{2(4)} + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_5 = \alpha_{2(5)} + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
\]</span></p>
<p>Here, data point 1 is in group 1, 2 is in group 3, 3 is in group 1, 4 is in group 2, and 5 is in group 2. Both <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_3\)</span> are in group 1, for example, so will have the same predicted value. However, the error (<span class="math inline">\(\epsilon\)</span>) for each is different.</p>
<p>This is called the “group means”, “cell means”, or “dummy” coding. Think of it like representing each group with a binary indicator telling you if each point is in the group (a “dummy” variable).</p>
<p>We can illustrate dummy coding with an example:</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="week-8-lecture.html#cb399-1" tabindex="-1"></a>iris.sub <span class="ot">&lt;-</span> iris[<span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(iris), <span class="at">size =</span> <span class="dv">12</span>), ]</span>
<span id="cb399-2"><a href="week-8-lecture.html#cb399-2" tabindex="-1"></a><span class="fu">model.matrix</span>( <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> iris.sub<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>##    iris.sub$Speciessetosa iris.sub$Speciesversicolor iris.sub$Speciesvirginica
## 1                       1                          0                         0
## 2                       0                          0                         1
## 3                       0                          1                         0
## 4                       0                          0                         1
## 5                       1                          0                         0
## 6                       1                          0                         0
## 7                       0                          1                         0
## 8                       1                          0                         0
## 9                       0                          0                         1
## 10                      0                          0                         1
## 11                      0                          0                         1
## 12                      1                          0                         0
## attr(,&quot;assign&quot;)
## [1] 1 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$`iris.sub$Species`
## [1] &quot;contr.treatment&quot;</code></pre>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="week-8-lecture.html#cb401-1" tabindex="-1"></a>iris.means <span class="ot">&lt;-</span> <span class="fu">ddply</span>(iris.sub, .(Species), summarise, <span class="at">Sepal.Length =</span> <span class="fu">mean</span>(Sepal.Length))</span></code></pre></div>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="week-8-lecture.html#cb402-1" tabindex="-1"></a>dummy <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(iris.sub<span class="sc">$</span>Sepal.Length <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> iris.sub<span class="sc">$</span>Species))</span>
<span id="cb402-2"><a href="week-8-lecture.html#cb402-2" tabindex="-1"></a>dummy<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                            Estimate Std. Error  t value     Pr(&gt;|t|)
## iris.sub$Speciessetosa         4.96  0.1681269 29.50152 2.883981e-10
## iris.sub$Speciesversicolor     6.70  0.2658320 25.20389 1.171294e-09
## iris.sub$Speciesvirginica      6.70  0.1681269 39.85084 1.962875e-11</code></pre>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="week-8-lecture.html#cb404-1" tabindex="-1"></a>iris.fig <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(iris.sub, <span class="fu">aes</span>(<span class="at">x =</span> Species, <span class="at">y =</span> Sepal.Length)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb404-2"><a href="week-8-lecture.html#cb404-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> iris.means, <span class="fu">aes</span>(<span class="at">x =</span> Species, <span class="at">y =</span> Sepal.Length, <span class="at">col =</span> Species), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb404-3"><a href="week-8-lecture.html#cb404-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sepal length&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> font.size))</span>
<span id="cb404-4"><a href="week-8-lecture.html#cb404-4" tabindex="-1"></a>iris.fig</span></code></pre></div>
<p><img src="Week-8-lecture_files/figure-html/unnamed-chunk-4-1.png" width="480" /></p>
<p>Don’t worry about the syntax, but look closely at the figure. The figure is illustrating the values for each group, and the red, green, and blue filled circles represent the mean within each group. Our simple “dummy coding” model simply states that each value (the individual black dots) can be modelled as the mean for that group PLUS the residual difference (i.e. <span class="math inline">\(\epsilon\)</span>) between the group mean and that individual value. Note that the function we use to fit the linear model ‘lm’ is one that we haven’t formally introduced yet (but that will come soon). For now, just observe how the output of the model represents the parameters we are interested in (in this case, the group means).</p>
<p>We write the equation in matrix form as</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 1 \\
  1 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 \\
  0 &amp; 1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}\]</span></p>
<p>Like with the continuous example, <span class="math inline">\(Y\)</span> is the response vector of length <span class="math inline">\(n\)</span>, the design matrix is an <span class="math inline">\(n \times p\)</span> matrix of ones and zeroes, the vector of parameters has length <span class="math inline">\(p\)</span>, and <span class="math inline">\(\epsilon\)</span> is the error vector of length <span class="math inline">\(n\)</span>.</p>
<p>Note that we can still write the equation in the compact form:</p>
<p><span class="math display">\[
\overrightarrow{Y} = \overleftrightarrow{X}\overrightarrow{\beta} + \overrightarrow{\epsilon}
\]</span></p>
<p>Notice that we have a system of equations, one for each group. (We may have several data points in each group – so our model wont fit each data point exactly; the error term takes care of this…) We also have three unknowns, which are the three group means. The three model parameters <span class="math inline">\(\alpha_{1}\)</span>, <span class="math inline">\(\alpha_{2}\)</span>, and <span class="math inline">\(\alpha_{3}\)</span> are all uniquely specified by the data.</p>
<p>However, there are other equivalent ways of writing the same model. For example, we could write</p>
<p><span class="math display">\[
Y_{i} \sim \bar{\alpha} + \alpha_{j(i)} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
\]</span></p>
<p>where <strong>now <span class="math inline">\(\bar{\alpha}\)</span> represents the average of all the group means and <span class="math inline">\(\alpha_{j(i)}\)</span> is the DIFFERENCE between the mean of group j and the mean of all the groups</strong>. (If the number of data points within each group are equal, than the mean of the group means is simply the mean of all the data, and we often refer to this as the ‘grand’ mean. However, if the groups are unbalanced, this may not be the case.)</p>
<p>We can explicitly write out what this model means for each and every data point <span class="math inline">\(Y_{i}\)</span>:</p>
<p><span class="math display">\[
Y_1 = \bar{\alpha} + \alpha_{1(1)} + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_2 = \bar{\alpha} + \alpha_{3(2)} + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_3 = \bar{\alpha} + \alpha_{1(3)} + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_4 = \bar{\alpha} + \alpha_{2(4)} + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
Y_5 = \bar{\alpha} + \alpha_{2(5)} + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
\]</span></p>
<p>We write the equation in matrix form as</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 0 &amp; 0 &amp; 1 \\
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
  \bar{\alpha} \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}\]</span></p>
<p>We have rewritten the model in terms of an overall mean (the mean of all the group means) and the effect of membership in each of the groups is added to this. Because the differences of each group to the overall mean sum to zero (do you see why this is?)</p>
<p>Notice that now we have introduced a fourth parameter without changing the data. We now have three group means and four parameters, so we cannot uniquely estimate all four parameters independently. If we estimate the overall global mean <span class="math inline">\(\bar{\alpha}\)</span>, then we can only estimate two of the three group differences. The difference associated with the last group is completely constrained. This is analogous to our discussion of the number of degrees of freedom. We say that this model is <strong>over parametrized</strong>. We can solve this problem is a number of ways. We can remove a parameter altogether (in our first example, we implicitly removed the global mean <span class="math inline">\(\bar{\alpha}\)</span>), or we can create new groups (linear combinations of the old groups) that are uniquely parameterized. Parameters that are not estimated because they are fully constrained by the other parameters are said to be <strong>aliased</strong>.</p>
<p><strong>Question:</strong> What parameter was removed/aliased in the dummy coding scheme? In that case, <span class="math inline">\(\bar{\alpha}\)</span> was removed/aliased.</p>
<p>We have now introduced two perfectly “correct” ways of writing this linear model with different design matrices and correspondingly different interpretations of the model coefficients. These different ways of parametrizing the model for categorical variables are called “coding schemes”. While the models represent the same math, the different ways of writing the models down permits different hypothesis tests about the model parameters.</p>
<p>In the case where we are looking at group means</p>
<p><span class="math display">\[
Y_{i} \sim \alpha_{j(i)} + \epsilon \mbox{, where } \epsilon \sim N(0,\sigma^{2})
\]</span>
the (implicit) null hypothesis is that the group means are zero. Mathematically,</p>
<p><span class="math display">\[
H_{0}: \alpha_{j} = 0
\]</span></p>
<p>However, in the second case,</p>
<p><span class="math display">\[
Y_{i} \sim \bar{\alpha} + \alpha_{j(i)} + \epsilon \mbox{, where } \epsilon \sim N(0,\sigma^{2})
\]</span>
the same null hypothesis</p>
<p><span class="math display">\[
H_{0}: \alpha_{j} = 0
\]</span>
means something very different. Therefore, we need to make sure to use coding schemes that will allow us to most directly test the hypothesis of interest. (This is why there is no one “correct” approach.)</p>
</div>
<div id="resolving-overparameterization-using-contrasts" class="section level2 hasAnchor" number="14.6">
<h2><span class="header-section-number">14.6</span> Resolving overparameterization using contrasts<a href="week-8-lecture.html#resolving-overparameterization-using-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can reduce the number of parameters in our model by creating a new set of parameters, each of which is a linear combination of groups, rather than a single group effect. Each parameter can include contributions from any number of the groups. We will now introduce of series of potential options for doing this, each of which is designed to test a specific hypothesis about how groups may differ. It is occassionally the case that none of these “off the shelf” options are appropriate for your analysis and you may need to design a design matrix that is tailored specifically for your research question, so the options we introduce here are just a few of the options and meant to illustrate the overall idea. The following decision tree is designed to help you choose the correct coding scheme for your specific analysis.</p>
<p>We know now that there are many ways in which to write a linear model with categorical predictors, and that the choices made about how to contrast the various levels of the factors is important for interpreting the results. In addition to the “dummy” or “cell means” coding scheme introduced above, we will go over 4 common “off-the-shelf” coding schemes that are used. Keep in mind that these are not all the possible coding schemes that could be used to write a model. Many others exist, and often it is necessary to write your own custom coding schemes to answer a particular question of interest.</p>
<p>Before getting into the math that describes the various options, we’ll work through a simple example. Lets say we have a study in which we measure the average lifespan of four species of penguins. For arguments sake, lets say we track 10 individuals in each of four species (40 penguins total). The mean lifespan in each group is 13, 12, 9, and 6. One way to write this model would be to simply estimate the mean of each group. This is the ‘cell means’ coding we described above. Nothing is being contrasted here. In other words, no group is being compared to any other group.</p>
<table>
<colgroup>
<col width="23%" />
<col width="22%" />
<col width="18%" />
<col width="18%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Method<span class="math inline">\(\#1\)</span></th>
<th align="center">Method<span class="math inline">\(\#2\)</span></th>
<th align="center">Method<span class="math inline">\(\#3\)</span></th>
<th align="center">Method<span class="math inline">\(\#4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td align="center">aliased</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{1}\)</span></td>
<td align="center">13</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{2}\)</span></td>
<td align="center">12</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{3}\)</span></td>
<td align="center">9</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{4}\)</span></td>
<td align="center">6</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Now let’s say we want to write the model differently. Now we want to explicitly compare the lifespan of penguins in spp 2, 3, and 4 to the lifespan of penguins of spp 1. In other words, we are contrasting spp 2 vs. spp 1, spp 3 vs. spp 1, and spp 4 vs. spp 1. The estimates we get for these contrasts will allow us to test hypotheses about how each group compares to the first.</p>
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="21%" />
<col width="20%" />
<col width="20%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Method<span class="math inline">\(\#1\)</span></th>
<th align="center">Method<span class="math inline">\(\#2\)</span></th>
<th align="center">Method<span class="math inline">\(\#3\)</span></th>
<th align="center">Method<span class="math inline">\(\#4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td align="center">aliased</td>
<td align="center">aliased</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{1}\)</span></td>
<td align="center">13</td>
<td align="center">13</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{2}\)</span></td>
<td align="center">12</td>
<td align="center">-1</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{3}\)</span></td>
<td align="center">9</td>
<td align="center">-4</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{4}\)</span></td>
<td align="center">6</td>
<td align="center">-7</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Let’s try another set of contrasts. In this case, let’s say that, for whatever reason, we want to compare the Mean(spp 1,spp 2) vs. Mean(spp 1), as well as Mean(spp 1, spp 2, spp 3) vs. Mean(spp 1,spp 2), and the Mean(spp 1, spp 2, spp 3, spp 4) vs. Mean(spp 1, spp 2, spp 3). That would yield the following parameter estimates. (Note that in this case, we have pulled out the group mean as a parameter, and the mean of spp 1 has been aliased, so its not being estimated.)</p>
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="21%" />
<col width="20%" />
<col width="20%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Method<span class="math inline">\(\#1\)</span></th>
<th align="center">Method<span class="math inline">\(\#2\)</span></th>
<th align="center">Method<span class="math inline">\(\#3\)</span></th>
<th align="center">Method<span class="math inline">\(\#4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td align="center">aliased</td>
<td align="center">aliased</td>
<td align="center">10</td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{1}\)</span></td>
<td align="center">13</td>
<td align="center">13</td>
<td align="center">aliased</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{2}\)</span></td>
<td align="center">12</td>
<td align="center">-1</td>
<td align="center">-0.5</td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{3}\)</span></td>
<td align="center">9</td>
<td align="center">-4</td>
<td align="center">-1.1667</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{4}\)</span></td>
<td align="center">6</td>
<td align="center">-7</td>
<td align="center">-1.3333</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Finally, imagine we want to compare each group to the grand mean (<span class="math inline">\(\mu\)</span>). In this case we pull out the <span class="math inline">\(\mu\)</span> and estimate it. Than we compare spp 1 vs. <span class="math inline">\(\mu\)</span>, and spp 2 vs. <span class="math inline">\(\mu\)</span>, and spp 3 vs. <span class="math inline">\(\mu\)</span>. We can’t also do spp 4 vs. <span class="math inline">\(\mu\)</span> because once we’ve calculated the other four quantities (<span class="math inline">\(\mu\)</span>, and the three contrasts with <span class="math inline">\(\mu\)</span>), the final quantity (spp 4 vs. <span class="math inline">\(\mu\)</span>) is completely determined and therefore is aliased.</p>
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="21%" />
<col width="20%" />
<col width="20%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Method<span class="math inline">\(\#1\)</span></th>
<th align="center">Method<span class="math inline">\(\#2\)</span></th>
<th align="center">Method<span class="math inline">\(\#3\)</span></th>
<th align="center">Method<span class="math inline">\(\#4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td align="center">aliased</td>
<td align="center">aliased</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{1}\)</span></td>
<td align="center">13</td>
<td align="center">13</td>
<td align="center">aliased</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{2}\)</span></td>
<td align="center">12</td>
<td align="center">-1</td>
<td align="center">-0.5</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{3}\)</span></td>
<td align="center">9</td>
<td align="center">-4</td>
<td align="center">-1.1667</td>
<td align="center">-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{4}\)</span></td>
<td align="center">6</td>
<td align="center">-7</td>
<td align="center">-1.3333</td>
<td align="center">aliased</td>
</tr>
</tbody>
</table>
<p>As a review, the first way of writing the model does not contrast anything. It simply estimates the group means, and the model for group <span class="math inline">\(j\)</span> is simply the group mean for <span class="math inline">\(j\)</span> plus an error term. The other three model parameterizations explicitly contrast the groups, but in different ways.</p>
<p>Hopefully, by working through a few examples, you can see that there are many ways we can write a model, and that each way contrasts different things and allows for an explicit hypothesis test on differences that are of interest. In this example, Method #1 is the ‘cell means’ coding that we started with, Method #2 is called ‘effect coding’ or ‘treatment contrasts’, Method #3 is called Helmert contrasts, and Method #4 is called the ‘sum-to-zero’ contrast we already briefly introduced. We’ll walk through a more formal introduction of these contrasts now.</p>
</div>
<div id="effect-codingtreatment-constrast" class="section level2 hasAnchor" number="14.7">
<h2><span class="header-section-number">14.7</span> Effect coding/Treatment constrast<a href="week-8-lecture.html#effect-codingtreatment-constrast" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In effect coding or treatment contrasts, you set aside one group as the “control” and estimate the difference of every other group from that control group. The estimated parameters therefore represent the “effect” of moving from the control group to any other group (which often represents the impact of a “treatment” relative to the control group).</p>
<p><span class="math display">\[
Y_i = \alpha_{\text{control}} + \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>where <span class="math inline">\(\alpha_{control}\)</span> is the mean of the control group, and <span class="math inline">\(\alpha_{j&gt;1}\)</span> is the difference between group <span class="math inline">\(j\)</span> and the control group. If we have <span class="math inline">\(k\)</span> groups, we only have <span class="math inline">\(k-1\)</span> <span class="math inline">\(\alpha_{j}\)</span> to estimate since we use one group as the control. We can think of <span class="math inline">\(\alpha_{control}\)</span> as an intercept.</p>
<p>We can summary this as</p>
<table style="width:100%;">
<colgroup>
<col width="29%" />
<col width="42%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Interpretation</th>
<th align="right">Null hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\alpha_{control}\)</span></td>
<td align="center">mean of control group (<span class="math inline">\(\mu_1\)</span>)</td>
<td align="right"><span class="math inline">\(H_0: \mu_1 = 0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_2\)</span></td>
<td align="center">difference between mean of group 2 and mean of control group (<span class="math inline">\(\mu_2 - \mu_1\)</span>)</td>
<td align="right"><span class="math inline">\(H_0: \mu_2 - \mu_1 = 0\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_3\)</span></td>
<td align="center">difference between mean of group 3 and mean of control group (<span class="math inline">\(\mu_3 - \mu_1\)</span>)</td>
<td align="right"><span class="math inline">\(H_0: \mu_3 - \mu_1 = 0\)</span></td>
</tr>
</tbody>
</table>
<p>This basically uses the first group as the “intercept” of the model, so instead of this</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 \\
  0 &amp; 0 &amp; 1 \\
  0 &amp; 0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
  \mu_1 \\
  \mu_2 \\
  \mu_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}\]</span></p>
<p>we have</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; 0 &amp; 0 \\
  1 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 1 \\
  1 &amp; 0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
  \alpha_{control} = \mu_1 \\
  \alpha_{2} = \mu_2 - \mu_{1}\\
  \alpha_{3} = \mu_3 - \mu_{1}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}\]</span></p>
<p>Note that the last two data points come from the same group, and so those two rows will always be the same. In theory, there are many many rows NOT shown.</p>
<p>The first version specifies the model in terms of the group means. The second version specifies the model in terms of linear combinations of the group means. The latter approach permits a straightforward way of estimating quantities of biological interest (usually we are interested in testing hypotheses about differences).</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="week-8-lecture.html#cb405-1" tabindex="-1"></a><span class="fu">contrasts</span>(iris.sub<span class="sc">$</span>Species) <span class="ot">&lt;-</span> <span class="st">&quot;contr.treatment&quot;</span></span>
<span id="cb405-2"><a href="week-8-lecture.html#cb405-2" tabindex="-1"></a><span class="fu">model.matrix</span>(<span class="sc">~</span> iris.sub<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>##    (Intercept) iris.sub$Speciesversicolor iris.sub$Speciesvirginica
## 1            1                          0                         0
## 2            1                          0                         1
## 3            1                          1                         0
## 4            1                          0                         1
## 5            1                          0                         0
## 6            1                          0                         0
## 7            1                          1                         0
## 8            1                          0                         0
## 9            1                          0                         1
## 10           1                          0                         1
## 11           1                          0                         1
## 12           1                          0                         0
## attr(,&quot;assign&quot;)
## [1] 0 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$`iris.sub$Species`
## [1] &quot;contr.treatment&quot;</code></pre>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="week-8-lecture.html#cb407-1" tabindex="-1"></a>treatment <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(iris.sub<span class="sc">$</span>Sepal.Length <span class="sc">~</span> iris.sub<span class="sc">$</span>Species))</span>
<span id="cb407-2"><a href="week-8-lecture.html#cb407-2" tabindex="-1"></a>treatment<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                            Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)                    4.96  0.1681269 29.501519 2.883981e-10
## iris.sub$Speciesversicolor     1.74  0.3145367  5.531946 3.648166e-04
## iris.sub$Speciesvirginica      1.74  0.2377674  7.318077 4.477216e-05</code></pre>
<p>In this case, you can see that the model is estimating the mean of the first group (setosa) and then the difference between the second and first group and the difference between the third and first groups. This allows you to test hypotheses about the differences, which is often more meaningful than testing hypotheses about the group means themselves.</p>
</div>
<div id="helmert-contrasts" class="section level2 hasAnchor" number="14.8">
<h2><span class="header-section-number">14.8</span> Helmert contrasts<a href="week-8-lecture.html#helmert-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Helmert contrasts include a mean treatment effect and then the remaining parameters estimate the group mean relative to the mean treatment effect of the groups before it. This is usually only sensible when the categories have some natural order to them, and are not very common in ecology but they are the default in SPLUS (but not R fortunately). (There are many different versions, but as long as you keep the interpretation of each coefficient straight, it doesn’t matter which you use. I think this version is the clearest.) The equation is difficult to write compactly but the Helmert contrast can be understood using the following example involving three groups.</p>
<table style="width:100%;">
<colgroup>
<col width="29%" />
<col width="42%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Interpretation</th>
<th align="right">Null hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\bar{\alpha}\)</span></td>
<td align="center">mean of group means</td>
<td align="right"><span class="math inline">\(H_0: \frac{\mu_1 + \mu_2 + \mu_3}{3} = 0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_2\)</span></td>
<td align="center">difference between mean of groups 1 and 2 and mean of group 1</td>
<td align="right"><span class="math inline">\(H_0: \frac{\mu_1 + \mu_2}{2} - \mu_1 = 0\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_3\)</span></td>
<td align="center">difference between mean of groups 1-3 and mean of groups 1 and 2</td>
<td align="right"><span class="math inline">\(H_0: \frac{\mu_1 + \mu_2 + \mu_3}{3} - \frac{\mu_1 + \mu_2}{2} = 0\)</span></td>
</tr>
</tbody>
</table>
<p>This can be expressed as:</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; -1 &amp; -1 \\
  1 &amp; 1 &amp; -1 \\
  1 &amp; 0 &amp; 2 \\
  1 &amp; 0 &amp; 2
\end{bmatrix}
\begin{bmatrix}
  \alpha_{\small{\mbox{mean of group means}}} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3} \\
  \alpha_{2} = \frac{\mu_{1}+\mu_{2}}{2}-\mu_{1} \\
  \alpha_{3} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3}-\frac{\mu_{1}+\mu_{2}}{2}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}\]</span></p>
<p>where I have assumed it is <span class="math inline">\(\alpha_{1}\)</span> that is aliased (although different books define this differently, so you may find versions in which it is parameterized differently).</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="week-8-lecture.html#cb409-1" tabindex="-1"></a><span class="fu">contrasts</span>(iris.sub<span class="sc">$</span>Species) <span class="ot">&lt;-</span> <span class="st">&quot;contr.helmert&quot;</span></span>
<span id="cb409-2"><a href="week-8-lecture.html#cb409-2" tabindex="-1"></a><span class="fu">model.matrix</span>(<span class="sc">~</span> iris.sub<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>##    (Intercept) iris.sub$Species1 iris.sub$Species2
## 1            1                -1                -1
## 2            1                 0                 2
## 3            1                 1                -1
## 4            1                 0                 2
## 5            1                -1                -1
## 6            1                -1                -1
## 7            1                 1                -1
## 8            1                -1                -1
## 9            1                 0                 2
## 10           1                 0                 2
## 11           1                 0                 2
## 12           1                -1                -1
## attr(,&quot;assign&quot;)
## [1] 0 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$`iris.sub$Species`
## [1] &quot;contr.helmert&quot;</code></pre>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="week-8-lecture.html#cb411-1" tabindex="-1"></a>helmert <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(iris.sub<span class="sc">$</span>Sepal.Length <span class="sc">~</span> iris.sub<span class="sc">$</span>Species))</span>
<span id="cb411-2"><a href="week-8-lecture.html#cb411-2" tabindex="-1"></a>helmert<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)           6.12  0.1188837 51.478884 1.977812e-12
## iris.sub$Species1     0.87  0.1572683  5.531946 3.648166e-04
## iris.sub$Species2     0.29  0.0767391  3.779039 4.355452e-03</code></pre>
</div>
<div id="sum-to-zero-contrasts" class="section level2 hasAnchor" number="14.9">
<h2><span class="header-section-number">14.9</span> Sum-to-zero contrasts<a href="week-8-lecture.html#sum-to-zero-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sum-to-zero contrast set the intercept to the mean treatment effect (the mean of the group means) and then the remaining parameters estimate the mean treatment effect of that group relative to the mean treatment effect. (We introduced this contrast briefly earlier.)</p>
<p><span class="math display">\[
Y_i = \bar{\alpha} + \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>where <span class="math inline">\(\bar{\alpha}\)</span> is the mean treatment effect for all groups, and <span class="math inline">\(\alpha_{j}\)</span> is the difference between group <span class="math inline">\(j\)</span> and <span class="math inline">\(\bar{\alpha}\)</span>.</p>
<p>We can summarize this as</p>
<table style="width:100%;">
<colgroup>
<col width="29%" />
<col width="42%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Interpretation</th>
<th align="right">Null hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\bar{\alpha}\)</span></td>
<td align="center">mean of group means</td>
<td align="right"><span class="math inline">\(H_0: \frac{\mu_q}{p} = \frac{\mu_1 + \mu_2 + \mu_3}{3} = 0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_1\)</span></td>
<td align="center">difference between mean of group 1 and mean of group means</td>
<td align="right"><span class="math inline">\(H_0: \mu_1 - \frac{\mu_q}{p} = 0\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_2\)</span></td>
<td align="center">difference between mean of group 2 and mean of group means</td>
<td align="right"><span class="math inline">\(H_0: \mu_2 - \frac{\mu_q}{p} = 0\)</span></td>
</tr>
</tbody>
</table>
<p>This can be expressed as:</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4
\end{bmatrix} =
\begin{bmatrix}
  1 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 1 \\
  1 &amp; -1 &amp; -1 \\
  1 &amp; -1 &amp; -1
\end{bmatrix}
\begin{bmatrix}
  \bar{\alpha} = \frac{\mu_{q}}{p} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3} \\
  \alpha_{1} = \mu_{1}-\frac{\mu_{q}}{p} \\
  \alpha_{2} = \mu_{2}-\frac{\mu_{q}}{p}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}\]</span></p>
<p>where we see that it is the last parameter (<span class="math inline">\(\alpha_{3}\)</span>), the one associated with the group mean of the last group (<span class="math inline">\(\mu_{3}\)</span>) which is aliased.</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="week-8-lecture.html#cb413-1" tabindex="-1"></a><span class="fu">contrasts</span>(iris.sub<span class="sc">$</span>Species) <span class="ot">&lt;-</span> <span class="st">&quot;contr.sum&quot;</span></span>
<span id="cb413-2"><a href="week-8-lecture.html#cb413-2" tabindex="-1"></a><span class="fu">model.matrix</span>(<span class="sc">~</span> iris.sub<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>##    (Intercept) iris.sub$Species1 iris.sub$Species2
## 1            1                 1                 0
## 2            1                -1                -1
## 3            1                 0                 1
## 4            1                -1                -1
## 5            1                 1                 0
## 6            1                 1                 0
## 7            1                 0                 1
## 8            1                 1                 0
## 9            1                -1                -1
## 10           1                -1                -1
## 11           1                -1                -1
## 12           1                 1                 0
## attr(,&quot;assign&quot;)
## [1] 0 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$`iris.sub$Species`
## [1] &quot;contr.sum&quot;</code></pre>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="week-8-lecture.html#cb415-1" tabindex="-1"></a>sumtozero <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(iris.sub<span class="sc">$</span>Sepal.Length <span class="sc">~</span> iris.sub<span class="sc">$</span>Species))</span>
<span id="cb415-2"><a href="week-8-lecture.html#cb415-2" tabindex="-1"></a>sumtozero<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)           6.12  0.1188837 51.478884 1.977812e-12
## iris.sub$Species1    -1.16  0.1534782 -7.558077 3.475057e-05
## iris.sub$Species2     0.58  0.1941363  2.987592 1.526041e-02</code></pre>
<p>There is one final “off-the-shelf” contrast that we will learn, and that is polynomial contrasts.</p>
</div>
<div id="polynomial-contrasts" class="section level2 hasAnchor" number="14.10">
<h2><span class="header-section-number">14.10</span> Polynomial contrasts<a href="week-8-lecture.html#polynomial-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Polynomial contrasts simply involve fitting polynomial terms of increasing order; polynomial contrasts only make sense if the covariate represents an ordinal value, so that the different levels of the factor are ordered and have equal spacing (income, years of education, etc.). This is really just a form of multiple regression (so a preview of what we will be doing in a couple of weeks…)</p>
<p>This one is a little hard to summarize in an equation, but is easily understood by example.</p>
<table style="width:100%;">
<colgroup>
<col width="29%" />
<col width="42%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="center">Interpretation</th>
<th align="right">Null hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td align="center">Y intercept</td>
<td align="right"><span class="math inline">\(H_0: \beta_0 = 0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td align="center">Partial slope for the linear term</td>
<td align="right"><span class="math inline">\(H_0: \beta_1 = 0\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td align="center">Partial slope for the quadratic term</td>
<td align="right"><span class="math inline">\(H_0: \beta_2 = 0\)</span></td>
</tr>
</tbody>
</table>
<p>This can be expressed as:</p>
<p><span class="math display">\[\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4
\end{bmatrix} =
\begin{bmatrix}
  0.58 &amp; -0.71 &amp; 0.41 \\
  0.58 &amp; 0 &amp; -0.82 \\
  0.58 &amp; 0.71 &amp; 0.41 \\
  0.58 &amp; 0.71 &amp; 0.41
\end{bmatrix}
\begin{bmatrix}
  \beta_{0} = \mbox{mean} \\
  \beta_{1} = \mbox{coefficient for the linear term} \\
  \beta_{2} = \mbox{coefficient for the quadratic term}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}\]</span></p>
<p>where the coefficients (<span class="math inline">\(\beta\)</span>) can be calculated using the values in the contrast matrix (we will call this <span class="math inline">\(\mathbf{P}\)</span>) associated with each group. Data point 1 is in group 1, 2 in group 3, 3 in group 1, 4 in group 2, and 5 in group 2.</p>
<p><span class="math display">\[
\beta_{k - 1} = \frac{\sum_{j} P_{kj} \bar{Y}_j}{\sum_{j} P_{kj}^2}
\]</span></p>
<p><span class="math display">\[
\beta_0 = \frac{1 \times \bar{Y}_{grp 1} + 1 \times \bar{Y}_{grp 2} + 1 \times \bar{Y}_{grp 3}}{1^2 + 1^2 + 1^2}
\]</span></p>
<p><span class="math display">\[
\beta_1 = \frac{-0.71 \times \bar{Y}_{grp 1} + 0 \times \bar{Y}_{grp 2} + 0.71 \times \bar{Y}_{grp 3}}{(-0.71)^2 + 0^2 + 0.71^2}
\]</span>
and so forth…</p>
<p>The values in the design matrix look complicated, but they aren’t really. Each column is scaled to have length 1 (which means all the denominators are actually just 1).</p>
</div>
<div id="visualizing-hypotheses-for-different-coding-schemes" class="section level2 hasAnchor" number="14.11">
<h2><span class="header-section-number">14.11</span> Visualizing hypotheses for different coding schemes<a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The different contrasts are different ways of comparing groups to different baselines. You can either compare each group to zero, a control group, to the mean of the group means (often called the grand mean), to the average of all previous groups, etc.</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="week-8-lecture.html#cb417-1" tabindex="-1"></a>baselines <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Baseline =</span> <span class="fu">c</span>(<span class="st">&quot;Mean of group means&quot;</span>, <span class="st">&quot;Control&quot;</span>, <span class="st">&quot;Zero&quot;</span>), <span class="at">Value =</span> <span class="fu">c</span>(<span class="fu">mean</span>(iris.means<span class="sc">$</span>Sepal.Length), iris.means<span class="sc">$</span>Sepal.Length[<span class="dv">1</span>], <span class="dv">0</span>))</span>
<span id="cb417-2"><a href="week-8-lecture.html#cb417-2" tabindex="-1"></a>iris.fig.baselines <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(iris.sub, <span class="fu">aes</span>(<span class="at">x =</span> Species, <span class="at">y =</span> Sepal.Length)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb417-3"><a href="week-8-lecture.html#cb417-3" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">data =</span> baselines, <span class="fu">aes</span>(<span class="at">yintercept =</span> Value), <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb417-4"><a href="week-8-lecture.html#cb417-4" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> baselines, <span class="fu">aes</span>(<span class="at">label =</span> Baseline, <span class="at">x =</span> <span class="cn">Inf</span>, <span class="at">y =</span> Value), <span class="at">hjust =</span> <span class="fl">1.0</span>, <span class="at">vjust =</span> <span class="fl">1.0</span>, <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb417-5"><a href="week-8-lecture.html#cb417-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> iris.means, <span class="fu">aes</span>(<span class="at">x =</span> Species, <span class="at">y =</span> Sepal.Length, <span class="at">col =</span> Species), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb417-6"><a href="week-8-lecture.html#cb417-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sepal length&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> font.size))</span>
<span id="cb417-7"><a href="week-8-lecture.html#cb417-7" tabindex="-1"></a>iris.fig.baselines</span></code></pre></div>
<p><img src="Week-8-lecture_files/figure-html/unnamed-chunk-11-1.png" width="576" /></p>
</div>
<div id="orthogonal-vs.-non-orthogonal-contrasts" class="section level2 hasAnchor" number="14.12">
<h2><span class="header-section-number">14.12</span> Orthogonal vs. Non-orthogonal contrasts<a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="perpendicular.png" alt="The segment AB is orthogonal to the segement CD. Source: Wikipedia" width="50%" />
<p class="caption">
Figure 8.1: The segment AB is orthogonal to the segement CD. Source: Wikipedia
</p>
</div>
<p>In a broad sense, orthogonal refers to things that are perpendicular. For some cases, that’s easy to think about. But what does it mean with respect to categorical comparisons in linear models?</p>
<p>In a simple model with one categorical predictor with five different groups (a, b, c, d, and e): <span class="math inline">\(Y_i = \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)\)</span>, we may be interested in comparing a vs. b, a vs. c, or even a, b, and c vs. d and e. There are many ways in which we might want to interpret differences among the groups, and each comparison is called a <strong>contrast</strong>. There are a large number of contrasts, but few <em>orthogonal</em> contrasts. For example, if we compare a vs. b and a vs. c, then a third comparison, b vs. c is not orthogonal because it is implicity included in the first two contrasts. Note that treatment contrasts are not orthogonal.</p>
<p>Mathematically, contrasts are orthogonal if their dot product is equal to zero or, equivalently, if the product of the pairwise coefficients sums to zero. In other words, let’s say we have three groups (three “levels of our factor”). Two possible contrasts would be</p>
<table>
<thead>
<tr class="header">
<th>Group</th>
<th align="center">Contrast A</th>
<th align="right">Contrast B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">2</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>2</td>
<td align="center">-1</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="center">-1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>Contrast A compares Group 1 against the average of Group 2 and 3. Contrast B compares Group 2 vs. Group 3. Are these two contrasts orthogonal? Yes!</p>
<p><span class="math display">\[
\sum\limits_{i=1}^{3} \alpha_{i}\beta_{i} = (2)(0)+(-1)(1)+(-1)(-1) = 0
\]</span></p>
<p><strong>If you have <span class="math inline">\(k\)</span> levels of a factor (<span class="math inline">\(k\)</span> numbers of groups), you only have <span class="math inline">\(k - 1\)</span> orthogonal contrasts.</strong></p>
<p>Why does this matter? With non-orthogonal contrasts, the order in which you make you comparisons in an Analysis of Variance will change your results and interpretations. See Aho section 10.3.2 for details.</p>
</div>
<div id="error-structure-of-linear-models" class="section level2 hasAnchor" number="14.13">
<h2><span class="header-section-number">14.13</span> Error structure of linear models<a href="week-8-lecture.html#error-structure-of-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Response = Deterministic function + Stochastic function</p>
<p>Only part of a statistical model is deterministic. It’s important to understand whether the stochastic part of the model is properly specified.</p>
<p>We have been referring to “error” as being normally distributed, <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>, and this is often assumed for linear models (including regression).</p>
<p>Imagine you have modeled the number of plants in quadrats. Is your error normally distributed?</p>
<p>Other models for error can be more appropriate, depending on the data and the model, e.g., <span class="math inline">\(\epsilon \sim \text{Bernoulli}()\)</span> or <span class="math inline">\(\epsilon \sim \text{Poisson}()\)</span>.</p>
<p>Everything to this point has assumed that the residuals for each data point (i.e. <span class="math inline">\(\epsilon_{i}\)</span>) are independent from one another and identifcally distributed (as <span class="math inline">\(N(0,\sigma^{2}\)</span>)).</p>
<p><span class="math display">\[
Y_i = \alpha_{j(i)} + \epsilon_i \text{, where }\epsilon_i \sim N(0, \sigma^2 \mathbf{I})
\]</span></p>
<p><span class="math display">\[
\sigma^2 \mathbf{I} = \sigma^2 \begin{bmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix} = \begin{bmatrix}
  \sigma^2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; \sigma^2 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; \sigma^2 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; \sigma^2 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma^2
\end{bmatrix}
\]</span></p>
<p>Diagonal elements represent the <span class="math inline">\(\mathrm{Var}[Y_i]\)</span> and the off-diagonal elements are <span class="math inline">\(\mathrm{Cov}[Y_i, Y_j]\)</span>. Here, <span class="math inline">\(\mathrm{Cov}[Y_i, Y_j] = 0\)</span>, which means that the error for data point <span class="math inline">\(Y_i\)</span> is independent of the error for data point <span class="math inline">\(Y_j\)</span>.</p>
<p>But, this is not always the case. Sometimes, a positive error/residual for one data point is coupled with a positive residual for another data point, or vice versa. This happens often in ecology and evolution, because we often work with data that has spatial dependence, temporal dependence, or phylogenetic dependence. Then, the error matrix is no longer diagonal, and may look like:</p>
<p><span class="math display">\[
\epsilon_i \sim N(0, \mathbf{\Sigma}) \text{, where } \mathbf{\Sigma} = \begin{bmatrix}
  4.5 &amp; 0.3 &amp; -4.7 &amp; 0 &amp; 0 \\
  0.3 &amp; 9.0 &amp; 0 &amp; 0 &amp; 0 \\
  -4.7 &amp; 0 &amp; 0.4 &amp; 0.3 &amp; 0 \\
  0 &amp; 0 &amp; 0.3 &amp; 5.2 &amp; 0.1 \\
  0 &amp; 0 &amp; 0 &amp; 0.1 &amp; 0.8
\end{bmatrix}
\]</span></p>
<p>The matrix is symmetric, because <span class="math inline">\(\mathrm{Cov}[Y_i, Y_j] = \mathrm{Cov}[Y_j, Y_i]\)</span>. Thus, it is important that we think of the error term as a matrix that describes the correlation structure of the data. Correlated errors will affect model fit. This is important because we want to downweight information from highly correlated data points because the information that each contributes is not independent from one another.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-7-lecturelab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-8-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
