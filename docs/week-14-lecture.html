<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>26 Week 14 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="26 Week 14 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="26 Week 14 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="26 Week 14 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-01-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-13-lab.html"/>
<link rel="next" href="week-14-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.8</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.9</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.9.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.9.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.9.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.9.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.9.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.10</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#short-digression-degrees-of-freedom"><i class="fa fa-check"></i><b>7.2</b> Short digression: Degrees of freedom</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.3</b> t-distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.4</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.5</b> F distribution</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.6</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.7" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.7</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-14-lecture" class="section level1 hasAnchor" number="26">
<h1><span class="header-section-number">26</span> Week 14 Lecture<a href="week-14-lecture.html#week-14-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-14-readings" class="section level2 hasAnchor" number="26.1">
<h2><span class="header-section-number">26.1</span> Week 14 Readings<a href="week-14-lecture.html#week-14-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Sections 9.17.</p>
<p>There are also a suite of papers we will discuss as part of our end-of-semester review for the final exam. I suggest reading them all but I will assign you one to focus on (and present) before class: <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Anderson_etal_2001.pdf">Anderson et al. (2001)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Beninger_etal_2012.pdf">Beninger et al. (2012)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Borer_etal_2009.pdf">Borer et al. (2009)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Gelman_2011.pdf">Gelman (2011)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Guthery_etal_2001.pdf">Guthery et al. (2001)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Makin_Xivry_2019.pdf">Makin and Xivry (2019)</a>, and <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Murtaugh_2007.pdf">Murtaugh (2007)</a>.</p>
</div>
<div id="what-does-multivariate-mean" class="section level2 hasAnchor" number="26.2">
<h2><span class="header-section-number">26.2</span> What does ‘multivariate’ mean?<a href="week-14-lecture.html#what-does-multivariate-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multivariate data includes more than one variable recorded from each experimental unit (replicate). For example, Lovett et al. (2000) measured many water chemistry variables (e.g., concentration of nitrate, ammonia, organic N) in each of 39 streams in the Catskill Mountains.</p>
<p>We’ve already worked with multivariate data when we fit multiple regressions or ANOVA models with more than one factors. We will be looking into some other commonly used multivariate analyses.</p>
</div>
<div id="multivariate-associations" class="section level2 hasAnchor" number="26.3">
<h2><span class="header-section-number">26.3</span> Multivariate associations<a href="week-14-lecture.html#multivariate-associations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are familiar with the process of measuring <strong>variation within a variable</strong> in linear regression using sums of squares and variances (which are equivalent to scaled sums of squares). For covariate 1, <span class="math inline">\(X_1\)</span>:</p>
<p><span class="math display">\[
\text{SS} (X_1) = \sum_{i = 1}^n ( X_{1i} - \bar{X}_{1} ) ^2
\]</span></p>
<p><span class="math display">\[
\mathrm{Var} (X_1) = \frac{1}{n - 1} \sum_{i = 1}^n ( X_{1i} - \bar{X}_{1} ) ^2
\]</span>
With multivariate data, we also look at the <strong>covariation between variables</strong>, as the sums of cross products or the covariances. For covariates 1 and 2, <span class="math inline">\(X_1, X_2\)</span>:</p>
<p><span class="math display">\[
\text{SCP} (X_1, X_2) = \sum_{i = 1}^n ( X_{1i} - \bar{X}_{1} ) ( X_{2i} - \bar{X}_{2} )
\]</span></p>
<p><span class="math display">\[
\mathrm{Cov} (X_1, X_2) = \frac{1}{n - 1} \sum_{i = 1}^n ( X_{1i} - \bar{X}_{1} ) ( X_{2i} - \bar{X}_{2} )
\]</span></p>
<p>We can do this for all of our covariates and get a matrix of the sums of squares and sums of cross products or a matrix of the variances and covariances. We could also standardize the variances and covariances by their standard deviations and get a matrix of correlations (<span class="math inline">\(r\)</span>).</p>
<p>For multivariate analyses, we use the matrix of associations to find <strong>linear combinations of the covariates in the original dataset</strong>. We do this in order to summarize the variation in the original data set using new derived covariates (each new covariate involves all of the original covariates). Only the first couple of the new derived covariates may explain a lot of the variation in the data, which is useful when we have a lot of covariates.</p>
<p>When we have <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p\)</span> covariates, our new covariate <span class="math inline">\(z_{j}\)</span> for observation <span class="math inline">\(j\)</span> is:</p>
<p><span class="math display">\[z_j = c_1 X_{1j} + c_2 X_{2j} + \dots + c_p X_{pj}\]</span></p>
<p><span class="math inline">\(c_1\)</span> through <span class="math inline">\(c_p\)</span> are coefficients that describe how much each original covariate contributes to the new covariate, <span class="math inline">\(z\)</span>. There will be as many new covariates <span class="math inline">\(z\)</span> as there were original ones (<span class="math inline">\(z_{1j}, z_{2j}, \dots, z_{pj}\)</span>).</p>
<p><em>Note that you could also calculate <span class="math inline">\(z\)</span> and <span class="math inline">\(c\)</span> using the original matrix of data rather than the matrix of associations (check your R package information to see which method is used).</em></p>
<p>What do we estimate in a multivariate analysis?</p>
<p>Each <span class="math inline">\(z_j\)</span> (remember, there are <span class="math inline">\(p\)</span> of these for each observation <span class="math inline">\(j\)</span>) is called a <strong>score</strong>. The entire equation above (again, there are <span class="math inline">\(p\)</span> of these), is called a <strong>component</strong>.</p>
<p>The set of coefficients for each <span class="math inline">\(z\)</span>, <span class="math inline">\(c\)</span>, are called <strong>eigenvectors</strong>. So, there are <span class="math inline">\(p\)</span> eigenvectors, each of length <span class="math inline">\(p\)</span>. They are often scaled, depending on your multivariate method.</p>
<p>The total amount of variation explained by each eigenvector is called the <strong>eigenvalue</strong>, <span class="math inline">\(\lambda\)</span>. There are <span class="math inline">\(p\)</span> eigenvalues.</p>
<p>The eigenvectors and eigenvalues of the <span class="math inline">\(p\)</span> components is what we use in a principal components analysis, canonical correlaion analysis, or a correspondence analysis.</p>
<p>If we have data that can be grouped, we can estimate the components in order to maximize between-group differences relative to within-group differences. This is what we get out of multivariate ANOVA (MANOVA) or discriminant function analysis.</p>
<p>Note that if you are working with species abundances, species presence-absences, or genetic data, we might calculate the dissimilarity between observations using a dissimilarity matrix.</p>
<p>For example, we might have the species abundances of aquatic invertebrates sampled from 30 ponds. Each pond has different species and abundances of species. We can calculate the dissimilarity of each pond to every other pond based on the species composition of each (for example, using Bray-Curtis distance, or another metric of distance). Then, we can use a multivariate analysis to summarize the variation among all ponds. Ponds that share species and have similar abundances will be closer together in multivariate space.</p>
<div class="sourceCode" id="cb983"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb983-1"><a href="week-14-lecture.html#cb983-1" tabindex="-1"></a><span class="fu">library</span>(vegan)</span>
<span id="cb983-2"><a href="week-14-lecture.html#cb983-2" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb983-3"><a href="week-14-lecture.html#cb983-3" tabindex="-1"></a><span class="fu">data</span>(varespec)  <span class="co"># % cover of lichen spp. in 24 sites</span></span>
<span id="cb983-4"><a href="week-14-lecture.html#cb983-4" tabindex="-1"></a>vare.dis <span class="ot">&lt;-</span> <span class="fu">vegdist</span>(varespec)</span>
<span id="cb983-5"><a href="week-14-lecture.html#cb983-5" tabindex="-1"></a>vare.dis</span></code></pre></div>
<pre><code>##           18        15        24        27        23        19        22
## 15 0.5310021                                                            
## 24 0.6680661 0.3597783                                                  
## 27 0.5621247 0.4055610 0.4934947                                        
## 23 0.3747078 0.3652097 0.5020306 0.4286111                              
## 19 0.5094738 0.4560757 0.5092318 0.4878190 0.3606242                    
## 22 0.6234419 0.3579517 0.5010050 0.4655224 0.4812706 0.4726483          
## 16 0.5337610 0.3976674 0.5907623 0.5683930 0.4094312 0.4496731 0.2678031
## 28 0.8418209 0.5225414 0.5736665 0.3027802 0.6979519 0.6431734 0.5985666
## 13 0.3453347 0.6063846 0.7576747 0.7543736 0.6221471 0.5739244 0.6948736
## 14 0.5449810 0.4803756 0.6533606 0.7467915 0.5645808 0.6331942 0.5357609
## 20 0.3879069 0.3784188 0.4346892 0.4957833 0.2877014 0.3953776 0.4627020
## 25 0.6318891 0.3376115 0.3369098 0.5001593 0.4258617 0.4311299 0.3822981
## 7  0.3603697 0.6717391 0.7931069 0.7792917 0.6390838 0.6958570 0.7459886
## 5  0.4955699 0.7178612 0.8561753 0.8732190 0.7295255 0.7898205 0.8611451
## 6  0.3382309 0.6355122 0.7441373 0.7496935 0.6252483 0.5684030 0.7249162
## 3  0.5277480 0.7578503 0.8382119 0.8090236 0.7128798 0.5302756 0.8026152
## 4  0.4694018 0.6843974 0.8309875 0.8413800 0.7117919 0.5177604 0.8015314
## 2  0.5724092 0.8206269 0.8372551 0.7581924 0.7249869 0.5389222 0.8321464
## 9  0.6583569 0.7761039 0.7590517 0.7415898 0.6693889 0.5393143 0.7725082
## 12 0.4688038 0.6794199 0.6894538 0.6253616 0.5384762 0.4288556 0.7051751
## 10 0.6248996 0.7644564 0.7842829 0.7096540 0.6625476 0.5059910 0.7875328
## 11 0.4458523 0.4716274 0.5677373 0.6322919 0.4710280 0.3293493 0.5812219
## 21 0.5560864 0.7607281 0.7272727 0.5456001 0.4951221 0.5315894 0.6771167
##           16        28        13        14        20        25         7
## 15                                                                      
## 24                                                                      
## 27                                                                      
## 23                                                                      
## 19                                                                      
## 22                                                                      
## 16                                                                      
## 28 0.7015360                                                            
## 13 0.5514941 0.8600122                                                  
## 14 0.4826350 0.8239667 0.5547565                                        
## 20 0.3737797 0.6963560 0.5785542 0.5115258                              
## 25 0.4306058 0.6086150 0.7412605 0.5541517 0.4518556                    
## 7  0.6596144 0.8960202 0.4533054 0.6550830 0.5959162 0.7556726          
## 5  0.7184789 0.9539592 0.5148988 0.7257681 0.7153827 0.8600858 0.3237446
## 6  0.6509879 0.9014440 0.3515673 0.6227473 0.5439118 0.7343872 0.1754713
## 3  0.6837953 0.9234485 0.4965478 0.7836661 0.6690479 0.8168684 0.5154487
## 4  0.6462648 0.9381169 0.3881748 0.6734743 0.6771854 0.8400134 0.5601721
## 2  0.7354202 0.9053213 0.5968691 0.8592489 0.6951539 0.8179089 0.6465777
## 9  0.8185866 0.8686670 0.7292530 0.8282497 0.6982486 0.7884243 0.8318435
## 12 0.6342166 0.8543167 0.5902386 0.7507074 0.5182426 0.7062564 0.6991666
## 10 0.7656598 0.9016604 0.7160439 0.8304088 0.6706349 0.7845955 0.7697453
## 11 0.5172825 0.7544064 0.4272808 0.6743277 0.4461712 0.6175930 0.5262233
## 21 0.7474559 0.7248773 0.7212772 0.8096450 0.6320431 0.7466232 0.7933350
##            5         6         3         4         2         9        12
## 15                                                                      
## 24                                                                      
## 27                                                                      
## 23                                                                      
## 19                                                                      
## 22                                                                      
## 16                                                                      
## 28                                                                      
## 13                                                                      
## 14                                                                      
## 20                                                                      
## 25                                                                      
## 7                                                                       
## 5                                                                       
## 6  0.3984538                                                            
## 3  0.5634432 0.4517627                                                  
## 4  0.5377506 0.4665100 0.3592689                                        
## 2  0.7257597 0.5552754 0.2099203 0.4841145                              
## 9  0.9014583 0.7223126 0.3885811 0.6222340 0.2330286                    
## 12 0.7808641 0.5762462 0.2641851 0.4870742 0.1846147 0.2277228          
## 10 0.8504191 0.6567926 0.3413378 0.5776062 0.1456729 0.1117280 0.1793368
## 11 0.5563798 0.4077948 0.3002597 0.3215966 0.4209596 0.5145260 0.3688102
## 21 0.8888316 0.6720141 0.7507773 0.7641304 0.6779661 0.5952563 0.5602137
##           10        11
## 15                    
## 24                    
## 27                    
## 23                    
## 19                    
## 22                    
## 16                    
## 28                    
## 13                    
## 14                    
## 20                    
## 25                    
## 7                     
## 5                     
## 6                     
## 3                     
## 4                     
## 2                     
## 9                     
## 12                    
## 10                    
## 11 0.5043578          
## 21 0.6147874 0.6713363</code></pre>
</div>
<div id="model-criticism-for-multivariate-analyses" class="section level2 hasAnchor" number="26.4">
<h2><span class="header-section-number">26.4</span> Model criticism for multivariate analyses<a href="week-14-lecture.html#model-criticism-for-multivariate-analyses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="transforming-your-data" class="section level3 hasAnchor" number="26.4.1">
<h3><span class="header-section-number">26.4.1</span> Transforming your data<a href="week-14-lecture.html#transforming-your-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just like last week, we are still looking at the linear relationships. The only difference is that now we are looking at the relationships among covariates (last week, we looked for a linear relationship between the response and the covariate). This means that we may need to transform covariates that do not have linear relationships with one another.</p>
</div>
</div>
<div id="standardizing-your-data" class="section level2 hasAnchor" number="26.5">
<h2><span class="header-section-number">26.5</span> Standardizing your data<a href="week-14-lecture.html#standardizing-your-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may want to standardize your data (divide by the standard deviation) if the covariates are on different scales from one another, or have very different variances.</p>
<p>However, if the scales are comparable, leaving the data unstandardized can be informative. You may want to conduct your multivariate analyses on both standardized and unstandardized data. The default behavior in R packages is not obvious, be sure to check the help files.</p>
</div>
<div id="multivariate-outliers" class="section level2 hasAnchor" number="26.6">
<h2><span class="header-section-number">26.6</span> Multivariate outliers<a href="week-14-lecture.html#multivariate-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Outliers can have a lot of influence in multivariate analyses too. You can look for outliers in multivariate space (values far away from most of the density of the dataset) by calculating the Mahalanobis distances of each observation and comparing it to the null distribution (chi-square distribution with <em>p</em> degrees of freedom).</p>
<p>Other than using a different criterion to determine what points are outliers, the procedures for dealing with outliers are the same.</p>
</div>
<div id="brief-overview-of-multivariate-analyses" class="section level2 hasAnchor" number="26.7">
<h2><span class="header-section-number">26.7</span> Brief overview of multivariate analyses<a href="week-14-lecture.html#brief-overview-of-multivariate-analyses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you plan on using multivariate analyses in your work, I recommend chapters 15 through 18 in Quinn and Keough 2002 (the previous textbook for this course, available for free online through the SBU library). Multivariate analyses are much easier to implement in R than they are to understand, and Quinn and Keough give a detailed but readable overview of these methods. The goal in lecture today is to make sure that you are aware of these methods.</p>
</div>
<div id="manova-and-dfa" class="section level2 hasAnchor" number="26.8">
<h2><span class="header-section-number">26.8</span> MANOVA and DFA<a href="week-14-lecture.html#manova-and-dfa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you have two or more response variables and one or more categorical covariates, you may be tempted to fit multiple ANOVAs. Though a single ANOVA is an omnibus test that controls for Type I error, fitting multiple ANOVAs risks inflating Type I error. You could account for this using a correction for familywise error rate. However, this reduces power.</p>
<p>Instead, you could fit a multivariate analysis of variance (MANOVA), which fits a linear combination of the response variables in a way that maximizes the between-group to within-group variances. Discriminant function analysis uses the same methods, but the goal is instead to predict group membership.</p>
</div>
<div id="scaling-or-ordination-techniques" class="section level2 hasAnchor" number="26.9">
<h2><span class="header-section-number">26.9</span> Scaling or ordination techniques<a href="week-14-lecture.html#scaling-or-ordination-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The goals of these types of analyses (principal components analysis, correspondence analysis, canonical correspondence analysis, multidimensional scaling) are (1) to reduce a large set of covariates to a smaller number of derived covariates, and (2) to reveal patterns in the data in multidimensional space. We will only be able to go over PCA today, but the other methods are covered in Quinn and Keough (2002).</p>
<div class="sourceCode" id="cb985"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb985-1"><a href="week-14-lecture.html#cb985-1" tabindex="-1"></a><span class="co"># example of MDS using lichen data by site</span></span>
<span id="cb985-2"><a href="week-14-lecture.html#cb985-2" tabindex="-1"></a>vare.mds0 <span class="ot">&lt;-</span> <span class="fu">isoMDS</span>(vare.dis, <span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb985-3"><a href="week-14-lecture.html#cb985-3" tabindex="-1"></a><span class="fu">ordiplot</span>(vare.mds0, <span class="at">type =</span> <span class="st">&quot;t&quot;</span>)</span></code></pre></div>
<p><img src="Week-14-lecture_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
</div>
<div id="principal-components-analysis-pca" class="section level2 hasAnchor" number="26.10">
<h2><span class="header-section-number">26.10</span> Principal components analysis (PCA)<a href="week-14-lecture.html#principal-components-analysis-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The goal of PCA is to reduce a set of variables to a smaller set that captures most of the variability in your data. Mathematically, PCA represents a transformation of your original axes to a new set of <strong>orthogonal</strong> axes which are ordered in terms of how much variation they explain (the <span class="math inline">\(z\)</span> components from before). Each axis is called a principal component (PC). You will have as many PCs as you have covariates.</p>
<p>The first principal component (PC1) is the axis which explains the most variation in the data. PC2 is the axis which is orthogonal (perpendicular) to PC1 and explains the next greatest amount of remaining variation. PC3 the axis orthogonal to both PC1 and PC2 and explains the next greatest amount of remaining variation, etc.</p>
</div>
<div id="principal-components-analysis-pca-1" class="section level2 hasAnchor" number="26.11">
<h2><span class="header-section-number">26.11</span> Principal components analysis (PCA)<a href="week-14-lecture.html#principal-components-analysis-pca-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Using the example I described at the beginning of the lecture, Lovett et al. (2000) measured water chemistry variables in each of 39 streams in the Catskill Mountains. Lovett and colleagues looked at each covariate and decided to log transform dissolved organic C, Cl, and H.</p>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="week-14-lecture.html#cb986-1" tabindex="-1"></a>streams.init <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;_data/lovett2.csv&quot;</span>)</span>
<span id="cb986-2"><a href="week-14-lecture.html#cb986-2" tabindex="-1"></a>streams <span class="ot">&lt;-</span> streams.init[, <span class="fu">c</span>(<span class="dv">6</span><span class="sc">:</span><span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">17</span>, <span class="dv">13</span>, <span class="dv">14</span>, <span class="dv">18</span>)]</span>
<span id="cb986-3"><a href="week-14-lecture.html#cb986-3" tabindex="-1"></a><span class="fu">row.names</span>(streams) <span class="ot">&lt;-</span> streams.init[, <span class="dv">1</span>]</span>
<span id="cb986-4"><a href="week-14-lecture.html#cb986-4" tabindex="-1"></a><span class="fu">str</span>(streams)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    38 obs. of  10 variables:
##  $ NO3 : num  24.2 25.4 29.7 22.1 13.1 27.5 28.1 31.2 22.6 35.9 ...
##  $ TON : num  5.6 4.9 4.4 6.1 5.7 3 4.7 5.4 3.1 4.9 ...
##  $ TN  : num  29.9 30.3 33 28.3 17.6 30.8 32.8 37.1 26 39.8 ...
##  $ LDOC: num  2.26 2.04 2.02 1.93 1.92 ...
##  $ NH4 : num  0.8 1.4 0.8 1.4 0.6 1.1 1.4 2.5 3.1 1.4 ...
##  $ SO4 : num  50.6 55.4 56.5 57.5 58.3 63 66.5 64.5 63.4 58.4 ...
##  $ LCL : num  1.19 1.21 1.23 1.23 1.26 ...
##  $ CA  : num  54.7 58.4 65.9 59.5 54.6 68.5 84.6 73.1 71.1 91.2 ...
##  $ MG  : num  14.4 17 19.6 19.5 21.9 22.4 26.2 25.4 21.8 22.2 ...
##  $ LH  : num  -0.319 -0.62 -0.328 -0.638 -0.432 ...</code></pre>
<p>Some of the covariates are correlated, and would be collinear in a regression. The components from a PCA are orthogonal, removing multicollinearity.</p>
<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb988-1"><a href="week-14-lecture.html#cb988-1" tabindex="-1"></a><span class="fu">pairs</span>(streams)</span></code></pre></div>
<p><img src="Week-14-lecture_files/figure-html/unnamed-chunk-5-1.png" width="576" /></p>
</div>
<div id="pca-in-r" class="section level2 hasAnchor" number="26.12">
<h2><span class="header-section-number">26.12</span> PCA in R<a href="week-14-lecture.html#pca-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can fit a PCA using <code>prcomp()</code>. There are a lot of functions for PCAs, carefully read the help files to understand how the function is fitting the PCA before using in your own analyses.</p>
<p>The variances of the water chemistry covariates are very variable. It is probably best if we scale our data with the argument <code>scale = TRUE</code> (note this is not the default in R!).</p>
<div class="sourceCode" id="cb989"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb989-1"><a href="week-14-lecture.html#cb989-1" tabindex="-1"></a><span class="fu">apply</span>(<span class="at">X =</span> streams, <span class="at">MARGIN =</span> <span class="dv">2</span>, var)  <span class="co"># variance of each covariate</span></span></code></pre></div>
<pre><code>##          NO3          TON           TN         LDOC          NH4          SO4 
##  74.18850640   1.63670697  65.60399004   0.02144044   0.53607397  27.24332859 
##          LCL           CA           MG           LH 
##   0.02407754 194.74177098  26.24953058   0.08566604</code></pre>
<div class="sourceCode" id="cb991"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb991-1"><a href="week-14-lecture.html#cb991-1" tabindex="-1"></a>pca.streams <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(streams, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>We can look at our eigenvalues in R using <code>summary()</code>. The square root of the eigenvalue is labeled <code>Standard deviation</code>. We can also see the proportion of variation explained in each component (labeled <code>Proportion of Variance</code>).</p>
<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb992-1"><a href="week-14-lecture.html#cb992-1" tabindex="-1"></a><span class="fu">summary</span>(pca.streams)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6     PC7
## Standard deviation     1.8504 1.5725 1.0822 0.96516 0.86381 0.80397 0.61930
## Proportion of Variance 0.3424 0.2473 0.1171 0.09315 0.07462 0.06464 0.03835
## Cumulative Proportion  0.3424 0.5897 0.7068 0.79995 0.87456 0.93920 0.97755
##                            PC8     PC9    PC10
## Standard deviation     0.36161 0.30237 0.04784
## Proportion of Variance 0.01308 0.00914 0.00023
## Cumulative Proportion  0.99063 0.99977 1.00000</code></pre>
<p>We have as many eigenvalues as covariates (10). PC1 explains 34% of the variation in the data, PC2 explains 25%, and PC3 explains 12%. So, we can use just the first three components to describe over 70% of the total variance in the data.</p>
<p>We can look at PC1 more closely by looking at the eigenvectors, or the vector of the coefficients of the component equation. The further away each coefficient is from 0, the greater contribution that variable makes to the component.</p>
<p><span class="math display">\[z_j = c_1 (\text{NO}_3)_j + c_2 (\text{total organic N})_j + c_3 (\text{total N})_j + \\
c_4 (\text{NH}_4)_j + c_5 (\text{log dis. organic C})_j + c_6 (\text{SO}_4)_j \\
c_7 (\text{log Cl})_j + c_8 (\text{Ca})_j + c_9 (\text{Mg})_j + c_{10} (\text{log H})_j\]</span></p>
<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb994-1"><a href="week-14-lecture.html#cb994-1" tabindex="-1"></a><span class="fu">round</span>(pca.streams<span class="sc">$</span>rotation[, <span class="dv">1</span>], <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   NO3   TON    TN  LDOC   NH4   SO4   LCL    CA    MG    LH 
## -0.26  0.15 -0.23 -0.29  0.23  0.37  0.36  0.28  0.47 -0.40</code></pre>
<p>Large values of PC1 are associated with having higher Mg, higher log Cl, higher S04, and lower log H.</p>
<p>We can visualize our PCA using either the <code>biplot()</code> function in base R or <code>fviz_pca_biplot()</code> in the package <code>factoextra</code>.</p>
<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb996-1"><a href="week-14-lecture.html#cb996-1" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb996-2"><a href="week-14-lecture.html#cb996-2" tabindex="-1"></a><span class="fu">fviz_pca_biplot</span>(pca.streams, <span class="at">axes =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">repel =</span> <span class="cn">TRUE</span>,</span>
<span id="cb996-3"><a href="week-14-lecture.html#cb996-3" tabindex="-1"></a>                <span class="at">col.var =</span> <span class="st">&quot;dodgerblue3&quot;</span>, <span class="at">col.ind =</span> <span class="st">&quot;gray60&quot;</span>, <span class="at">title =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<p><img src="Week-14-lecture_files/figure-html/unnamed-chunk-9-1.png" width="576" /></p>
<p>The coordinates of each stream <span class="math inline">\(j\)</span> are equal to the scores <span class="math inline">\(z_j\)</span> for PC1 and PC2 (<span class="math inline">\(z_{1j}, z_{2j}\)</span>).</p>
<div class="sourceCode" id="cb997"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb997-1"><a href="week-14-lecture.html#cb997-1" tabindex="-1"></a><span class="co"># PCA scores for PC1 and PC2 of the first five streams</span></span>
<span id="cb997-2"><a href="week-14-lecture.html#cb997-2" tabindex="-1"></a>pca.streams<span class="sc">$</span>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>##                     PC1        PC2
## Santa Cruz   -3.7157577 -1.3260985
## Colgate      -2.1068761 -0.3092199
## Halsey       -2.4108484  0.3108805
## Batavia Hill -1.1467251 -0.6644800
## Windham Ridg -0.8618198 -2.0450124</code></pre>
<p>The vectors are proportional to the eigenvectors, or the contribution of each covariate to the component.</p>
<div class="sourceCode" id="cb999"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb999-1"><a href="week-14-lecture.html#cb999-1" tabindex="-1"></a><span class="co"># Eigenvectors for PC1 and PC2</span></span>
<span id="cb999-2"><a href="week-14-lecture.html#cb999-2" tabindex="-1"></a>pca.streams<span class="sc">$</span>rotation[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>##             PC1         PC2
## NO3  -0.2608012  0.51882617
## TON   0.1470633 -0.29927264
## TN   -0.2284551  0.51019471
## LDOC -0.2882753 -0.14662403
## NH4   0.2279503 -0.07531484
## SO4   0.3684647  0.22497994
## LCL   0.3578632 -0.15795720
## CA    0.2811469  0.44581193
## MG    0.4716302  0.01538673
## LH   -0.3972261 -0.28148575</code></pre>
<p>It is possible to fit linear regression in which the covariates are principal components from a PCA. This way, the covariates are not collinear. The problem is that the regression coefficients are particularly difficult to interpret. However, if you have a PCA with easily interpretable eigenvectors, this might be a useful method.</p>
<p>Additionally, be aware that the components that explain most of the variance in the covariates may not explain the variation in the response variable.</p>
</div>
<div id="missing-data" class="section level2 hasAnchor" number="26.13">
<h2><span class="header-section-number">26.13</span> Missing data<a href="week-14-lecture.html#missing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In large, multivariate data sets, you may have observations that are missing values for some of the covariates. We saw this in lab last week.</p>
<div class="sourceCode" id="cb1001"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1001-1"><a href="week-14-lecture.html#cb1001-1" tabindex="-1"></a>mammal <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;_data/MammalLifeHistory.csv&quot;</span>,<span class="at">header=</span>T)</span>
<span id="cb1001-2"><a href="week-14-lecture.html#cb1001-2" tabindex="-1"></a>carnivore.init <span class="ot">&lt;-</span> mammal[<span class="fu">which</span>(mammal<span class="sc">$</span>Order <span class="sc">==</span> <span class="st">&quot;Carnivora&quot;</span>), ]</span>
<span id="cb1001-3"><a href="week-14-lecture.html#cb1001-3" tabindex="-1"></a>carnivore.init<span class="sc">$</span>AFR[<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>]</span></code></pre></div>
<pre><code>##  [1] 24.00    NA 12.00 10.50 12.00 10.50    NA 12.00    NA    NA    NA 16.81</code></pre>
<p>Last week, we removed the entire row for observations with one or more missing values for covariates. This is the default behavior in R, for example, when fitting multiple regressions with <code>lm()</code>. However, this is throwing away all of the information we do have for the observation.</p>
</div>
<div id="imputing-missing-data" class="section level2 hasAnchor" number="26.14">
<h2><span class="header-section-number">26.14</span> Imputing missing data<a href="week-14-lecture.html#imputing-missing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In problem set 13, you were asked to impute missing data by sampling with replacement from the nonmissing data. There are other ways of dealing with missing data too.</p>
<p>You could replace the missing data with the mean value for that covariate. This means that the missing point won’t contribute to the estimate of the partial slope for the missing covariate. However, it will lead to an underestimate of the variance for the estimate (doesn’t add to SSE). For the regression:</p>
<p><span class="math display">\[
\text{Maximum lifespan} \sim \text{Intercept} + \text{AFR}
\]</span></p>
<p>We could impute missing values of AFR with the mean value of AFR.</p>
<div class="sourceCode" id="cb1003"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1003-1"><a href="week-14-lecture.html#cb1003-1" tabindex="-1"></a>carnivore <span class="ot">&lt;-</span> carnivore.init</span>
<span id="cb1003-2"><a href="week-14-lecture.html#cb1003-2" tabindex="-1"></a>missing.AFR <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">is.na</span>(carnivore.init<span class="sc">$</span>AFR))</span>
<span id="cb1003-3"><a href="week-14-lecture.html#cb1003-3" tabindex="-1"></a>mean.AFR <span class="ot">&lt;-</span> <span class="fu">mean</span>(carnivore.init<span class="sc">$</span>AFR, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1003-4"><a href="week-14-lecture.html#cb1003-4" tabindex="-1"></a>carnivore<span class="sc">$</span>AFR[missing.AFR] <span class="ot">&lt;-</span> mean.AFR</span></code></pre></div>
<p>You could also use a regression to predict the missing value for one covariate given the other covariates in the data. Using the same example, we could impute missing values of AFR using the coefficients estimated from a regression where AFR is the response variable with one or more other covariates as the predictors. For example:</p>
<p><span class="math display">\[
\text{AFR} \sim \text{Intercept} + \text{Litter size}
\]</span></p>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="week-14-lecture.html#cb1004-1" tabindex="-1"></a>imputation.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> AFR <span class="sc">~</span> LitterSize, <span class="at">data =</span> carnivore.init)</span>
<span id="cb1004-2"><a href="week-14-lecture.html#cb1004-2" tabindex="-1"></a>predicted.AFR <span class="ot">&lt;-</span> imputation.lm<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> imputation.lm<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> </span>
<span id="cb1004-3"><a href="week-14-lecture.html#cb1004-3" tabindex="-1"></a>  carnivore.init<span class="sc">$</span>LitterSize[missing.AFR] </span>
<span id="cb1004-4"><a href="week-14-lecture.html#cb1004-4" tabindex="-1"></a>carnivore<span class="sc">$</span>AFR[missing.AFR] <span class="ot">&lt;-</span> predicted.AFR</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-13-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-14-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
