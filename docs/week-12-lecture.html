<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>22 Week 12 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="22 Week 12 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="22 Week 12 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="22 Week 12 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-01-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-11-lab.html"/>
<link rel="next" href="week-12-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.8</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.9</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.9.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.9.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.9.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.9.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.9.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.10</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#short-digression-degrees-of-freedom"><i class="fa fa-check"></i><b>7.2</b> Short digression: Degrees of freedom</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.3</b> t-distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.4</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.5</b> F distribution</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.6</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.7" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.7</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-12-lecture" class="section level1 hasAnchor" number="22">
<h1><span class="header-section-number">22</span> Week 12 Lecture<a href="week-12-lecture.html#week-12-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-12-readings" class="section level2 hasAnchor" number="22.1">
<h2><span class="header-section-number">22.1</span> Week 12 Readings<a href="week-12-lecture.html#week-12-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Sections 7.65 and 10.8-10.14, as well as Logan Chapters 11-12. You will also need to read <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Hurlbert_1984.pdf">Hurlbert (1984)</a>. Note that I have long considered eliminating this paper because Hurlbert has made statements in the public sphere that many find offensive (regarding politics, not statistics), I continue to assign this paper because the paper itself is so well known that it would be a disservice not to familiarize everyone with the topics and terminology introduced here. We will discuss this paper at length in class and while it is a bit long, I would encourage you to read it carefully. Here is a nice simple overview on <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Krzywinski_Altman_2014d.pdf">two factor designs</a> worth reading as well. There are several other (very short) papers that may help clarify some of the more nuanced issues of this week, on <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Krzywinski_Altman_2014c.pdf">‘blocking’ in ANOVA</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Altman_Krzywinski_2015b.pdf">split plot designs</a>, and <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Krzywinski_etal_2014.pdf">nested designs</a>.</p>
</div>
<div id="week-12-outline" class="section level2 hasAnchor" number="22.2">
<h2><span class="header-section-number">22.2</span> Week 12 outline<a href="week-12-lecture.html#week-12-outline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>ANOVA for two-way factorial design</p></li>
<li><ol style="list-style-type: lower-alpha">
<li><p>Dealing with unbalanced design – unequal sample sizes: Type I, II, and III SS</p></li>
<li><p>Dealing with unbalanced design- missing cells</p></li>
</ol></li>
<li><p>ANOVA for two-way nested design</p></li>
<li><p>Some more experimental design</p></li>
</ol>
</div>
<div id="review-anova-with-one-factor" class="section level2 hasAnchor" number="22.3">
<h2><span class="header-section-number">22.3</span> Review: ANOVA with one factor<a href="week-12-lecture.html#review-anova-with-one-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With single factor ANOVA, we compared multiple levels of a factor. Another way to say this is that we only had one categorical covariate in our linear model (e.g., the effect of zinc concentration on diatom diversity).</p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
</div>
<div id="anova-with-more-than-one-factor" class="section level2 hasAnchor" number="22.4">
<h2><span class="header-section-number">22.4</span> ANOVA with more than one factor<a href="week-12-lecture.html#anova-with-more-than-one-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With two-way (and higher) ANOVA, we look at more than one factor at a time (e.g., the effect of density and season on limpet egg production). There are different ways in which multiple factors can be modeled, and this depends on the design of your study. We will focus our discussion primarily on two ways in which multiple factors can be applied in an experimental design: Nested vs Factorial. Note that in some complicated experiments, there may be elements that are nested and others that are factorial. This will become clear as we work through some examples.</p>
<p>In this example below, the factors are crossed (two-way factorial design).</p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-3-1.png" width="528" /></p>
<p>The two factors are fully crossed, or all combinations of factors are included in the design and every level of every factor occurs in combination with every level of the other factors.</p>
<p>We will start with factorial designs because these are the most logical extension of the one-way analyses we discussed last week.</p>
</div>
<div id="two-way-anova-factorial-designs" class="section level2 hasAnchor" number="22.5">
<h2><span class="header-section-number">22.5</span> Two-way ANOVA factorial designs<a href="week-12-lecture.html#two-way-anova-factorial-designs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous example, the number of egg masses per limpet was measured at different densities (factor A) and in different seasons (factor B). Two-way factorial designs can be represented in a table, where each cell is a combination of factor A and factor B. Each cell has multiple replicates.</p>
<table>
<thead>
<tr class="header">
<th>Factor level</th>
<th align="center"><span class="math inline">\(B_1\)</span></th>
<th align="center"><span class="math inline">\(B_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center">8 animals/enclosure, spring</td>
<td align="center">8 animals/enclosure, summer</td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center">15 animals/enclosure, spring</td>
<td align="center">15 animals/enclosure, summer</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_3\)</span></td>
<td align="center">30 animals/enclosure, spring</td>
<td align="center">30 animals/enclosure, summer</td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_4\)</span></td>
<td align="center">45 animals/enclosure, spring</td>
<td align="center">45 animals/enclosure, summer</td>
</tr>
</tbody>
</table>
<p>In a two-way factorial design, adding in an additional factor gives us a new issue to contend with. We are trying to measure the effect of factor A, marginalizing across factor B, or the effect of factor B, marginalizing across factor A. The marginal means for density (averaged across both seasons) and for season (averaged across all densities) are shown:</p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-4-1.png" width="288" /><img src="Week-12-lecture_files/figure-html/unnamed-chunk-4-2.png" width="288" /></p>
<p>But, what if season has an effect on density, or, the two factors influence one another? With two-way designs we need to account for this. We call this potential <span class="math inline">\(\text{factor} \times \text{factor}\)</span> influence an <strong>interaction</strong>, which we include in our model as a parameter that allows for the effect of factor A to depend on factor B.</p>
<p>For a hypothetical example, let’s say we are measuring plant biomass in different fertilizer treatments (factor A, levels 1-3: low N, ambient N, and high N) and watering treatments (factor B, levels 1-4: no water, low water, ambient water, excess water), where <span class="math inline">\(Y_{ijk}\)</span> is the <span class="math inline">\(k^{\text{th}}\)</span> individual with fertilizer treatment <span class="math inline">\(i\)</span> and watering treatment <span class="math inline">\(j\)</span>. All possible combinations of treatments were measured. Interaction term <span class="math inline">\({AB}_{ij}\)</span> allows for the effect of watering treatment to depend on fertilizer treatment (and vice versa).</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2 )
\]</span></p>
<p><strong>Question: What is <span class="math inline">\(A_i\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The effect of fertilizer treatment <span class="math inline">\(i\)</span>, <span class="math inline">\(\mu_i - \mu\)</span>. The effect of treatment <span class="math inline">\(i\)</span> is the mean for factor A, level <span class="math inline">\(i\)</span>, pooling across all levels for factor B.
</span>
</details>
<p>
 
</p>
<p><strong>Question: How do we write the model for the 2nd individual plant in the fertilizer treatment “low N” and the watering treatment “excess water”?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(Y_{142} = \mu + A_1 + B_4 + {AB}_{14} + \epsilon_{142} \text{, where } \epsilon_{142} \sim \mathrm{N} ( 0, \sigma^2 )\)</span>
</span>
</details>
<p>
 
</p>
<p>Each level of one factor is applied to all levels of the other factor, and all combinations are replicated.</p>
<p>Factor A, fertilizer treatment, has 3 levels and factor B (watering treatment) has 4 levels, so we have 12 possible combinations of factor A and B that must be represented. In a balanced design, you would have the same number of samples in each of the 12 combinations of factors A and B.</p>
<table>
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor level</th>
<th align="center"><span class="math inline">\(B_1\)</span></th>
<th align="center"><span class="math inline">\(B_2\)</span></th>
<th align="center"><span class="math inline">\(B_3\)</span></th>
<th align="center"><span class="math inline">\(B_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_1 = Y_{111}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{112}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{113}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_2 = Y_{121}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{122}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{123}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_3 = Y_{131}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{132}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{133}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_4 = Y_{141}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{142}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{143}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_1 = Y_{211}\)</span> <br> <span class="math inline">\(A_2 , B_1 = Y_{212}\)</span> <br> <span class="math inline">\(A_2 , B_1 = Y_{213}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_2 = Y_{221}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{222}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{223}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_3 = Y_{231}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{232}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{233}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_4 = Y_{241}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{242}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{243}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_3\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_1 = Y_{311}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{312}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{313}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_2 = Y_{321}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{322}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{323}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_3 = Y_{331}\)</span> <br> <span class="math inline">\(A_3 , B_3 = Y_{332}\)</span> <br> <span class="math inline">\(A_3 , B_3 = Y_{333}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_4 = Y_{341}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{342}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{343}\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Question: How would you find the mean biomass for factor B, level 1 (no water)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\bar{Y}_j = \bar{Y}_1\)</span> = mean of all biomass values in column 1
</span>
</details>
<p>
 
</p>
<p><strong>Question: What about the mean for factor A, level 3 (high N)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\bar{Y}_i = \bar{Y}_3\)</span> = mean of all biomass values in row 3
</span>
</details>
<p>
 
</p>
<p><strong>Question: What about the mean for factor A, level 2 AND factor B, level 4 (ambient N, excess water)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\bar{Y}_{ij} = \bar{Y}_{24}\)</span> = mean of all biomass values in the cell in row 2 and column 4
</span>
</details>
<p>
 
</p>
<p>Interactions occur when:</p>
<p><span class="math display">\[
\text{Effect of factor A alone} + \text{Effect of factor B alone} \neq \text{Effect of A and B together}
\]</span></p>
<p>Let’s go through multiple examples of hypothetical two-way factorial ANOVA results to interpret the main effects and interactions in each outcome.</p>
<p>
 
</p>
<p><strong><span style="color: red;">Example #1</span></strong></p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-5-1.png" width="384" /></p>
<p>A large positive value for factor A is associated with an increase in the response variable, a large positive value for factor B is associated with an increase in the response variable, and there is an interaction of A <span class="math inline">\(\times\)</span> B, showing that B has little effect at low A, but a large effect at high A. We can see this more clearly in the diagram below. For evaluating the main effect of factor B (left panel), we average across (‘marginalize out’) all the A levels and see the impact of changing the B level. Likewise, evaluating the main effect of factor A (right panel) requires us to average across all the B levels. In both cases, when we average across the levels of the other factor, we see that both A and B are associated with a change in the mean response. (There is also an interaction here as well.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="Two_Way_Main_Effects_1.png" alt="The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level." width="100%" />
<p class="caption">
Figure 7.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.
</p>
</div>
<p>
 
</p>
<p><strong><span style="color: red;">Example #2</span></strong></p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-7-1.png" width="384" /></p>
<p>Once again, we can see the main effects by averaging across the levels of the other factor. Here both A and B have a main effect on the response, but there is no interaction because the effect of factor A is the same for both levels of factor B (and vice versa).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="Two_Way_Main_Effects_2.png" alt="The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level." width="100%" />
<p class="caption">
Figure 20.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.
</p>
</div>
<p>
 
</p>
<p><strong><span style="color: red;">Example #3</span></strong></p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-9-1.png" width="384" /></p>
<p>Below we can see that there is no main effect for Factor A because when we average over the levels of B, A has no effect on the response (right panel). There is, however, a main effect of B (left panel). There is no interaction of A <span class="math inline">\(\times\)</span> B.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="Two_Way_Main_Effects_3.png" alt="The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level." width="100%" />
<p class="caption">
Figure 22.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.
</p>
</div>
<p>
 
</p>
<p><strong><span style="color: red;">Example #4</span></strong></p>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-11-1.png" width="384" /></p>
<p>Below we see that Factor A has no main effect (right panel) and Factor B also has no main effect (left panel). There is, however, an interaction of A <span class="math inline">\(\times\)</span> B. In fact, the effect of A is negative for B=Low and positive for B=High.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="Two_Way_Main_Effects_4.png" alt="The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level." width="100%" />
<p class="caption">
Figure 8.1: The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.
</p>
</div>
<p>Let’s continue interpreting two-way ANOVA plots of main effects and interactions with Aho Fig. 10.9. The response variable is biomass. Factor A, water level, is shown on the x-axis, and factor B, nutrient level, is shown with the line type, where a solid line represents added N, and a dashed line represents the control.</p>
<pre><code>## Loading required package: tcltk</code></pre>
<p><img src="Week-12-lecture_files/figure-html/unnamed-chunk-13-1.png" width="576" /></p>
<p><strong>Question: How would you interpret these results for each scenario (a,b,c, and d)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
a) Water has a positive effect on biomass, Fertilization has a positive effect on biomass, and there is no significant interaction. b) There is no effect of water on biomass, Fertilization has a positive effect on biomass and there is no significant interaction. c) A significant interaction obscures the main effects. In wet conditions, Fertilizer has a negative effect on biomass. In dry conditions, Fertilizer has a positive effect on biomass. d) There is still an interaction, but it doesn’t change (or obscure) the main effects. Ferilizer always has a positive effect on biomass. In wet conditions, this effect is more extreme.
</span>
</details>
<p>
 
</p>
<p>Let’s come back to the equation for a two-factor factorial design:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2 )
\]</span></p>
<p>In the two-way factorial ANOVA model, there are separate null hypotheses for the two main effects and for the interaction term. The null hypotheses for the main effects (factors A and B) are:</p>
<p><span class="math display">\[
H_0 (A): \mu_1 = \mu_2 = ... = \mu_i \text{ or } A_1 = A_2 = \dots = A_i = 0
\]</span></p>
<p><span class="math display">\[
H_0 (B): \mu_1 = \mu_2 = ... = \mu_j \text{ or } B_1 = B_2 = \dots = B_j = 0
\]</span></p>
<p><strong>Question: What’s the difference between writing the null hypothesis as all <span class="math inline">\(\mu_i\)</span> equal vs. all <span class="math inline">\(A_i = 0\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
First way: all population group means are equal, second way: all treatment effects (differences between group means and overall mean) are zero.
</span>
</details>
<p>
 
</p>
<p>Null hypothesis for interaction (factors A and B)</p>
<p><span class="math display">\[
H_0 (AB): {AB}_{11} = {AB}_{12} = {AB}_{13} = \dots = {AB}_{ij} = 0
\]</span></p>
<p>Under the null hypothesis for the interaction term, the effects of factors A and B are additive. Another way to say this is, the effect of having factor A level 1 (low N treatment) and factor B level 3 (ambient water treatment) is equal to the sum of the effect of factor A and the effect of factor B.</p>
<table>
<colgroup>
<col width="36%" />
<col width="21%" />
<col width="21%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups (factor A)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(a - 1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{A}}{\text{DOF}_{A}}\)</span></td>
</tr>
<tr class="even">
<td>Among groups (factor B)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{j} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(b - 1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{B}}{\text{DOF}_{B}}\)</span></td>
</tr>
<tr class="odd">
<td>Interaction</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{ij} - \bar{Y}_{i} - \bar{Y}_{j} + \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\((a - 1) (b - 1)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{AB}}{\text{DOF}_{AB}}\)</span></td>
</tr>
<tr class="even">
<td>Within groups (residual)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y}_{ij})^2\)</span></td>
<td align="center"><span class="math inline">\(ab (n - 1)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{within}}}{\text{DOF}_{\text{within}}}\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(abn - 1\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Given our example of plant biomass (<span class="math inline">\(Y\)</span>) under different fertilizer (factor A) and watering treatments (factor B).</p>
<p><strong>Question: What is <span class="math inline">\(\bar{Y}\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The overall mean biomass, including all factors and levels.
</span>
</details>
<p>
 
</p>
<p><strong>Question: What is <span class="math inline">\(\bar{Y}_i\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The mean biomass in fertilizer (factor A) <span class="math inline">\(i\)</span>, averaged across all watering treatments (factor B).
</span>
</details>
<p>
 
</p>
<p><strong>Question: What is <span class="math inline">\(\bar{Y}_j\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The mean biomass in watering treatment (factor B) <span class="math inline">\(j\)</span>, marginalized across fertilizer treatments (factor A).
</span>
</details>
<p>
 
</p>
<p><strong>Question: What is <span class="math inline">\(\bar{Y}_{ij}\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The mean biomass in treatment combination fertilizer (factor A) <span class="math inline">\(i\)</span> and water level (factor B) <span class="math inline">\(j\)</span>. We will refer to this as the <strong>cell mean</strong>.
</span>
</details>
<p>
 
</p>
<p><strong>Question: What is going on with the sums of squares for factor A? There are no <span class="math inline">\(k\)</span> or <span class="math inline">\(j\)</span> subscripts in the formula <span class="math inline">\((\bar{Y}_{i} - \bar{Y})^2\)</span>, yet the summations <span class="math inline">\(\sum^b_{j = 1}\)</span> and <span class="math inline">\(\sum^n_{k = 1}\)</span> are included in the equation.</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
You end up multiplying the same value, <span class="math inline">\((\bar{Y}_{i} - \bar{Y})^2\)</span> multiple times. The sums of squares for factor A is equal to <span class="math inline">\(n b \sum^a_{i = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span> when you have a balanced design (equal number of replicates in each cell).
</span>
</details>
<p>
 
</p>
<p><strong>Question: Why is <span class="math inline">\(\text{DOF}_{A} = a - 1\)</span> and <span class="math inline">\(\text{DOF}_{B} = b - 1\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We estimate <span class="math inline">\(a\)</span> or <span class="math inline">\(b\)</span> group means, minus one for the overall mean.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Why is <span class="math inline">\(\text{DOF}_{AB} = (a - 1) (b - 1)\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Multiply this out. <span class="math inline">\(ab\)</span> = number of combinations, we estimate <span class="math inline">\(ab\)</span> cell means, minus <span class="math inline">\(a\)</span>, minus <span class="math inline">\(b\)</span> for each of the factor means, and minus one for the overall mean.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Why is <span class="math inline">\(\text{DOF}_{\text{within}} = a b (n - 1)\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Multiply this out. <span class="math inline">\(abn\)</span> = total number of data points. Subtract <span class="math inline">\(ab\)</span> for the cell means.
</span>
</details>
<p>
 
</p>
<p>The confusing sums of squares for the interaction can be rearranged to make more sense:</p>
<p><span class="math display">\[
\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{ij} - \bar{Y}_{i} - \bar{Y}_{j} + \bar{Y})^2 = \sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} ((\bar{Y}_{ij} - \bar{Y}) - (\bar{Y}_{i} - \bar{Y}) - (\bar{Y}_{j} - \bar{Y}))^2
\]</span></p>
<p>The interaction term represents the difference between the cell mean and overall mean relative to the difference between the main effects mean and the overall mean. This tells us how “special” this cell is relative to being in factor A alone or factor B alone.</p>
<p>To test the null hypothesis that factor A has no effect (<span class="math inline">\(H_0(A): A_i = 0\)</span>), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: <span class="math inline">\(P ( X \ge F^*), X \sim F_{[\text{DOF}_{A}, \text{DOF}_{\text{within}}]}\)</span>.</p>
<p>To test the null hypothesis that factor B has no effect (<span class="math inline">\(H_0(B): B_j = 0\)</span>), <span class="math inline">\(P ( X \ge F^*), X \sim F_{[\text{DOF}_{B}, \text{DOF}_{\text{within}}]}\)</span>.</p>
<p>Last, to test the null hypothesis that there is no interaction (<span class="math inline">\(H_0(AB): AB_{ij} = 0\)</span>), <span class="math inline">\(P ( X \ge F^*), X \sim F_{[\text{DOF}_{AB}, \text{DOF}_{\text{within}}]}\)</span>.</p>
<p>In our data, there is some total amount of variation:</p>
<p><span class="math display">\[
\text{SS}_{\text{total}} = \sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2
\]</span></p>
<p>We have partitioned the variance into the variance explained by factor A (<span class="math inline">\(\text{SS}_{A}\)</span>), the variance explained by factor B (<span class="math inline">\(\text{SS}_{B}\)</span>), the variance explained by the interaction between factor A and factor B (<span class="math inline">\(\text{SS}_{AB}\)</span>), and the unexplained (residual) variance (<span class="math inline">\(\text{SS}_{\text{within}}\)</span>).</p>
<p>So, <strong>if our ANOVA is balanced</strong>:</p>
<p><span class="math display">\[
\textbf{SS}_{\text{total}} = \textbf{SS}_{A} + \textbf{SS}_{B} + \textbf{SS}_{AB} + \textbf{SS}_{\text{within}}
\]</span>
What if factors A and B are both random effects? Our null hypotheses would be:</p>
<p><span class="math display">\[
H_0: \sigma^2_A = 0 \text{ and } \sigma^2_B = 0
\]</span></p>
<p>These null hypotheses mean that there is no added variance due to the levels within factors A or B, respectively.</p>
<p>What about the interaction in this case?</p>
<p><span class="math display">\[
H_0: \sigma^2_{AB} = 0
\]</span></p>
<p>This means that there is no added variance due to the <em>combination</em> of A and B.</p>
<p>If both effects are random, most of the ANOVA table is exactly the same, however, the <strong>F-ratio is calculated differently</strong>.</p>
<table style="width:100%;">
<colgroup>
<col width="16%" />
<col width="27%" />
<col width="27%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">F ratio for fixed effects only</th>
<th align="center">F ratio for random effects only</th>
<th align="center">F ratio for A random / B fixed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{A}}{\text{MS}_{AB}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}\)</span></td>
</tr>
<tr class="even">
<td>Factor B</td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{B}}{\text{MS}_{\text{within}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{B}}{\text{MS}_{AB}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{B}}{\text{MS}_{AB}}\)</span></td>
</tr>
<tr class="odd">
<td>Interaction</td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}\)</span></td>
</tr>
</tbody>
</table>
<p>The F ratio is the mean squared error (MS) of the factor of interest divided by the mean squared error for the term that has everything <em>but</em> the factor of interest. When you have factor A and are considering interactions with a random effects variable B, that adds a new component to the expected variance of A (remember the new variance term, <span class="math inline">\(\sigma_{AB}^2\)</span> with random effects). Therefore, the appropriate comparison for the F ratio test is the mean squared error for the interaction term, which includes both the within group error and this additional variance component associated with the random factor.</p>
<p>Remember from last week:</p>
<p><span class="math display">\[
\text{Among group variance (group effect + error)}
\]</span></p>
<p>When factors A and B are random, the estimate of <span class="math inline">\(\text{MS}_{A}\)</span> can be described as:</p>
<p><span class="math display">\[
\text{Among group variance component}_A \text{(group A variance + interaction variance + residual variance)}
\]</span></p>
<p>And when factors A and B are random, the estimate of <span class="math inline">\(\text{MS}_{AB}\)</span> can be described as:</p>
<p><span class="math display">\[
\text{Interaction variance component}_{AB} \text{(interaction variance + residual variance)}
\]</span></p>
<p>So, the F ratio is the mean squares of the factor of interest divided by the MS for the term that has everything but the factor of interest.</p>
<p><strong>Review question: How did we estimate the F ratio with a single factor ANOVA with a random effect?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The F ratio was exactly the same as for fixed effects (denominator was mean square within groups). Only your interpretation of the test changes with a single factor random effects ANOVA.
</span>
</details>
<p>
 
</p>
</div>
<div id="why-bother-with-random-effects" class="section level2 hasAnchor" number="22.6">
<h2><span class="header-section-number">22.6</span> Why bother with random effects?<a href="week-12-lecture.html#why-bother-with-random-effects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You have a study measuring the effect of different levels of zinc contamination on diatom diversity, measured in multiple streams. You could model the data with or without stream as a random effect. The model without a random effect (pooling all data from different streams), where <span class="math inline">\(A_i\)</span> is the effect of zinc level <span class="math inline">\(i\)</span>, would be:</p>
<p><span class="math display">\[
Y_{ij} = \mu + A_i + \epsilon_{ij} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma_{\epsilon1}^2 )
\]</span></p>
<p>The model with stream as a random effect, where <span class="math inline">\(B_j\)</span> is the random effect of stream, and <span class="math inline">\(AB_{ij}\)</span> is also a random effect describing whether the effect of zinc is consistent across streams, would be:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma_{\epsilon2}^2 ) \\
B_j \sim \mathrm{N}(0, \sigma^2_B) \\
{AB}_{ij} \sim \mathrm{N}(0, \sigma^2_{AB})
\]</span></p>
<p>Your data is exactly the same in both cases, but <strong>in the model with stream as a random effect, you have decided to let the random effect of stream absorb some of the residual/unexplained variation in the model</strong>. Therefore, <span class="math inline">\(\sigma_{\epsilon2} \leq \sigma_{\epsilon1}\)</span>. In the case where you ignored the effect of stream, the unexplained variation included some variation that could have been explained by stream. By lumping these together, you lose statistical power to test the null hypothesis on <span class="math inline">\(A_{i}\)</span>.</p>
</div>
<div id="mixed-model" class="section level2 hasAnchor" number="22.7">
<h2><span class="header-section-number">22.7</span> Mixed model<a href="week-12-lecture.html#mixed-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What if we have one fixed effect and one random effect. Models with both fixed and random effects are called mixed models. Our null hypothesis in this case is:</p>
<p>Factor A (fixed effect) <span class="math inline">\(H_0: A_1 = A_2 = \dots = 0\)</span></p>
<p><strong>Question: How can we interpret this null hypothesis?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The means of each population of factor A, pooled over all levels of random factor B, are equal.
</span>
</details>
<p>
 
</p>
<p>Factor B (random effect) <span class="math inline">\(H_0: \sigma^2_B = 0\)</span></p>
<p><strong>Question: How can we interpret this null hypothesis?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
There is no added variance due to the levels within factor B.
</span>
</details>
<p>
 
</p>
<p>Interaction <span class="math inline">\(H_0: \sigma^2_{AB} = 0\)</span></p>
<p>Important: The interaction between a fixed factor and a random factor is a random factor.</p>
<p><strong>Question: How can we interpret this null hypothesis?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
There is no added variance due to the combinations of factor A or B.
</span>
</details>
<p>
 
</p>
</div>
<div id="unbalanced-designs" class="section level2 hasAnchor" number="22.8">
<h2><span class="header-section-number">22.8</span> Unbalanced designs<a href="week-12-lecture.html#unbalanced-designs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Unbalanced designs have a surprising influence on our estimates in ANOVA.</p>
<p>Earlier, we discussed partitioning the total variance in our data into many components:</p>
<p><span class="math display">\[
\text{SS}_{\text{total}} = \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
\]</span></p>
<p>When we have an unbalanced design, the assignment of the variance to different components becomes ambiguous, and depends on the order that we assign variance. There is no longer a simple way to partition the variance into components of <span class="math inline">\(\text{SS}_{\text{total}}\)</span>. The formulae in the two-way factorial ANOVA table are no longer applicable. Were we to calculate these variance components:</p>
<p><span class="math display">\[
\text{SS}_{\text{total}} \ne \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
\]</span></p>
<p>There are multiple ways that a design can be unbalanced, for example, having different sample sizes in different treatments:</p>
<table>
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor level</th>
<th align="center"><span class="math inline">\(B_1\)</span></th>
<th align="center"><span class="math inline">\(B_2\)</span></th>
<th align="center"><span class="math inline">\(B_3\)</span></th>
<th align="center"><span class="math inline">\(B_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_1 = Y_{111}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{112}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{113}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_2 = Y_{121}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{122}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{123}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_3 = Y_{131}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{132}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{133}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_4 = Y_{141}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{142}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{143}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_1 = Y_{211}\)</span> <br> <span class="math inline">\(A_2 , B_1 = Y_{212}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_2 = Y_{221}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{222}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{223}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_3 = Y_{231}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{232}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{233}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_4 = Y_{241}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{242}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{243}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_3\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_1 = Y_{311}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{312}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{313}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_2 = Y_{321}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{322}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{323}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_3 = Y_{331}\)</span> <br> <span class="math inline">\(A_3 , B_3 = Y_{332}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_4 = Y_{341}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{342}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{343}\)</span></td>
</tr>
</tbody>
</table>
<p>Or, one or more cells (factor A B combination) may be missing entirely:</p>
<table>
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor level</th>
<th align="center"><span class="math inline">\(B_1\)</span></th>
<th align="center"><span class="math inline">\(B_2\)</span></th>
<th align="center"><span class="math inline">\(B_3\)</span></th>
<th align="center"><span class="math inline">\(B_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_1 = Y_{111}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{112}\)</span> <br> <span class="math inline">\(A_1 , B_1 = Y_{113}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_2 = Y_{121}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{122}\)</span> <br> <span class="math inline">\(A_1 , B_2 = Y_{123}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_3 = Y_{131}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{132}\)</span> <br> <span class="math inline">\(A_1 , B_3 = Y_{133}\)</span></td>
<td align="center"><span class="math inline">\(A_1 , B_4 = Y_{141}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{142}\)</span> <br> <span class="math inline">\(A_1 , B_4 = Y_{143}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(A_2 , B_2 = Y_{221}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{222}\)</span> <br> <span class="math inline">\(A_2 , B_2 = Y_{223}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_3 = Y_{231}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{232}\)</span> <br> <span class="math inline">\(A_2 , B_3 = Y_{233}\)</span></td>
<td align="center"><span class="math inline">\(A_2 , B_4 = Y_{241}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{242}\)</span> <br> <span class="math inline">\(A_2 , B_4 = Y_{243}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_3\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_1 = Y_{311}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{312}\)</span> <br> <span class="math inline">\(A_3 , B_1 = Y_{313}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_2 = Y_{321}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{322}\)</span> <br> <span class="math inline">\(A_3 , B_2 = Y_{323}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_3 = Y_{331}\)</span> <br> <span class="math inline">\(A_3 , B_3 = Y_{332}\)</span> <br> <span class="math inline">\(A_3 , B_3 = Y_{333}\)</span></td>
<td align="center"><span class="math inline">\(A_3 , B_4 = Y_{341}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{342}\)</span> <br> <span class="math inline">\(A_3 , B_4 = Y_{343}\)</span></td>
</tr>
</tbody>
</table>
<div id="unbalanced-design-different-sample-sizes" class="section level3 hasAnchor" number="22.8.1">
<h3><span class="header-section-number">22.8.1</span> Unbalanced design – Different sample sizes<a href="week-12-lecture.html#unbalanced-design-different-sample-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll look into the first situation first. Again, when you have an unbalanced design, not only is your ANOVA more sensitive to deviations from the assumptions of ANOVA (i.e., cells with different sample sizes might have different variances), but the sum of squares can no longer be neatly partitioned as we have assumed in the past:</p>
<p><span class="math display">\[
\text{SS}_{\text{total}} \neq \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
\]</span></p>
<p>Because the partitioning of variance is ambiguous and depends on the order in which we estimate the components, there are three different ways to calculate the sums of squares for the main effects terms. <strong>These are called Type I, Type II, and Type III sums of squares</strong>.</p>
<p>Why does unbalanced design change estimates of SS?</p>
<p>In a two-way ANOVA, there are two ways to interpret the main effects:</p>
<ol style="list-style-type: decimal">
<li><p>What is the effect of factor A on <span class="math inline">\(Y\)</span>, <strong>IGNORING</strong> factor B?</p></li>
<li><p>What is the effect of factor A on <span class="math inline">\(Y\)</span>, <strong>CONTROLLING</strong> for factor B?</p></li>
</ol>
<p>It turns out that if you have equal numbers of observations in each cell, then these are the same question, but if you have an unbalanced design, then these are actually different questions. Why?</p>
<p>Let’s say that we have unbalanced data. In our sample, men are more likely to have PhDs than women. Also, PhDs make more money than non-PhDs. In this case, if you just considered the influence of gender on salary, you might conclude that men make more money than women even if there is actually no influence of gender.</p>
<p>In other words, the factors gender and education are correlated, and there is some amount of overlap in the variance explained by each predictor.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="Logan_BalancedUnbalanced.png" alt="Source: Logan (2010)" width="100%" />
<p class="caption">
Figure 22.2: Source: Logan (2010)
</p>
</div>
<p>In our previous example, <strong>“ignoring”</strong> education would mean that you let gender “take credit” for all of the variance it explains, even the variance that it shares with education. <em>Note that this is implicitly the case with all variables not included in your study (hidden explanatory variables).</em></p>
<p><strong>“Controlling”</strong> for education would mean that you are testing the effect of gender only after the effect of education had already been taken into account.</p>
<p>Another way of looking at it:</p>
<p>These two approaches actually address different hypotheses. “Ignoring” education tests whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample. “Controlling for” education tests whether men make more money than women in a population in which all educational levels are equally likely.</p>
<p>Usually, we are interested in inference where we are controlling for the other variables. When we have unbalanced designs, this is the only approach that makes much sense. But a word of warning (that will be often repeated), this is not the default in some major ANOVA functions in R!</p>
<p>We will now define each of the types of sums of squares. Note that each definition assumes the two-way factorial design: <span class="math inline">\(Y \sim A + B + A \times B\)</span>.</p>
</div>
<div id="type-i-sequential-sums-of-squares" class="section level3 hasAnchor" number="22.8.2">
<h3><span class="header-section-number">22.8.2</span> Type I (sequential) sums of squares<a href="week-12-lecture.html#type-i-sequential-sums-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\text{SS}(A)\)</span> for factor A</p>
<p><span class="math inline">\(\text{SS}(B | A)\)</span> for factor B</p>
<p>With sequential sums of squares, we first test the main effect A. Then, we estimate the main effect of B AFTER the main effect of A has “taken up” the shared variation. Bringing this back to our earlier example, with Type I SS, we would be “ignoring” education by testing whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample.</p>
<p>The order in which you add factors in a model has a huge influence. This is the default in R’s function <code>anova()</code>, despite often not being what you are interested in!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-15"></span>
<img src="Logan_TypeISS.png" alt="Source: Logan (2010)" width="100%" />
<p class="caption">
Figure 22.3: Source: Logan (2010)
</p>
</div>
<p>The sums of squares for factors A and B in Type I models are estimated using the differences in the sums of squares error for a model with just the overall mean, to a model with just factor A, to a model with factor A and factor B (but no interaction). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with just the overall mean to the model with just factor A:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + \epsilon_{ijk}
\]</span></p>
<p>Notice that the variance associated with <span class="math inline">\(\epsilon_{ijk}\)</span> on the left hand side gets divided up: some will be ‘assigned’ to factor <span class="math inline">\(A\)</span> and some will still be left over in <span class="math inline">\(\epsilon_{ijk}\)</span> on the right hand side.</p>
<p>In other words,</p>
<p><span class="math display">\[
\epsilon_{ijk} \sim N(0,\sigma^2_{\epsilon1}) \longrightarrow \epsilon_{ijk} \sim N(0,\sigma^2_{\epsilon2})
\]</span></p>
<p>where <span class="math inline">\(\sigma^2_{\epsilon_2} &lt; \sigma^2_{\epsilon_1}\)</span> because some of that variation is now explained by factor <span class="math inline">\(A\)</span>. We use that difference in residual variation as a measure of how much variation is ‘taken up’ or ‘explained’ by the factor <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[
\text{SS}_{A} = \text{SSE}(\mu) - \text{SSE}(A)
\]</span></p>
<p>In the above equation, <span class="math inline">\(SS_{A}\)</span> is the sum of squares associated with the factor A. In other words, it is the sum of squares error that factor A “takes credit for”. SSE is the sum-of-squares error, or the residual sum-of-squares variation left over after the model. The model with no factors is comparing each data point to the grand mean <span class="math inline">\(\mu\)</span>, so here <span class="math inline">\(SSE(\mu)\)</span> is just the total sum-of-squares variation. (In other words, with no covariates, <em>all</em> variation is residual.) <span class="math inline">\(SSE(A)\)</span> is the residual sum-of-squares variation with <span class="math inline">\(A\)</span> in the model.</p>
<p>To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor A to the model with factor A and factor B (but no interaction). In other words, we add the factor <span class="math inline">\(B\)</span></p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
\]</span></p>
<p>and calculate the decrease in the residual variation in going from an A-only model to an (A+B) model.</p>
<p><span class="math display">\[
\text{SS}_{B} = \text{SSE}(A) - \text{SSE}(A + B)
\]</span></p>
</div>
<div id="type-ii-hierarchical-sums-of-squares" class="section level3 hasAnchor" number="22.8.3">
<h3><span class="header-section-number">22.8.3</span> Type II (hierarchical) sums of squares<a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\text{SS}(A | B)\)</span> for factor A</p>
<p><span class="math inline">\(\text{SS}(B | A)\)</span> for factor B</p>
<p>With hierarchical sums of squares, we assume no significant interaction. This does not depend on the order that factors are input. This can be done using the <code>Anova()</code> function in the package <code>car</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="Logan_TypeIISS.png" alt="Source: Logan (2010)" width="50%" />
<p class="caption">
Figure 22.4: Source: Logan (2010)
</p>
</div>
<p>The sums of squares for each main effect are calculated by comparing the sums of squares error in a model with the factor of interest to a model without it (including all other terms at the same or lower level). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with factor A to the model without it (notice the missing interaction):</p>
<p><span class="math display">\[
Y_{ijk} = \mu + B_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
\]</span></p>
<p><span class="math display">\[
\text{SS}_{A} = \text{SSE}(B) - \text{SSE}(A + B)
\]</span></p>
<p>To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor B to the model without it:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
\]</span></p>
<p><span class="math display">\[
\text{SS}_{B} = \text{SSE}(A) - \text{SSE}(A + B)
\]</span></p>
</div>
<div id="type-iii-marginal-sums-of-squares" class="section level3 hasAnchor" number="22.8.4">
<h3><span class="header-section-number">22.8.4</span> Type III (marginal) sums of squares<a href="week-12-lecture.html#type-iii-marginal-sums-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\text{SS}(A | B, AB)\)</span> for factor A</p>
<p><span class="math inline">\(\text{SS}(B | A, AB)\)</span> for factor B</p>
<p>With marginal sums of squares, we are estimating the marginal effect of a factor after the effect of the other factors (and interactions) have been taken into account. Back to our earlier example, we would be “controlling for” education, by testing whether men make more money than women in a population in which all educational levels are equally likely, or “controlling” for education by estimating the effect of gender only after the effect of education had already been taken into account. This can be done using the <code>Anova()</code> function in the package <code>car</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="Logan_TypeIIISS.png" alt="Source: Logan (2010)" width="100%" />
<p class="caption">
Figure 22.5: Source: Logan (2010)
</p>
</div>
<p>The sums of squares are estimated in Type III models by comparing the difference in the sums of squares between the full model and the model without the main effect being measured. To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the full model and the model missing factor A:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + B_i + (AB)_{ij} + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}
\]</span></p>
<p><span class="math display">\[
\text{SS}_{A} = \text{SSE}(B + A:B) - \text{SSE}(A + B + A:B)
\]</span></p>
<p>To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the full model and the model missing factor B:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + (AB)_{ij} + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}
\]</span></p>
<p><span class="math display">\[
\text{SS}_{B} = \text{SSE}(A + A:B) - \text{SSE}(A + B + A:B)
\]</span></p>
</div>
<div id="comparing-type-i-ii-and-iii-ss" class="section level3 hasAnchor" number="22.8.5">
<h3><span class="header-section-number">22.8.5</span> Comparing type I, II, and III SS<a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the design is balanced (equal sample sizes in each category), the factors are “orthogonal,” and Types I,II, and III all give equivalent results. When the interaction AB is not significant, then Type II and III SS estimates are equivalent. Aho 10.14 contains more detail.</p>
<p>Note that <span class="math inline">\(\text{SS}_{AB}\)</span> and <span class="math inline">\(\text{SS}_{\text{within}}\)</span> are the same for all three ways of calculating SS. <span class="math inline">\(\text{SS}_{\text{within}}\)</span> is the sum of squared deviations between each fitted data point <span class="math inline">\(\hat{Y}_{ijk} = \bar{Y}_{ij}\)</span> and the overall mean for the full model <span class="math inline">\(Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}\)</span> and <span class="math inline">\(\text{SS}_{AB}\)</span> is calculated as the difference in sums of squares between the full model and the model without the interaction term. There are only differences in estimates of SS for the main effects terms for the three different SS methods because it depends on the way you calculate <strong>marginal</strong> means.</p>
</div>
</div>
<div id="unbalanced-design-missing-cell" class="section level2 hasAnchor" number="22.9">
<h2><span class="header-section-number">22.9</span> Unbalanced design – Missing cell<a href="week-12-lecture.html#unbalanced-design-missing-cell" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When an entire cell (a combination of factors) is missing, it is not possible to test all the main effects and interactions. One solution is to fit a large single factor ANOVA with as many levels as there are cells, and then compare combinations using specific contrasts to tests hypotheses of interest. This is like treating each factor combination like a dummy coded variate. This is called a “cell means model.” This approach is worked out in Logan 12.6.</p>
</div>
<div id="two-factor-nested-anova" class="section level2 hasAnchor" number="22.10">
<h2><span class="header-section-number">22.10</span> Two factor nested ANOVA<a href="week-12-lecture.html#two-factor-nested-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In nested designs, the categories of the nested factor within each level of the main factor are unique. Usually this happens because 1) you have unique organisms within each treatment, or 2) you have unique plots within each treatment. The nested factors are usually random effects (but not always). Nested designs refer to any design in which there is subsampling within the replicates. Nested designs, or hierarchical designs, can have many levels. For example, if you were interested in barnacle diversity, you could have subsamples within replicates, replicates nested within intertidal zones, intertidal zones nested with shores, shores nested within regions, etc.</p>
<p>Let’s imagine we are measuring the amount of glycogen in rat livers. We have <strong>three treatments</strong> that we gave rats. We included <strong>two rats in each treatment</strong>. We took six liver samples from each rat and measured each one of those liver samples once. (We will tackle a slightly more complex nested version in the lab.) There are 36 total measurements.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="RatDesign_Simplified.png" alt="Nested design of the rat experiment." width="100%" />
<p class="caption">
Figure 16.1: Nested design of the rat experiment.
</p>
</div>
<p>What would our model equation look like?</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + B_{j(i)} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2)
\]</span></p>
<p><span class="math inline">\(A_i\)</span> is the effect of treatment, and <span class="math inline">\(B_{j(i)}\)</span> is the nesting factor (rats within treatment). <span class="math inline">\(Y_{ijk}\)</span> is the <span class="math inline">\(k^{th}\)</span> liver sample from rat <span class="math inline">\(j\)</span> nested in treatment <span class="math inline">\(i\)</span>. It would be impossible to have a crossed design for this experiment, for example, rat 1 cannot be in treatment 1, 2, and 3.</p>
<p>Why use a nested structure?</p>
<p>Nested designs are actually quite powerful. Single units in an experimental design may not adequately represent the populations. By only measuring the response in single units in an experiment, this can actually <strong>increases</strong> the unexplained variation in the system, which can mask the true differences.</p>
<p><strong><span style="color: read;">By subsampling within each sample, we get a more precise estimate of the mean response within each sampling unit (think: Central Limit Theorem).</span></strong> If we estimate the response in three subsamples, we can get a more precise estimate of the treatment mean than if we only had one sample per replication. This way, we reduce unexplained variability.</p>
<p>Now we know why subsampling is helpful, but why not just average among the subsamples? A nested analysis allows you to partition the variance into the amount of variation at each level of the hierarchy. Why not just <em>ignore</em> the nestedness of the data? Because by ignoring the nestedness of the data, you would treat all the data as being independent, but because of the nested samping design, this is inappropriate.</p>
<table>
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among groups (factor A)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(a - 1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{among grp.}}}{\text{DOF}_{\text{among grp.}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{\text{among grp.}}}{\text{MS}_{\text{among rep.}}}\)</span></td>
</tr>
<tr class="even">
<td>Among replicates within groups (nesting factor B)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{j(i)} - \bar{Y}_{i})^2\)</span></td>
<td align="center"><span class="math inline">\(a (b - 1)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{among rep.}}}{\text{DOF}_{\text{among rep.}}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{\text{among rep.}}}{\text{MS}_{\text{residual}}}\)</span></td>
</tr>
<tr class="odd">
<td>Subsamples within replicates (residual)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y}_{j(i)})^2\)</span></td>
<td align="center"><span class="math inline">\(ab (n - 1)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{residual}}}{\text{DOF}_{\text{residual}}}\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Total</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(abn - 1\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>In the above table, the notation is as follows:</p>
<p><span class="math inline">\(\bar{Y}\)</span> is the grand mean (i.e. the average across all 36 measurements)</p>
<p><span class="math inline">\(\bar{Y}_{i}\)</span> is the mean within each group. In the rat example, this is the mean for each treatment.</p>
<p><span class="math inline">\(\bar{Y}_{j(i)}\)</span> = is the mean within of the subsets within that group. In the rat example, this would be the mean within each rat.</p>
<p>Notice that there are 6 measurements for each rat and there is variation <em>within</em> these measurements. Given this experimental set-up, this is the residual variation. The mean squared error at this bottom level of the model is <span class="math inline">\(\text{MS}_{residual}\)</span>. <strong>Large residual variation is going to make it difficult to say that variation between groups higher in the hierarchy are actually statistically significant.</strong></p>
<p>To test the null hypothesis that factor A has no effect (<span class="math inline">\(H_0(A): A_i = 0\)</span>), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: <span class="math inline">\(P ( X \ge F^*), X \sim F_{[\text{DOF}_{\text{among grp.}}, \text{DOF}_{\text{among rep.}}]}\)</span>. To test the null hypothesis that nesting factor B has no effect (<span class="math inline">\(H_0(B): B_j = 0\)</span>), <span class="math inline">\(P ( X \ge F^*), X \sim F_{[\text{DOF}_{\text{among rep.}}, \text{DOF}_{\text{residual}}]}\)</span>. Again, there is no interaction term with nested designs.</p>
<p>Finally, nested design arise frequently in a spatial context, where you might have multiple plots within fields receiving different treatments. The most common scenario would look like this:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-19"></span>
<img src="NestedANOVA_WithSubsampling.png" alt="Nested ANOVA with sub-sampling" width="60%" />
<p class="caption">
Figure 22.6: Nested ANOVA with sub-sampling
</p>
</div>
<div id="potential-issues-with-nested-designs" class="section level3 hasAnchor" number="22.10.1">
<h3><span class="header-section-number">22.10.1</span> Potential issues with nested designs<a href="week-12-lecture.html#potential-issues-with-nested-designs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nested designs are often mis-analyzed, because people often incorrectly treat subsamples as independent replicates. This ignores correlation among subsamples and artificially boosts sample size. As a result, the chance of Type I error increases dramatically.</p>
<p>Nested designs are difficult if not impossible to analyze correctly if sample sizes are not the same in each group.</p>
<p>The power of ANOVA designs is more sensitive to the number of independent replicates than it is to the precision with which you can estimate the mean of each replicate, which means that you should never divert resources away from more independent replicates in favor of subsampling the replicates you have.</p>
<p><strong>All of these potential issues can be resolved with careful experimental design.</strong></p>
</div>
</div>
<div id="experimental-design" class="section level2 hasAnchor" number="22.11">
<h2><span class="header-section-number">22.11</span> Experimental design<a href="week-12-lecture.html#experimental-design" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Experimental design is a very important part of statistics that deals with the structure of an experiment or sampling scheme (experimental design is still important even with observational studies). Proper experimental design is essential to correctly be able to make inferences. More on this topic in Aho Ch. 7 and the Hurlbert (1989) paper.</p>
<div id="completely-randomized-design" class="section level3 hasAnchor" number="22.11.1">
<h3><span class="header-section-number">22.11.1</span> Completely randomized design<a href="week-12-lecture.html#completely-randomized-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We didn’t say so explicitly, but when we discussed single-factor ANOVA before we assumed a <strong>completely randomized design</strong>. In a completely randomized design, treatments are assigned at random to experimental units (plots, organisms, patients). A negative of completely randomized design is that if there is an environmental gradient, or “noise,” then the completely randomized design will have low power. This is because differences among treatments will be swamped by background “noise” not accounted for in the model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="EnvGradient.png" alt="Environmental gradient" width="50%" />
<p class="caption">
Figure 22.7: Environmental gradient
</p>
</div>
</div>
<div id="randomized-block-design" class="section level3 hasAnchor" number="22.11.2">
<h3><span class="header-section-number">22.11.2</span> Randomized block design<a href="week-12-lecture.html#randomized-block-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A “block” is a unit of space or time within which conditions are considered to be relatively homogeneous. Blocks may be placed randomly or systematically, but they should be arranged so that <strong>environmental conditions are more are more similar within blocks than between them</strong>.Within each block, treatments are assigned randomly. Each treatment is assigned once per block. Blocks should be placed far enough apart that blocks can be considered independent.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="RandomizedBlockDesign.png" alt="Randomized block design" width="50%" />
<p class="caption">
Figure 22.8: Randomized block design
</p>
</div>
<p>Randomized block designs <strong>can account for some of the background heterogeneity that completely randomized designs miss</strong>. When environmental heterogeneity is present, the randomized block design is more efficient than a completely randomized layout and will need fewer replicates for the same statistical power. A negative of randomized block design is that we assume no interaction between the block and the treatments (in other words, it assumes that the ranking of treatment responses will be the same in each block).</p>
<p>Randomized block design models are very similar to two-way factorial designs, but <strong>without an interaction term</strong>. <span class="math inline">\(Y_{ijk}\)</span> is the <span class="math inline">\(k^{\text{th}}\)</span> observation from the <span class="math inline">\(j^{\text{th}}\)</span> level in blocking factor B and the <span class="math inline">\(i^{\text{th}}\)</span> level in factor A (a fixed effect).</p>
<p><span class="math display">\[
Y_{ijk} = \mu + A_i + B_j +\epsilon_{ijk}, \text{where } \epsilon_{ijk} \sim \mathrm{N} (0, \sigma^2)
\]</span></p>
<p>Our null hypotheses are the effect of treatment factor A is zero (<span class="math inline">\(H_0(A): \text{all } A_i = 0\)</span>), and the effect of blocking factor B is zero (<span class="math inline">\(H_0(B): \text{all } B_i = 0\)</span>). If we reject the null hypothesis that block effects are significant, we know that using blocks as replicates was effective in addressing the homogeneity among replicates. We fully expect that the block effect will be significant, since this was our rationale for using a randomized block design!</p>
<table>
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th align="center">SS</th>
<th align="center">DOF</th>
<th align="center">MS</th>
<th align="center">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Among treatments (factor A)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} (\bar{Y}_{i} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(a - 1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{A}}{\text{DOF}_{A}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}\)</span></td>
</tr>
<tr class="even">
<td>Among blocks (factor B)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} (\bar{Y}_{j} - \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\(b - 1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{B}}{\text{DOF}_{B}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{MS}_{B}}{\text{MS}_{\text{within}}}\)</span></td>
</tr>
<tr class="odd">
<td>Within groups (residual)</td>
<td align="center"><span class="math inline">\(\sum^a_{i = 1} \sum^b_{j = 1} (Y_{ij} - \bar{Y}_{j} - \bar{Y}_{i} + \bar{Y})^2\)</span></td>
<td align="center"><span class="math inline">\((b - 1)(a - 1)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\text{SS}_{\text{within}}}{\text{DOF}_{\text{within}}}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>There is no <span class="math inline">\(\sum^n_{k = 1}\)</span> because there is only one replicate of each treatment in each block. Also notice the similarity of the residual SS to the interaction mean squares in other designs.</p>
</div>
<div id="latin-square-design" class="section level3 hasAnchor" number="22.11.3">
<h3><span class="header-section-number">22.11.3</span> Latin square design<a href="week-12-lecture.html#latin-square-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A Latin square design is a special case of a randomized block design for cases where environmental heterogeneity may occur along two dimensions (east-west and north-south for example). In this case you want each treatment to occur exactly once in each row and in each column (like Sudoku).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="LatinSquare.png" alt="Latin square design. Source: Wikipedia" width="40%" />
<p class="caption">
Figure 22.9: Latin square design. Source: Wikipedia
</p>
</div>
</div>
<div id="split-plot-design" class="section level3 hasAnchor" number="22.11.4">
<h3><span class="header-section-number">22.11.4</span> Split plot design<a href="week-12-lecture.html#split-plot-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With a split plot design, we have an experimental design that exists at two scales: a large unit (which has a whole plot treatment assigned), and smaller units within the large unit (which have split plot treatments). This is a hierarchical design in that the large unit treatments are pseudoreplicates, but the split plots have informative factor levels that mean exactly the same thing in other plots (unlike rat individuals in nested designs). For example, the large unit could be “pond,” to which you apply different nutrient addition treatments, and the smaller unit could be “predation treatment” where you multiple different cage setups within each pond. The predation treatments mean the same thing in the other ponds.</p>
<p>Often one factor is applied at the block level because of logistical constraints. One example might be an experiment to test insecticide application vs. grass seed type. If insecticides are being applied by airplane, then it is easier to apply one insecticide treatment to a whole field instead of just to a portion of field, in which case insecticide might be applied at the field level but grass variety might vary among subplots within the larger field (“block”).</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-11-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-12-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
