<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>18 Week 10 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="18 Week 10 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="18 Week 10 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="18 Week 10 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-9-lab.html"/>
<link rel="next" href="week-10-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-10-lecture" class="section level1 hasAnchor" number="18">
<h1><span class="header-section-number">18</span> Week 10 Lecture<a href="week-10-lecture.html#week-10-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-10-readings" class="section level2 hasAnchor" number="18.1">
<h2><span class="header-section-number">18.1</span> Week 10 Readings<a href="week-10-lecture.html#week-10-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Section 9.20, as well as Logan Chapters 9 and 16-17. You will also need to read <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Siddhartha_etal_1989.pdf">Siddhartha et al. (1989)</a>. This paper is another one that is quite important and should be read carefully. We will also discuss it at length in class. There are two other papers some of you will find worthwhile as well, <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/OHara_etal_2010.pdf">this paper on why not to log-transform count data</a> (please, don’t do it, it you have count data and don’t know what to do, ask me!) and <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Warton_etal_2016.pdf">another paper laying out when to use GLM and when to use OLS regression</a>.</p>
</div>
<div id="week-10-outline" class="section level2 hasAnchor" number="18.2">
<h2><span class="header-section-number">18.2</span> Week 10 outline<a href="week-10-lecture.html#week-10-outline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This week we will cover my favorite topic of all: Generalized Linear Regression and Multiple Regression!</p>
<p>An outline for this week’s materials:</p>
<ol style="list-style-type: decimal">
<li><p>Basic idea behind GLM</p></li>
<li><p>Logistic regression</p></li>
<li><p>Poisson regression</p></li>
<li><p>Deviance</p></li>
<li><p>LOESS/spline smoothing</p></li>
<li><p>Generalized additive models</p></li>
<li><p>Multiple regression</p></li>
</ol>
</div>
<div id="an-example" class="section level2 hasAnchor" number="18.3">
<h2><span class="header-section-number">18.3</span> An example<a href="week-10-lecture.html#an-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a model of the presence or absence of a wood-boring beetle as a function of the wood density of decaying aspen trees in Quebec. The data look like this:</p>
<pre><code>## Loading required package: tcltk</code></pre>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
<p>So, let’s do what we know, and we’ll fit a linear regression to the data using <code>lm()</code> (like we did last week). Then we’ll plot the model fit.</p>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="week-10-lecture.html#cb611-1" tabindex="-1"></a>beetle.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> ANAT <span class="sc">~</span> Wood.density, <span class="at">data =</span> beetle)</span>
<span id="cb611-2"><a href="week-10-lecture.html#cb611-2" tabindex="-1"></a>new.predictor <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">Wood.density =</span> <span class="fu">seq</span>(<span class="fu">min</span>(beetle<span class="sc">$</span>Wood.density), <span class="fu">max</span>(beetle<span class="sc">$</span>Wood.density), <span class="at">length.out =</span> <span class="dv">24</span>))</span>
<span id="cb611-3"><a href="week-10-lecture.html#cb611-3" tabindex="-1"></a>beetle.vals <span class="ot">&lt;-</span> <span class="fu">predict</span>(beetle.fit, <span class="at">newdata =</span> new.predictor)</span>
<span id="cb611-4"><a href="week-10-lecture.html#cb611-4" tabindex="-1"></a>beetle.predict <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">WoodDensity =</span> new.predictor[[<span class="dv">1</span>]], <span class="at">Presence =</span> beetle.vals)</span>
<span id="cb611-5"><a href="week-10-lecture.html#cb611-5" tabindex="-1"></a></span>
<span id="cb611-6"><a href="week-10-lecture.html#cb611-6" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> beetle, <span class="fu">aes</span>(<span class="at">x =</span> Wood.density, <span class="at">y =</span> ANAT)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray55&quot;</span>) <span class="sc">+</span> </span>
<span id="cb611-7"><a href="week-10-lecture.html#cb611-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> beetle.predict, <span class="fu">aes</span>(<span class="at">x =</span> WoodDensity, <span class="at">y =</span> Presence)) <span class="sc">+</span></span>
<span id="cb611-8"><a href="week-10-lecture.html#cb611-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Wood density (g cm&quot;</span> <span class="sc">^</span> <span class="st">&quot;-3&quot;</span>, <span class="st">&quot;)&quot;</span>)), <span class="at">y =</span> <span class="st">&quot;Presence of wood-boring beetle&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb611-9"><a href="week-10-lecture.html#cb611-9" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>))</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
<p>How well does this model fit, and how well does it represent the data? Let’s predict beetle presence for woody density of 0.1 g/cm<sup>3</sup> and then 0.4 g/cm<sup>3</sup>.</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="week-10-lecture.html#cb612-1" tabindex="-1"></a><span class="fu">unname</span>(beetle.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> beetle.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>## [1] 1.622327</code></pre>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="week-10-lecture.html#cb614-1" tabindex="-1"></a><span class="fu">unname</span>(beetle.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> beetle.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.4</span>)</span></code></pre></div>
<pre><code>## [1] -0.03375074</code></pre>
<p>Last, we’ll look at the residuals for our model fit.</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="week-10-lecture.html#cb616-1" tabindex="-1"></a>beetle.resid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">PredictedValues =</span> <span class="fu">predict</span>(beetle.fit), <span class="at">Residuals =</span> <span class="fu">residuals</span>(beetle.fit))</span>
<span id="cb616-2"><a href="week-10-lecture.html#cb616-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> beetle.resid, <span class="fu">aes</span>(<span class="at">x =</span> PredictedValues, <span class="at">y =</span> Residuals)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray55&quot;</span>) <span class="sc">+</span></span>
<span id="cb616-3"><a href="week-10-lecture.html#cb616-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Predicted values from linear regression&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Residuals&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb616-4"><a href="week-10-lecture.html#cb616-4" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
</div>
<div id="generalized-linear-models" class="section level2 hasAnchor" number="18.4">
<h2><span class="header-section-number">18.4</span> Generalized linear models<a href="week-10-lecture.html#generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The regression models that we have been introduced to thus far all assume two things:</p>
<ol style="list-style-type: decimal">
<li><p>A <strong>linear</strong> relationship between the independent variable and the dependent variables</p></li>
<li><p>Normally distributed errors</p></li>
</ol>
<p>Linear regression can be thought of as</p>
<p><span class="math display">\[
Y_i \sim \mathrm{N}(\beta_0 + \beta_1 X_i, \sigma^2)
\]</span></p>
<p>or, equivalently,</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \text{, where } \epsilon_i \sim \mathrm{N}(0, \sigma^2)
\]</span>
Generalized linear models are extensions of what we have been talking about that free us from these two constraints. We can think of the above case as having two parts, the first is a model for the mean (or the expected value <span class="math inline">\(E[Y_{i}]\)</span>).</p>
<p><span class="math display">\[
\mathrm{E}[Y_i] = \mu_i = \beta_0 + \beta_1 X_i
\]</span>
and the second being a model for the variance</p>
<p><span class="math display">\[
\epsilon_i \sim \mathrm{N}(0, \sigma^2)
\]</span></p>
<p>The Normal distribution is “special”. The mean (<span class="math inline">\(\mathrm{E}(X)\)</span>) and the variance (<span class="math inline">\(\mathrm{Var}(X)\)</span>) of the distribution are directly related to the two parameters of the distribution (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, respectively). This means the mean and variance are decoupled.</p>
<p>For other distributions, the parameters of the distribution often do not map uniquely to the mean and variance. GLMs allow us to model the response as some distribution (no longer has to be a Normal), and to model the parameters of that distribution (or a function of those parameters) using a linear model. In other words,</p>
<p><span style="color: orangered;">
<span class="math display">\[
\text{Response} \sim \text{Distribution}(\text{parameters})
\]</span></p>
<p><span class="math display">\[
g(\text{parameters}) = \beta_0 + \beta_1 X_i
\]</span>
</span></p>
<p><span class="math inline">\(g()\)</span> is a function of the parameters. For most distributions other than the Normal, the mean and variance are linked.</p>
<p><span class="math display">\[
\epsilon_i \sim \text{Variance function}(\text{usually determined by the distribution for the mean})
\]</span></p>
<p>The function that relates the parameters of the distribution to the parameters of the linear model is called a <strong>link function</strong> (here, <span class="math inline">\(g()\)</span>). Not all functions are acceptable, there are a few frequently used functions that serve this role (depends on the distribution of our data).</p>
<p>Traditional linear regression may fail to represent our data (predicting nonsensical data, etc.) in many cases in ecology and evolution. Think about the error structure you’d expect with each of these types of data:</p>
<ul>
<li><p>Count data</p></li>
<li><p>Binary data</p></li>
<li><p>Proportion data</p></li>
</ul>
<p>In this lecture we will go through logistic regression for binomial data and Poisson regression for Poisson (count) data. Keep in mind that other GLMs exist, specifically beta regression and gamma regression, among others.</p>
<p>The order of complexity (and flexibility) of the models we will be discussing is:</p>
<p>Standard linear regression <span class="math inline">\(&lt;\)</span> Generalized linear models (GLMs) <span class="math inline">\(&lt;\)</span> Generalized Additive Models (GAMs).</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="18.5">
<h2><span class="header-section-number">18.5</span> Logistic regression<a href="week-10-lecture.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression is used to model Bernoulli or binomial response variables. Bernoulli data includes:</p>
<ul>
<li><p>Survival for individual organisms</p></li>
<li><p>Presence/absence for individual organisms</p></li>
<li><p>Allele or phenotype of type 1 or type 2 for an individual</p></li>
</ul>
<p>The underlying equation is</p>
<p><span class="math display">\[
Y_i \sim \mathrm{Bernoulli}(p_i)
\]</span>
The response is either 0 or 1, with probability <span class="math inline">\(p\)</span>. For Bernoulli data (or binomial), the link function <span class="math inline">\(g()\)</span> is</p>
<p><span class="math display">\[
\log \left( \frac{p_i}{1 - p_i} \right) = \log \left( &#39;&#39;odds&#39;&#39; \right) = \mathrm{logit}(p_i)
\]</span></p>
<p>This function is known as the <strong>logit function</strong> and also represents the log odds of a response. By using the logit function as the link function for a logistic regression, we are saying that the logit function maps the linear model (<span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>) to the parameter(s) of the Bernoulli (or binomial). Why use the logit function? We want to map values in the range <span class="math inline">\([0, 1]\)</span> to <span class="math inline">\([- \infty, \infty]\)</span>.</p>
<p>The variance function describes the distribution of the error in the GLM:</p>
<p><span class="math display">\[
\epsilon_i \sim \text{Variance function}
\]</span></p>
<p>Roughly, a variance function relates the mean of the distribution to the variance (see Aho 9.20.7.3 for more details). For example, the variance function of the normal distribution is 1, <span class="math inline">\(\mathrm{V}[\theta] = 1\)</span> (describing constant variance, i.e., homoscedasticity). <em>Note that <span class="math inline">\(\mathrm{V}[\theta]\)</span> is not equivalent to <span class="math inline">\(\mathrm{Var}[\theta]\)</span>, though they are related (see Appendix A.6 in Aho for details).</em> The variance of <span class="math inline">\(Y_{i}\)</span> for the Bernoulli, where <span class="math inline">\(n_i=1\)</span>is given by:</p>
<p><span class="math display">\[
\mathrm{Var}(Y_i) = p_i (1 - p_i)
\]</span></p>
<p>While for a normal linear regression we need errors to be the same (homoscedasctic), with a logistic regression, variation is largest around <span class="math inline">\(p = 0.5\)</span> and shrinks to zero at the end points of <span class="math inline">\(p=0\)</span> and <span class="math inline">\(p=1\)</span>.</p>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="week-10-lecture.html#cb617-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb617-2"><a href="week-10-lecture.html#cb617-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> p, <span class="at">y =</span> p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>The complete model for a logistic regression with Bernouilli data <span class="math inline">\(Y_i\)</span> and covariate <span class="math inline">\(X_i\)</span> is:</p>
<p><span class="math display">\[
Y_i \sim \mathrm{Bernoulli}(p_i) \text{, where } \log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
\]</span></p>
<p>Keep in mind that for Bernoulli data, each data point is either a 0 or a 1 (a “success” or a “fail”). The probability <span class="math inline">\(p_{i}\)</span> for each data point <span class="math inline">\(Y_{i}\)</span> is a continuous variable from [0,1]. The value of the covariate <span class="math inline">\(X_{i}\)</span> determines the probability <span class="math inline">\(p_{i}\)</span>, which in turn “weights the coin” and controls the probability that the outcome <span class="math inline">\(Y_{i}\)</span> will be 0 or 1.</p>
<p>The binomial is closely related to a Bernoulli. Whereas the Bernoulli represents a single “coin flip”, the bimomial represents a collection of coin flips. To model a binomial response variable:</p>
<p><span class="math display">\[
Y_i \sim \mathrm{Binomial}(n_i, p_i) \text{, where } \log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
\]</span></p>
<p>Now, the variance of <span class="math inline">\(Y_i\)</span> is:</p>
<p><span class="math display">\[
\mathrm{Var}(Y_i) = n_i p_i (1 - p_i)
\]</span></p>
<p>Some examples of data that are best modelled as either Bernoulli or Binomial are:</p>
<p>Evidence for joint damage in a rocket (this is a dataset we will tackle in the lab):</p>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="week-10-lecture.html#cb618-1" tabindex="-1"></a>Challenger_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;_data/Challenger_data.csv&quot;</span>)</span>
<span id="cb618-2"><a href="week-10-lecture.html#cb618-2" tabindex="-1"></a><span class="fu">plot</span>(Challenger_data<span class="sc">$</span>Temp,Challenger_data<span class="sc">$</span>O.ring.failure,<span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">0</span>,<span class="fl">0.55</span>,<span class="fl">0.4</span>,<span class="fl">0.6</span>),<span class="at">xlab=</span><span class="st">&quot;Temperature&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Presence of at least one failed O-ring&quot;</span>)</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>or the presence/absence of a species (in this case, a newt) plotted as a function of habitat suitability (data from <a href="https://www.dataanalytics.org.uk/publications/statistics-book-for-ecologists/support/data/">this website</a>):</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="week-10-lecture.html#cb619-1" tabindex="-1"></a>Newt_HSI <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;_data/Newt HSI.csv&quot;</span>)</span>
<span id="cb619-2"><a href="week-10-lecture.html#cb619-2" tabindex="-1"></a><span class="fu">plot</span>(Newt_HSI<span class="sc">$</span>HSI,Newt_HSI<span class="sc">$</span>presence,<span class="at">pch=</span><span class="dv">16</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">0</span>,<span class="fl">0.55</span>,<span class="fl">0.4</span>,<span class="fl">0.3</span>),<span class="at">xlab=</span><span class="st">&quot;Habitat Suitability Index&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Species presence (1)/Absence (0)&quot;</span>)</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>By modeling our data as it exists, we have solved four problems</p>
<ol style="list-style-type: decimal">
<li><p>The errors now reflect the binomial process</p></li>
<li><p>The variance reflects the binomial process</p></li>
<li><p>The response <span class="math inline">\(Y_i\)</span> is now bounded <span class="math inline">\([0, 1]\)</span></p></li>
<li><p>We have maintained the information on sample size for each data point (only relevant when data are binomial)</p></li>
</ol>
<p>An example with the beetle dataset (To be worked out on your own)</p>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="week-10-lecture.html#cb620-1" tabindex="-1"></a>beetle.glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> ANAT <span class="sc">~</span> Wood.density, <span class="at">data =</span> beetle, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb620-2"><a href="week-10-lecture.html#cb620-2" tabindex="-1"></a>beetle.glm.vals <span class="ot">&lt;-</span> <span class="fu">predict</span>(beetle.glm.fit, <span class="at">newdata =</span> new.predictor, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb620-3"><a href="week-10-lecture.html#cb620-3" tabindex="-1"></a>beetle.glm.predict <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">WoodDensity =</span> new.predictor[[<span class="dv">1</span>]], <span class="at">Presence =</span> beetle.glm.vals)</span>
<span id="cb620-4"><a href="week-10-lecture.html#cb620-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> beetle, <span class="fu">aes</span>(<span class="at">x =</span> Wood.density, <span class="at">y =</span> ANAT)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray55&quot;</span>) <span class="sc">+</span> </span>
<span id="cb620-5"><a href="week-10-lecture.html#cb620-5" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> beetle.glm.predict, <span class="fu">aes</span>(<span class="at">x =</span> WoodDensity, <span class="at">y =</span> Presence)) <span class="sc">+</span></span>
<span id="cb620-6"><a href="week-10-lecture.html#cb620-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Wood density (g cm&quot;</span> <span class="sc">^</span> <span class="st">&quot;-3&quot;</span>, <span class="st">&quot;)&quot;</span>)), <span class="at">y =</span> <span class="st">&quot;Presence of wood-boring beetle&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb620-7"><a href="week-10-lecture.html#cb620-7" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-9-1.png" width="480" /></p>
<p><strong>Question: How do we interpret this GLM?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
With increasing wood density, the probability of finding wood-boring beetles decreases.
</span>
</details>
<p>And let’s see our predictions of the response again:</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="week-10-lecture.html#cb621-1" tabindex="-1"></a><span class="fu">predict</span>(beetle.glm.fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">Wood.density =</span> <span class="fl">0.1</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.9999694</code></pre>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="week-10-lecture.html#cb623-1" tabindex="-1"></a><span class="fu">predict</span>(beetle.glm.fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">Wood.density =</span> <span class="fl">0.4</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##           1 
## 0.004522846</code></pre>
<p>The predictions are <span class="math inline">\(\hat{Y}_i\)</span>, which is equal to the expected value of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(X_i\)</span> (<span class="math inline">\(\mathrm{E}(Y_i)\)</span>). With a normally distributed regression, <span class="math inline">\(\hat{Y}_i = \mu_i = \beta_0 + \beta_1 X_i\)</span>. With GLMs, we have to consider the link function. Now, <span class="math inline">\(\mathrm{E}(Y_i) = p_i = \hat{Y}_i\)</span>. Thus, <span class="math inline">\(\hat{Y}_i\)</span> are probabilities in logistic regression. This is why <code>predict()</code> doesn’t give us 0’s and 1’s.</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="week-10-lecture.html#cb625-1" tabindex="-1"></a><span class="fu">predict</span>(beetle.glm.fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">Wood.density =</span> <span class="fl">0.1</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.9999694</code></pre>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="week-10-lecture.html#cb627-1" tabindex="-1"></a><span class="fu">predict</span>(beetle.glm.fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">Wood.density =</span> <span class="fl">0.4</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##           1 
## 0.004522846</code></pre>
<p>The argument <code>type = "response"</code> in <code>predict()</code> back-transforms the predicted response using the inverse logit function.</p>
<p>In general, the outputs (<span class="math inline">\(\hat{Y}_i\)</span>) from logistic regression are in logit units because of the link function we applied, <span class="math inline">\(\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i\)</span>. This also means that the relationship between the link function and the predictor, expressed with the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is linear on the scale of the logit probability. We can transform the logit output to units of odds (using the exponential function) or probability (using the inverse logit function).</p>
<p>If the probability of an event is 0.3:</p>
<p>The odds of the event occurring: <span class="math inline">\(\text{odds} = \frac{0.3}{0.7} = 0.43\)</span></p>
<p>The log odds (logit) of the event occurring: <span class="math inline">\(\text{log odds} = \ln \frac{0.3}{0.7} = -0.85\)</span></p>
<p>The probability of the event (which we already knew) can be obtained using the logit using the inverse logit: <span class="math inline">\(\frac{\exp(\text{log odds})}{1 + \exp(\text{log odds})}\)</span></p>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-12-1.png" width="384" /></p>
<p>Adapted from <a href="http://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf">this website</a>.</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="week-10-lecture.html#cb629-1" tabindex="-1"></a><span class="fu">summary</span>(beetle.glm.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = ANAT ~ Wood.density, family = &quot;binomial&quot;, data = beetle)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5141  -0.1748   0.1050   0.4717   2.0057  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)    15.659      6.856   2.284   0.0224 *
## Wood.density  -52.632     23.838  -2.208   0.0272 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 31.755  on 23  degrees of freedom
## Residual deviance: 14.338  on 22  degrees of freedom
## AIC: 18.338
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>The relationship between the link function and the predictor, <span class="math inline">\(\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i\)</span>, is <strong>linear</strong> on the scale of <strong>logit units</strong> (see Aho example 9.28).</p>
<p>If the logistic model is fit without an intercept, the best fit line predicts <span class="math inline">\(p = 0.5\)</span> when <span class="math inline">\(X = 0\)</span>. The intercept <span class="math inline">\(\hat{\beta_0}\)</span> shifts the probability for <span class="math inline">\(X = 0\)</span> up or down relative to 0.5. In logit units, the probability is shifted up (approximating 1) when <span class="math inline">\(X = 0\)</span>.</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="week-10-lecture.html#cb631-1" tabindex="-1"></a>beetle.glm.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>]  <span class="co"># logit units</span></span></code></pre></div>
<pre><code>## (Intercept) 
##     15.6586</code></pre>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="week-10-lecture.html#cb633-1" tabindex="-1"></a>inv.logit <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb633-2"><a href="week-10-lecture.html#cb633-2" tabindex="-1"></a>  <span class="fu">exp</span>(x) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(x))</span>
<span id="cb633-3"><a href="week-10-lecture.html#cb633-3" tabindex="-1"></a>}</span>
<span id="cb633-4"><a href="week-10-lecture.html#cb633-4" tabindex="-1"></a><span class="fu">inv.logit</span>(beetle.glm.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>])  <span class="co"># units of probability (but no longer linear)</span></span></code></pre></div>
<pre><code>## (Intercept) 
##   0.9999998</code></pre>
<p>Since the parameters are linear on the scale of logit units, the logit (log odds) of beetle presence decreases by 52.6 given a 1 g/cm<sup>3</sup> increase in wood density (decreases to almost 0 probability).</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="week-10-lecture.html#cb635-1" tabindex="-1"></a>beetle.glm.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>]  <span class="co"># logit units</span></span></code></pre></div>
<pre><code>## Wood.density 
##     -52.6317</code></pre>
<p>Be aware that the significance of coefficients fit for GLMs is done using a Wald test, which relies on asymptotic (large-sample) estimates of standard errors of the coefficients. The Wald test uses a test statistic, <span class="math inline">\(z\)</span>, that is the parameter estimate divided by the estimated standard error of the parameter estimate, which is asymptotically standard normal under <span class="math inline">\(H_0 = 0\)</span>.</p>
</div>
<div id="fitting-a-glm" class="section level2 hasAnchor" number="18.6">
<h2><span class="header-section-number">18.6</span> Fitting a GLM<a href="week-10-lecture.html#fitting-a-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To fit GLMs, we need another method besides ordinary least squares (which assumes data are normally distributed). We need to fit the parameters using maximum likelihood.</p>
<p>We’ll use binomial data as an example. The likelihood for binomial data, where we have <span class="math inline">\(N\)</span> data points <span class="math inline">\(Y = \{ Y_1, Y_2, ...,Y_N \}\)</span>, each a draw from <span class="math inline">\(\mathrm{Binomial}(n_i, p_i)\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}(Y_i | p_i) = \prod^N_{i = 1} \frac{n_i !}{Y_i! (n_i - Y_i)!} p_i^{Y_i} (1 - p_i)^{n_i - Y_i}
\]</span>
Note that I’ll use <span class="math inline">\(\mathcal{L}\)</span> to represent likelihood and <span class="math inline">\(\ell\)</span> to represent log-likelihood (as in Aho).</p>
<p>We can express <span class="math inline">\(p_i\)</span> as a function of the model parameters by rearranging the model equation:</p>
<p><span class="math display">\[
\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
\]</span></p>
<p><span class="math display">\[
p_i = \frac{\exp(\beta_0 + \beta_1 X_i)}{1 + \exp (\beta_0 + \beta_1 X_i)}
\]</span>
So, we can substitute this expression for <span class="math inline">\(p_i\)</span> into the likelihood (after converting to negative log-likelihood), then we get the likelihood as a function of the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\text{NLL}(\beta_0, \beta_1 | X_i)\)</span>. We can solve for the maximum likelihood estimate (MLE) for <span class="math inline">\(\hat{\beta_0}\)</span>:</p>
<p><span class="math display">\[
\frac{\partial \text{NLL}}{\partial \beta_0} = 0 \rightarrow \hat{\beta_{0}}
\]</span>
<span class="math display">\[
\frac{\partial \text{NLL}}{\partial \beta_1} = 0 \rightarrow \hat{\beta_{1}}
\]</span></p>
<p>These equations cannot be solved algebraically. They must be solved numerically, for example, using iteratively weighted least squares with optimizing algorithms. When you think about the computing power needed to fit GLMs (which is no big deal today), it makes sense why it was traditional to transform your data to become normally distributed rather than model your data as it exists using GLM.</p>
<p><strong>How do we interpret the coefficients?</strong></p>
<p>If we fit a logistic model without an intercept, the best fit line must predict p=0.5 when X=0. So the intercept in this case shifts the probability for X=0 up or down relative to 0.5, and it does so by shifting the entire curve left and right along the x axis. This is illustrated in the figure that follows, where I have added “some amount <span class="math inline">\(i\)</span> to the intercept for <span class="math inline">\(i=1\)</span> to <span class="math inline">\(i=12\)</span> and plotted the colors using the rainbow color set.</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="week-10-lecture.html#cb637-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">typ=</span><span class="st">&quot;l&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;X value&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Logistic probability&quot;</span>)</span>
<span id="cb637-2"><a href="week-10-lecture.html#cb637-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)</span>
<span id="cb637-3"><a href="week-10-lecture.html#cb637-3" tabindex="-1"></a>{</span>
<span id="cb637-4"><a href="week-10-lecture.html#cb637-4" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span>i<span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span>i<span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))),<span class="at">col=</span><span class="fu">rainbow</span>(<span class="dv">15</span>)[i])</span>
<span id="cb637-5"><a href="week-10-lecture.html#cb637-5" tabindex="-1"></a>}</span>
<span id="cb637-6"><a href="week-10-lecture.html#cb637-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb637-7"><a href="week-10-lecture.html#cb637-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span>)),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb637-8"><a href="week-10-lecture.html#cb637-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">5</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">5</span>)),<span class="at">col=</span><span class="fu">rainbow</span>(<span class="dv">15</span>)[<span class="dv">5</span>],<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Note how the change of intercept from <span class="math inline">\(\beta_{0}=-3\)</span> to <span class="math inline">\(\beta_{0}=5\)</span> shifts the distribution to the left and as a result increases the value where the curve crosses the y-axis. (If the slope were negative, a shift to the left would have the opposite effect.)</p>
<p>The “slope” (<span class="math inline">\(\widehat{\beta_{1}}\)</span>)̂ can be interpreted as follows:</p>
<p><span class="math display">\[
e^{\widehat{\beta_{1}}}=\mbox{=&quot;change in the odds for a 1 unit change in X&quot;}
\]</span>
Another way to think about the slope term in logistic regression is that is controls how steeply the curve shifts between 0 and 1. Consider the following figure, where I reduce the slope from <span class="math inline">\(\beta_{1}=1\)</span> to <span class="math inline">\(\beta_{1}=0.077\)</span>, Notice how the lines get flatter as the slope decreases (but without changing the intercept).</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="week-10-lecture.html#cb638-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span><span class="dv">1</span><span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">typ=</span><span class="st">&quot;l&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;X value&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Logistic probability&quot;</span>)</span>
<span id="cb638-2"><a href="week-10-lecture.html#cb638-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)</span>
<span id="cb638-3"><a href="week-10-lecture.html#cb638-3" tabindex="-1"></a>{</span>
<span id="cb638-4"><a href="week-10-lecture.html#cb638-4" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>),<span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span>(<span class="dv">1</span><span class="sc">-</span>(i<span class="sc">/</span><span class="dv">13</span>))<span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">+</span>(<span class="dv">1</span><span class="sc">-</span>(i<span class="sc">/</span><span class="dv">13</span>))<span class="sc">*</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))),<span class="at">col=</span><span class="fu">rainbow</span>(<span class="dv">15</span>)[i])</span>
<span id="cb638-5"><a href="week-10-lecture.html#cb638-5" tabindex="-1"></a>}</span>
<span id="cb638-6"><a href="week-10-lecture.html#cb638-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p><strong>Important note</strong> Note that the sampling variability is fully prescribed by the model used to describe the response</p>
<p><span class="math display">\[
Y_i \sim Binomial(n_i,p_i)
\]</span></p>
<p>In other words, by using a logistic model, we have implicitly stated that the variability in the response that is due to sampling variation (‘residual error’) is entirely driven by the variation that would be expected by a Binomial distribution. We do not have a separate “error model”. We have one model that contains information both on the expected (mean) value of the response for each value of the covariate, and on the amount of random noise in that response that we would expect from one data point to the next (for the same value of the covariate).</p>
</div>
<div id="poisson-regression" class="section level2 hasAnchor" number="18.7">
<h2><span class="header-section-number">18.7</span> Poisson regression<a href="week-10-lecture.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Poisson regression is used to model response variables that are counts. Poisson data includes:</p>
<ul>
<li><p>Number of plants is a given area</p></li>
<li><p>Number of offspring for an individual</p></li>
<li><p>Number of bacterial colonies in a Petri dish</p></li>
</ul>
<p>The underlying model of the data is</p>
<p><span class="math display">\[
Y_i \sim \mathrm{Poisson}(\lambda_i)
\]</span></p>
<p><strong>Question: What are the issues with modeling Poisson data as Normal?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
If you fit a normal linear regression to Poisson data, your regression might predict values that are negative. Also, the variance for Poisson is equal to the mean (the Normal has no such restriction).
</span>
</details>
<p>For Poisson data, the link function <span class="math inline">\(g()\)</span> is</p>
<p><span class="math display">\[
g (\lambda_i) = \log (\lambda_i)
\]</span>
This means that we map the linear model <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> to the parameter of the Poisson using a log function. Why use a log function? We want to map values that are non-negative integers to <span class="math inline">\([- \infty, \infty]\)</span>.</p>
<p>The variance of <span class="math inline">\(Y_i\)</span> for a Poisson regression is:</p>
<p><span class="math display">\[
\mathrm{Var}(Y_i) = \lambda_i
\]</span></p>
<p>The complete model for a Poisson regression with data <span class="math inline">\(Y_i\)</span> and covariate <span class="math inline">\(X_i\)</span> is:</p>
<p><span class="math display">\[
Y_i \sim \mathrm{Poisson}(\lambda_i) \text{, where } \log (\lambda_i) = \beta_0 + \beta_1 X_i
\]</span></p>
<p>To fit this GLM using maximum likelihood, we could plug in <span class="math inline">\(\lambda_i = \exp(\beta_0 + \beta_1 X_i)\)</span> to the likelihood function for a Poisson distributed variable.</p>
<p><span class="math display">\[
\mathcal{L}(Y_i | \lambda_i) = \prod^N_{i = 1} \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
\]</span>
As before, we solve for the parameters <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> by plugging into the likelihood <span class="math inline">\(\lambda_{i} = e^{\beta_{0}+\beta_{1}X_{i}}\)</span>, taking the partial derivatives with respect to <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, respectively, setting those partial derivatives to zero, and solving for <span class="math inline">\(\hat{\beta_{0}}\)</span> and <span class="math inline">\(\hat{\beta_{1}}\)</span>.</p>
<p>By modeling our data as it exists, we have solved three problems:</p>
<ol style="list-style-type: decimal">
<li><p>The errors now reflect the Poisson process</p></li>
<li><p>The variance reflects the Poisson process</p></li>
<li><p>The response is now restricted to non-negative integers</p></li>
</ol>
<p>However, there are things you need to understand about your data before fitting a Poisson regression:</p>
<ol style="list-style-type: decimal">
<li><p>Data are often <strong>overdispersed</strong>, or the variance is larger than the mean. To resolve this, you can add an additional term to the model. These models are called <strong>quasi-poisson models</strong>.</p></li>
<li><p>Data may have an unusually large number of zeros. To resolve this, you can fit a <strong>zero-inflated Poisson model</strong>, which models two components, the zero component (as a logistic regression), and the Poisson regression component. These classes of models (with multiple distributions) are more generally called mixture models.</p></li>
</ol>
<p>These types of Poisson regressions are common in ecology and evolution!</p>
</div>
<div id="deviance" class="section level2 hasAnchor" number="18.8">
<h2><span class="header-section-number">18.8</span> Deviance<a href="week-10-lecture.html#deviance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we were fitting linear regression models, we assessed model fit using the coefficient of determination <span class="math inline">\(R^{2}\)</span>. To measure fit of GLMs we use the <strong>deviance</strong>. Conceptually, it is not all that different from other fit metrics, and is analogous to the residual sum of squares for a linear model (actually it is equal to the residual sum of squares, see table 9.3 in Aho).</p>
<p>The deviance compares the NLL of the proposed model with the NLL of the “saturated” model. The saturated model is a model in which the predicted mean response matches each data point exactly, with one parameter for each observation. Note that the saturated model is a theoretical construct used to calculate deviance, not a model that you actually fit yourself.</p>
<p>Deviance is calculated as:</p>
<p><span class="math display">\[
D = 2 (\ell_{\text{saturated model}} - \ell_{\text{proposed model}})
\]</span>
If the proposed model fits the data nearly as well as the saturated model, then</p>
<p><span class="math display">\[
D | H_0 \sim \chi^2_{n - p}
\]</span>
The degrees of freedom for the chi-square distribution is calculated using <span class="math inline">\(n\)</span>, the number of parameters in the saturated model (usually the number of data points), and <span class="math inline">\(p\)</span>, the number of parameters in the proposed model. The saturated model has a likelihood greater than or equal to your proposed model, so the deviance is positive. We calculate the <em>P</em>-value as <span class="math inline">\(P(D | H_0 \ge D_{\text{observed}})\)</span> (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the model fit is poor.</p>
<p>In addition to assessing how well the model fits the data, we can use deviance to compare two models. We can use deviance to compare <strong>nested</strong> models. When one model is a reduced version of another model, the models are nested.</p>
<p>When comparing nested models, we no longer need to use the idea of the saturated model. We just compare the <em>difference</em> in deviance between the two nested models.</p>
<p><span class="math display">\[
\Delta D = 2 (\ell_{\text{larger model}} - \ell_{\text{smaller model}}) = -2 (\ell_{\text{smaller model}} - \ell_{\text{larger model}})
\]</span>
Under the null hypothesis that the smaller model is the true model:</p>
<p><span class="math display">\[
\Delta D | H_0 \sim \chi^2_{\text{additional parameters in larger model}}
\]</span>
Models with more parameters have higher likelihood, because any additional parameters allow the model more “freedom” to fit the data. This means that the difference in log likelihoods (<span class="math inline">\(\ell_{\text{larger model}} - \ell_{\text{smaller model}}\)</span>) will be positive and therefore the deviance difference will be negative.</p>
<p>Adding additional parameters <strong>always</strong> improves model fit. BUT, we want to know whether the fit improves above and beyond what would be expected purely by adding additional parameters to the model? We calculate the <em>P</em>-value as <span class="math inline">\(P(\Delta D | H_0 \ge \Delta D_{\text{observed}})\)</span> (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the larger model fits the data above and beyond what we would expect.</p>
<p>We already used this concept of deviance to construct 95% confidence intervals on a maximum likelihood estimate!</p>
<p>From Week 4’s Problem Set, “Using your function for the negative log-likelihood, calculate the MLE for <span class="math inline">\(\lambda\)</span> and the 95th percentile confidence interval. What would the 99th percentile confidence interval be? (4 pts) [Hint: The 95th percentile cut-off is <span class="math inline">\(0.5 \times \chi^2_{df = 1}(0.95)\)</span> or <code>0.5 * qchisq(0.95, df = 1) = 1.92</code> in R; in other words, the NLL would have to be &gt;1.92 higher than the minimum to fall outside the 95th percentile confidence interval. Likewise, the 99th percentile cut-off is <span class="math inline">\(0.5 \times \chi^2_{df = 1}(0.99) = 3.32\)</span>.]</p>
<p>So, you used the difference in NLL required to find the 95% CIs, but now you can see where that comes from.</p>
<p>An alternative approach to assessing the “importance” of a variable is to look at the test statistic for the model parameters the same way that we did for ordinary linear regression.</p>
<p><span class="math display">\[
z=\frac{\mbox{parameter estimate}}{\mbox{s.e. of parameter estimate}}
\]</span></p>
<p>The test statistic z is approximately normal for large sample sizes.</p>
</div>
<div id="other-methods-loess-splines-gams" class="section level2 hasAnchor" number="18.9">
<h2><span class="header-section-number">18.9</span> Other methods – LOESS, splines, GAMs<a href="week-10-lecture.html#other-methods-loess-splines-gams" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional regression methods are incredibly useful for predicting or explaining empirical data. However, in some situations, these methods are not flexible enough. For these cases, there are non-parametric approaches to curve fitting.</p>
<p>These methods may be useful for:</p>
<ul>
<li><p>Visualization of patterns</p></li>
<li><p>Interpolating discrete data</p></li>
<li><p>Identifying a threshold</p></li>
<li><p>Prediction (e.g., niche modeling)</p></li>
</ul>
<p>Keep in mind that LOESS, splines, and GAMS do not produce models that are straightforward and mechanistic. It is difficult/impossible to explain what the coefficients they produce actually mean.</p>
<p>LOESS (LOWESS), or locally weighted regression, is a non-parameteric method for fitting a curve through data. The dependent variable is fit in a moving fashion (like a moving window average)</p>
<p><span class="math display">\[
Y_i = g(X_i) + \epsilon_i \text{, where } \epsilon_i \sim \mathrm{N} (0, \sigma^2)
\]</span></p>
<p>The only assumption about the function <span class="math inline">\(g()\)</span> is that it is a smooth function. In LOESS, the regression curve is fit using a moving window and weighted linear regression where each point in the neighborhood is weighted according to its distance from <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
Y_i = \beta_0 + w_1(X_i - X_0) + w_2 (X_i - X_0)^2 + ... + w_p (X_i - X_0)^p + \epsilon_i
\]</span></p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="week-10-lecture.html#cb639-1" tabindex="-1"></a>bomregions2012<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;~/Dropbox/Biometry/Week 10 Multiple regression and GLMs/Week 10 Lecture/bomregions2012.csv&quot;</span>)</span>
<span id="cb639-2"><a href="week-10-lecture.html#cb639-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> bomregions2012, <span class="fu">aes</span>(<span class="at">x =</span> Year, <span class="at">y =</span> northRain)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>) <span class="sc">+</span></span>
<span id="cb639-3"><a href="week-10-lecture.html#cb639-3" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Northern rainfall&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb639-4"><a href="week-10-lecture.html#cb639-4" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size)) <span class="sc">+</span> <span class="fu">theme_classic</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-18-1.png" width="384" /></p>
<p>A smoothing spline is a model whose metric of fit is the sum of a measure of residuals, as well as a measure of roughness:</p>
<p><span class="math display">\[\sum^n_{i = 1} (Y_i - f(X_i))^2 + h \int^{x_{\text{max}}}_{x_{\text{min}}} f&#39;&#39;(X_i)^2 dx\]</span></p>
<p>The first term is the residual sum of squares, and the second term is a roughness penalty. <span class="math inline">\(h\)</span> is a smoothing parameter. A larger <span class="math inline">\(h\)</span> means a smoother line, because curves are more heavily penalized. A curve with <span class="math inline">\(h = 0\)</span> will just interpolate the data and a curve with a very large <span class="math inline">\(h\)</span> will fit a linear regression. The endpoints of the integral enclose the data.</p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="week-10-lecture.html#cb641-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> bomregions2012, <span class="fu">aes</span>(<span class="at">x =</span> Year, <span class="at">y =</span> northRain)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb641-2"><a href="week-10-lecture.html#cb641-2" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">formula =</span> y <span class="sc">~</span> splines<span class="sc">::</span><span class="fu">bs</span>(x, <span class="dv">10</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb641-3"><a href="week-10-lecture.html#cb641-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Northern rainfall&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb641-4"><a href="week-10-lecture.html#cb641-4" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size)) <span class="sc">+</span> <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-19-1.png" width="384" /></p>
<p>Generalized additive models (GAMs) allow us specify both smoothing and conventional parametric terms for models. GAMs are compatible with nonlinearity and nonnormal errors by using link functions (like with GLMs). With one covariate:</p>
<p><span class="math display">\[Y = \beta_0 + g(X)\]</span></p>
<p>This function, <span class="math inline">\(g()\)</span>, could be smoothing splines, LOESS smoothers, kernel smoothers, etc.</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="week-10-lecture.html#cb642-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> bomregions2012, <span class="fu">aes</span>(<span class="at">x =</span> Year, <span class="at">y =</span> northRain)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb642-2"><a href="week-10-lecture.html#cb642-2" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;gam&quot;</span>, <span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">bs =</span> <span class="st">&quot;cs&quot;</span>)) <span class="sc">+</span></span>
<span id="cb642-3"><a href="week-10-lecture.html#cb642-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Northern rainfall&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb642-4"><a href="week-10-lecture.html#cb642-4" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size)) <span class="sc">+</span> <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="Week-10-lecture_files/figure-html/unnamed-chunk-20-1.png" width="384" /></p>
<p>Personally, I really dislike GAMs and would discourage their use. GAMs rarely if ever provide information that you couldn’t see with the naked eye, and their use lends an air of statistical rigor to an analysis that is usually unjustified. I often see GAMs used as a crutch to avoid thinking seriously about the statistical model, and it tends to produce features that are artifacts of the data rather than meaningful information about the underlying process.</p>
<p>Multiple regression is <strong>not</strong> fundamentally different from what we have discussed in the past three weeks. The major differences are that we need to be careful about what covariates we include, as well as our interpretations of linear model coefficients. <em>Note that covariates, predictors, and explanatory variables all refer to the same thing, what I’ve referred to in linear regression as <span class="math inline">\(X_i\)</span>.</em></p>
<p>Multiple regression is useful to:</p>
<ol style="list-style-type: decimal">
<li><p>Build better predictive models</p></li>
<li><p>Investigate the relative effects of each covariate standardized across the effects of the other covariates. With multiple regression, we use <strong>partial regression lines</strong>, where the slope of any single partial regression line is the effect of that covariate <em>holding all the other covariates at their mean value.</em></p></li>
</ol>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + ... + \beta_{p - 1} X_{(p - 1)i} + \epsilon_i
\]</span></p>
<p>Where <span class="math inline">\(Y_i\)</span> is the response for the <span class="math inline">\(i^{\text{th}}\)</span> data point, <span class="math inline">\(X_{(p - 1)i}\)</span> is the covariate <span class="math inline">\(p - 1\)</span> for the <span class="math inline">\(i^{\text{th}}\)</span> data point. There are <span class="math inline">\(p\)</span> parameters in the model (<span class="math inline">\(p - 1\)</span> partial regression slopes and one intercept).</p>
<p>Note that we could write this is matrix form like we did with the linear models in Week 8:</p>
<p><span class="math display">\[\mathbf{Y} =
\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix}
\mathbf{X} =
\begin{bmatrix}
  1 &amp; X_{11} &amp; ... &amp; X_{(p - 1)1} \\
  1 &amp; X_{12} &amp; ... &amp; X_{(p - 1)2} \\
  1 &amp; \vdots &amp; \ddots &amp; \vdots \\
  1 &amp; X_{1n} &amp; ... &amp; X_{(p - 1)n}
\end{bmatrix}
\mathbf{b} =
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \vdots \\
  \beta_{p - 1}
\end{bmatrix}
\mathbf{e} =
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}\]</span></p>
<p>The null hypothesis for each of the regression coefficients is: <span class="math inline">\(\beta_0 = 0\)</span> (the intercept is equal to zero), <span class="math inline">\(\beta_1 = 0\)</span>, …, <span class="math inline">\(\beta_{p - 1} = 0\)</span> (the partial regression slope of <span class="math inline">\(X_{p - 1}\)</span> on <span class="math inline">\(Y\)</span> is zero). Multiple regression is fit the exact same way as simple linear regression (see Aho 9.5.1).</p>
<p>The first three assumptions are the same as linear regression with one covariate. The final assumption is new and relevant only with multiple covariates.</p>
<ol style="list-style-type: decimal">
<li><p>Linearity (A linear model appropriately describes the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>)</p></li>
<li><p>Normality (For any given value of <span class="math inline">\(X\)</span>, the sampled <span class="math inline">\(Y\)</span> values are independent with normally distributed errors)</p></li>
<li><p>Homogeneity of variances (Variances are constant along the regression line)</p></li>
<li><p>The covariates, <span class="math inline">\(X_1, X_2, ..., X_{p-1}\)</span>, are uncorrelated. <strong>Multicollinearity</strong> occurs when covariates are correlated with one another.</p></li>
</ol>
<p>Multicollinearity does NOT bias parameter estimates! But it still can be a big problem.</p>
<ol style="list-style-type: decimal">
<li><p>It causes instability of estimated partial regression slopes, meaning that small changes in the data cause large changes in the parameter estimates.</p></li>
<li><p>It inflates standard errors and confidence intervals of parameter estimates, increasing the chance of a Type II error. ANother way to say this is that multicollinearity reduces power, or increases the chance that you won’t find a significant result even if the null hypothesis is false.</p></li>
</ol>
<p>For simple linear regression with a single covariate, the slope parameter had the following sampling distribution:</p>
<p><span class="math display">\[
\hat{\beta_1} \sim \mathrm{N} {\left( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2} \right)}
\]</span></p>
<p><strong>Review Question: What is <span class="math inline">\(\sigma_\epsilon^2\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The population error variance (the expected value of squared errors). In practice, we estimate this as <span class="math inline">\(s^2_\epsilon\)</span> from the data. It’s also called the mean squared error.
</span>
</details>
<p>When we have multiple covariates, the sampling distribution for the partial slope parameter (the slope for the <span class="math inline">\(X_1\)</span> covariate, for example) is:</p>
<p><span class="math display">\[
\hat{\beta_1} \sim \mathrm{N} {\left ( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_{1i} - \bar{X_1})^2} \frac{1}{1 - R^2_1} \right)}
\]</span></p>
<p>where <span class="math inline">\(R^2_1\)</span> is the multiple coefficient of determination, or the fraction of variance in the covariate 1 explained by all other covariates, <span class="math inline">\(X_1 \sim X_2 + X_3 + ... + X_{p - 1}\)</span>.</p>
<p>The bigger that <span class="math inline">\(R^2_1\)</span> is, the larger the standard error of <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="week-10-lecture.html#cb643-1" tabindex="-1"></a>Var.inflation <span class="ot">&lt;-</span> <span class="cf">function</span>(R2<span class="fl">.1</span>) {</span>
<span id="cb643-2"><a href="week-10-lecture.html#cb643-2" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> R2<span class="fl">.1</span>)</span>
<span id="cb643-3"><a href="week-10-lecture.html#cb643-3" tabindex="-1"></a>}</span>
<span id="cb643-4"><a href="week-10-lecture.html#cb643-4" tabindex="-1"></a><span class="fu">Var.inflation</span>(<span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>## [1] 1.111111</code></pre>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="week-10-lecture.html#cb645-1" tabindex="-1"></a><span class="fu">Var.inflation</span>(<span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 2.5</code></pre>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="week-10-lecture.html#cb647-1" tabindex="-1"></a><span class="fu">Var.inflation</span>(<span class="fl">0.9</span>)</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<p>How do we diagnose multicollinearity?</p>
<ol style="list-style-type: decimal">
<li><p>Look at the pairwise correlations between all of he predictor variables either by a correlation matrix or by a scatterplot matrix (try the package <code>corrplot</code>)</p></li>
<li><p>Calculate the Variance Inflation Factor. A large VIF may be &gt; 5 or &gt; 10 (depends who you ask). Two covariates have to be very correlated for this to be an issue.</p></li>
</ol>
<p><span class="math display">\[
\text{VIF} = \frac{1}{1 - R^2_{p - 1}}
\]</span>
3. Do a principal components analysis (PCA) to see which covariates are highly correlated (we will get into this Week 14).</p>
<p>How do we deal with multicollinearity?</p>
<ol style="list-style-type: decimal">
<li><p>If the goal of your model is <strong>prediction</strong>, multicollinearity isn’t necessarily a problem. It may be best to leave all predictors in the model, even if they are collinear.</p></li>
<li><p>Remove the highly correlated predictors, starting with the least biologically interesting ones</p></li>
<li><p>Do a PCA, because the principal components are guaranteed to be orthogonal (completely uncorrelated). However, this does make hypothesis testing difficult, because the parameters will be combinations of the original variables.</p></li>
</ol>
<p>How can we test hypotheses about model parameters in multiple regression?</p>
<ol style="list-style-type: decimal">
<li>Compare individual parameters using a <em>t</em>-test</li>
</ol>
<p><span class="math display">\[
\frac{\hat{\beta_1} - \beta_{1 | H_0}}{\text{SE}_{\hat{\beta_1}}} \sim t_{df}
\]</span></p>
<p>How many degrees of freedom do we have for each model parameter? Let’s jump back to the distribution for the partial slope of covariate <span class="math inline">\(X_1\)</span> out of <span class="math inline">\(p - 1\)</span> total covariates. In our estimate of the unknown error variance <span class="math inline">\(\sigma^2_\epsilon\)</span> (back from Week 9 lecture):</p>
<p><span class="math display">\[
s_\epsilon^2 = \frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
\]</span></p>
<p>Calculating <span class="math inline">\(\hat{Y_i}\)</span> requires <span class="math inline">\(\bar{Y}\)</span> as well as the mean of each covariate (<span class="math inline">\(\bar{X_1}, \bar{X_2}, ..., \bar{X}_{p - 1}\)</span>), so we lose <span class="math inline">\(p\)</span> degrees of freedom.</p>
<ol start="2" style="list-style-type: decimal">
<li>Compare the full model to the reduced model in which that particular term has been removed:</li>
</ol>
<p><span class="math display">\[
\text{Full model: } Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \epsilon_i
\]</span></p>
<p><span class="math display">\[
\text{Reduced model: } Y_i = \beta_0 + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \epsilon_i
\]</span></p>
<p><strong>Question: How can we compare these two models?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Since the models are nested, we can use the deviance difference.
</span>
</details>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-9-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-10-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
