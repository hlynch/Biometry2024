<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>24 Week 13 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="24 Week 13 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="24 Week 13 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="24 Week 13 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-12-lab.html"/>
<link rel="next" href="week-13-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-13-lecture" class="section level1 hasAnchor" number="24">
<h1><span class="header-section-number">24</span> Week 13 Lecture<a href="week-13-lecture.html#week-13-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-13-readings" class="section level2 hasAnchor" number="24.1">
<h2><span class="header-section-number">24.1</span> Week 13 Readings<a href="week-13-lecture.html#week-13-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The topic covered this week is huge, and here I provide a number of readings that might be helpful to your understanding: <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Aho_etal_2014.pdf">Aho et al. (2014)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Burnham_etal_2011.pdf">Burnham et al. (2011)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Fox_Chapter6.pdf">Fox (2002)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Grueber_etal_2011.pdf">Grueber et al. (2011)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Hutto_2012.pdf">Hutto (2012)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Johnson_Omland_2004.pdf">Johnson and Omland (2004)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Nakagawa_Cuthill_2007.pdf">Nakagawa and Cuthill (2007)</a>, <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Tong_2019.pdf">Tong (2019)</a>, and <a href="https://github.com/hlynch/Biometry2022/tree/master/_data/Zuur_etal_2010.pdf">Zuur et al. (2010)</a>.</p>
</div>
<div id="model-criticism" class="section level2 hasAnchor" number="24.2">
<h2><span class="header-section-number">24.2</span> Model criticism<a href="week-13-lecture.html#model-criticism" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we can decide on what the “best” model is, we need to decide what we mean by the best model. There are two reasonable definitions of the best model based on the two separate and distinct goals of <em>testing a mechanism</em> and <em>making predictions</em>.</p>
<p><span class="math display">\[
Y = f(X) + \epsilon
\]</span></p>
<p>Models are always imperfect representation of reality (“All models are wrong but some are useful”). However, when you fit a model, you need to show that the model is a good representation of the data. This is important because if your data do not fit the assumptions of the model, the model can misrepresent the data. Model criticism and selection involves three parts:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Assumptions/Robustness:</strong> Are any of the assumptions of the model violated? Are any model assumptions having an undue impact on the results?</p></li>
<li><p><strong>Assessment:</strong> Does the model provide adequate fit to the data?</p></li>
<li><p><strong>Selection:</strong> Which model (or models) should we choose for final inference or prediction?</p></li>
</ol>
<p>The difference between the predicted value (based on the regression equation) and the actual, observed value. Remember that a lot of the assumptions for linear regression have to do with the residuals, (e.g., we often assumed <span class="math inline">\(\epsilon_i \sim \mathrm{N} (0, \sigma^2)\)</span>).</p>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-2-1.png" width="480" /></p>
<p>Plots of residuals should not change with the fitted values. Sometimes, systematic features in residual plots can suggest specific failures of model assumptions.</p>
<p>We will look at a past Biometry exam question to both practice for the exam and link what we are talking about today to past lectures.</p>
<p>Below are four plots depicting the residuals of a linear model plotted as a function of <span class="math inline">\(\hat{Y}\)</span>. For each panel, state whether the model violates any of the assumptions of linear regression and, if yes, which assumption(s) of linear regression are violated.</p>
<p><img src="Residuals.png" /></p>
<ol style="list-style-type: lower-alpha">
<li><p>No apparent violations</p></li>
<li><p>Variance is heteroscedastic (not constant). Larger fitted values have larger variance. Depending on your data, what type of regression may be more appropriate here? Poisson regression (why? mean = variance). You could also consider transforming the data or fitting a weighted regression to resolve this pattern.</p></li>
<li><p>Linear pattern in the residuals, the errors are not independent</p></li>
<li><p>Nonlinear pattern in the residuals (consider transforming data or fitting the predictor quadratically).</p></li>
</ol>
</div>
<div id="residuals" class="section level2 hasAnchor" number="24.3">
<h2><span class="header-section-number">24.3</span> Residuals<a href="week-13-lecture.html#residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In linear regression, an outlier is an observation with a <strong>large residual</strong>. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity, a data entry error, or other problem.</p>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-3-1.png" width="480" /></p>
</div>
<div id="leverage" class="section level2 hasAnchor" number="24.4">
<h2><span class="header-section-number">24.4</span> Leverage<a href="week-13-lecture.html#leverage" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An observation with an extreme value for a <strong>predictor variable/independent variable/covariate</strong> (often <span class="math inline">\(X\)</span>) is a point with high leverage. Leverage for a point is high when <span class="math inline">\((X - \bar{X})^2\)</span> is large. X-values further from <span class="math inline">\(\bar{X}\)</span> influence <span class="math inline">\(\hat{Y}_i\)</span> more than those close to <span class="math inline">\(\bar{X}\)</span>. High leverage points may have a large effect on the estimate of regression coefficients.</p>
<p>The leverage of the <span class="math inline">\(i^{\text{th}}\)</span> point is called <span class="math inline">\(h_{ii}\)</span> (the <span class="math inline">\(i^{\text{th}}\)</span> diagonal element in the hat matrix). The hat matrix relates the response <span class="math inline">\(Y\)</span> to the predicted response <span class="math inline">\(\hat{Y}\)</span>. This hat value, <span class="math inline">\(h_{ii}\)</span> thus is a measure of the contribution of data point <span class="math inline">\(Y_i\)</span> to all fitted values <span class="math inline">\(\hat{Y}\)</span>.</p>
<p>Average leverage is considered to be equal to <span class="math inline">\(p / n\)</span>, or the number of parameters divided by the sample size. Twice the average leverage is considered high.</p>
</div>
<div id="influence" class="section level2 hasAnchor" number="24.5">
<h2><span class="header-section-number">24.5</span> Influence<a href="week-13-lecture.html#influence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. Influence can be thought of as the <strong>product of leverage and outlierness</strong>.</p>
<p>We assess influence by looking at the effect of deleting that data point. We compare the least squares estimate of coefficient <span class="math inline">\(\beta\)</span> for the full dataset to the least squares estimate of coefficient <span class="math inline">\(\beta\)</span> when the <span class="math inline">\(i^{\text{th}}\)</span> data point is removed.</p>
<p><span class="math display">\[
\text{Influence} \propto \hat{\beta} - \hat{\beta}_{-i}
\]</span></p>
</div>
<div id="comparing-residuals-leverage-and-influence" class="section level2 hasAnchor" number="24.6">
<h2><span class="header-section-number">24.6</span> Comparing residuals, leverage, and influence<a href="week-13-lecture.html#comparing-residuals-leverage-and-influence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-4-1.png" width="576" /></p>
<p>The red and the blue points have large residuals.</p>
<p>The red and the green points have high leverage (large X values).</p>
<p>Only the red point here has high influence. Note that in your own work you would support these claims with statistics (which we will go over next).</p>
<p>Important:</p>
<ol style="list-style-type: decimal">
<li><p>Outliers might not be influential. (Depends on the <span class="math inline">\(X\)</span> value)</p></li>
<li><p>High leverage values may not be influential. (May fit right in with predicted line)</p></li>
<li><p>Influential observations may not be outliers. (May just have high leverage)</p></li>
</ol>
<p>We will use our two linear regressions of spider web length as a function of temperature, <code>web.fit</code>, and percent of males earning $3500 or more as a function of high school graduation rate, <code>Duncan.fit</code>, to go through the process of diagnosing a model. Do these models meet the assumptions of linear regression? Do they fit the data well?</p>
<p>Remember that certain analyses have certain assumptions. With a Poisson regression, you need to check for overdispersion or an abundance of zeros, and use the appropriate model.</p>
<p>If you have more than one covariate, you need to assess collinearity, which inflates Type II error and may mask covariate significance. We will go through this process in more detail in lab.</p>
<p>Are the residuals normally distributed? Histogram the residuals. They should be centered on zero and approximately normal (assuming a basic linear regression). Remember that with some analyses, like ANOVA, you assume normality within each group, so you would not look at all residuals together in that case. Also note that some analyses are fairly robust to non-normality.</p>
<div class="sourceCode" id="cb818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb818-1"><a href="week-13-lecture.html#cb818-1" tabindex="-1"></a><span class="fu">hist</span>(web.fit<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
<p>Are the residuals normally distributed? Look at a Q-Q plot comparing the quantiles of the residuals against the quantiles of the distribution we expect them to match (usually normal).</p>
<div class="sourceCode" id="cb819"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb819-1"><a href="week-13-lecture.html#cb819-1" tabindex="-1"></a><span class="fu">plot</span>(web.fit, <span class="at">which =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>Are the residuals independent and homoscedasctic? Plot the residuals vs. the fitted values. The residuals should be uncorrelated with the fitted values and the variance of the residuals should not change as a function of the fitted values.</p>
<div class="sourceCode" id="cb820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb820-1"><a href="week-13-lecture.html#cb820-1" tabindex="-1"></a><span class="fu">plot</span>(web.fit, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-7-1.png" width="384" /></p>
<p>Are the residuals normally distributed? We can formally check the residuals are normally distributed using the Kolmogorov-Smirnov test (null hypothesis is that the two vectors come from the same distribution). In this case, we would check against a normal distribution with mean of 0 and standard deviation of <span class="math inline">\(\sigma\)</span> from the regression.</p>
<div class="sourceCode" id="cb821"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb821-1"><a href="week-13-lecture.html#cb821-1" tabindex="-1"></a><span class="fu">ks.test</span>(web.fit<span class="sc">$</span>residuals, <span class="at">y =</span> <span class="st">&quot;pnorm&quot;</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">summary</span>(web.fit)<span class="sc">$</span>sigma)</span></code></pre></div>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  web.fit$residuals
## D = 0.075099, p-value = 0.9205
## alternative hypothesis: two-sided</code></pre>
<p><strong>Question: How do we interpret the results of this hypothesis test?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We have insufficient evidence to reject the null hypothesis that the residuals are normally distributed (both samples come from the same distribution).
</span>
</details>
<p>
 
</p>
<p>Are the residuals independent? Use the Durbin-Watson test, where the null hypothesis is that there is no correlation among residuals. If errors are correlated, we cannot assume that our errors are independent (an assumption of regression). These kind of correlations may be temporal or spatial, and can be controlled for with different modeling approaches. See Zuur et al. (2010) “Step 8” for details on how to deal with correlated errors.</p>
<p><span class="math display">\[d = \frac{\sum_{i = 2}^n (\epsilon_i - \epsilon_{i - 1})^2}{\sum_{i = 1}^n (\epsilon_i)^2}\]</span></p>
<div class="sourceCode" id="cb823"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb823-1"><a href="week-13-lecture.html#cb823-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb823-2"><a href="week-13-lecture.html#cb823-2" tabindex="-1"></a><span class="fu">durbinWatsonTest</span>(web.fit)</span></code></pre></div>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1     -0.03369874      2.059546   0.928
##  Alternative hypothesis: rho != 0</code></pre>
<p><strong>Question: How do we interpret the results of this hypothesis test?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We have insufficient evidence to reject the null hypothesis that the residuals are independent.
</span>
</details>
<p>
 
</p>
<p>Also be sure to look for outliers and points with high influence. There are a couple of ways you can determine if a point really is an outlier. We will use the Duncan regression to assess outliers, <code>Duncan.fit</code>.</p>
<p>What do you do with an outlier?</p>
<p>Well…</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="Outlier_cartoon.png" alt="One possible solution to outliers. (No!! Just kidding!! Do not do this.) Source: John Fox, Regression Diagnostics (1991)" width="25%" />
<p class="caption">
Figure 22.1: One possible solution to outliers. (No!! Just kidding!! Do not do this.) Source: John Fox, Regression Diagnostics (1991)
</p>
</div>
<p>Don’t remove it unless you can verifiably say it was recorded or measured in error.</p>
<ol style="list-style-type: decimal">
<li><p>Present the model with and without outliers, providing an explanation for the outliers (is there a potential unmeasured covariate that could explain the outlier?)</p></li>
<li><p>Use a robust method (tests based on rank, weighted models, etc.).</p></li>
</ol>
<p>Calculate the leave-one-out residuals for each data point and scale them by the standard deviation of the leave-one-out residuals. Data points with <span class="math inline">\(t_i &gt; 2\)</span> are potential outliers, <span class="math inline">\(t_i &gt; 3\)</span> are more serious outliers.</p>
<p><span class="math display">\[
t_i = \frac{Y_i - \hat{Y}_{i, (-i)}}{\sqrt{\mathrm{Var} (Y_i - \hat{Y}_{i, (-i)})}}
\]</span></p>
<div class="sourceCode" id="cb825"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb825-1"><a href="week-13-lecture.html#cb825-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb825-2"><a href="week-13-lecture.html#cb825-2" tabindex="-1"></a>Duncan[outliers, ]</span></code></pre></div>
<pre><code>##             type income education prestige
## conductor     wc     76        34       38
## RR.engineer   bc     81        28       67</code></pre>
<div class="sourceCode" id="cb827"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb827-1"><a href="week-13-lecture.html#cb827-1" tabindex="-1"></a><span class="fu">studres</span>(Duncan.fit)[<span class="fu">which</span>(<span class="fu">studres</span>(Duncan.fit) <span class="sc">&gt;</span> <span class="dv">2</span>)]</span></code></pre></div>
<pre><code>##   conductor RR.engineer 
##    2.919721    3.646444</code></pre>
<p>Cook’s distances are related to the studentized residuals in that both use leave-one-out residuals:</p>
<p><span class="math display">\[
D_i = \frac{\sum_{j = 1}^n (\hat{Y}_j - \hat{Y}_{j, (-i)})^2}{p \times \mathrm{MSE}}
\]</span></p>
<p>where <em>p</em> is number of parameters in the model, and MSE is the mean squared error. There are different cut-offs for what might be considered significant, but <em>D</em> &gt; 1 would be a basic guideline. Another guideline from Chatterjee and Hadi is to adjust <em>D</em> for sample size, <span class="math inline">\(4 / (n - p - 1)\)</span>. Cook’s distance describes the <strong>influence</strong> of a point (a function of both leverage and outlierness).</p>
<div class="sourceCode" id="cb829"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb829-1"><a href="week-13-lecture.html#cb829-1" tabindex="-1"></a>cutoff <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="sc">/</span> (<span class="fu">nrow</span>(Duncan) <span class="sc">-</span> <span class="dv">3</span> <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb829-2"><a href="week-13-lecture.html#cb829-2" tabindex="-1"></a><span class="fu">cooks.distance</span>(Duncan.fit)[<span class="fu">which</span>(<span class="fu">cooks.distance</span>(Duncan.fit) <span class="sc">&gt;</span> cutoff)]  <span class="co"># in car package</span></span></code></pre></div>
<pre><code>##    minister   conductor RR.engineer 
##   0.1415350   0.1162735   0.2025096</code></pre>
</div>
<div id="residuals-for-glms" class="section level2 hasAnchor" number="24.7">
<h2><span class="header-section-number">24.7</span> Residuals for GLMs<a href="week-13-lecture.html#residuals-for-glms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note that the definition of residuals for GLMs is a bit different. Residuals can be defined as <span class="math inline">\(\hat{Y}_i - Y_i\)</span>. For normally distributed linear models, this is equal to the statistical error <span class="math inline">\(\epsilon_i = \mathrm{E}(Y_i) - Y_i\)</span>. Remember that <span class="math inline">\(\mathrm{E}(Y_i)\)</span> or <span class="math inline">\(\hat{Y}_i\)</span> for a binomial or Poisson GLM is <span class="math inline">\(\hat{p_i}\)</span> or <span class="math inline">\(\hat{\lambda_i}\)</span>, respectively, so not on the same scale as the data (<span class="math inline">\(Y_i\)</span> for binomial or Poisson is an integer 0 through <em>n</em>). Also, nonconstant variance is an inherent part of GLMs.</p>
<p>We can use different types of residuals for model criticism of GLMs. We can look at the residuals on the scale of the fitted values or response (using the inverse link function): <code>residuals.glm(crab.glm, type = "response")</code>, or residuals that are scaled by the variance inherent to the GLM: deviance residuals <code>type = "deviance"</code> or Pearson residuals <code>type = "pearson"</code> (see 9.20.7.1 in Aho for details). R provides these in diagnostic plots.</p>
<div class="sourceCode" id="cb831"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb831-1"><a href="week-13-lecture.html#cb831-1" tabindex="-1"></a>crabs <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;_data/crabs.csv&quot;</span>)</span>
<span id="cb831-2"><a href="week-13-lecture.html#cb831-2" tabindex="-1"></a>crab.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(satell <span class="sc">~</span> width,  <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> crabs)</span>
<span id="cb831-3"><a href="week-13-lecture.html#cb831-3" tabindex="-1"></a><span class="fu">plot</span>(crab.glm, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-13-1.png" width="384" /></p>
<div class="sourceCode" id="cb832"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb832-1"><a href="week-13-lecture.html#cb832-1" tabindex="-1"></a>crab.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(satell <span class="sc">~</span> width,  <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> crabs)</span>
<span id="cb832-2"><a href="week-13-lecture.html#cb832-2" tabindex="-1"></a><span class="fu">plot</span>(crab.glm, <span class="at">which =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-14-1.png" width="384" /></p>
<div class="sourceCode" id="cb833"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb833-1"><a href="week-13-lecture.html#cb833-1" tabindex="-1"></a>crab.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(satell <span class="sc">~</span> width,  <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> crabs)</span>
<span id="cb833-2"><a href="week-13-lecture.html#cb833-2" tabindex="-1"></a><span class="fu">plot</span>(crab.glm, <span class="at">which =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-15-1.png" width="384" /></p>
</div>
<div id="model-selection-vs.-model-criticism" class="section level2 hasAnchor" number="24.8">
<h2><span class="header-section-number">24.8</span> Model selection vs. model criticism<a href="week-13-lecture.html#model-selection-vs.-model-criticism" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we move on to model selection, let’s review the difference between model selection and model criticism: Model criticism involves determining whether the model is appropriate, meets assumptions, and accurately represents the data. Model selection gives us the best model, the best suite of models, or the best averaged model.</p>
<p>Your analyses should always involve model criticism, and may also additionally involve model selection.</p>
<p>To decide what model is best, we first need to determine what our goal was in modeling. If our goal was to make predictions, or definition of what makes a model the best is different from if our goal was to test a mechanism.</p>
<p>We will revisit some ideas from the Shmueli (2010) paper that was assigned Week 8.</p>
<p>If our goal is to explain, then we are interested in the amount of variation in the response variable that is explained by each of the covariates.</p>
<p>With mechanism-based, explanatory models, we use Occam’s razor—the law of <strong>parsimony</strong>. By applying this principle, we identify models that are simple but effective, and we do not consider models that are needlessly complex. In other words, given two models, <strong>we prefer the smaller model unless the larger model is significantly better</strong>.</p>
<p>Additionally, all parameters should be <strong>biologically important</strong>, as identified by the modeler.</p>
<p>If our goal is to predict, then we want to <strong>minimize the expected prediction error</strong>, or our expectation of the difference between a new predicted value and the fitted value. For a model:</p>
<p><span class="math display">\[
Y = f(X) + \epsilon, \text{ where } \epsilon \sim \mathrm{N} (0, \sigma^2 )
\]</span></p>
<p>our expected prediction error of new data point <span class="math inline">\((X^\ast, Y^\ast)\)</span> is:</p>
<p><span class="math display">\[
\text{EPE} = \mathrm{E} [ (Y^\ast - \hat{Y})^2] = \mathrm{E} [ (Y^\ast - f(X^\ast))^2]
\]</span></p>
<p>The expected prediction error boils down to three components of variance:</p>
<p><span class="math display">\[
\sigma^2 + (\text{bias}_{\text{prediction}})^2 + \mathrm{Var} (\text{prediction})
\]</span></p>
<p><span class="math inline">\(\sigma^2\)</span> represents the variation inherent to the process. This error is irreducible.</p>
<p><strong>Bias</strong>, <span class="math inline">\((\text{bias}_{\text{prediction}})^2\)</span>, represents the difference between the expected prediction of our model (if we could repeat the process of collecting data and building the model many times) and the underlying true value we are trying to predict.</p>
<p><strong>Variance</strong>, <span class="math inline">\(\mathrm{Var} (\text{prediction})\)</span>, represents the variability of model predictions for a given data point, if we could repeat the data-gathering and model building process many times.</p>
<p>If we had both: 1) the true model (an exact picture of the true mean behavior of the mechanism), and 2) infinite data (no random fluctuations due to finite sampling), we could theoretically minimize both bias and variance. However, that’s not the case in reality, and we tend to find a trade-off between bias and variance.</p>
<p><span class="math display">\[
\sigma^2 + (\text{bias}_{\text{prediction}})^2 + \mathrm{Var} (\text{prediction})
\]</span>
<img src="BiasVariance.png" /></p>
</div>
<div id="comparing-two-models" class="section level2 hasAnchor" number="24.9">
<h2><span class="header-section-number">24.9</span> Comparing two models<a href="week-13-lecture.html#comparing-two-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we are generating models to test mechanisms, we can use the following three criteria to decide which is best. All three use the likelihood of the models.</p>
<ol style="list-style-type: decimal">
<li><p>Likelihood ratio test (LRT)</p></li>
<li><p>Akaike’s Information Criterion (AIC)</p></li>
<li><p>Bayesian Information Criterion (BIC)</p></li>
</ol>
<p>The likelihood ratio test is one way of comparing two nested models. Before we get into the details of the LRT, let’s review for a second what we mean by <em>nested</em>.</p>
<div id="nested-or-not" class="section level3 hasAnchor" number="24.9.1">
<h3><span class="header-section-number">24.9.1</span> Nested or not?<a href="week-13-lecture.html#nested-or-not" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Question: Are these models nested?</strong></p>
<p><span class="math display">\[
Y \sim X_1 + X_2 + X_3 \\
Y \sim X_1 + X_2 + X_4
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
No, neither model is a subset of the other
</span>
</details>
<p>
 
</p>
<p><strong>Question: What about these models?</strong></p>
<p><span class="math display">\[
Y \sim X_1 + X_2 + X_3 \\
Y \sim X_1 + X_2
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Yes, the second model is a subset of the first model
</span>
</details>
<p>
 
</p>
<p>Note that this usage of “nested” is completely different from the way we used it with nested ANOVAs.</p>
<p><strong>Question: Nested?</strong></p>
<span class="math display">\[
Y = a \\
Y = a + b X
\]</span>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Nested. The second model is equal to the first model with <span class="math inline">\(b = 0\)</span>.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Nested?</strong></p>
<span class="math display">\[
Y = a + b X \\
Y = a + b X + c X^2
\]</span>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Nested. The second model is equal to the first model with <span class="math inline">\(c = 0\)</span>.
</span>
</details>
<p>
 
</p>
<p><strong>Question: Nested?</strong></p>
<p><span class="math display">\[
Y = r \left ( 1 - \frac{N_t}{K} \right ) \\
Y = r \left ( 1 - \left ( \frac{N_t}{K} \right ) ^\gamma \right )
\]</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Nested. The second model is equal to the first model with <span class="math inline">\(\gamma = 1\)</span>. This model is called the Ricker model (top model is the usual form and the bottom model is the generalized version). It’s used in ecology to model the abundance of animal populations with density dependence.
</span>
</details>
<p>
 
</p>
</div>
<div id="likelihood-ratio-test-lrt" class="section level3 hasAnchor" number="24.9.2">
<h3><span class="header-section-number">24.9.2</span> Likelihood Ratio Test (LRT)<a href="week-13-lecture.html#likelihood-ratio-test-lrt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Likelihoods are crucial for model selection, they can be used generally for a wide variety of models. We’ll start with a brief review of likelihood.</p>
<p>When we were fitting statistical distributions to data, we assumed that each data point was independent. Therefore, for independent events <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>:</p>
<p><span class="math display">\[
\mathrm{P}(X_1 \cap X_2 \cap X_3) = \mathrm{P}(X_1) \times \mathrm{P}(X_2) \times \mathrm{P}(X_3)
\]</span></p>
<p>We then calculate the joint density of the data as a function of the unknown parameters (multiplying the probability of each data point given the PDF and as a function of the unknown parameters):</p>
<p><span class="math display">\[
\mathcal{L}(\text{parameters} | \text{data}) = \prod^N_{i = 1} \text{PDF} (X_i | \text{parameters})
\]</span></p>
<p>Last we find the value(s) of the parameter(s) that maximize the likelihood (in practice, by minimizing the log-likelihood).</p>
<p>Now we are using the same idea to fit models to data:</p>
<p><span class="math display">\[
\mathcal{L}(\text{model parameters} | \text{data}) = \prod^N_{i = 1} \text{PDF} (X_i | \text{model parameters})
\]</span></p>
<p><strong>Question: What are the parameters in a linear regression with one continuous predictor?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>
</span>
</details>
<p>
 
</p>
<p><strong>Question: What are the parameters in a linear model with one categorical predictor (which has four levels)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, <span class="math inline">\(\alpha_3\)</span> (depends on how you code the model, but there will be four parameters describing the mean behavior if the model is not overparameterized), and <span class="math inline">\(\sigma\)</span>
</span>
</details>
<p>
 
</p>
<p><strong>Question: What are the parameters in a Poisson generalized linear model with one continuous predictor?</strong></p>
<p><em>Hint:</em> <span class="math inline">\(Y_i \sim \mathrm{Pois} (\lambda_i), \quad log(\lambda_i) = \beta_0 + \beta_1 X_i\)</span></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
</span>
</details>
<p>
 
</p>
<p>Remember the deviance difference from Week 10? Under the null hypothesis that the larger model fits no better than the smaller model, the deviance difference goes as a chi-square distribution.</p>
<p><span class="math display">\[D = 2 * \log \frac{\mathcal{L}_{\text{larger model}}}{\mathcal{L}_{\text{smaller model}}} = 2 (\ell_{\text{larger model}} - \ell_{\text{smaller model}}) \\
D | H_0 \sim \chi^2_{\text{additional parameters in larger model}}\]</span></p>
<p>This is called a likelihood ratio test. A large value for <span class="math inline">\(D\)</span> (likelihood ratio), unexpected under the null distribution, would indicate a significant <strong>improvement</strong> in using the larger model. In this case, we would prefer the larger model over the smaller model. Note that we can only do this for nested models.</p>
<p><strong>Question: Will the likelihood be larger for the larger model or for the smaller model?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The likelihood will always be larger for the larger model (more parameters = more ability to fit the data).
</span>
</details>
<p>
 
</p>
<p><strong>Question: Will the deviance difference, D, be positive or negative?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Because the likelihood will always be larger for the larger model, D will always be positive. This makes sense because the <span class="math inline">\(\chi^2\)</span> distribution is always positive.
</span>
</details>
<p>
 
</p>
<div class="sourceCode" id="cb834"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb834-1"><a href="week-13-lecture.html#cb834-1" tabindex="-1"></a>Duncan.smaller <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> prestige <span class="sc">~</span> income, <span class="at">data =</span> Duncan)</span>
<span id="cb834-2"><a href="week-13-lecture.html#cb834-2" tabindex="-1"></a>Duncan.larger <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> prestige <span class="sc">~</span> income <span class="sc">+</span> education, <span class="at">data =</span> Duncan)</span>
<span id="cb834-3"><a href="week-13-lecture.html#cb834-3" tabindex="-1"></a><span class="fu">logLik</span>(Duncan.larger)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -178.9822 (df=4)</code></pre>
<div class="sourceCode" id="cb836"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb836-1"><a href="week-13-lecture.html#cb836-1" tabindex="-1"></a><span class="fu">logLik</span>(Duncan.smaller)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -191.3776 (df=3)</code></pre>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb838-1"><a href="week-13-lecture.html#cb838-1" tabindex="-1"></a>Duncan.D <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="fu">logLik</span>(Duncan.larger) <span class="sc">-</span> <span class="fu">logLik</span>(Duncan.smaller))</span>
<span id="cb838-2"><a href="week-13-lecture.html#cb838-2" tabindex="-1"></a>Duncan.D</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 24.79077 (df=4)</code></pre>
<div class="sourceCode" id="cb840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb840-1"><a href="week-13-lecture.html#cb840-1" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="at">q =</span> Duncan.D, <span class="at">df =</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 6.390288e-07 (df=4)</code></pre>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb842-1"><a href="week-13-lecture.html#cb842-1" tabindex="-1"></a>xvals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">40</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb842-2"><a href="week-13-lecture.html#cb842-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb842-3"><a href="week-13-lecture.html#cb842-3" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> xvals, <span class="at">y =</span> <span class="fu">dchisq</span>(<span class="at">x =</span> xvals, <span class="at">df =</span> <span class="dv">1</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Quantiles of chi-square[DOF = 1]&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>)</span>
<span id="cb842-4"><a href="week-13-lecture.html#cb842-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">1</span>), <span class="at">col =</span> <span class="st">&quot;indianred2&quot;</span>)</span>
<span id="cb842-5"><a href="week-13-lecture.html#cb842-5" tabindex="-1"></a>  <span class="fu">text</span>(<span class="at">x =</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">4</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;indianred2&quot;</span>, <span class="at">labels =</span> <span class="st">&quot;Critical value&quot;</span>)</span>
<span id="cb842-6"><a href="week-13-lecture.html#cb842-6" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> Duncan.D, <span class="at">col =</span> <span class="st">&quot;dodgerblue2&quot;</span>)</span>
<span id="cb842-7"><a href="week-13-lecture.html#cb842-7" tabindex="-1"></a>    <span class="fu">text</span>(<span class="at">x =</span> Duncan.D <span class="sc">+</span> <span class="dv">6</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue2&quot;</span>, <span class="at">labels =</span> <span class="st">&quot;Observed test stat.&quot;</span>)</span></code></pre></div>
<p><img src="Week-13-lecture_files/figure-html/unnamed-chunk-17-1.png" width="480" /></p>
<p><strong>Question: How do we interpret the results of our likelihood ratio test?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The larger model explains significantly more variation in the data than we would expect.
</span>
</details>
<p>
 
</p>
</div>
<div id="akaikes-information-criterion-aic" class="section level3 hasAnchor" number="24.9.3">
<h3><span class="header-section-number">24.9.3</span> Akaike’s Information Criterion (AIC)<a href="week-13-lecture.html#akaikes-information-criterion-aic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>AIC is just one of the many <strong>information theoretic</strong> alternatives that have been developed to compare the “divergence” between our particular model (which is an approximation of the true mechanism) and the “true” model (the mechanism that’s actually going on in the real world). We always lose some information when approximating the truth using a model.</p>
<p>If you have a set of models that you could consider different alternative hypotheses, information theoretic methods allow you to rank them.</p>
<p>In practice, all information theoretic methods reduce to finding the model that minimizes some criterion that includes the sum of two terms: one is based on the <strong>likelihood</strong> and the other is a <strong>penalty term</strong> which penalizes for increasing model complexity. For AIC:</p>
<p><span class="math display">\[
\mathrm{AIC} = -2 \ell + 2 k
\]</span></p>
<p>where <span class="math inline">\(\ell\)</span> is the log-likelihood and <span class="math inline">\(k\)</span> is the total number of parameters in the model (including variance as a parameter when appropriate, <span class="math inline">\(\sigma^2\)</span>). <strong>Smaller AIC means better model fit</strong>.</p>
<div class="sourceCode" id="cb843"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb843-1"><a href="week-13-lecture.html#cb843-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">logLik</span>(Duncan.smaller) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> <span class="dv">3</span></span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 388.7552 (df=3)</code></pre>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb845-1"><a href="week-13-lecture.html#cb845-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">logLik</span>(Duncan.larger) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> <span class="dv">4</span></span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 365.9645 (df=4)</code></pre>
<p><strong>Question: Which model fits best?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The larger model.
</span>
</details>
<p>
 
</p>
<p><span class="math display">\[
\mathrm{AIC} = -2 \ell + 2 k
\]</span></p>
<p>The penalty term in AIC enforces parsimony, because adding an additional parameter will increase AIC by 2 unless the more complex model considerably increases the log-likelihood.</p>
<p>When you have small sample size, or <span class="math inline">\(n / k &lt; 40\)</span>, use the small sample corrected AIC (AICc). This converges to AIC when sample size is large.</p>
<p><span class="math display">\[
\mathrm{AICc} = \mathrm{AIC} + \frac{2 k (k + 1)}{n - k - 1}
\]</span></p>
<div class="sourceCode" id="cb847"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb847-1"><a href="week-13-lecture.html#cb847-1" tabindex="-1"></a><span class="fu">AIC</span>(Duncan.smaller) <span class="sc">+</span> (<span class="dv">2</span> <span class="sc">*</span> <span class="dv">3</span> <span class="sc">*</span> (<span class="dv">3</span> <span class="sc">+</span> <span class="dv">1</span>)) <span class="sc">/</span> (<span class="fu">nrow</span>(Duncan) <span class="sc">-</span> <span class="dv">3</span> <span class="sc">-</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 389.3406</code></pre>
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb849-1"><a href="week-13-lecture.html#cb849-1" tabindex="-1"></a><span class="fu">AIC</span>(Duncan.larger) <span class="sc">+</span> (<span class="dv">2</span> <span class="sc">*</span> <span class="dv">4</span> <span class="sc">*</span> (<span class="dv">4</span> <span class="sc">+</span> <span class="dv">1</span>)) <span class="sc">/</span> (<span class="fu">nrow</span>(Duncan) <span class="sc">-</span> <span class="dv">4</span> <span class="sc">-</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 366.9645</code></pre>
<p><strong>Question: Which model fits best?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The larger model.
</span>
</details>
<p>
 
</p>
<p>We can calculate the AIC of all possible candidate models and compare by calculating the difference in AIC between model <em>i</em> and the AIC of the best candidate model:</p>
<p><span class="math display">\[
\Delta \mathrm{AIC}_i = \mathrm{AIC}_i - \mathrm{AIC}_{\min}
\]</span></p>
<p>Information Criterion methods <strong>do not allow us say that one model is significantly better</strong>. We do not have a sampling distribution for differences in AIC, and we cannot use AIC to calculate a <em>P</em>-value for the significance of a model term. That said, there are some rules of thumb:</p>
<p><span class="math inline">\(\Delta \mathrm{AIC} &lt; 2\)</span> are considered equivalent</p>
<p><span class="math inline">\(4 &lt; \Delta \mathrm{AIC} &lt; 7\)</span> are considered clearly different</p>
<p><span class="math inline">\(\Delta \mathrm{AIC} &gt; 10\)</span> are definitely different</p>
<p><strong>Question: Given this definition, is the larger model definitely still the better model?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Yes, <span class="math inline">\(\Delta \mathrm{AIC} &gt; 10\)</span>.
</span>
</details>
<p>
 
</p>
</div>
<div id="bayesian-information-criterion-bic" class="section level3 hasAnchor" number="24.9.4">
<h3><span class="header-section-number">24.9.4</span> Bayesian Information Criterion (BIC)<a href="week-13-lecture.html#bayesian-information-criterion-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>BIC uses the same idea as AIC, but with a different penalty for model complexity. This is sometimes called the Schwarz criterion. <strong>Smaller BIC means better model fit</strong>.</p>
<p><span class="math display">\[
\mathrm{BIC} = -2 \ell + 2 k \times \ln(n)
\]</span></p>
<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb851-1"><a href="week-13-lecture.html#cb851-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">logLik</span>(Duncan.smaller) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">nrow</span>(Duncan))</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 405.5952 (df=3)</code></pre>
<div class="sourceCode" id="cb853"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb853-1"><a href="week-13-lecture.html#cb853-1" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">logLik</span>(Duncan.larger) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> <span class="dv">4</span> <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">nrow</span>(Duncan))</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; 388.4178 (df=4)</code></pre>
<p><strong>Question: Which model fits best?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The larger model.
</span>
</details>
<p>
 
</p>
<p>The penalty for BIC is different than for AIC. For AIC, the probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on <span class="math inline">\(k\)</span> but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as sample size, <span class="math inline">\(n\)</span>, gets large. Proponents of BIC like that it tends to select simpler models (fewer parameters) relative to AIC, because it is a consistent estimator of model fit.</p>
<p>However, Johnson and Omland (2004) include a strong critique of BIC. They state that it is not based in KL information theory, and it relies on the assumption that there is one true model and it is contained in the set of candidate models.</p>
</div>
<div id="comparing-lrt-and-aicbic" class="section level3 hasAnchor" number="24.9.5">
<h3><span class="header-section-number">24.9.5</span> Comparing LRT and AIC/BIC<a href="week-13-lecture.html#comparing-lrt-and-aicbic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>LRT allows you to put a p-value on model terms. Because you have a sampling distribution for the difference in deviances, you can compare the difference in deviance to the quantiles on the chi-squared distribution to get a p-value.</p></li>
<li><p>AIC/BIC values do not allow you to refer to one model as being significantly different than another. No p-values.</p></li>
<li><p>LRT require nested models and Information Theoretic Criterion like AIC do not. This is a major advantage for AIC/BIC!</p></li>
</ol>
<p>Information theoretic critera (AIC, AICc, BIC) allows you to compare all candidate models at once. We’ll go through the process of model seelction and model averaging using AIC as an example, though you could use the same approach with any criterion.</p>
<ol style="list-style-type: decimal">
<li><p>Decide on your candidate set of models</p></li>
<li><p>Compute the likelihood and AIC for each of the candidate models</p></li>
<li><p>Rank the models in order of increasing AIC</p></li>
</ol>
<p>After that, you have three options:</p>
<p><strong>Option 1:</strong> Choose the model with the lowest AIC.
This is not recommended! You may have several models that are very close in AIC, so it is arbitrary to select the lowest AIC model as the unambiguously best model.</p>
<p><strong>Option 2:</strong> Report all the models with <span class="math inline">\(\Delta \mathrm{AIC} = 2\)</span> of the best model.
This is better, because you are reporting all models that are basically indistinguishable from the best model, but there is no way to say how much better one model is from another.</p>
<p><strong>Option 3:</strong> Model weighting. You consider the parameter estimates from all candidate models, where the better models are weighted more than the worse models.</p>
<p>Burnham and Anderson define model weights from the AIC values, where the weight is the proportion of times under repeated sampling that the <span class="math inline">\(i^{\text{th}}\)</span> model would be the best model.</p>
<p><span class="math display">\[
\Delta_i = \mathrm{AIC}_i - \min (\mathrm{AIC}) \\
w_i = \frac{\mathrm{e}^{-\Delta_i / 2}}{\sum_{i = 1}^M \mathrm{e}^{-\Delta_i / 2}}
\]</span></p>
<p>The denominator is summed over all candidate models. Burnham et al. (2011) describe the model weight as the strength of evidence, or the probability of model <span class="math inline">\(i\)</span> given the data and all <span class="math inline">\(M\)</span> models.</p>
</div>
</div>
<div id="model-weighting" class="section level2 hasAnchor" number="24.10">
<h2><span class="header-section-number">24.10</span> Model weighting<a href="week-13-lecture.html#model-weighting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model weights provide a number of benefits:</p>
<ol style="list-style-type: decimal">
<li><p>They explicitly address the fact that you don’t know the best model, so it is a straightforward way of presenting the relative strength of evidence for each of the models in the set of candidate models.</p></li>
<li><p>The model weights present the possibility of calculating weighted parameter estimates. This is the idea behind AIC <strong>model averaging</strong>. With AIC model averaging, we use the Akaike weights to weight the parameter estimates and variances (i.e., standard errors) from each model and combine those. Thus, we incorporate model selection uncertainty directly into the parameter estimates via the Akaike weights.</p></li>
</ol>
<p>Model weighting and model averaging account for two types of uncertainty: the uncertainty about the parameters themselves, and the uncertainty about the model.</p>
<p>Let’s work through an example. Let’s say we have four possible candidate models – note that they are linear but not nested. Weighted parameters only work with linear models because the parameters have the same interpretation across the models.</p>
<p><span class="math display">\[
\text{Model 1}: Y = a, \quad \hat{a} = 1 \\
\text{Model 2}: Y = a + b X_1, \quad \hat{a} = 2, \hat{b} = 3 \\
\text{Model 3}: Y = a + b X_1 + c X_2, \quad \hat{a} = 1.5, \hat{b} = 2, \hat{c} = 7 \\
\text{Model 4}: Y = a + b X_1 + d X_3, \quad \hat{a} = 1.2, \hat{b} = 4, \hat{d} = 4
\]</span></p>
<p>Let’s say we calculated the following model weights:</p>
<p><span class="math display">\[
\Delta_i = \mathrm{AIC}_i - \min (\mathrm{AIC}) \\
w_i = \frac{\mathrm{e}^{-\Delta_i / 2}}{\sum_{i = 1}^M \mathrm{e}^{-\Delta_i / 2}}
\]</span></p>
<p>Model 1: <em>w</em> = 0.6,
Model 2: <em>w</em> = 0.1,
Model 3: <em>w</em> = 0.15,
Model 4: <em>w</em> = 0.15,</p>
<p>The model averaged estimate of parameter <span class="math inline">\(a\)</span> would be calculated by</p>
<p><span class="math display">\[
0.6(1) + 0.1(2) + 0.15(1.5) + 0.15(1.2) = 1.205
\]</span></p>
<p>How do we calculate the model averaged estimate of <span class="math inline">\(b\)</span>? This is a little trickier, because <span class="math inline">\(b\)</span> only appears in models 2, 3, 4.</p>
<p>One method we can use is to calculate new weights for the only set of models in which <span class="math inline">\(b\)</span> appears, so our new model weights are:</p>
<p><span class="math display">\[
\text{Model 2}: w = 0.1 / (0.1 + 0.15 + 0.15) = 0.25 \\
\text{Models 3 and 4}:w = 0.15 / (0.1 + 0.15 + 0.15) = 0.375
\]</span></p>
<p>and our model averaged estimate for <span class="math inline">\(b\)</span> is:</p>
<p><span class="math display">\[
0.25(3) + 0.375(2) + 0.375(4) = 3
\]</span></p>
<p>However, this doesn’t take into account that <span class="math inline">\(b\)</span> may not appear in a model because it doesn’t explain much variation in the response. We could also set <span class="math inline">\(b\)</span> to zero in models where it isn’t included. This essentially “shrinks” the parameter estimate for <span class="math inline">\(b\)</span> if it doesn’t appear in many models, bringing it closer to zero.</p>
<p><span class="math display">\[
0.6(0) + 0.1(3) + 0.15(2) + 0.15(4) = 1.2
\]</span></p>
<p>Be sure to explicitly state which of these two methods you used to weight parameter estimates in a report/thesis/manuscript.</p>
<p>What happens if all of your models are pretty bad, and you use model selection methods like the ones we’ve been discussing?</p>
<p>You’ll still get a best/most parsimonious model!</p>
<p>You need to also provide evidence of model explanatory power using metrics like the coefficient of determination <span class="math inline">\(R^2\)</span> (or adjusted <span class="math inline">\(R^2\)</span>, or marginal/conditional <span class="math inline">\(R^2\)</span> for mixed/hierarchical models, or goodness-of-fit tests) in addition to model fit diagnostics like AIC. Also don’t forget about your coefficient hypothesis tests! See Mac Nally et al. (2017) for more on comparing model fit and model explanatory power.</p>
<p><span class="math display">\[
R^2 = \frac{\mathrm{SSR}}{\mathrm{SST}} = \frac{\sum_{i = 1}^n (Y_i - \hat{Y}_i)^2}{\sum_{i = 1}^n (Y_i - \bar{Y})^2}
\]</span></p>
<p>Which models to include in model selection?</p>
<p>In many situations, you might have a large number of potential covariates to include in a model, all of which could have biological significance to your system. You have many choices to make as the investigator. You could:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Generate a subset of biologically reasonable models to test</strong>. In this case you would support your selections by whatever theory is reasonable for your system.</p></li>
<li><p><strong>Test all possible subsets of models</strong>. In this case, you would calculate AIC/BIC for models with all possible combinations of covariates. This becomes computationally expensive as the list of predictors increases in length, especially if you are testing for interactions as well as main effects.</p></li>
<li><p><strong>Use stepwise regression</strong>. Here, you would add in or remove variables one at a time to find the best model based on some criterion.</p></li>
</ol>
</div>
<div id="stepwise-regression" class="section level2 hasAnchor" number="24.11">
<h2><span class="header-section-number">24.11</span> Stepwise regression<a href="week-13-lecture.html#stepwise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With stepwise regression, you add in or remove parameters one at a time, then a test is done to check whether some variables can be deleted without appreciably increasing the residual sum of squares (RSS) or some other criterion (for example, AIC can be used). The procedure stops when the available improvement falls below some critical value. Using this approach, you do not test all possible models, because the algorithm only compares models to the optimal model from the previous step.</p>
<p>The main approaches are:</p>
<ul>
<li><p>Forward selection, which involves starting with no variables in the model, trying out the variables one by one and including them if they are ‘statistically significant’.</p></li>
<li><p>Backward elimination, which involves starting with all candidate variables and testing them one by one for statistical significance, deleting any that are not significant.</p></li>
<li><p>Methods that are a combination of the above, testing at each stage for variables to be included or excluded.</p></li>
</ul>
<div id="criticism-of-stepwise-regression" class="section level3 hasAnchor" number="24.11.1">
<h3><span class="header-section-number">24.11.1</span> Criticism of stepwise regression<a href="week-13-lecture.html#criticism-of-stepwise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A sequence of F-tests is often used to control the inclusion or exclusion of variables, but these are carried out on the same data and so there will be problems of multiple comparisons for which many correction criteria have been developed. It is therefore difficult to interpret the <em>P</em>-values associated with these tests, since each is conditional on the previous tests of inclusion and exclusion (see “dependent tests” in false discovery rate). When estimating the degrees of freedom, the number of the candidate independent variables from the best fit selected is smaller than the total number of final model variables, causing the fit to appear better than it is when adjusting the <span class="math inline">\(R^2\)</span> value for the number of degrees of freedom. It is important to consider how many degrees of freedom have been used in the entire model, not just count the number of independent variables in the resulting fit.</p>
</div>
<div id="criticism-of-data-dredging" class="section level3 hasAnchor" number="24.11.2">
<h3><span class="header-section-number">24.11.2</span> Criticism of data dredging<a href="week-13-lecture.html#criticism-of-data-dredging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Automated model selection can be considered data dredging (methods 2 and 3). Critics regard data dredging as substituting intense computation for subject area expertise. You may have many predictors even in a carefully thought out model because biological systems are complex and we may want to know which predictors, from a set of reasonable predictors, best explain the variation in the data. However, we should be careful to recognize that we don’t really understand the Type I error rate for the entire model selection process and our findings should be considered exploratory; where automated model selection has been used to find the best set of predictors, you may need to use that information to propose new hypotheses to be tested more rigorously with a new dataset.</p>
</div>
<div id="final-thoughts-on-model-selection" class="section level3 hasAnchor" number="24.11.3">
<h3><span class="header-section-number">24.11.3</span> Final thoughts on model selection<a href="week-13-lecture.html#final-thoughts-on-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the goal is hypothesis testing, think carefully about the models ahead of time and consider a smaller set of candidate models, all of which make biological sense.</p>
<p>If the goal is prediction, you can start with a wider set of candidate models, and you are less worried that the correlations stem from true causation.</p>
<p><strong>Don’t let the computation drive the biology.</strong> It is OK and often appropriate to leave covariates in the model even if they are not statistically significant if you believe them to be biologically significant.</p>
</div>
</div>
<div id="week-13-faq" class="section level2 hasAnchor" number="24.12">
<h2><span class="header-section-number">24.12</span> Week 13 FAQ<a href="week-13-lecture.html#week-13-faq" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Question: How to we calculate explanatory power?</strong></p>
<p><span class="math display">\[
r^{2} = \frac{SSR}{SST} = \frac{1-SSE}{SST} = \frac{1-\mbox{sum-of-squares error}}{\mbox{sum-of-squares total}}
\]</span></p>
<p>Going back to Week 9, we note that the sum-of-squares total is given by</p>
<p><span class="math display">\[
SST = \sum_{i=1}^{n}(Y_{i}-\bar{Y})^{2}
\]</span>
and the sum-of-squares error</p>
<p><span class="math display">\[
SSE = \sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
\]</span></p>
<p><strong>Question: How exactly does Mallow’s Cp work?</strong></p>
<p>Mallows Cp is used when you are comparing models with different numbers of parameters. If the model with p parameters is correct than Cp will tend to be close to or smaller than p. Mallows Cp will be close to p if the model is unbiased and so Mallows Cp is used if unbiasedness is a criteria of particular interest.</p>
<p><strong>Question: What is the rational behind the penalty factor in BIC and why does the penalty grow with n?</strong></p>
<p>If you are comparing two nested models, AIC is equivalent to using a cutoff for the difference in -2LogLik between the two models equal to 2*k, where k is the difference in the number of parameters between the two models. (Note that AIC does not require nested models, but the case of nested models allows us to understand the penalty factor for BIC.) The probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on “k” but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as n gets large.</p>
<p><strong>Question: How do we do model averaging when the parameter in question is not included in all models?</strong></p>
<p>In these cases, we have to restrict attention to the models that include the parameter of question, and recalculate the AIC weights within that subset. For example, in the example presented in lecture, “b” only appears in Models 2,3, and 4.</p>
<p>Step 1 therefore is to recalculate AIC weights as follows:</p>
<p><span class="math display">\[
w_{AIC, model2} = \frac{0.1}{0.1+0.15+0.15} = 0.25
\]</span>
<span class="math display">\[
w_{AIC, model3} = \frac{0.15}{0.1+0.15+0.15} = 0.375
\]</span></p>
<p><span class="math display">\[
w_{AIC, model4} = \frac{0.15}{0.1+0.15+0.15} = 0.375
\]</span></p>
<p>The weighted parameter estimate for “b” would then be</p>
<p><span class="math display">\[
\hat{b} = (0.25 \times 3) + (0.375 \times 2) + (0.375 \times 4) = 3
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-12-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-13-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
