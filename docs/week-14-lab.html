<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>27 Week 14 Lab | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="27 Week 14 Lab | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="27 Week 14 Lab | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="27 Week 14 Lab | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-14-lecture.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-14-lab" class="section level1 hasAnchor" number="27">
<h1><span class="header-section-number">27</span> Week 14 Lab<a href="week-14-lab.html#week-14-lab" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In lab we’ll go through</p>
<ol style="list-style-type: decimal">
<li><p>Some practice with PCA using the semester survey results</p></li>
<li><p>Some practice with GLMs using the semester survey results</p></li>
</ol>
<p>There are a number of functions you could use in R to do principal components analysis. We will use the ‘prcomp’ function, but there is a very closely related function called ‘princomp’ as well as a function called ‘principal’ which is in the ‘psych’ package.</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="week-14-lab.html#cb1013-1" tabindex="-1"></a>readings<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;~/Dropbox/Biometry/Week 14 Multivariate analyses and Review/Week 14 Lab/Readings 2023.csv&quot;</span>,<span class="at">header=</span>T)</span>
<span id="cb1013-2"><a href="week-14-lab.html#cb1013-2" tabindex="-1"></a></span>
<span id="cb1013-3"><a href="week-14-lab.html#cb1013-3" tabindex="-1"></a>missing<span class="ot">&lt;-</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Useful)<span class="sc">|</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Difficult)<span class="sc">|</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Interesting)</span>
<span id="cb1013-4"><a href="week-14-lab.html#cb1013-4" tabindex="-1"></a>Useful<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Useful[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1013-5"><a href="week-14-lab.html#cb1013-5" tabindex="-1"></a></span>
<span id="cb1013-6"><a href="week-14-lab.html#cb1013-6" tabindex="-1"></a>Difficult<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Difficult[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1013-7"><a href="week-14-lab.html#cb1013-7" tabindex="-1"></a></span>
<span id="cb1013-8"><a href="week-14-lab.html#cb1013-8" tabindex="-1"></a>Interesting<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Interesting[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1013-9"><a href="week-14-lab.html#cb1013-9" tabindex="-1"></a></span>
<span id="cb1013-10"><a href="week-14-lab.html#cb1013-10" tabindex="-1"></a><span class="co">#Length.means.readings&lt;-aggregate(readings$Length[!missing], by=list(Index=readings$Index[!missing]),FUN=mean)$x</span></span>
<span id="cb1013-11"><a href="week-14-lab.html#cb1013-11" tabindex="-1"></a></span>
<span id="cb1013-12"><a href="week-14-lab.html#cb1013-12" tabindex="-1"></a>pca.result<span class="ot">&lt;-</span><span class="fu">prcomp</span>(<span class="sc">~</span>Useful<span class="sc">+</span>Interesting<span class="sc">+</span>Difficult,<span class="at">retx=</span>T)</span></code></pre></div>
<p>Before printing out the result, let’s make sure everyone understands what I was doing with the aggregate commands, and how the ‘prcomp’ function input works.</p>
<p>To print out a summary of the PCA, we use</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="week-14-lab.html#cb1014-1" tabindex="-1"></a><span class="fu">summary</span>(pca.result)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3
## Standard deviation     0.9046 0.4566 0.29753
## Proportion of Variance 0.7337 0.1869 0.07937
## Cumulative Proportion  0.7337 0.9206 1.00000</code></pre>
<p>We see that PCA1 is associated with over 73% of the variation in responses. So, what is PCA1?</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="week-14-lab.html#cb1016-1" tabindex="-1"></a>pca.result<span class="sc">$</span>rotation</span></code></pre></div>
<pre><code>##                    PC1       PC2        PC3
## Useful       0.2565179 0.7697893  0.5844853
## Interesting  0.5521851 0.3796006 -0.7422904
## Difficult   -0.7932781 0.5131548 -0.3276918</code></pre>
<p>PCA1 is an axis which describes papers that are more Interesting and less Difficult, with a very small weight towards papers that are Useful. In other words, a large positive PCA1 score would be associated with an interesting paper that was easy to read. Note that the principal components denote an axis, but the direction is arbitrary. Since no direction is implied by the sign, we do not interpret this as saying that most papers were interesting and easy. Instead we would say that the papers largely fall along a common axis in which Interesting/Easy to read papers are at one end, and Boring/Difficult to read papers are at the other end. (For now I am ignoring the smaller influence of Useful on PCA1.)</p>
<p>We can visualize this using the function ‘biplot’</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="week-14-lab.html#cb1018-1" tabindex="-1"></a><span class="fu">biplot</span>(pca.result)</span></code></pre></div>
<p><img src="Week-14-lab_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Biplots take some getting used to, and when they have many more dimensions, they become increasingly difficult to interpret. However, papers high on PC1 are generally Interesting and Easy to read and papers low on PC1 are generally Boring and more Difficult to read. Papers high on PC2 are generally more Useful and slightly more Difficult and papers low on PC2 are generally less Useful but less Difficult to read. It’s worth noting that there is a nice spread of papers in this bi-plot, so at least along these two axes, there were no real outliers. Also, keep in mind that since the PCA is picking up relative differences, it is not possible for all papers to end up in one corner of the plot. While of course we would want all papers to be highly Interesting and highly Useful, even if all papers ranked high on both measures overall, the PCA will identify axes of <em>differences</em> so there will always be some that show up as “relatively less Interesting” - does this make sense?</p>
<p>Which papers were highly positive on PC2 and also positive on PC1? These are papers that were Useful and also more Interesting than the average.</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="week-14-lab.html#cb1019-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">15</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Siddhartha, R. D., E. B. Fowlkes, and B. Hoadley. 1989. Risk analysis of the space shuttle: Pre-challenger prediction of failure. Journal of the American Statistical Association 84(408): 945-957.&quot;</code></pre>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="week-14-lab.html#cb1021-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">21</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Aho, K., D. DeWayne, and T. Peterson. 2014. Model selection for ecologists: the worldviews of AIC and BIC. Ecology 95(3): 631-636.&quot;</code></pre>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="week-14-lab.html#cb1023-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">4</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Johnson, D.H. 2002. The role of hypothesis testing in wildlife science. The Journal of Wildlife Management 66(2): 272-276.&quot;</code></pre>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="week-14-lab.html#cb1025-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">25</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Johnson, J.B., and K.S. Omland. 2004. Model selection in ecology and evolution. TRENDS in Ecology and Evolution 19(2): 101-108.&quot;</code></pre>
<p>I totally agree on the Siddhartha paper! Johnson’s paper’s also fall in this qudrant as well.</p>
<p>You can play around with this yourself and see why I added the [1] at the end. When I pull out the rows with the Index identified by the PCA, I get the list of all entries (since we had &gt;1 team rating the papers) and so I only print the first one.</p>
<p>Which paper fell out along the Difficult axis?</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="week-14-lab.html#cb1027-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">22</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Burnham et al. 2011. AIC model selection and multimodel inference in behavioral ecology: some background, observations, and comparisons. Behavior, Ecology, and Sociobiology 65: 23-35.&quot;</code></pre>
<p>Burnham’s AIC model selection paper. Also,</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="week-14-lab.html#cb1029-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">6</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Bender, R., and S. Lange. 2001. Adjusting for multiple testing – when and how? Journal of Clinical Epidemiology 54: 343-349.&quot;</code></pre>
<p>Usually Bolker holds this honor! Bolker is often rated as difficult. I keep this chapter around because his thinking is so “spot on” and the material in his book will serve you well if you continue on doing quantitative modelling. I’m a little surprised to see Bender and Lange here, but every year is different.</p>
<p>The real quadrant that I investigate closely are papers that are considered Difficult but not all that Useful. Through this PCA exeercise, I have eliminated papers consistently falling into this corner. Let’s see what 16 is…</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="week-14-lab.html#cb1031-1" tabindex="-1"></a>readings[readings<span class="sc">$</span>Index<span class="sc">==</span><span class="dv">16</span>,<span class="dv">1</span>][<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Altman, N., and M. Krzywinski. 2015. Sources of variation. Nature Methods 12(1): 5-6. (optional)&quot;</code></pre>
<p>I’m surprised to see Altman here, as the Points of Significance papers are so short and I think quite well written. I think this probably means we just need to spend a bit of time going over this one next year.</p>
<p>One thing to keep in mind is that a PCA identifies <em>variation</em> in the dataset. It’s worth putting these numbers in context of the overall means.</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="week-14-lab.html#cb1033-1" tabindex="-1"></a><span class="fu">mean</span>(Useful)</span></code></pre></div>
<pre><code>## [1] 3.874055</code></pre>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="week-14-lab.html#cb1035-1" tabindex="-1"></a><span class="fu">mean</span>(Difficult)</span></code></pre></div>
<pre><code>## [1] 2.08303</code></pre>
<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1037-1"><a href="week-14-lab.html#cb1037-1" tabindex="-1"></a><span class="fu">mean</span>(Interesting)</span></code></pre></div>
<pre><code>## [1] 3.372958</code></pre>
<p>So the average reading scored pretty high for being Useful and Interesting and was rated below average for Difficulty, so on the whole, I’m fairly pleased with these ratings.</p>
<p>You might be interested in how these ratings have changed over time (I was!). Let’s start with the readings.</p>
<div class="sourceCode" id="cb1039"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1039-1"><a href="week-14-lab.html#cb1039-1" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb1039-2"><a href="week-14-lab.html#cb1039-2" tabindex="-1"></a>Biometry_change <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;_data/Biometry_change.xlsx&quot;</span>)</span>
<span id="cb1039-3"><a href="week-14-lab.html#cb1039-3" tabindex="-1"></a>fitR_U<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>R_U_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1039-4"><a href="week-14-lab.html#cb1039-4" tabindex="-1"></a>fitR_U</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$R_U_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.36439 -0.26399  0.06366  0.16202  0.38224 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          -85.55852   51.82955  -1.651    0.150
## Biometry_change$Year   0.04416    0.02569   1.719    0.136
## 
## Residual standard error: 0.2946 on 6 degrees of freedom
## Multiple R-squared:   0.33,  Adjusted R-squared:  0.2183 
## F-statistic: 2.955 on 1 and 6 DF,  p-value: 0.1364</code></pre>
<div class="sourceCode" id="cb1041"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1041-1"><a href="week-14-lab.html#cb1041-1" tabindex="-1"></a><span class="fu">plot</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>R_U_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Response (out of 5)&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="fl">4.5</span>),<span class="at">main=</span><span class="st">&quot;Useful (black), Difficult (red), Interesting (green)&quot;</span>)</span>
<span id="cb1041-2"><a href="week-14-lab.html#cb1041-2" tabindex="-1"></a></span>
<span id="cb1041-3"><a href="week-14-lab.html#cb1041-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1041-4"><a href="week-14-lab.html#cb1041-4" tabindex="-1"></a>{</span>
<span id="cb1041-5"><a href="week-14-lab.html#cb1041-5" tabindex="-1"></a>  <span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>R_U_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_U_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>R_U_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_U_se[i])</span>
<span id="cb1041-6"><a href="week-14-lab.html#cb1041-6" tabindex="-1"></a>}</span>
<span id="cb1041-7"><a href="week-14-lab.html#cb1041-7" tabindex="-1"></a></span>
<span id="cb1041-8"><a href="week-14-lab.html#cb1041-8" tabindex="-1"></a>fitR_D<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>R_D_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1041-9"><a href="week-14-lab.html#cb1041-9" tabindex="-1"></a>fitR_D</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$R_D_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.29893 -0.12886  0.03503  0.12404  0.31271 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          -39.63356   39.88751  -0.994    0.359
## Biometry_change$Year   0.02076    0.01977   1.050    0.334
## 
## Residual standard error: 0.2267 on 6 degrees of freedom
## Multiple R-squared:  0.1553, Adjusted R-squared:  0.01454 
## F-statistic: 1.103 on 1 and 6 DF,  p-value: 0.334</code></pre>
<div class="sourceCode" id="cb1043"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1043-1"><a href="week-14-lab.html#cb1043-1" tabindex="-1"></a><span class="fu">points</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>R_D_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Difficult&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1043-2"><a href="week-14-lab.html#cb1043-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1043-3"><a href="week-14-lab.html#cb1043-3" tabindex="-1"></a>{</span>
<span id="cb1043-4"><a href="week-14-lab.html#cb1043-4" tabindex="-1"></a>  <span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>R_D_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_D_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>R_D_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_D_se[i],<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1043-5"><a href="week-14-lab.html#cb1043-5" tabindex="-1"></a>}</span>
<span id="cb1043-6"><a href="week-14-lab.html#cb1043-6" tabindex="-1"></a></span>
<span id="cb1043-7"><a href="week-14-lab.html#cb1043-7" tabindex="-1"></a>fitR_I<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>R_I_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1043-8"><a href="week-14-lab.html#cb1043-8" tabindex="-1"></a>fitR_I</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$R_I_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.19171 -0.11413 -0.03823  0.12481  0.21891 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)          -72.68056   29.30218  -2.480   0.0478 *
## Biometry_change$Year   0.03766    0.01452   2.593   0.0410 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1665 on 6 degrees of freedom
## Multiple R-squared:  0.5284, Adjusted R-squared:  0.4498 
## F-statistic: 6.724 on 1 and 6 DF,  p-value: 0.04105</code></pre>
<div class="sourceCode" id="cb1045"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1045-1"><a href="week-14-lab.html#cb1045-1" tabindex="-1"></a><span class="fu">points</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>R_I_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Difficult&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb1045-2"><a href="week-14-lab.html#cb1045-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1045-3"><a href="week-14-lab.html#cb1045-3" tabindex="-1"></a>{</span>
<span id="cb1045-4"><a href="week-14-lab.html#cb1045-4" tabindex="-1"></a>  <span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>R_I_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_I_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>R_I_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>R_I_se[i],<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb1045-5"><a href="week-14-lab.html#cb1045-5" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Week-14-lab_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>So it looks like while the overall trend is towards readings that are more Interesting, more Difficult, and more Useful, the only one of these that is statistically significant is the trend towards more Interesting.</p>
<p>The problem sets show exactly the same trends, with no significant changes in Useful or Difficult but a significant trend towards ore Interesting problem sets.</p>
<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1046-1"><a href="week-14-lab.html#cb1046-1" tabindex="-1"></a>PS<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;~/Dropbox/Biometry/Week 14 Multivariate analyses and Review/Week 14 Lab/ProblemSets 2022.csv&quot;</span>,<span class="at">header=</span>T)</span>
<span id="cb1046-2"><a href="week-14-lab.html#cb1046-2" tabindex="-1"></a></span>
<span id="cb1046-3"><a href="week-14-lab.html#cb1046-3" tabindex="-1"></a>missing<span class="ot">&lt;-</span><span class="fu">is.na</span>(PS<span class="sc">$</span>Useful)<span class="sc">|</span><span class="fu">is.na</span>(PS<span class="sc">$</span>Difficult)<span class="sc">|</span><span class="fu">is.na</span>(PS<span class="sc">$</span>Interesting)</span>
<span id="cb1046-4"><a href="week-14-lab.html#cb1046-4" tabindex="-1"></a></span>
<span id="cb1046-5"><a href="week-14-lab.html#cb1046-5" tabindex="-1"></a>Useful.means.PS<span class="ot">&lt;-</span><span class="fu">aggregate</span>(PS<span class="sc">$</span>Useful[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>PS<span class="sc">$</span>Week[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1046-6"><a href="week-14-lab.html#cb1046-6" tabindex="-1"></a></span>
<span id="cb1046-7"><a href="week-14-lab.html#cb1046-7" tabindex="-1"></a>Difficult.means.PS<span class="ot">&lt;-</span><span class="fu">aggregate</span>(PS<span class="sc">$</span>Difficult[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Week=</span>PS<span class="sc">$</span>Week[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1046-8"><a href="week-14-lab.html#cb1046-8" tabindex="-1"></a></span>
<span id="cb1046-9"><a href="week-14-lab.html#cb1046-9" tabindex="-1"></a>Interesting.means.PS<span class="ot">&lt;-</span><span class="fu">aggregate</span>(PS<span class="sc">$</span>Interesting[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Week=</span>PS<span class="sc">$</span>Week[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1046-10"><a href="week-14-lab.html#cb1046-10" tabindex="-1"></a></span>
<span id="cb1046-11"><a href="week-14-lab.html#cb1046-11" tabindex="-1"></a>pca.result<span class="ot">&lt;-</span><span class="fu">prcomp</span>(<span class="sc">~</span>Useful.means.PS<span class="sc">+</span>Interesting.means.PS<span class="sc">+</span>Difficult.means.PS,<span class="at">data=</span>PS,<span class="at">retx=</span>T)</span></code></pre></div>
<p>Notice that it has simply labeled them in order, so 7=Week #9 PS, 8=Week #10 PS, 9=Week #11 PS, 10=Week #12 PS, and 11=Week #13 PS.</p>
<p>To print out a summary of the PCA, we use</p>
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="week-14-lab.html#cb1047-1" tabindex="-1"></a><span class="fu">summary</span>(pca.result)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3
## Standard deviation     0.8474 0.6995 0.2560
## Proportion of Variance 0.5641 0.3844 0.0515
## Cumulative Proportion  0.5641 0.9485 1.0000</code></pre>
<p>We see that for the problem sets, PC1 is less dominant (56% of the variation). So, what is PCA1?</p>
<div class="sourceCode" id="cb1049"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1049-1"><a href="week-14-lab.html#cb1049-1" tabindex="-1"></a>pca.result<span class="sc">$</span>rotation</span></code></pre></div>
<pre><code>##                             PC1        PC2        PC3
## Useful.means.PS      -0.7894404 -0.1122903 -0.6034689
## Interesting.means.PS -0.4937346 -0.4679620  0.7329650
## Difficult.means.PS    0.3647054 -0.8765857 -0.3139865</code></pre>
<p>PC1 combines all three factors with the largest component being focused on “Useful”, and the axis divides problem sets judged Useless/Boring/Difficult and those that are Useful/Interesting/Easy. (Reminder: the signs of the PCs is <em>arbitrary</em>, so the signs on the rotation could have all be flipped.) Looking across all the PC axes, we want papers that are low (negative) on PC1 and low (negative) on PC2 (though these are also slightly less Useful). PC3 is a toss up, because that axis represents a trade-off between Useful and Interesting.</p>
<div class="sourceCode" id="cb1051"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1051-1"><a href="week-14-lab.html#cb1051-1" tabindex="-1"></a><span class="fu">biplot</span>(pca.result)</span></code></pre></div>
<p><img src="Week-14-lab_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We can see that problem set 6 is the one that is really driving variation here! (As always) If we were to eliminate week 6, the others are all varying primarily on PC2.</p>
<p>Again, looking at the means:</p>
<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1052-1"><a href="week-14-lab.html#cb1052-1" tabindex="-1"></a><span class="fu">mean</span>(Useful.means.PS)</span></code></pre></div>
<pre><code>## [1] 4.348485</code></pre>
<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1054-1"><a href="week-14-lab.html#cb1054-1" tabindex="-1"></a><span class="fu">mean</span>(Difficult.means.PS)</span></code></pre></div>
<pre><code>## [1] 3.643939</code></pre>
<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1056-1"><a href="week-14-lab.html#cb1056-1" tabindex="-1"></a><span class="fu">mean</span>(Interesting.means.PS)</span></code></pre></div>
<pre><code>## [1] 4.155303</code></pre>
<p>The problem sets overall rated as being very Useful and Interesting but also sort of Difficult.</p>
<p>How have things changed over time?</p>
<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1058-1"><a href="week-14-lab.html#cb1058-1" tabindex="-1"></a>fitPS_U<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>PS_U_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1058-2"><a href="week-14-lab.html#cb1058-2" tabindex="-1"></a>fitPS_U</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$PS_U_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35934 -0.10749 -0.06325  0.05777  0.54458 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          -73.58452   49.66576  -1.482    0.189
## Biometry_change$Year   0.03848    0.02461   1.563    0.169
## 
## Residual standard error: 0.2823 on 6 degrees of freedom
## Multiple R-squared:  0.2894, Adjusted R-squared:  0.171 
## F-statistic: 2.444 on 1 and 6 DF,  p-value: 0.169</code></pre>
<div class="sourceCode" id="cb1060"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1060-1"><a href="week-14-lab.html#cb1060-1" tabindex="-1"></a><span class="fu">plot</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>PS_U_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Response (out of 5)&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">5</span>),<span class="at">main=</span><span class="st">&quot;Useful (black), Difficult (red), Interesting (green)&quot;</span>)</span>
<span id="cb1060-2"><a href="week-14-lab.html#cb1060-2" tabindex="-1"></a></span>
<span id="cb1060-3"><a href="week-14-lab.html#cb1060-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1060-4"><a href="week-14-lab.html#cb1060-4" tabindex="-1"></a>{</span>
<span id="cb1060-5"><a href="week-14-lab.html#cb1060-5" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>PS_U_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_U_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>PS_U_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_U_se[i])</span>
<span id="cb1060-6"><a href="week-14-lab.html#cb1060-6" tabindex="-1"></a>}</span>
<span id="cb1060-7"><a href="week-14-lab.html#cb1060-7" tabindex="-1"></a></span>
<span id="cb1060-8"><a href="week-14-lab.html#cb1060-8" tabindex="-1"></a>fitPS_D<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>PS_D_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1060-9"><a href="week-14-lab.html#cb1060-9" tabindex="-1"></a>fitPS_D</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$PS_D_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.40591 -0.18510  0.03417  0.16826  0.38526 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          -30.20979   50.25999  -0.601    0.570
## Biometry_change$Year   0.01655    0.02491   0.664    0.531
## 
## Residual standard error: 0.2856 on 6 degrees of freedom
## Multiple R-squared:  0.0685, Adjusted R-squared:  -0.08675 
## F-statistic: 0.4412 on 1 and 6 DF,  p-value: 0.5312</code></pre>
<div class="sourceCode" id="cb1062"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1062-1"><a href="week-14-lab.html#cb1062-1" tabindex="-1"></a><span class="fu">points</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>PS_D_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Difficult&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1062-2"><a href="week-14-lab.html#cb1062-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1062-3"><a href="week-14-lab.html#cb1062-3" tabindex="-1"></a>{</span>
<span id="cb1062-4"><a href="week-14-lab.html#cb1062-4" tabindex="-1"></a>  <span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>PS_D_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_D_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>PS_D_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_D_se[i],<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1062-5"><a href="week-14-lab.html#cb1062-5" tabindex="-1"></a>}</span>
<span id="cb1062-6"><a href="week-14-lab.html#cb1062-6" tabindex="-1"></a></span>
<span id="cb1062-7"><a href="week-14-lab.html#cb1062-7" tabindex="-1"></a></span>
<span id="cb1062-8"><a href="week-14-lab.html#cb1062-8" tabindex="-1"></a>fitPS_I<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Biometry_change<span class="sc">$</span>PS_I_mean<span class="sc">~</span>Biometry_change<span class="sc">$</span>Year))</span>
<span id="cb1062-9"><a href="week-14-lab.html#cb1062-9" tabindex="-1"></a>fitPS_I</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Biometry_change$PS_I_mean ~ Biometry_change$Year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.44739 -0.17128 -0.01723  0.15788  0.44258 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)          -137.90978   55.40599  -2.489   0.0472 *
## Biometry_change$Year    0.07004    0.02746   2.551   0.0435 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3149 on 6 degrees of freedom
## Multiple R-squared:  0.5202, Adjusted R-squared:  0.4403 
## F-statistic: 6.506 on 1 and 6 DF,  p-value: 0.04345</code></pre>
<div class="sourceCode" id="cb1064"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1064-1"><a href="week-14-lab.html#cb1064-1" tabindex="-1"></a><span class="fu">points</span>(Biometry_change<span class="sc">$</span>Year,Biometry_change<span class="sc">$</span>PS_I_mean,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean Difficult&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb1064-2"><a href="week-14-lab.html#cb1064-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb1064-3"><a href="week-14-lab.html#cb1064-3" tabindex="-1"></a>{</span>
<span id="cb1064-4"><a href="week-14-lab.html#cb1064-4" tabindex="-1"></a>  <span class="fu">segments</span>(<span class="at">x0=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">x1=</span>Biometry_change<span class="sc">$</span>Year[i], <span class="at">y0=</span>Biometry_change<span class="sc">$</span>PS_I_mean[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_I_se[i],<span class="at">y1=</span>Biometry_change<span class="sc">$</span>PS_I_mean[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Biometry_change<span class="sc">$</span>PS_I_se[i],<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb1064-5"><a href="week-14-lab.html#cb1064-5" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Week-14-lab_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Interestingly, this is the same pattern seen for the Readings, with everything getting slightly (but not significantly) more Useful and Difficult over time but statistically significantly more Interesting.</p>
<div id="missing-at-random---practice-with-glms" class="section level2 hasAnchor" number="27.1">
<h2><span class="header-section-number">27.1</span> Missing at random - practice with GLMs<a href="week-14-lab.html#missing-at-random---practice-with-glms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the things we can do with this dataset is to ask whether data were missing at random, since not all readings were given ratings.</p>
<div class="sourceCode" id="cb1065"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1065-1"><a href="week-14-lab.html#cb1065-1" tabindex="-1"></a>missing<span class="ot">&lt;-</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Useful)<span class="sc">|</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Difficult)<span class="sc">|</span><span class="fu">is.na</span>(readings<span class="sc">$</span>Interesting)</span>
<span id="cb1065-2"><a href="week-14-lab.html#cb1065-2" tabindex="-1"></a>Useful<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Useful[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1065-3"><a href="week-14-lab.html#cb1065-3" tabindex="-1"></a></span>
<span id="cb1065-4"><a href="week-14-lab.html#cb1065-4" tabindex="-1"></a>Difficult<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Difficult[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1065-5"><a href="week-14-lab.html#cb1065-5" tabindex="-1"></a></span>
<span id="cb1065-6"><a href="week-14-lab.html#cb1065-6" tabindex="-1"></a>Interesting<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Interesting[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span>
<span id="cb1065-7"><a href="week-14-lab.html#cb1065-7" tabindex="-1"></a></span>
<span id="cb1065-8"><a href="week-14-lab.html#cb1065-8" tabindex="-1"></a>Length.means.readings<span class="ot">&lt;-</span><span class="fu">aggregate</span>(readings<span class="sc">$</span>Length[<span class="sc">!</span>missing], <span class="at">by=</span><span class="fu">list</span>(<span class="at">Index=</span>readings<span class="sc">$</span>Index[<span class="sc">!</span>missing]),<span class="at">FUN=</span>mean)<span class="sc">$</span>x</span></code></pre></div>
<p>One could ask the question, are these data missing at random? In the problem set for Week #13, we completed the dataset using random imputation. In other words, we assumed that data were missing at random and we drew with replacement from the other values to replace missing datapoints. However, in this case, it seems likely that data are not missing at random. I suspect that papers were not evaluated because no one read them, and that something about the papers may predict whether the papers were read or not. We can answer this question by constructing a model for “missingness” which assumes that the probability of being evaluated is distributed as Binom(n,p) where p is the probability of being evaluated (and presumably, of having been read in the first place).</p>
<p>First, I need to go through the data and figure out how many times a paper was evaluated.</p>
<div class="sourceCode" id="cb1066"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1066-1"><a href="week-14-lab.html#cb1066-1" tabindex="-1"></a>num.missing<span class="ot">&lt;-</span><span class="fu">vector</span>(<span class="at">length=</span><span class="fu">max</span>(readings<span class="sc">$</span>Index))</span>
<span id="cb1066-2"><a href="week-14-lab.html#cb1066-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">max</span>(readings<span class="sc">$</span>Index))</span>
<span id="cb1066-3"><a href="week-14-lab.html#cb1066-3" tabindex="-1"></a>{</span>
<span id="cb1066-4"><a href="week-14-lab.html#cb1066-4" tabindex="-1"></a>  num.missing.useful<span class="ot">&lt;-</span><span class="fu">sum</span>(<span class="fu">as.numeric</span>(<span class="fu">is.na</span>(readings<span class="sc">$</span>Useful[readings<span class="sc">$</span>Index<span class="sc">==</span>i])))</span>
<span id="cb1066-5"><a href="week-14-lab.html#cb1066-5" tabindex="-1"></a>  num.missing.difficult<span class="ot">&lt;-</span><span class="fu">sum</span>(<span class="fu">as.numeric</span>(<span class="fu">is.na</span>(readings<span class="sc">$</span>Difficult[readings<span class="sc">$</span>Index<span class="sc">==</span>i])))</span>
<span id="cb1066-6"><a href="week-14-lab.html#cb1066-6" tabindex="-1"></a>  num.missing.interesting<span class="ot">&lt;-</span><span class="fu">sum</span>(<span class="fu">as.numeric</span>(<span class="fu">is.na</span>(readings<span class="sc">$</span>Interesting[readings<span class="sc">$</span>Index<span class="sc">==</span>i])))</span>
<span id="cb1066-7"><a href="week-14-lab.html#cb1066-7" tabindex="-1"></a>  max.missing<span class="ot">&lt;-</span><span class="fu">max</span>(num.missing.useful,num.missing.difficult,num.missing.interesting)</span>
<span id="cb1066-8"><a href="week-14-lab.html#cb1066-8" tabindex="-1"></a>  num.missing[i]<span class="ot">&lt;-</span>max.missing</span>
<span id="cb1066-9"><a href="week-14-lab.html#cb1066-9" tabindex="-1"></a>}</span></code></pre></div>
<p>For simplicity, I am considering “evaluated” as evaluated for all three categories (Useful, Difficult, and Interesting).</p>
<p>Now I use a Binomial GLM to model the probability of being evaluated as a function of Useful, Interesting, and Difficult (as rated by the other groups). Note that there were 11 groups total, so n=11.</p>
<div class="sourceCode" id="cb1067"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1067-1"><a href="week-14-lab.html#cb1067-1" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">glm</span>(<span class="fu">cbind</span>(<span class="dv">11</span><span class="sc">-</span>num.missing,num.missing)<span class="sc">~</span>Useful<span class="sc">+</span>Difficult<span class="sc">+</span>Interesting,<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb1067-2"><a href="week-14-lab.html#cb1067-2" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(11 - num.missing, num.missing) ~ Useful + 
##     Difficult + Interesting, family = &quot;binomial&quot;)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8889   0.2226   0.3517   0.5553   1.0689  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  -8.9095     4.7986  -1.857   0.0634 .
## Useful        1.8824     1.0529   1.788   0.0738 .
## Difficult     0.6784     0.8084   0.839   0.4013  
## Interesting   1.3888     1.2207   1.138   0.2553  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33.997  on 34  degrees of freedom
## Residual deviance: 20.274  on 31  degrees of freedom
## AIC: 41.242
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>None of the covariates are significant.</p>
<p>We might suspect a high degree of multicollinearity among the predictors. We can use PCA to create new orthogonal covariates which (more efficiently) capture the variability in the survey results.</p>
<p>I will rerun the PCA for the readings.</p>
<div class="sourceCode" id="cb1069"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1069-1"><a href="week-14-lab.html#cb1069-1" tabindex="-1"></a>pca.result<span class="ot">&lt;-</span><span class="fu">prcomp</span>(<span class="sc">~</span>Useful<span class="sc">+</span>Interesting<span class="sc">+</span>Difficult,<span class="at">retx=</span>T)</span>
<span id="cb1069-2"><a href="week-14-lab.html#cb1069-2" tabindex="-1"></a><span class="fu">summary</span>(pca.result)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3
## Standard deviation     0.9046 0.4566 0.29753
## Proportion of Variance 0.7337 0.1869 0.07937
## Cumulative Proportion  0.7337 0.9206 1.00000</code></pre>
<div class="sourceCode" id="cb1071"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1071-1"><a href="week-14-lab.html#cb1071-1" tabindex="-1"></a>pca.result<span class="sc">$</span>rotation</span></code></pre></div>
<pre><code>##                    PC1       PC2        PC3
## Useful       0.2565179 0.7697893  0.5844853
## Interesting  0.5521851 0.3796006 -0.7422904
## Difficult   -0.7932781 0.5131548 -0.3276918</code></pre>
<p>PCA1 captures about 73% of the variability, so we try using just PCA1 in our GLM.</p>
<div class="sourceCode" id="cb1073"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1073-1"><a href="week-14-lab.html#cb1073-1" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">glm</span>(<span class="fu">cbind</span>(<span class="dv">11</span><span class="sc">-</span>num.missing,num.missing)<span class="sc">~</span>pca.result<span class="sc">$</span>x[,<span class="dv">1</span>],<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb1073-2"><a href="week-14-lab.html#cb1073-2" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(11 - num.missing, num.missing) ~ pca.result$x[, 
##     1], family = &quot;binomial&quot;)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0307   0.3584   0.5512   0.6803   1.2636  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         4.0506     0.4411   9.182   &lt;2e-16 ***
## pca.result$x[, 1]   0.9334     0.4302   2.170     0.03 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33.997  on 34  degrees of freedom
## Residual deviance: 28.604  on 33  degrees of freedom
## AIC: 45.572
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Aha! So none of the individual covariates are statistically significant, but in part this is because they are co-linear and so the variances are inflated. When we use the first PCA axis, the model does show a statistically significant trend between papers that are Useful, Interesting, and Easy and the probability that the paper was read.</p>
<p>Is there an effect of paper length to consider? After all, that’s probably the most salient feature of a paper when you first download it.</p>
<div class="sourceCode" id="cb1075"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1075-1"><a href="week-14-lab.html#cb1075-1" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">glm</span>(<span class="fu">cbind</span>(<span class="dv">11</span><span class="sc">-</span>num.missing,num.missing)<span class="sc">~</span>Length.means.readings,<span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<div class="sourceCode" id="cb1077"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1077-1"><a href="week-14-lab.html#cb1077-1" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(11 - num.missing, num.missing) ~ Length.means.readings, 
##     family = &quot;binomial&quot;)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.58226   0.00104   0.00981   0.10240   0.95209  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)            -0.7617     1.3958  -0.546   0.5853  
## Length.means.readings   1.3101     0.6014   2.178   0.0294 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33.9971  on 34  degrees of freedom
## Residual deviance:  6.4514  on 33  degrees of freedom
## AIC: 23.419
## 
## Number of Fisher Scoring iterations: 10</code></pre>
<p>Yes, but not in the direction we would anticipate. Longer papers are more likely to be read than shorter papers. Digging into the data a bit, the papers that were not rated and presumably not read were the very short Points of Significance papers, so this is certainly driving this unanticipated response. We have assumed throughout that these readings are all independent data points, but can you see why that might not be the case here? If students decide not to read the Points of Significance papers, that will impact all of those papers, and it might be that independent decisions are not being made for each one individually. (They were also assigned in the same week, so perhaps that plays a role as well.)</p>
</div>
<div id="finally-a-word-about-grades" class="section level2 hasAnchor" number="27.2">
<h2><span class="header-section-number">27.2</span> Finally, a word about grades<a href="week-14-lab.html#finally-a-word-about-grades" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The question came up as to how grades have changed over time. Behold, the grade distribution over time (A=4, A-=3.67, B+=3.33, B=3, etc.).</p>
<div class="sourceCode" id="cb1079"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1079-1"><a href="week-14-lab.html#cb1079-1" tabindex="-1"></a>year<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">2012</span>,<span class="dv">2013</span>,<span class="dv">2014</span>,<span class="dv">2015</span>,<span class="dv">2016</span>,<span class="dv">2017</span>,<span class="dv">2018</span>,<span class="dv">2020</span>,<span class="dv">2021</span>,<span class="dv">2022</span>)</span>
<span id="cb1079-2"><a href="week-14-lab.html#cb1079-2" tabindex="-1"></a>mean.grade<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">3.37</span>,<span class="fl">2.88</span>,<span class="fl">2.62</span>,<span class="fl">3.45</span>,<span class="fl">3.11</span>,<span class="fl">3.22</span>,<span class="fl">3.00</span>,<span class="fl">3.25</span>,<span class="fl">2.45</span>,<span class="fl">2.47</span>)</span>
<span id="cb1079-3"><a href="week-14-lab.html#cb1079-3" tabindex="-1"></a>sem.grade<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">0.15</span>,<span class="fl">0.21</span>,<span class="fl">0.26</span>,<span class="fl">0.13</span>,<span class="fl">0.18</span>,<span class="fl">0.19</span>,<span class="fl">0.23</span>,<span class="fl">0.20</span>,<span class="fl">0.33</span>,<span class="fl">0.32</span>)</span>
<span id="cb1079-4"><a href="week-14-lab.html#cb1079-4" tabindex="-1"></a><span class="fu">plot</span>(year,mean.grade,<span class="at">pch=</span><span class="dv">15</span>,<span class="at">xlab=</span><span class="st">&quot;Year&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Mean grade&quot;</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">1</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="dv">4</span>))</span>
<span id="cb1079-5"><a href="week-14-lab.html#cb1079-5" tabindex="-1"></a><span class="fu">points</span>(year[<span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">9</span>)],mean.grade[<span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">9</span>)],<span class="at">pch=</span><span class="dv">15</span>,<span class="at">cex=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="dv">4</span>))</span>
<span id="cb1079-6"><a href="week-14-lab.html#cb1079-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(mean.grade))</span>
<span id="cb1079-7"><a href="week-14-lab.html#cb1079-7" tabindex="-1"></a>{</span>
<span id="cb1079-8"><a href="week-14-lab.html#cb1079-8" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0=</span>year[i], <span class="at">x1=</span>year[i], <span class="at">y0=</span>mean.grade[i]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>sem.grade[i],<span class="at">y1=</span>mean.grade[i]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>sem.grade[i],<span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb1079-9"><a href="week-14-lab.html#cb1079-9" tabindex="-1"></a>}</span>
<span id="cb1079-10"><a href="week-14-lab.html#cb1079-10" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0=</span>year[<span class="dv">8</span>], <span class="at">x1=</span>year[<span class="dv">8</span>], <span class="at">y0=</span>mean.grade[<span class="dv">8</span>]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>sem.grade[<span class="dv">8</span>],<span class="at">y1=</span>mean.grade[<span class="dv">8</span>]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>sem.grade[<span class="dv">8</span>],<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1079-11"><a href="week-14-lab.html#cb1079-11" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0=</span>year[<span class="dv">9</span>], <span class="at">x1=</span>year[<span class="dv">9</span>], <span class="at">y0=</span>mean.grade[<span class="dv">9</span>]<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>sem.grade[<span class="dv">9</span>],<span class="at">y1=</span>mean.grade[<span class="dv">9</span>]<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>sem.grade[<span class="dv">9</span>],<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-14-lab_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-14-lecture.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
