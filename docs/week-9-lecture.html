<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>16 Week 9 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="16 Week 9 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="16 Week 9 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="16 Week 9 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-8-lab.html"/>
<link rel="next" href="week-9-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.3</b> Weighted regression</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.4</b> Robust regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.5</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.6</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-9-lecture" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">16</span> Week 9 Lecture<a href="week-9-lecture.html#week-9-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-9-readings" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> Week 9 Readings<a href="week-9-lecture.html#week-9-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Chapters 8 and 9 up to and including Section 9.16, as well as Logan Chapter 8. I also want to share <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Warton_etal_2006.pdf">a classic paper on how to model “scaling-type” relationships - I’m looking at you morphometrics folks!</a>, though this is just a recommended reading and not something we will cover in Biometry.</p>
</div>
<div id="correlation" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> Correlation<a href="week-9-lecture.html#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Correlation and regression are two ways of looking at the relationship between two continuous variables. Correlation is testing the null hypothesis that the two variables are uncorrelated, and there is complete symmetry between the two variables. On the other hand, regression is concerned with using one variable to PREDICT another, and there is an asymmetry between the predictor and the response variable. A regression model for <span class="math inline">\(Y \sim X\)</span> is different than <span class="math inline">\(X \sim Y\)</span>.</p>
<p>Regression and correlation are not the same.</p>
<p>Let’s take two paired samples A and B and introduce a quantity called the correlation coefficient r:</p>
<p><span class="math display">\[
r = \frac{Cov(A,B)}{\sqrt{Var(A) \times Var(B)}}
\]</span></p>
<p>More precisely, this is called “Pearson’s product moment correlation”.</p>
<p>We know from before that the sample variance of A (a.k.a. Var(A)) is given by</p>
<p><span class="math display">\[
Var(A) = \frac{1}{n-1}\sum_{i=1}^{n}(A_{i}-\bar{A})(A_{i}-\bar{A})
\]</span>
Sample variance is the squared deviation of a random variable from its expected value, scaled by the number of data points. When we use squared deviations, our values are always positive.</p>
<p>Let’s say <span class="math inline">\(A_1 = 12\)</span> and <span class="math inline">\(\bar{A} = 14\)</span>. The squared deviation is then <span class="math inline">\((12 - 14)^2 = 4\)</span>. Now let’s say <span class="math inline">\(A_2 = 30\)</span>. The squared deviation is then <span class="math inline">\((30 - 14)^2 = 256\)</span>. When using squared deviations, values far away from the mean contribute greatly to the variance (they have a large influence).</p>
<p>By extension, the sample covariance of A and B (i.e. Cov(A,B)) is given by</p>
<p><span class="math display">\[
Cov(A,B) = \frac{1}{n-1}\sum_{i=1}^{n}(A_{i}-\bar{A})(B_{i}-\bar{B})
\]</span>
Why do we only lose 1 degree of freedom when we have to estimate the mean of both the A group and the B group? So as not to distract from the main thread here, I have answered that below in the <a href="week-9-lecture.html#W9FAQ">FAQ</a>.</p>
<p>Why does this expression make sense? The covariance measures how much two things go up and down together.</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="week-9-lecture.html#cb487-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb487-2"><a href="week-9-lecture.html#cb487-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>)</span>
<span id="cb487-3"><a href="week-9-lecture.html#cb487-3" tabindex="-1"></a>(<span class="dv">1</span> <span class="sc">/</span> (<span class="fu">length</span>(A) <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((A <span class="sc">-</span> <span class="fu">mean</span>(A)) <span class="sc">*</span> (B <span class="sc">-</span> <span class="fu">mean</span>(B))) </span></code></pre></div>
<pre><code>## [1] 1.666667</code></pre>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="week-9-lecture.html#cb489-1" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb489-2"><a href="week-9-lecture.html#cb489-2" tabindex="-1"></a>(<span class="dv">1</span> <span class="sc">/</span> (<span class="fu">length</span>(A) <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((A <span class="sc">-</span> <span class="fu">mean</span>(A)) <span class="sc">*</span> (B <span class="sc">-</span> <span class="fu">mean</span>(B)))</span></code></pre></div>
<pre><code>## [1] -1.666667</code></pre>
<p>However, the covariance isn’t meaningful by itself because it depends on the absolute scale of variability in A and B, so it has to be divided by some aggregate measure of variability in the two samples (i.e. the geometric mean). We can see this by scaling everything by a factor of 10 and recalculating Covariance:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="week-9-lecture.html#cb491-1" tabindex="-1"></a>(<span class="dv">1</span> <span class="sc">/</span> (<span class="fu">length</span>(A) <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((A <span class="sc">-</span> <span class="fu">mean</span>(A)) <span class="sc">*</span> (B <span class="sc">-</span> <span class="fu">mean</span>(B)))</span></code></pre></div>
<pre><code>## [1] -1.666667</code></pre>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="week-9-lecture.html#cb493-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> A <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb493-2"><a href="week-9-lecture.html#cb493-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> B <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb493-3"><a href="week-9-lecture.html#cb493-3" tabindex="-1"></a>(<span class="dv">1</span> <span class="sc">/</span> (<span class="fu">length</span>(A) <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((A <span class="sc">-</span> <span class="fu">mean</span>(A)) <span class="sc">*</span> (B <span class="sc">-</span> <span class="fu">mean</span>(B))) </span></code></pre></div>
<pre><code>## [1] -166.6667</code></pre>
<p>This is why to calculate <span class="math inline">\(r\)</span> we scale the covariance by the geometric mean of the product of the variances for the two samples.</p>
<p><span class="math display">\[
r = \frac{Cov(A,B)}{\sqrt{Var(A) \times Var(B)}}
\]</span>
Pearson’s product moment correlation is bounded between -1 and 1. Correlation coefficients of 1 or -1 occur when you set deviations in sample <span class="math inline">\(B\)</span> to be exactly matched in sample <span class="math inline">\(A\)</span>. These measures are unitless.</p>
<p>Let’s look at a few examples…</p>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-4-1.png" width="240" /></p>
<pre><code>## [1] 0.04997418</code></pre>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-4-2.png" width="240" /></p>
<pre><code>## [1] 0.9048269</code></pre>
</div>
<div id="hypothesis-testing---pearsons-r" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">16.3</span> Hypothesis testing - Pearson’s <em>r</em><a href="week-9-lecture.html#hypothesis-testing---pearsons-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before getting into hypothesis testing, some terminology:</p>
<p><span class="math inline">\(\rho\)</span>=true (population) correlation coefficient of A and B</p>
<p><span class="math inline">\(r\)</span>=empirical (sample) correlation coefficient of A and B</p>
<p>How can we test hypotheses about correlation?</p>
<ol style="list-style-type: decimal">
<li><p>Parametric approach using distribution theory</p></li>
<li><p>Nonparametric approaches: bootstrap or randomization</p></li>
</ol>
<p>We’ll start with 1. If <span class="math inline">\(\rho\)</span> is the population (true) correlation coefficient of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> and <span class="math inline">\(r\)</span> is the sample (empirical) correlation coefficient of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we will test whether <span class="math inline">\(\rho\)</span> is significantly different from zero.</p>
<p>How do we assess whether our estimate of <span class="math inline">\(\rho\)</span> (which, following our notation from earlier in the semester, we will call <span class="math inline">\(\hat{\rho}\)</span>) is significantly different from zero?</p>
<p>Note before we begin that calculating Pearson’s correlation coefficient itself does not require that the data (what we’ve been calling <em>A</em> and <em>B</em>) are bivariate normal, but hypothesis testing/constructing CIs based on distribution theory does.</p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are from a bivariate normal distribution and <span class="math inline">\(\rho = 0\)</span>, the sample correlation coefficient is normally distributed (for large sample sizes):</p>
<p><span class="math display">\[
r | (H_0: \rho = 0) \sim \mathrm{N} {\left( 0, \frac{1 - r^2}{n - 2} \right)}
\]</span></p>
<p>and therefore we use the following test statistic:</p>
<p>Then we convert this to the following test statistic:
<span class="math display">\[
T^* = r \sqrt{\frac{n - 2}{1 - r^2}} | (H_0: \rho = 0) \sim \mathrm{N}(0, 1)
\]</span></p>
<p>Actually, it is more precise to say that under the null hypothesis,</p>
<p><span class="math display">\[
T^* = r \sqrt{\frac{n - 2}{1 - r^2}} | (H_0: \rho = 0) \sim t_{n-2}
\]</span></p>
<strong>Question: How did we generate this test statistic, <span class="math inline">\(T^*\)</span> from the original test statistic, <em>r</em>?</strong>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
By standardizing the distribution of <span class="math inline">\(r | H_0\)</span> so that the normal distribution has a standard deviation/variance of 1
</span>
</details>
<p><strong>Question: Why did we use this test statistic instead of <em>r</em>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The standard normal distribution and the t-distribution are conventional distributions with associated tables of quantiles that were used before computing was as powerful and widespread. Also, the t-distribution is more appropriate for small sample sizes (it has fatter tails).
</span>
</details>
<p><strong>Question: Where did we lose two degrees of freedom?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
To estimate <em>r</em>, we calculated the mean of sample A and sample B
</span>
</details>
<p>The Pearson’s product moment correlation assumes the following about the two samples:</p>
<ol style="list-style-type: decimal">
<li><p>The joint distribution <span class="math inline">\((A, B)\)</span> is bivariate normal (necessary for hypothesis testing using distribution theory)</p></li>
<li><p>The relationship between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is linear (always necessary)</p></li>
<li><p>Data are independent samples from the joint distribution.</p></li>
</ol>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="week-9-lecture.html#cb497-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb497-2"><a href="week-9-lecture.html#cb497-2" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb497-3"><a href="week-9-lecture.html#cb497-3" tabindex="-1"></a>A <span class="ot">&lt;-</span> A[<span class="fu">order</span>(A)]</span>
<span id="cb497-4"><a href="week-9-lecture.html#cb497-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">A =</span> A, <span class="at">B =</span> (A <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb497-5"><a href="week-9-lecture.html#cb497-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> A, <span class="at">y =</span> B)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-5-1.png" width="288" /></p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="week-9-lecture.html#cb498-1" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>A, df<span class="sc">$</span>B)</span></code></pre></div>
<pre><code>## [1] -0.1232354</code></pre>
<p>Even though <em>A</em> and <em>B</em> clearly have a strong relationship, correlation is only effective if the relationship is <strong>linear</strong>.</p>
<p>Other considerations:</p>
<p>What if the data are not bivariate normal? If sample size is large, you’re probably okay.</p>
<p>Pearson’s <span class="math inline">\(r\)</span> is also very sensitive to outliers, what can you do in that case? Robust correlation measures (Spearman’s or Kendall’s <span class="math inline">\(r\)</span>) are robust to deviations from bivariate normality and outliers. Confidence intervals on <span class="math inline">\(r\)</span> can also be derived using bootstrap and permutation methods. As these are far more intuitive, require no assumptions, and (with modern computers) are fast, it is often better to use these other methods (which may be slightly less powerful, another topic to be covered later).</p>
<p><strong>Question: How could you bootstrap the correlation coefficient given that the samples (<em>A</em> and <em>B</em>) are paired?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<p><span style="color: blueviolet;">You need to sample (with replacement) A-B pairs. This will preserve the correlated structure of the data while allowing you to resample a “new” dataset with which to calculate a confidence interval. In other words, sample with replacement from the <strong>row indices</strong>, preserving the relationship between the two samples</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="week-9-lecture.html#cb500-1" tabindex="-1"></a>iterations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb500-2"><a href="week-9-lecture.html#cb500-2" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb500-3"><a href="week-9-lecture.html#cb500-3" tabindex="-1"></a>B <span class="ot">&lt;-</span> A <span class="sc">-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb500-4"><a href="week-9-lecture.html#cb500-4" tabindex="-1"></a>cor.obs <span class="ot">&lt;-</span> <span class="fu">cor</span>(A, B)</span>
<span id="cb500-5"><a href="week-9-lecture.html#cb500-5" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(A, B)</span>
<span id="cb500-6"><a href="week-9-lecture.html#cb500-6" tabindex="-1"></a>cor.boot <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb500-7"><a href="week-9-lecture.html#cb500-7" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb500-8"><a href="week-9-lecture.html#cb500-8" tabindex="-1"></a>  index.boot <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(dat), <span class="at">size =</span> <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb500-9"><a href="week-9-lecture.html#cb500-9" tabindex="-1"></a>  dat.boot <span class="ot">&lt;-</span> dat[index.boot, ]</span>
<span id="cb500-10"><a href="week-9-lecture.html#cb500-10" tabindex="-1"></a>  cor.boot[i] <span class="ot">&lt;-</span> <span class="fu">cor</span>(<span class="at">x =</span> dat.boot<span class="sc">$</span>A, <span class="at">y =</span> dat.boot<span class="sc">$</span>B)</span>
<span id="cb500-11"><a href="week-9-lecture.html#cb500-11" tabindex="-1"></a>}</span>
<span id="cb500-12"><a href="week-9-lecture.html#cb500-12" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;The 95% confidence interval for the estimated correlation coefficient, &quot;</span>, <span class="fu">round</span>(cor.obs, <span class="at">digits =</span> <span class="dv">3</span>), <span class="st">&quot; is (&quot;</span>, <span class="fu">round</span>(<span class="fu">quantile</span>(cor.boot, <span class="fl">0.025</span>), <span class="at">digits =</span> <span class="dv">3</span>), <span class="st">&quot;, &quot;</span>, <span class="fu">round</span>(<span class="fu">quantile</span>(cor.boot, <span class="fl">0.975</span>), <span class="at">digits =</span> <span class="dv">3</span>), <span class="st">&quot;)&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The 95% confidence interval for the estimated correlation coefficient, 0.899 is (0.854, 0.934)&quot;</code></pre>
</span>
</details>
<p><strong>Question: How would we conduct a permutation test for the correlation coefficient (with <span class="math inline">\(H_0: \rho = 0\)</span>)?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Shuffle <em>A</em> independently of <em>B</em> and calculate the correlation of the shuffled data to create a null distribution of correlations. Compare the observed correlation to this distribution.
</span>
</details>
</div>
<div id="fishers-z" class="section level2 hasAnchor" number="16.4">
<h2><span class="header-section-number">16.4</span> Fisher’s <span class="math inline">\(z\)</span><a href="week-9-lecture.html#fishers-z" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you have large sample sizes, you can test other null hypotheses (i.e. <span class="math inline">\(H_{0}: \rho=\rho_{0}\)</span>) and put confidence intervals on <span class="math inline">\(\rho\)</span> using what is called “Fisher’s transformation”. Using Fisher’s <span class="math inline">\(z\)</span> transformation, we convert a statistic with a bounded distribution to an unbounded (normal) distribution. This is useful for calculating confidence intervals. Also with this method, we can test hypotheses other than <span class="math inline">\(\rho = 0\)</span>. We can transform <span class="math inline">\(r\)</span> to <span class="math inline">\(z\)</span> using the the inverse hyperbolic tangent:</p>
<p><span class="math display">\[
z = 0.5 \ln \left( \frac{1 + r}{1 - r} \right) = \tanh^{-1} (r)
\]</span></p>
<p>With this transformation, <span class="math inline">\(z\)</span> is approximately normally distributed for all values of <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[
z \sim \mathrm{N} {\left( 0.5 \ln \left( \frac{1 + \rho}{1 - \rho} \right), \frac{1}{n - 3} \right)}
\]</span>
Now, the test statistic that we can use to test the null hypothesis <span class="math inline">\(H_0: \rho = \rho_0\)</span> is:</p>
<p><span class="math display">\[
T^* = \frac{z - 0.5 \ln \left( \frac{1 + \rho_0}{1 - \rho_0} \right)}{\sqrt{1 / (n - 3)}} \approx \mathrm{N}(0, 1)
\]</span></p>
<p>Why did we rewrite a new test statistic (what was wrong with <span class="math inline">\(z\)</span>)? The null distribution for <span class="math inline">\(z\)</span> is more difficult to reference (compared to the quantiles of the standard normal, for example). For a bit of evidence showing that both ways of writing the test statistic are fine, see the following code:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="week-9-lecture.html#cb502-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb502-2"><a href="week-9-lecture.html#cb502-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> A <span class="sc">-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb502-3"><a href="week-9-lecture.html#cb502-3" tabindex="-1"></a>r.obs <span class="ot">&lt;-</span> <span class="fu">cor</span>(A, B)</span>
<span id="cb502-4"><a href="week-9-lecture.html#cb502-4" tabindex="-1"></a><span class="co"># using z as test statistic</span></span>
<span id="cb502-5"><a href="week-9-lecture.html#cb502-5" tabindex="-1"></a>z.obs <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>((<span class="dv">1</span> <span class="sc">+</span> r.obs) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> r.obs)) <span class="co"># note that z under the null distribution is 0</span></span>
<span id="cb502-6"><a href="week-9-lecture.html#cb502-6" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> z.obs, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">100</span> <span class="sc">-</span> <span class="dv">3</span>)), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 2.569189e-58</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="week-9-lecture.html#cb504-1" tabindex="-1"></a><span class="co"># using transformed test statistic, T*</span></span>
<span id="cb504-2"><a href="week-9-lecture.html#cb504-2" tabindex="-1"></a>test.stat <span class="ot">&lt;-</span> z.obs <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">100</span> <span class="sc">-</span> <span class="dv">3</span>))</span>
<span id="cb504-3"><a href="week-9-lecture.html#cb504-3" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> test.stat, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 2.569189e-58</code></pre>
<p>To simply the notation, let us define</p>
<p><span class="math display">\[
\zeta_{0} = \frac{1}{2}ln\left(\frac{1+\rho_{0}}{1-\rho_{0}}\right)
\]</span></p>
<p>We can also use this to construct confidence intervals for <span class="math inline">\(\zeta_{0}\)</span>:</p>
<p><span class="math display">\[
P \left( z - \frac{t_{[1 - \alpha / 2](\infty)}}{\sqrt{n - 3}} \leq \zeta_{0} \leq z + \frac{t_{[1 - \alpha / 2](\infty)}}{\sqrt{n - 3}} \right) = 1 - \alpha
\]</span></p>
<p>(Note that we need to do some back-transforming to get from here to a confidence interval on <span class="math inline">\(\rho\)</span>.)</p>
<p>Implicit in the calculation of the correlation coefficient is that the quantities being compared are either interval or ratio variables. If this is not the case, then you need to use a more general statistical test which uses only the ranks of the data. The underlying principle is that you rank the data in each sample and then compare ranks. There are two common ones:</p>
<ol style="list-style-type: decimal">
<li>Spearmans rank correlation: Similar to above except instead of using raw values, the data are first transformed into ranks (separately for each sample) before calculating <span class="math inline">\(r\)</span>.</li>
</ol>
<p><span class="math display">\[
r_{s} = \frac{Cov(ranks_A,ranks_B)}{\sqrt{Var(ranks_A) \times Var(ranks_B)}}
\]</span></p>
<p>where <span class="math inline">\(ranks_A\)</span> and <span class="math inline">\(ranks_B\)</span> are the ranks of the two datasets.</p>
<p><span class="math display">\[
r_s | H_0 \sim \sqrt{\frac{1}{n - 1}} \mathrm{N} (0, 1)
\]</span>
Ties are dealt with in a different way (see Aho 8.3.1.1 for more detail).</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="week-9-lecture.html#cb506-1" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">4</span>)</span>
<span id="cb506-2"><a href="week-9-lecture.html#cb506-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb506-3"><a href="week-9-lecture.html#cb506-3" tabindex="-1"></a><span class="fu">cov</span>(<span class="fu">rank</span>(A), <span class="fu">rank</span>(B)) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(<span class="fu">rank</span>(A)) <span class="sc">*</span> <span class="fu">var</span>(<span class="fu">rank</span>(B)))</span></code></pre></div>
<pre><code>## [1] -0.8</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="week-9-lecture.html#cb508-1" tabindex="-1"></a><span class="fu">cor.test</span>(A, B, <span class="at">method =</span> <span class="st">&quot;spearman&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  A and B
## S = 18, p-value = 0.3333
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##  rho 
## -0.8</code></pre>
<p>Q: What information is lost when using Spearman’s rather than Pearson’s <span class="math inline">\(r\)</span>?</p>
<ol start="2" style="list-style-type: decimal">
<li>Kendall’s <span class="math inline">\(\tau\)</span> or “Kendall’s coefficient of rank correlation”:</li>
</ol>
<p>The basic idea is that for sample X, you can compare all possible combinations of <span class="math inline">\(X_{i}\)</span> to see which values are larger. For a second sample Y, you can do the same. Kendall’s tau tallies how many pairs share the same relationship. If (<span class="math inline">\(X_{i}\)</span>&gt; <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(Y_{i} &gt; Y_{j}\)</span>) OR (<span class="math inline">\(X_{i}&lt; X_{j}\)</span> and <span class="math inline">\(Y_{i}&lt; Y_{j}\)</span>), then X and Y are considered concordant for that combination. If (<span class="math inline">\(X_{i}&gt; X_{j}\)</span> and <span class="math inline">\(Y_{i}&lt; Y_{j}\)</span>) OR (<span class="math inline">\(X_{i}&lt; X_{j}\)</span> and <span class="math inline">\(Y_{i}&gt; Y_{j}\)</span>), then X and Y are considered disconcordant for that combination.</p>
<p>This is easier to describe in practice:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="week-9-lecture.html#cb510-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4306</span>)</span>
<span id="cb510-2"><a href="week-9-lecture.html#cb510-2" tabindex="-1"></a>total <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, <span class="at">size =</span> <span class="dv">8</span>)</span>
<span id="cb510-3"><a href="week-9-lecture.html#cb510-3" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">A =</span> total[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">B =</span> total[<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>])</span>
<span id="cb510-4"><a href="week-9-lecture.html#cb510-4" tabindex="-1"></a>dat</span></code></pre></div>
<pre><code>##    A B
## 1 10 8
## 2 15 6
## 3 14 2
## 4  3 1</code></pre>
<p><span class="math display">\[
\tau = \frac{\#\text{concordant pairs} - \#\text{discordant pairs}}{0.5 n (n - 1)}
\]</span>
where the denominator is simply the number of possible combinations.</p>
<p>Once we calculate <span class="math inline">\(\tau\)</span>, how do we know if that value is significant or not? One method would be to do a randomization test. We can also use the following approximation for the test statistic for large (n&gt;10) sample sizes:</p>
<p><span class="math display">\[
\tau | H_0 \sim \mathrm{N} \left( 0, \frac{2(2n + 5)}{9n (n - 1)} \right)
\]</span>
In other words,</p>
<p><span class="math display">\[
\frac{\tau}{\sqrt{\frac{2(2n + 5)}{9n (n - 1)}}} \sim N(0,1)
\]</span>
so we can use the quantiles of the standard normal we are so familiar with. In practice, Pearson’s and Kendall’s rank correlation tests often give very very similar results (if not in the actual value, certainly in the inference derived from them).</p>
</div>
<div id="regression" class="section level2 hasAnchor" number="16.5">
<h2><span class="header-section-number">16.5</span> Regression<a href="week-9-lecture.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regression is a linear model (in the vein of those we introduced last week) in which a continuous response is modeled by one or more continuous predictors. (If we have only discrete predictors we call this ANOVA…the names are silly since they are all just linear models.)</p>
<p>Linear regression simply finds the straight line that “best fits” a scatterplot of (x,y) data.
We will note at the outset that there are two distinct kinds of regression.</p>
<p>“Type I” regression assumes that the purpose of the analysis is to use x to predict y. Therefore, we assume x is without uncertainty and we want to minimize the error with which we predict y given x.</p>
<p>“Type II” regression, or major axis regression (or standardized major axis regression), assumes a symmetry between X and Y and that the goal is in finding a correlation between them. Type II regression is often used when X (as well as Y) is measured with error, although see Smith (2009) for a full discussion of this interpretation.</p>
<p>We will start by talking about “Type I” regression which is much more common, and come back to discussing Type II regression later. I have assigned a reading for this week that goes over all of this is some more detail. Before discussing regression, it is important to note that there is a distinction between <strong>regression</strong> and <strong>correlation</strong>. In regression, you are using one or more variables to predict another variable because you believe there is a cause-and-effect relationship (X causes Y). Correlation does not imply the same cause-and-effect type relationship, it just addresses whether two variables are associated with one another (X and Y covary).</p>
<p><strong>Remember correlation does not imply causation. To address causation, you usually need to do some kind of a manipulative experiment.</strong></p>
<p>As a first start, we will discuss linear regression (We introduced this model briefly when we introduced linear models last week.)</p>
<p><span class="math display">\[
Y_{i} = \beta_{0}+\beta_{1}X_{i} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the <strong>intercept</strong> and <span class="math inline">\(\beta_{1}\)</span> is the <strong>slope</strong>. Note that each data point <span class="math inline">\(Y_{i}\)</span> is associated with its own value of the corresponding covariate predictor (<span class="math inline">\(X_{i}\)</span>) and its own sample for the residual <span class="math inline">\(\epsilon_{i}\)</span>. Before we get into some more math, let’s just draw some data and visualize what the slope and intercept represent. Note that we are usually just interested in the line (<span class="math inline">\(\beta_{0}+\beta_{1}X\)</span>) that represents the effect of the covariate on the response variable and therefore our focus is on estimating the parameters <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>. We are often less interested in the variance associated with <span class="math inline">\(\epsilon\)</span> but keep in mind that <span class="math inline">\(\sigma\)</span> is a third parameter of this model and also must be estimated from the data.</p>
<p>How do we find the line that best fits the data? Maximum likelihood!</p>
</div>
<div id="estimating-the-slope-and-intercept-in-linear-regression" class="section level2 hasAnchor" number="16.6">
<h2><span class="header-section-number">16.6</span> Estimating the slope and intercept in linear regression<a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The traditional “no calculus” explanation for estimating regression slope and intercept is included below for completeness, but I will bypass this more convoluted approach for now in favor of using maximum likelihood, which we already know how to do.</p>
<p>Remember back to the Week 4 lab, the joint likelihood for <span class="math inline">\(n\)</span> i.i.d. Normally distributed data <span class="math inline">\(Y \sim N(\mu,\sigma^{2})\)</span> was given by:</p>
<p><span class="math display">\[
L(\mu,\sigma|Y_{1},Y_{2},...,Y_{n}) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(Y_{i}-\mu)^{2}}{\sigma^{2}}\right)}
\]</span>
(Before, we were calling the response variable X, but now we will switch to calling the response variable Y since we will use X for the covariates.) Here we are making one tiny change, we are replacing <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\beta_{0}+\beta_{1}X_{i}\)</span> to fit the model <span class="math inline">\(Y \sim N(\beta_{0}+\beta_{1}X_{i},\sigma^{2})\)</span>.</p>
<p><span class="math display">\[
L(\beta_{0},\beta_{1},\sigma|Y_{1},Y_{2},...,Y_{n}) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(Y_{i}-(\beta_{0}+\beta_{1}X_{i}))^{2}}{\sigma^{2}}\right)}
\]</span></p>
<p>Great! Now we have a joint likelihood for the new linear regression model, and we can find the MLE for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> just like we did before, by setting</p>
<p><span class="math display">\[
\frac{\partial NLL}{\partial \beta_{0}} =0
\]</span>
and</p>
<p><span class="math display">\[
\frac{\partial NLL}{\partial \beta_{1}} =0
\]</span></p>
<p>The values of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> that satisfy this equation are the maximum likelihood estimators for these two parameters. (Note that we will use this approach [writing down the joint likelihood and then minimizing with respect to the parameters] when we come to generalized linear models in another week.)</p>
<p><strong>If</strong> these conditions are met, then the OLS estimators we just derived are the best linear unbiased estimators (BLUE) and the sampling distributions for the slope and intercept are as follows:</p>
<p><span class="math display">\[
\hat{\beta_0} \sim \mathrm{N}\left( \beta_0, \frac{\sigma_\epsilon^2 \sum_{i = 1}^n X_i^2}{n \sum_{i = 1}^n (X_i - \bar{X})^2}\right) \text{, where } \sigma_\epsilon^2 = \mathrm{E}(\epsilon_i^2)
\]</span>
<span class="math display">\[
\hat{\beta_1} \sim \mathrm{N}\left( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right) \text{, where } \sigma_\epsilon^2 = \mathrm{E}(\epsilon_i^2)
\]</span></p>
<p><em>FYI:</em> these distributions for <span class="math inline">\(\beta\)</span> can be derived using the second derivatives of the NLL (but you don’t need to be able to do this).</p>
<p>These equations may look complex, but there are some features that should make sense. One: The expected value of <span class="math inline">\(\hat{\beta_{0}}\)</span> is <span class="math inline">\(\beta_{0}\)</span>, as you would expect for an unbiased estimator. Moreover, if <span class="math inline">\(\sigma_\epsilon^2\)</span> goes to zero, the uncertainty in the estimates go to zero. In other words, if there is no variation in the data (all data points lie exactly on the regression line), than there is no uncertainty in the value of the parameters. Also, note that as <span class="math inline">\(n \rightarrow \infty\)</span>, the uncertainty goes to zero as well (again, as we would expect).</p>
<p>Because we do not know the population (true) error variance <span class="math inline">\(\sigma_\epsilon^2\)</span>, we estimate it from the data using:</p>
<p><span class="math display">\[
s_\epsilon^2 = \frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
\]</span></p>
<p>We substitute <span class="math inline">\(s_\epsilon^2\)</span> for <span class="math inline">\(\sigma_\epsilon^2\)</span>. This is an unbiased and maximally efficient estimator for <span class="math inline">\(\sigma_\epsilon^2\)</span>. <span class="math inline">\(p\)</span> is the number of parameters required to estimate <span class="math inline">\(\sigma_\epsilon^2\)</span>. Note that Aho calls this mean squared error (MSE) but most sources reserve that term for the actual mean of the squared errors, whereas here we want an unbiased estimate of the variance from the large pool of residuals our own sample of residuals are drawn from. (In other words, we have <span class="math inline">\(n\)</span> data points and so our model yields <span class="math inline">\(n\)</span> residuals; these <span class="math inline">\(n\)</span> values are just a sample from a larger population of residuals and <strong>that’s</strong> the variance we want to estimate here, so we have to divide by the degrees of freedom, which is <span class="math inline">\(n-p\)</span>. There is a bit more discussion of this terminology in the lab.)</p>
<p><strong>Question: How many degrees of freedom do we have for <span class="math inline">\(s_\epsilon^2\)</span> in a simple linear regression?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(n - p = n - 2\)</span>. This is because <span class="math inline">\(\hat{Y}\)</span> involves <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\bar{X}\)</span>. This will be different with multiple regression (regression with more than one covariate).
</span>
</details>
<p><strong>Question: What is our null hypothesis for the parameter <span class="math inline">\(\beta_0\)</span>? What does it mean?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\beta_0 | H_0 = 0\)</span>. When <span class="math inline">\(X = 0\)</span> the estimated value for <span class="math inline">\(Y\)</span> is 0.
</span>
</details>
<p><strong>Question: What about for <span class="math inline">\(\beta_1\)</span>?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
<span class="math inline">\(\beta_1 | H_0 = 0\)</span>. The slope of the regression equals 0, <span class="math inline">\(X\)</span> has no linear effect on <span class="math inline">\(Y\)</span>.
</span>
</details>
<p>The test statistic and standard error for estimating <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
T^* = \frac{\hat{\beta_1} - \beta_{1 | H_0}}{\text{SE}_{\hat{\beta_1}}} \sim t_{n - p}
\]</span></p>
<p><span class="math display">\[
\text{SE}_{\hat{\beta_1}} = \sqrt{\frac{\frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}} = \sqrt{\frac{s_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}}
\]</span>
Note that usually we are interested in <span class="math inline">\(\beta_{1|H_{0}}=0\)</span>.</p>
<p>We construct confidence intervals as we always have</p>
<p><span class="math display">\[
P {\left( \hat{\beta_1 } - t_{(1 - \frac{\alpha}{2}) [n - p]} \text{SE}_{\hat{\beta_1}} \leq \beta_1 \leq \hat{\beta_1 } + t_{(1 - \frac{\alpha}{2}) [n - p]} \text{SE}_{\hat{\beta_1}} \right)} = 1 - \alpha
\]</span></p>
<p>You can use similar methods to estimate the CI for <span class="math inline">\(\beta_0\)</span>. We can use bootstrapping to calculate the standard error of the regression slope as well. We demonstrate this with an example on Thursday.</p>
<p><strong>Question: I’ve called this value <span class="math inline">\(\text{SE}_{\hat{\beta_1}}\)</span>. Aho refers to it as <span class="math inline">\({\hat{\sigma}_{\hat{\beta_1}}}\)</span>. Why are these names interchangeable?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The standard deviation of an estimated parameter (here, <span class="math inline">\(\hat{\beta}_1\)</span>) is equal to the standard error of the parameter.
</span>
</details>
</div>
<div id="ok-now-the-other-derivation-for-slope-and-intercept" class="section level2 hasAnchor" number="16.7">
<h2><span class="header-section-number">16.7</span> OK, now the “other” derivation for slope and intercept<a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here I present an alternative method for estimating the slope and intercept values that does not require knowing about maximum likelihood. As you will see, it is a little convoluted, but it does define some key vocabulary along the way, so here goes…</p>
<p>It probably makes intuitive sense that the mean value of X should be associated with the mean value of Y and this is in fact true. The best fit line passes through (<span class="math inline">\(\bar{X}\)</span>,<span class="math inline">\(\bar{Y}\)</span>). </p>
<p>First, we will go over the notation for linear models (and regression specifically) we’ll be using for the next couple of weeks.</p>
<table>
<colgroup>
<col width="19%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th align="center">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td align="center">data/response variable/dependent variable</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y_i\)</span></td>
<td align="center">value of <span class="math inline">\(i^\text{th}\)</span> data point</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bar{Y}\)</span></td>
<td align="center">mean of data</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y}_i\)</span></td>
<td align="center">estimate from the model for point <span class="math inline">\(i\)</span>. In regression, this is <span class="math inline">\(\hat{Y}_i = \beta_0 + \beta_1 X_i\)</span></td>
</tr>
</tbody>
</table>
<p>To make any further progress with this question, we need to “partition the variance”. We will not tackle this seriously until we get to ANOVA next week, but the basic idea is fairly straightforward.</p>
<p><img src="Regression.png" /></p>
<p>To determine how to specifically fit slope and intercept parameters, we need to talk about partitioning variance (a goal of modeling in general, and a major part of ANOVA). When modeling, variation is either explained or unexplained.
Let’s start from the beginning with this dataset of the relative length of spider webs. Of course, our data are not all exactly the same value, there is some variation:</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="week-9-lecture.html#cb512-1" tabindex="-1"></a><span class="fu">data</span>(webs)</span>
<span id="cb512-2"><a href="week-9-lecture.html#cb512-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span>  <span class="fu">theme</span>(<span class="at">axis.title.x =</span> <span class="fu">element_blank</span>(), <span class="at">axis.text.x =</span> <span class="fu">element_blank</span>(), <span class="at">axis.ticks.x =</span> <span class="fu">element_blank</span>(), <span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-11-1.png" width="288" /></p>
<p>Next, we fit a model with two parameters, one parameter to describe the mean behavior of the system and one parameter to describe the amount of variation around that mean, e.g., <span class="math inline">\(Y \sim \mathrm{N}(\mu, \sigma^2)\)</span>, where we estimate <span class="math inline">\(\mu\)</span> using <span class="math inline">\(\overline{Y}\)</span> (there are two parameters, but only <span class="math inline">\(\mu\)</span> describes the mean). Using this parameter <span class="math inline">\(\mu\)</span>, we have explained some of that variation in our data.</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="week-9-lecture.html#cb513-1" tabindex="-1"></a>y.bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(webs[, <span class="dv">2</span>])</span>
<span id="cb513-2"><a href="week-9-lecture.html#cb513-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> </span>
<span id="cb513-3"><a href="week-9-lecture.html#cb513-3" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(length)), <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span> </span>
<span id="cb513-4"><a href="week-9-lecture.html#cb513-4" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&#39;text&#39;</span>, <span class="at">x =</span> <span class="fl">0.01</span>, <span class="at">y =</span> y.bar<span class="fl">+0.0001</span>, <span class="at">label =</span> <span class="st">&quot;bar(Y)&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">col =</span><span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb513-5"><a href="week-9-lecture.html#cb513-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb513-6"><a href="week-9-lecture.html#cb513-6" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title.x =</span> <span class="fu">element_blank</span>(), <span class="at">axis.text.x =</span> <span class="fu">element_blank</span>(), <span class="at">axis.ticks.x =</span> <span class="fu">element_blank</span>(), <span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-12-1.png" width="288" /></p>
<p>Now, we might be able to explain more of this variation in the data if we add a covariate to the mean, so as to allow the mean to vary. This will “capture” some of the variability in the data and will leave less unexplained variation to be assigned to <span class="math inline">\(\sigma\)</span>. There are three total parameters in this more complicated model, two that describe the mean and one to describe the <strong>residual variation</strong>. (The parameter <span class="math inline">\(\sigma^2\)</span> is playing the same role in the model as before, but because we hope that at least some of the variation is explained by the covariate, we now refer to whats left as <strong>residual variation</strong>.)</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="week-9-lecture.html#cb514-1" tabindex="-1"></a>web.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> length <span class="sc">~</span> temp.C, <span class="at">data =</span> webs)</span>
<span id="cb514-2"><a href="week-9-lecture.html#cb514-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> </span>
<span id="cb514-3"><a href="week-9-lecture.html#cb514-3" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(length)), <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span> </span>
<span id="cb514-4"><a href="week-9-lecture.html#cb514-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">slope =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;palegreen1&quot;</span>) <span class="sc">+</span></span>
<span id="cb514-5"><a href="week-9-lecture.html#cb514-5" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&#39;text&#39;</span>, <span class="at">x =</span> <span class="dv">12</span>, <span class="at">y =</span> y.bar, <span class="at">label =</span> <span class="st">&quot;bar(Y)&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">col =</span><span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb514-6"><a href="week-9-lecture.html#cb514-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Temperature (in C)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-13-1.png" width="480" /></p>
<p>The total amount of variation to be explained is called the “SST” for “sums of squares total”.</p>
<p><span class="math display">\[
\text{Sums of squares total} = \sum_{i = 1}^n (Y_i - \bar{Y})^2
\]</span></p>
<p>(<span class="math inline">\(n\)</span> is the total number of data points.)</p>
<p>We can partition this total sums of squares into the component explain by the regression model and that which is left over as unexplained variation (which we often refer to as “error”, but it is “error” in the sense that it is not explained in the model and not “error” in the sense that we have done something wrong). Note that different sources/books will use different acronyms for partitioning variation and you should not be too invested in the notation I am presenting here. The important thing is to remember the equations and the idea behind partitioning variation. The <strong>sum of squares regression</strong> is the amount of variation expained by the regression line, or the squared deviations from the points estimated in the regression <span class="math inline">\(\hat{Y}_i\)</span>, which is described by <span class="math inline">\(\hat{Y}_i = \hat{\beta_0} + \hat{\beta_1} X_i\)</span>, and the estimated mean <span class="math inline">\(\bar{Y}\)</span>:</p>
<p><span class="math display">\[
\text{SSR} = \sum_{i = 1}^n{(\hat{Y}_i - \bar{Y})^2}
\]</span></p>
<p>You can think of the sum of squares regression as the amount of variation explained when going from a one parameter model describing the mean behavior (<span class="math inline">\(\mu\)</span> only) to the regression model (here, two parameters describing the mean behavior of the model, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="week-9-lecture.html#cb515-1" tabindex="-1"></a>x.i <span class="ot">&lt;-</span> webs[<span class="dv">36</span>, <span class="dv">3</span>]</span>
<span id="cb515-2"><a href="week-9-lecture.html#cb515-2" tabindex="-1"></a>y.i.hat <span class="ot">&lt;-</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> x.i</span>
<span id="cb515-3"><a href="week-9-lecture.html#cb515-3" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> </span>
<span id="cb515-4"><a href="week-9-lecture.html#cb515-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">x =</span> x.i, <span class="at">y =</span> y.i.hat, <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb515-5"><a href="week-9-lecture.html#cb515-5" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="at">x =</span> x.i <span class="sc">+</span> <span class="fl">1.5</span>, <span class="at">y =</span> y.i.hat, <span class="at">label =</span> <span class="st">&quot;hat(Y[i])&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb515-6"><a href="week-9-lecture.html#cb515-6" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(length)), <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span> </span>
<span id="cb515-7"><a href="week-9-lecture.html#cb515-7" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">slope =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;palegreen1&quot;</span>) <span class="sc">+</span></span>
<span id="cb515-8"><a href="week-9-lecture.html#cb515-8" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="dv">12</span>, <span class="at">y =</span> <span class="fl">0.9987</span>, <span class="at">label =</span> <span class="st">&quot;bar(Y)&quot;</span>, <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">col =</span><span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb515-9"><a href="week-9-lecture.html#cb515-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Temperature (in C)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-14-1.png" width="480" /></p>
<p>The <strong>sum of squares error</strong> is the amount of variation <em>not</em> expained by the regression line, or the squared deviations from the actual data points themselves, <span class="math inline">\(Y_i\)</span>, and the points estimated by the regression <span class="math inline">\(\hat{Y}_i\)</span>:</p>
<p><span class="math display">\[
\text{SSE} = \sum_{i = 1}^n{(Y_i - \hat{Y}_i)^2}
\]</span></p>
<p>The sum of squares error is the remaining (unexplained, residual) error after modeling.</p>
<p>Again, <span class="math inline">\(\hat{Y}_i = \beta_0 + \beta_1 X_i\)</span>, or the <em>Y</em> value predicted by the regression model. The best fit line minimizes the sum of squares error. All errors (residuals) are the <strong>vertical</strong> distances from the data points to the points estimated in the regression. This is because all error exists with respect to the response variable (here, <span class="math inline">\(Y\)</span>) only. <em>Note that there all alternatives to minimizing the sum of squares error that we will briefly discuss later.</em></p>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-15-1.png" width="480" /></p>
<p>To estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we could use multiple approaches. We could use the method we have been discussing, minimizing the sums of squares error, which is called <strong>ordinary least squares</strong> (OLS). We could also use maximum likelihood. As long as the assumptions of regression are met, these two methods are equivalent.</p>
<p>The best fit line will pass through <span class="math inline">\((\bar{X}, \bar{Y})\)</span> because this is the “center mass” of the data and because we assumed the expected value of the errors is zero. We use this fact (and some algebra) to show:</p>
<p><span class="math display">\[
\bar{Y} = \hat{\beta_0} + \hat{\beta_1} \bar{X}
\]</span></p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{Y} - \hat{\beta_1} \bar{X}
\]</span></p>
<p>Then, we solve for <span class="math inline">\(\hat{\beta_1}\)</span> by plugging in what we know back into the equation for SSE:</p>
<p><span class="math display">\[
\text{SSE} = \sum_{i = 1}^n{(Y_i - \hat{Y}_i)^2} = \sum_{i = 1}^n{(Y_i - \hat{\beta_0} - \hat{\beta_1} X_i)^2} = \sum_{i = 1}^n{(Y_i - \bar{Y} + \hat{\beta_1} \bar{X} - \hat{\beta_1} X_i)^2}
\]</span></p>
<p>Then we only have one unknown left <span class="math inline">\(\text{SSE} = f(\hat{\beta_1})\)</span>, so we can minimize the function by taking the derivative with respect to <span class="math inline">\(\hat{\beta_1}\)</span> and set it equal to 0.</p>
<p><span class="math display">\[
\frac{\partial \text{SSE}}{\partial \hat{\beta_1}} = 0
\]</span></p>
<p>After working this out,</p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i = 1}^n (X_i - \bar{X})^2}
\]</span></p>
<p>Try this derivation yourself and then check your work <a href="http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf">here</a>.</p>
</div>
<div id="assumptions-of-regression" class="section level2 hasAnchor" number="16.8">
<h2><span class="header-section-number">16.8</span> Assumptions of regression<a href="week-9-lecture.html#assumptions-of-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math display">\[
Y_{i} = \beta_{0}+\beta_{1}X_{i} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
\]</span></p>
<ol style="list-style-type: decimal">
<li>A linear model appropriately described the relationship between X and Y.</li>
<li>For any given value of X, the sampled Y values are independent with normally distributed errors.
We can express this assumption as follows:</li>
</ol>
<p><span class="math inline">\(E[\epsilon_{i}]=0\)</span></p>
<p><span class="math inline">\(E[(\epsilon_{i})^{2}]=\sigma_{\epsilon}^{2}\)</span></p>
<p><span class="math inline">\(E[(\epsilon_{i})(\epsilon_{j})]=0 \mbox{, where } i \neq j\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Variances are constant along the regression line. (Non-constant variances are an example of heteroskedacity. More on this in a second.)</li>
<li>An implicit assumption is that the regression fit is only valid over the range of X represented in the original data. It is very dangerous to extrapolate outside the range over which the regression line was originally fit.</li>
</ol>
<p>Our model allows us to make a prediction about the response expected for any given value of X, which may be a value of X in the original dataset or it may be a value of X <strong>not</strong> in the original dataset. (The model is only valid for X values contained within the range of the original data; more on this later.) We will refer to a new X value as <span class="math inline">\(X^{*}\)</span> and the corresponding estimates for Y as <span class="math inline">\(\hat{Y}\)</span>. Don’t forget that the fitted <span class="math inline">\(\hat{Y}\)</span> values are also estimated quantities, and as such come with uncertainty (i.e. standard errors and confidence intervals). It turns out there are really two ways to express “confidence”: one of which we call a “confidence interval”, the other we call a “prediction interval”.</p>
</div>
<div id="confidence-vs.-prediction-intervals" class="section level2 hasAnchor" number="16.9">
<h2><span class="header-section-number">16.9</span> Confidence vs. Prediction intervals<a href="week-9-lecture.html#confidence-vs.-prediction-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One way is to ask about variability in our predicted values is: “What are the 1-<span class="math inline">\(\alpha\)</span> confidence intervals on the <strong>mean</strong> value of Y given <span class="math inline">\(X^{*}\)</span>”? Equivalently, confidence intervals for <span class="math inline">\(\hat{Y}\)</span> mean, “there is a 95% chance that my 95th percentile CIs include the true underlying population parameter value, which is the <strong>mean value</strong> of <span class="math inline">\(\hat{Y}\)</span> (<span class="math inline">\(\mathrm{E}[Y|X^{*}]\)</span>).”</p>
<p>We calculate the CIs using</p>
<p><span class="math display">\[
\hat{Y^*} \pm t_{(1 - \frac{\alpha}{2}) [n - p]} \sqrt{s_\epsilon^2 {\left(\frac{1}{n} + \frac{(X^* - \bar{X})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right)}}
\]</span></p>
<p>Note the notation here. The above equation represents the confidence intervals on <span class="math inline">\(Y\)</span> associated with the value <span class="math inline">\(X^*\)</span>. The “hat” on Y reflects the fact that Y is an estimate based on the data, and the “star” is just meant to remind you that this interval is associated with the specific X value <span class="math inline">\(X^*\)</span>.</p>
<p>However, there is a second way to think about uncertainty in the response value. In stead of wanting to know our uncertainty in the MEAN predicted value, we may want an estimate for the variation expected for a future value of <span class="math inline">\(Y\)</span> for a given <span class="math inline">\(X^{*}\)</span>. Our estimates of uncertainty for the predicted values take into account both the mean of <span class="math inline">\(\hat{Y_h}\)</span> (uncertainty in the regression line itself) as well as the variance in <span class="math inline">\(\hat{Y_h}\)</span>. These are called <strong>prediction intervals</strong> (PIs).</p>
<p>We calculate PIs using</p>
<p><span class="math display">\[
\hat{Y^*} \pm t_{(1 - \frac{\alpha}{2}) [n - p]} \sqrt{s_\epsilon^2 {\left( 1 + \frac{1}{n} + \frac{(X^* - \bar{X})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right)}}
\]</span></p>
<p><strong>Question: What is the difference between these two formulas? Does it make sense why they are different?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
The formula for the prediction interval has a 1 in the parenthesis, which is associated with the addition of <span class="math inline">\(s^{2}_{\epsilon}\)</span>. This should make sense because it is the residual uncertianty that is being added to the uncertainty in the best fitting line itself. Prediction intervals are always wider than confidence intervals.
</span>
</details>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="week-9-lecture.html#cb516-1" tabindex="-1"></a>webs<span class="sc">$</span>CIl <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb516-2"><a href="week-9-lecture.html#cb516-2" tabindex="-1"></a>webs<span class="sc">$</span>CIu <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[, <span class="dv">3</span>]</span>
<span id="cb516-3"><a href="week-9-lecture.html#cb516-3" tabindex="-1"></a>webs<span class="sc">$</span>PIl <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb516-4"><a href="week-9-lecture.html#cb516-4" tabindex="-1"></a>webs<span class="sc">$</span>PIu <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[, <span class="dv">3</span>]</span>
<span id="cb516-5"><a href="week-9-lecture.html#cb516-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> </span>
<span id="cb516-6"><a href="week-9-lecture.html#cb516-6" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIl), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-7"><a href="week-9-lecture.html#cb516-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIu), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-8"><a href="week-9-lecture.html#cb516-8" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIl), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-9"><a href="week-9-lecture.html#cb516-9" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIu), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-10"><a href="week-9-lecture.html#cb516-10" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">slope =</span> web.fit<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;palegreen1&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-11"><a href="week-9-lecture.html#cb516-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Temperature (in C)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-16-1.png" width="576" /></p>
<p>Remember: Confidence intervals represent our uncertainty about where the line should be (the mean behavior). Prediction intervals represent our certainty about the predicted values you could expect (the mean behavior + the variance).</p>
<p>We can confirm the interpretation of the confidence interval by bootstrapping our data and refitting a line each time.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="week-9-lecture.html#cb517-1" tabindex="-1"></a>webs<span class="sc">$</span>CIl <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb517-2"><a href="week-9-lecture.html#cb517-2" tabindex="-1"></a>webs<span class="sc">$</span>CIu <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[, <span class="dv">3</span>]</span>
<span id="cb517-3"><a href="week-9-lecture.html#cb517-3" tabindex="-1"></a>webs<span class="sc">$</span>PIl <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb517-4"><a href="week-9-lecture.html#cb517-4" tabindex="-1"></a>webs<span class="sc">$</span>PIu <span class="ot">&lt;-</span> <span class="fu">predict</span>(web.fit, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[, <span class="dv">3</span>]</span>
<span id="cb517-5"><a href="week-9-lecture.html#cb517-5" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> webs, <span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> length)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;gray37&quot;</span>) <span class="sc">+</span> </span>
<span id="cb517-6"><a href="week-9-lecture.html#cb517-6" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIl), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-7"><a href="week-9-lecture.html#cb517-7" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIu), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-8"><a href="week-9-lecture.html#cb517-8" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIl), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-9"><a href="week-9-lecture.html#cb517-9" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIu), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-10"><a href="week-9-lecture.html#cb517-10" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Temperature (in C)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Relative length of spider webs&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> text.size))</span>
<span id="cb517-11"><a href="week-9-lecture.html#cb517-11" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)</span>
<span id="cb517-12"><a href="week-9-lecture.html#cb517-12" tabindex="-1"></a>{</span>
<span id="cb517-13"><a href="week-9-lecture.html#cb517-13" tabindex="-1"></a>  selection<span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(webs),<span class="at">replace=</span>T)</span>
<span id="cb517-14"><a href="week-9-lecture.html#cb517-14" tabindex="-1"></a>  webs.bootstrapped<span class="ot">&lt;-</span>webs[selection,]</span>
<span id="cb517-15"><a href="week-9-lecture.html#cb517-15" tabindex="-1"></a>  web.fit.new <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> length <span class="sc">~</span> temp.C, <span class="at">data =</span> webs.bootstrapped)</span>
<span id="cb517-16"><a href="week-9-lecture.html#cb517-16" tabindex="-1"></a>  p<span class="ot">&lt;-</span>p<span class="sc">+</span><span class="fu">geom_abline</span>(<span class="at">intercept =</span> web.fit.new<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">slope =</span> web.fit.new<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;palegreen1&quot;</span>)</span>
<span id="cb517-17"><a href="week-9-lecture.html#cb517-17" tabindex="-1"></a>}</span>
<span id="cb517-18"><a href="week-9-lecture.html#cb517-18" tabindex="-1"></a>p<span class="ot">&lt;-</span>p<span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIl), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-19"><a href="week-9-lecture.html#cb517-19" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> CIu), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue1&quot;</span>)  <span class="sc">+</span></span>
<span id="cb517-20"><a href="week-9-lecture.html#cb517-20" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIl), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) <span class="sc">+</span></span>
<span id="cb517-21"><a href="week-9-lecture.html#cb517-21" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> temp.C, <span class="at">y =</span> PIu), <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="at">col =</span> <span class="st">&quot;slateblue1&quot;</span>) </span>
<span id="cb517-22"><a href="week-9-lecture.html#cb517-22" tabindex="-1"></a><span class="fu">plot</span>(p)</span></code></pre></div>
<p><img src="Week-9-lecture_files/figure-html/unnamed-chunk-17-1.png" width="576" /></p>
<p>Notice that the confidence intervals “capture” the range of possible regression lines created by fitting a linear model to the bootstrapped datasets (each “playing the role”” of a re-do of the original data collection.)</p>
<p><strong>Question: Why are confidence and prediction intervals “bow-tie” shaped, i.e. wider further away from the middle of the line?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
Mathematically, the squared deviation from <span class="math inline">\(\bar{X}\)</span> is greater as you move away from <span class="math inline">\(\bar{X}\)</span>, which increases <span class="math inline">\(\text{SE}_{\hat{Y}}\)</span>. Intuitively, we can think about it this way: Because the best fitting line <em>has</em> to go through (<span class="math inline">\(\bar{X},\bar{Y}\)</span>), the confidence interval is narrowest at this point. Away from (<span class="math inline">\(\bar{X},\bar{Y}\)</span>), we have the additional uncertainty about what the slope should be moving away from (<span class="math inline">\(\bar{X},\bar{Y}\)</span>), and so the intervals “grow” as you move away from this point.
</span>
</details>
</div>
<div id="how-do-we-know-if-our-model-is-any-good" class="section level2 hasAnchor" number="16.10">
<h2><span class="header-section-number">16.10</span> How do we know if our model is any good?<a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Earlier we determined that we fit our regression by minimizing the sum of squares error (SSE). However, SSE depends on the units of the dependent variable, so that doesn’t help us decide if the model fits “well” (e.g., the model could be a best fit line, but still not very good). However, we can use the total variation, partitioned into 1) the explained variation in the regression, SSR, and 2) the unexplained residual variance, SSE, to create a useful metric of model fit.</p>
<p><span class="math display">\[
\sum_{i = 1}^n (Y_i - \bar{Y})^2 = \sum_{i = 1}^n (\hat{Y_i} - \bar{Y})^2 + \sum_{i = 1}^n (Y_i - \hat{Y}_i)^2
\]</span>
On the left side is the total sum of squares (SST), or how much each data point differs from the mean. The first term on the right is the sum of squares regression (SSR), or the amount of variation explained by the regression. The final term on the right is the sum of squares error (SSE), or how much each data point deviates from the best fit line.</p>
<p>The <strong>coefficient of determination</strong> <span class="math inline">\(R^2\)</span> is the proportion of variation in <span class="math inline">\(Y\)</span> explained by the regression model. Think of it like the ratio of variance explained relative to the total amount of variation in the data.</p>
<p><span class="math display">\[
R^2 = \frac{\text{SSR}}{\text{SST}}
\]</span></p>
<p>The coefficient of determination ranges from 0 to 1. It is equivalent to the square of Pearson’s <span class="math inline">\(r\)</span>. High <span class="math inline">\(R^2\)</span> means that the model has explanatory power. However, it is not evidence that the relationship is causal.</p>
<p>Lets go back to our calculations, and think about how many degrees of freedom we had for each</p>
<ol style="list-style-type: decimal">
<li><p>Total sum of squares = SST = <span class="math inline">\(\sum_{i=1}^{n}(Y_{i}-\bar{Y})(Y_{i}-\bar{Y})\)</span>. This required one parameter <span class="math inline">\(\bar{Y}\)</span>, be estimated from the data, so there remain n-1 degrees of freedom.</p></li>
<li><p>Sum of squared error = SSE = <span class="math inline">\(\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})(Y_{i}-\hat{Y_{i}})\)</span>. This required estimation of <span class="math inline">\(\hat{Y_{i}}\)</span>, which used two estimated parameters (slope and intercept) <span class="math inline">\(\rightarrow\)</span> n-2 d.o.f. remain.</p></li>
<li><p>Regression model sum of squares SSR: The regression d.o.f. reflect the number of <em>extra</em> parameters involved in going from the null model (<span class="math inline">\(\bar{Y}\)</span>) to the regression model, which in this case is 1.</p></li>
</ol>
<p>##Robust regression</p>
<p>Ordinary Least Squares regression as defined above is fairly sensitive to outliers, which can be seen in the definition of SSE:</p>
<p><span class="math display">\[
SSE = \sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
\]</span>
Because we square the residuals, points that are far off the regression line have more influence on the estimates of intercept and slope than points that are much closer to the best fit line. There are several methods that are more robust against outliers. Not surprisingly, these fall under the heading of “robust regression”.</p>
<p>The most common robust regression approach is called M-estimation. <strong>M-estimation</strong> minimizes any monotonic function of the error, allowing for methods other than squared error. These functions may associate less of a “cost” with having a large residual, or they may even have a saturation point which defines the maximum influence any point can have.</p>
<p>In other words, the metric that is being minimized can be any function f() of the difference between each point and the best-fit line:</p>
<p><span class="math display">\[
\sum_{i = 1}^n f(Y_i - \hat{Y_i})
\]</span>
Visualizing M-estimation (these are four functions, <span class="math inline">\(\rho\)</span>, that can be used to model the error, <span class="math inline">\(x\)</span>):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="RhoFunctions.png" alt="Possible shapes for the weighting function in robust regression. Source: Wikimedia Commons" width="50%" />
<p class="caption">
Figure 16.1: Possible shapes for the weighting function in robust regression. Source: Wikimedia Commons
</p>
</div>
<p>We won’t get into the mathematics, because they are complex (another benefit of least squares is simpler math) but it’s an avenue you may use in the future and is good to know about.</p>
</div>
<div id="robust-regression" class="section level2 hasAnchor" number="16.11">
<h2><span class="header-section-number">16.11</span> Robust regression<a href="week-9-lecture.html#robust-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes the relationship between X and Y is not linear and cannot be made linear using a transformation of the variables. In this case, you can carry out a non-parametric test for regression. Here we will discuss Kendall’s robust line fitting method.</p>
<p>Step 1 – Order the x and y values in order of increasing <span class="math inline">\(X\)</span> value
Step 2 – Compute the slope between every pair of <span class="math inline">\(X\)</span> values</p>
<p><span class="math display">\[
\mbox{Slope between i and j} = \frac{Y_{j}-Y_{i}}{X_{j}-X_{i}}
\]</span>
Step 3 –</p>
<p><span class="math display">\[
\widehat{\mbox{Slope}}_{non-parameteric} = \mbox{median of slopes}
\]</span>
<span class="math display">\[
\widehat{\mbox{Intercept}}_{non-parameteric} = \mbox{median of } Y_{i}-\widehat{\mbox{Slope}}_{non-parametric}X_{i}
\]</span>
<strong>Question: How to know if the slope is significant?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">Use Kendall’s rank correlation test.
</span>
</details>
</div>
<div id="type-i-and-type-ii-regression" class="section level2 hasAnchor" number="16.12">
<h2><span class="header-section-number">16.12</span> Type I and Type II Regression<a href="week-9-lecture.html#type-i-and-type-ii-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have only discussed Type I regression up to this point. With Type I regression, we use <span class="math inline">\(X\)</span> to predict <span class="math inline">\(Y\)</span>, and have only minimized the error in <span class="math inline">\(Y\)</span>. This means that we have assumed that there is no error in <span class="math inline">\(X\)</span> (<span class="math inline">\(X\)</span> is fixed and not random).</p>
<p>However, in many cases there could be error in <span class="math inline">\(X\)</span>. <strong>Major axis regression</strong> and <strong>standardized major axis regression</strong> (Type II or Model II regressions) model variance in both the explanatory and response variables. Importantly, however, these methods to not strictly predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span>, but rather calculate the major axis of variation in the bivariate dataset <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This means Type II regression models are not suggested for prediction.</p>
<p>Major axis regression places equal weight on deviations in the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> directions and minimizes the sum of squared <strong>perpendicular distances</strong> to the best fit line. The major axis regression line is the first principal component from PCA (stay tuned). This requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are measured on the same scale, using the same units.</p>
<p>Standardized major axis regression standardizes each variable by its own variance and then fits a best fit line, so that deviations in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are weighted accordingly.</p>
<p>When is MA and SMA used? (1) In tests of isometry, allometry, stoichiometry (is the relationship between X and Y 1? Is the ratio of C:N 1?), (2) Testing if two methods of measurement agree.</p>
</div>
<div id="W9FAQ" class="section level2 hasAnchor" number="16.13">
<h2><span class="header-section-number">16.13</span> Week 9 FAQ<a href="week-9-lecture.html#W9FAQ" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Question: Why do we divide by n-1 in the denominator of the sample covariance?</strong></p>
<p>The sample covariance is given by</p>
<p><span class="math display">\[
Cov(A,B) = \frac{1}{n-1}\sum_{i=1}^{n}(A_{i}-\bar{A})(B_{i}-\bar{B})
\]</span>
The best way of thinking about this is as follows: We start with 2n degrees of freedom, n for the A data and n for the B data. Where we lose degrees of freedom may be seen by re-writing the product in the sum as follows:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(A_{i}-\bar{A})(B_{i}-\bar{B})=\sum_{i=1}^{n}A_{i}(B_{i}-\bar{B})-\sum_{i=1}^{n}\bar{A}(B_{i}-\bar{B})
\]</span></p>
<p>We have terms on the left hand side, and lose one degree of freedom for the use of <span class="math inline">\(\bar{B}\)</span> in each term (for a total of <span class="math inline">\(n\)</span> d.o.f. lost) and we lose one additional d.o.f. for the one use of ̅<span class="math inline">\(\bar{B}\)</span> in the second term. Therefore we end up with <span class="math inline">\(2n-n-1=n-1\)</span> degrees of freedom for the whole product.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-8-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-9-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
