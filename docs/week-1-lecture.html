<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Week 1 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="1 Week 1 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Week 1 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Week 1 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-04-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="week-1-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#t-test"><i class="fa fa-check"></i><b>10.1</b> t-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.2</b> F-test</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.4" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.4</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#centering-the-covariates"><i class="fa fa-check"></i><b>17.3</b> Centering the covariates</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.4</b> Weighted regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.5</b> Robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.6</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.7" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.7</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-1-lecture" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Week 1 Lecture<a href="week-1-lecture.html#week-1-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-1-readings" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Week 1 Readings<a href="week-1-lecture.html#week-1-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest Aho Chapter 2, and Sections 4.1, 4.2, 4.3.1, 4.3.2, 4.3.3, 4.3.4, 4.3.7, and 4.5. Logan Chapter 1 will help you get started in R if you are not already familiar with it.</p>
<p>I also recommend reviewing the <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Algebra_rules.pdf">Algebra Rules sheet</a>.</p>
</div>
<div id="basic-outline" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Basic Outline<a href="week-1-lecture.html#basic-outline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>First half:
- R
- bootstrap, jackknife, and other randomization techniques
- hypothesis testing
- probability distributions
- the “classic tests” of statistics
- graphical analysis of data</p>
<p>Second half:</p>
<ul>
<li>regression (incl. ANOVA, ANCOVA)</li>
<li>model building</li>
<li>model criticism</li>
<li>non-linear regression</li>
<li>multivariate regression</li>
</ul>
<p>Class Structure</p>
<ul>
<li><p>Lecture on Tuesday</p></li>
<li><p>“Lab”” on Thursday</p></li>
<li><p>Problem sets are posted on Wednesdays (feel free to remind me via Slack if I forget), and are due <em>before lecture</em> the following Tuesday. This deadline is very strict, no exceptions. Turn in what you have before 8:00 am on Tuesday, even if its not complete.</p></li>
</ul>
<p>Communication</p>
<ul>
<li><p>Use slack!</p></li>
<li><p>Come to (both) office hours</p></li>
<li><p>I have instituted a new participation component to Biometry’s grading, see syllabus.</p></li>
</ul>
</div>
<div id="todays-agenda" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Today’s Agenda<a href="week-1-lecture.html#todays-agenda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Basic probability theory</li>
<li>An overview of univariate distributions</li>
<li>Calculating the expected value of a random variable</li>
<li>A brief introduction to the Scientific Method</li>
<li>Introduction to statistical inference</li>
</ol>
</div>
<div id="basic-probability-theory" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Basic Probability Theory<a href="week-1-lecture.html#basic-probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s imagine that we have a bag with a mix of regular and peanut M&amp;Ms. Each M&amp;M has two traits: Color and Type.</p>
<p><span class="math display">\[ \sum_{all \: colors} P(color) = 1 \]</span>
<span class="math display">\[ \sum_{all \: types} P(types) = ? \]</span>
Note that the probabilities have to sum to 1.0 and that the probabilities also have to be non-negative. It turns out these are the only two requirements for a “legal” probability function. (Here we have discrete categories and so the probabilities add to one through a straightforward summation. The distribution of probabilities across categories is called the probability <em>mass</em> function. If these were continuous probabilities, like the distribution describing the weight of each M&amp;M, the distribution would be called a probability <em>density</em> function and we would have to integrate [the continuous version of a sum] over all possible weights. Either way, the sum [or integral] over all possible values of the variable has to equal 1.0.)</p>
<p>The complement (indicated by a superscript C) of a trait represents every object that does <em>not</em> have that trait, so the probability of the complement to Green is the probability of getting an M&amp;M that is <em>anything but green</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="comp.png" alt="Red shading represents the complement. Source: Wikimedia Commons" width="25%" />
<p class="caption">
Figure 0.1: Red shading represents the complement. Source: Wikimedia Commons
</p>
</div>
<p><span class="math display">\[ P(Green^c) = 1 - P(Green)  \]</span></p>
<div id="intersection" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Intersection<a href="week-1-lecture.html#intersection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="inter.png" alt="Red shading represents the intersection. Source: Wikimedia Commons" width="25%" />
<p class="caption">
Figure 1.1: Red shading represents the intersection. Source: Wikimedia Commons
</p>
</div>
<p>Now let’s pull one M&amp;M out of the bag. <em>If</em> the color distribution of chocolate M&amp;Ms and peanut M&amp;Ms is the same, then these two traits are independent, and we can write the probability of being <em>both</em> Green and Peanut as</p>
<p><span class="math display">\[ P(Green \: AND \:  Peanut) = P(Green \cap Peanut) = P(Green) \cdot P(Peanut) \]</span></p>
<p>This is called a Joint Probability and we usually write it as <span class="math inline">\(P(Green,Peanut)\)</span>. <em>This only works if these two traits are independent of one another.</em> If color and type were not independent of one another, we would have to calculate the joint probability differently, but in the vast majority of cases we are working with data that we assuming are independent of one another. In these cases, the joint probability is simply the product of all the individual probabilities.</p>
<p>Note that</p>
<p><span class="math display">\[ P(Green \: AND \:  Blue) = P(Green \cap Blue) = 0\]</span>
because an M&amp;M cannot be Green and Blue at the same time. in this case,</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="EmptySet.png" alt="In the case of two different colors, the intersection is empty." width="25%" />
<p class="caption">
Figure 1.2: In the case of two different colors, the intersection is empty.
</p>
</div>
</div>
<div id="union" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Union<a href="week-1-lecture.html#union" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="union.png" alt="Red shading represents the union. Source: Wikimedia Commons" width="25%" />
<p class="caption">
Figure 1.3: Red shading represents the union. Source: Wikimedia Commons
</p>
</div>
<p><span class="math display">\[ \begin{align*}
P(Green \: OR \: Peanut) &amp;= P(Green \cup Peanut) \\
&amp;= P(Green) + P(Peanut) - P(Green \cap Peanut) \end{align*} \]</span></p>
<p><strong>Question: Why do we have to subtract off the intersection?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
If we do not subtract off the intersection, then the probability of Green AND Peanut will be double counted.
</span>
</details>
</div>
</div>
<div id="multiple-events" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Multiple events<a href="week-1-lecture.html#multiple-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider what happens when we pull 2 M&amp;Ms out of the bag</p>
<p><span class="math display">\[ P (Green \: AND \: THEN \: Blue) = P(Green) \cdot P(Blue) \]</span></p>
<p><strong>Question: What if we didn’t care about the order?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
If we do not care about the order, then the combination of one Green M&amp;M and one Blue M&amp;M could have come about because we drew a Blue M&amp;M and then a Green, or a Green and then a Blue. Because there are two ways to get this outcome (and they are mutually exclusive, so we can simply add the two probabilities), the total probability is simply 2 <span class="math inline">\(\times\)</span> P(Green) <span class="math inline">\(\times\)</span> P(Blue).
</span>
</details>
</div>
<div id="conditionals" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Conditionals<a href="week-1-lecture.html#conditionals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we will introduce the ideal of a conditional probability.</p>
<p><span class="math display">\[ P(A \mid B) = P(A \: conditional \: on \: B) \]</span>
Let’s say we have a bivariate distribution (that just means we have two traits being discussed, similar to M&amp;M color and M&amp;M type above) for discrete quantities such as hair color and eye color (which we will use because they are intuitive but <em>not</em> independent traits), and we survey a number (n=20 in this case) of students.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Brown</th>
<th align="center">Blond</th>
<th align="center">Red</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Blue eyes</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td>Brown eyes</td>
<td align="center">7</td>
<td align="center">2</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td>Green eyes</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>This table summarizes the joint distribution for hair color and eye color, which we would write as</p>
<p><span class="math display">\[
P(hair,eye)
\]</span></p>
<p>Remember that for any two traits A and B that are independent,</p>
<p><span class="math display">\[
P(A,B) = P(A) \times P(B)
\]</span></p>
<p>However, in this case, we don’t have any reason to believe that hair and eye color are independent traits. People with blue eyes have a different probability of having brown hair than people with brown eyes. These are called <em>conditional probabilities</em>. For example, the probability of having blue eyes conditional on having blond hair is given by 4/7. We write this as follows</p>
<p><span class="math display">\[
P(eyes=blue|hair=blond)
\]</span></p>
<p>The | symbol represents the “conditional on” statement.</p>
<p>We might also be interested in the <em>marginal</em> probabilities, which are those probabilities representing hair color irrespective of eye color, or eye color irrespective of hair color. In the example given, the marginal probability of having blue eyes is 7/20. The marginal probability of having blond hair is also 7/20. These are univariate probabilities, and are written as</p>
<p><span class="math display">\[
P(eye)
\]</span></p>
<p>or</p>
<p><span class="math display">\[
P(hair)
\]</span></p>
<p>The relationship between joint, marginal, and conditional distributions can be seen in the following statement</p>
<p><span class="math display">\[
P(\mbox{eyes}=\mbox{blue},\mbox{hair}=\mbox{blond})=P(\mbox{eyes}=\mbox{blue}|\mbox{hair}=\mbox{blond})P(\mbox{hair}=\mbox{blond})
\]</span></p>
<p>We can see that this works out as it should</p>
<p><span class="math display">\[
\frac{4}{20}=\frac{4}{7} \times \frac{7}{20}
\]</span></p>
<p>If we want to know the probability of having blue eyes (and didn’t care about hair color) than we would want to add up all the possibilities:</p>
<p><span class="math display">\[
P(\mbox{eyes}=\mbox{blue})=P(\mbox{eyes}=\mbox{blue}|\mbox{hair}=\mbox{blond})P(\mbox{hair}=\mbox{blond}) \\ +P(\mbox{eyes}=\mbox{blue}|\mbox{hair}=\mbox{brown})P(\mbox{hair}=\mbox{brown}) \\
+P(\mbox{eyes}=\mbox{blue}|\mbox{hair}=\mbox{red})P(\mbox{hair}=\mbox{red})
\]</span>
We can state this more simply as a sum:</p>
<p><span class="math display">\[
P(\mbox{eyes}=\mbox{blue})=\sum_{\mbox{all hair colors}}P(\mbox{eyes}=\mbox{blue}|\mbox{hair}=\square)P(\mbox{hair}=\square)
\]</span></p>
<p>For continuous distributions, the same principles apply. Let’s say we have a continuous bivariate distribution <span class="math inline">\(p(A,B)\)</span>. The marginal distribution for A can be calculated by integrating over B (we call this “marginalizing out” or “marginalizing over” B)</p>
<p><span class="math display">\[
p(A=a)=\int_{b=-\infty}^{b=\infty}p(A=a|B=b)p(B=b)db = \int_{b=-\infty}^{b=\infty}p(A=a,B=b)db
\]</span></p>
<p>Notice that, in all cases (discrete or continuous), the relationships among marginal, conditional, and joint distribution can written as</p>
<p><span class="math display">\[
p(A|B)\times p(B)=p(A,B)
\]</span>
or as</p>
<p><span class="math display">\[
p(B|A)\times p(A)=p(A,B)
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
p(A|B)=\frac{p(B|A)p(A)}{p(B)} = \frac{p(A,B)}{p(B)}
\]</span>
Ta da! We’ve arrived at Bayes Theorem, using nothing more than some basic definitions of probability.</p>
<p>In Bayesian analyses (which we will not get into this semester), we are using this to calculate the probability of certain model parameters conditional on the data you have. But to find out more, you’ll have to take BEE 569.</p>
<p><span class="math display">\[ P(parameters \mid data) \cdot P(data) = P(data \mid parameters) \cdot P(parameters) \]</span></p>
<p><span class="math display">\[ P(parameters \mid data) = \frac{P(data \mid parameters) \cdot P(parameters)}{P(data)}\]</span></p>
</div>
<div id="a-few-foundational-ideas" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> A few foundational ideas<a href="week-1-lecture.html#a-few-foundational-ideas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are a few statistics (a <em>statistic</em> is just something calculated from data) that we will need to know right at the beginning.</p>
<p>For illustration purposes, lets assume we have the following (sorted) series of data points: (1,3,3,4,7,8,13)</p>
<p>There are three statistics relating the “central tendency”: the <em>mean</em> (the average value; 5.57), the <em>mode</em> (the most common value; 3), and the <em>median</em> (the “middle” value; 4). We often denote the mean of a variable with a bar, as in <span class="math inline">\(\bar{x}\)</span>. There are also two statistics relating to how much variation there is in the data. The <em>variance</em> measures the average squared distance between each point and the mean. For reasons that we will discuss in lab, we estimate the variance using the following formula</p>
<p><span class="math display">\[
\mbox{variance}_{unbiased} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}
\]</span>
rather than the more intuitive</p>
<p><span class="math display">\[
\mbox{variance}_{biased} = \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}
\]</span></p>
<p>Why? It turns out that the unbiased estimator is not the sum of squared deviation <em>per data point</em> but rather the sum of squared deviation <em>per degree of freedom</em>. What’s a degree of freedom? Glad you asked…</p>
</div>
<div id="degrees-of-freedom" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> Degrees of freedom<a href="week-1-lecture.html#degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What is meant by “degrees of freedom”?</p>
<p>Draw five boxes on the board:</p>
<p>We want the mean to be 4 so the sum has to be 20. Let’s start filling in boxes…</p>
<p>Do we get to choose the last number? No! The last number is constrained by the fact that we already fixed the mean to be 4.</p>
<p>So we only have 4 degrees of freedom, the last box is prescribed by the mean. So if we know the mean of a sample of size n, we only have n-1 remaining degrees of freedom in the sample.</p>
<p>degrees of freedom (dof)=sample size (n)-number of parameters already estimated from the data (p)</p>
<p>If we go back for a second, it turns out that the real definition of the sample variance is</p>
<p><span class="math display">\[
\mbox{variance} =  \frac{\mbox{sum of squares}}{\mbox{degree of freedom}}
\]</span></p>
<p>What is the sum of squares?</p>
<p><span class="math display">\[
SS = \sum(Y-\bar{Y})^2
\]</span>
How many degrees of freedom did I start with? (n)</p>
<p>How many did I lose in the calculation of the SS? (1, for <span class="math inline">\(\bar{Y}\)</span>)</p>
<p>Therefore, an unbiased estimate of the population variance is:</p>
<p><span class="math display">\[
SS = \frac{\sum(Y-\bar{Y})^{2}}{n-1}
\]</span>
##Back to talking about variance and standard deviation</p>
<p>Keep in mind that variance measures a distance <em>squared</em>. So if your data represent heights in m, than the variance will have units <span class="math inline">\(m^{2}\)</span> or square-meters.</p>
<p>The <em>standard deviation</em> is simply the square-root of variance, and is often denoted by the symbol <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[
\sigma = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}
\]</span></p>
<p>If you were handed a distribution and you were asked to measure a characteristic “fatness” for the distribution, your estimate would be approximately <span class="math inline">\(\sigma\)</span>. Note that <span class="math inline">\(\sigma\)</span> has the same units as the original data, so if your data were in meters, <span class="math inline">\(\sigma\)</span> would also be in meters.</p>
</div>
<div id="quick-intro-to-the-gaussian-distribution" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Quick intro to the Gaussian distribution<a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We won’t get to Normal (a.k.a. Gaussian) distributions properly until Week 3, but we will need one fact about the “Standard Normal Distribution” now. The Standard Normal distribution is a Normal (or Gaussian, bell-shaped) distribution with mean equal to zero and standard deviation equal to 1. 68<span class="math inline">\(\%\)</span> of the probability is contained within 1 standard deviation of the mean (so from -<span class="math inline">\(\sigma\)</span> to +<span class="math inline">\(\sigma\)</span>), and 95<span class="math inline">\(\%\)</span> of the probability is contained within 2 standard deviations of the mean (so from -2<span class="math inline">\(\sigma\)</span> to +2<span class="math inline">\(\sigma\)</span>). (Actually, 95<span class="math inline">\(\%\)</span> is contained with 1.96 standard deviations, so sometimes we will use the more precise 1.96 and sometimes you will see this rounded to 2.)</p>
</div>
<div id="overview-of-univariate-distributions" class="section level2 hasAnchor" number="1.10">
<h2><span class="header-section-number">1.10</span> Overview of Univariate Distributions<a href="week-1-lecture.html#overview-of-univariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Discrete Distributions</p>
<ul>
<li><p>Binomial</p></li>
<li><p>Multinomial</p></li>
<li><p>Poisson</p></li>
<li><p>Geometric</p></li>
</ul>
<p>Continuous Distributions</p>
<ul>
<li><p>Normal/Gaussian</p></li>
<li><p>Beta</p></li>
<li><p>Gamma</p></li>
<li><p>Student’s t</p></li>
<li><p><span class="math inline">\(\chi^2\)</span></p></li>
</ul>
</div>
<div id="what-can-you-ask-of-a-distribution" class="section level2 hasAnchor" number="1.11">
<h2><span class="header-section-number">1.11</span> What can you ask of a distribution?<a href="week-1-lecture.html#what-can-you-ask-of-a-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><strong>Probability Density Function</strong>: <span class="math inline">\(P(x_1&lt;X&lt;x_2)\)</span> (continuous distributions)</li>
</ul>
<p><span style="color: green;">
<strong>Stop: Let’s pause for a second and discuss the probability density function. This is a concept that student’s often struggle with. What is the interpretation of <span class="math inline">\(P(x)\)</span>? What is <span class="math inline">\(P(x=3)\)</span>? Can <span class="math inline">\(P(x)\)</span> ever be negative? [No.] Can <span class="math inline">\(P(x)\)</span> ever be greater than 1? [Yes! Why?]</strong>
</span></p>
<ul>
<li><p><strong>Probability Mass Function</strong>: <span class="math inline">\(P(X=x_1)\)</span> (discrete distributions)</p></li>
<li><p><strong>Cumulative Density Function (CDF)</strong>: What is <span class="math inline">\(P(X \le X^*)\)</span>?</p></li>
<li><p><strong>Quantiles of the distributions</strong>: What is <span class="math inline">\(X^{*}\)</span> if <span class="math inline">\(P(X \le X^{*})=0.37\)</span>?</p></li>
<li><p><strong>Sample from the distribution</strong>: With a large enough sample, the histogram will come very close to the underlying PDF.</p></li>
</ul>
<p>Note that the CDF is the integration of the PDF, and the PDF is the derivative of the CDF, so if you have one of these you can always get the other. Likewise, you can always get from the quantiles to the CDF (and then to the PDF). These three things are all equally informative about the shape of the distribution.</p>
<div id="expected-value-of-a-random-variable" class="section level3 hasAnchor" number="1.11.1">
<h3><span class="header-section-number">1.11.1</span> Expected Value of a Random Variable<a href="week-1-lecture.html#expected-value-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In probability theory the expected value of a random variable is the weighted average of all possible values that this random variable can take on. The weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of continious random variable.</p>
</div>
<div id="discrete-case" class="section level3 hasAnchor" number="1.11.2">
<h3><span class="header-section-number">1.11.2</span> Discrete Case<a href="week-1-lecture.html#discrete-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[ X = \{X_1, X_2,...,X_k\} \\
E[X] = \sum_{i=1}^n{X_i \cdot P(X_i)}\]</span></p>
<ul>
<li>Example: Draw numbered balls with numbers 1, 2, 3, 4 and 5 with probabilities 0.1, 0.1, 0.1, 0.1, 0.6.</li>
</ul>
<p><span class="math display">\[ \begin{align*} E[X] &amp;= (0.1 \cdot 1) + (0.1 \cdot 2) + (0.1 \cdot 3) + (0.1 \cdot 4) + (0.6 \cdot 5) \\ &amp;=4 \end{align*}\]</span></p>
</div>
<div id="continuous-case" class="section level3 hasAnchor" number="1.11.3">
<h3><span class="header-section-number">1.11.3</span> Continuous Case<a href="week-1-lecture.html#continuous-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[ E[X] = \int_{-\infty}^{\infty}{X \cdot f(X)dX}\]</span></p>
<p>Note that the function <span class="math inline">\(f(x)\)</span> in the above equation is the probability density function.</p>
</div>
</div>
<div id="a-brief-introduction-to-inference-logic-and-reasoning" class="section level2 hasAnchor" number="1.12">
<h2><span class="header-section-number">1.12</span> A brief introduction to inference, logic, and reasoning<a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>INDUCTIVE reasoning:</p>
<p>A set of specific observations <span class="math inline">\(\rightarrow\)</span> A general principle</p>
<p>Example: I observe a number of elephants and they were all gray. Therefore, all elephants are gray.</p>
<p>DEDUCTIVE reasoning:</p>
<p>A general principle <span class="math inline">\(\rightarrow\)</span> A set of predictions or explanations</p>
<p>Example: All elephants are gray. Therefore, I predict that this new (as yet undiscovered) species of elephant will be gray.</p>
<p>QUESTION: If this new species of elephant is green, what does this do to our hypothesis that all elephants are gray?</p>
<p>Some terminology:</p>
<p><strong>Null Hypothesis</strong>: A statement encapsulating “no effect”</p>
<p><strong>Alternative Hypothesis</strong>: A statement encapsulating “an effect””</p>
<ul>
<li><p>Fisher: Null hypothesis only</p></li>
<li><p>Neyman and Pearson: H0 and H1, Weigh risk of of false positive against the false negative</p></li>
<li><p>We use a hyprid approach</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="popper.png" alt="Hypothetico-deductive view of the scientific method. Photo Source: LSE Library" width="75%" />
<p class="caption">
Figure 1.4: Hypothetico-deductive view of the scientific method. Photo Source: LSE Library
</p>
</div>
<p>Not all hypotheses are created equal. Consider the following two hypotheses:</p>
<p>H<span class="math inline">\(_{1}\)</span>: There are vultures in the local park</p>
<p>H<span class="math inline">\(_{2}\)</span>: There are no vultures in the local park</p>
<p>Which of these two ways of framing the null hypothesis can be rejected by data?</p>
<p><strong>Hypothesis can only be rejected, they can never be accepted!</strong></p>
<p> </p>
<p>“Based on the data obtained, we reject the null hypothesis that…”</p>
<p> or</p>
<p>“Based on the data obained, we fail to reject the null hypothesis that…”</p>
<p>More terminology</p>
<p><strong>Population</strong>: Entire collection of individuals a researcher is interested in.</p>
<p><strong>Model</strong>: Mathematical description of the quantity of interest. It combines a general description (functional form) with parameters (population parameters) that take specific values.</p>
<p><strong>Population paramater</strong>: Some measure of a population (mean, standard deviation, range, etc.). Because populations are typically very large this quantity is unknown (and usually unknowable).</p>
<p><strong>Sample</strong>: A subset of the population selected for the purposes of making inference for the popualtion.</p>
<p><strong>Sample Statistic</strong>: Some measure of this sample that is used to infer the true value of the associated populatipn parameter.</p>
<p>An example:</p>
<p><strong>Population</strong>: Fish density in a lake</p>
<p><strong>Sample</strong>: You do 30 net tows and count all the fish in each tow</p>
<p><strong>Model</strong>: <span class="math inline">\(Y_i \sim Binom(p,N)\)</span></p>
<p>The basic outline of statistical inference</p>
<p>sample(data) <span class="math inline">\(\rightarrow\)</span> sample statistics <span class="math inline">\(\rightarrow\)</span> ESTIMATOR <span class="math inline">\(\rightarrow\)</span> population parameter <span class="math inline">\(\rightarrow\)</span> underlying distribution</p>
<p>Estimators are imperfect tools</p>
<ul>
<li><p>Bias: The expected value <span class="math inline">\(\neq\)</span> population parameter</p></li>
<li><p>Not consistent: As <span class="math inline">\(n \to \infty\)</span> sample statistic <span class="math inline">\(\neq\)</span> population parameter</p></li>
<li><p>Variance</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-1-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
