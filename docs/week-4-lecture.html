<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Week 4 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="7 Week 4 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Week 4 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Week 4 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2024-04-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-3-lab.html"/>
<link rel="next" href="week-4-lab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface, data sets, and past exams</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a>
<ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#week-1-readings"><i class="fa fa-check"></i><b>1.1</b> Week 1 Readings</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.7</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#degrees-of-freedom"><i class="fa fa-check"></i><b>1.8</b> Degrees of freedom</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#quick-intro-to-the-gaussian-distribution"><i class="fa fa-check"></i><b>1.9</b> Quick intro to the Gaussian distribution</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.10</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.11</b> What can you ask of a distribution?</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.11.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.11.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.11.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.11.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.11.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-inference-logic-and-reasoning"><i class="fa fa-check"></i><b>1.12</b> A brief introduction to inference, logic, and reasoning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a>
<ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab.html"><a href="week-1-lab.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab.html"><a href="week-1-lab.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab.html"><a href="week-1-lab.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab.html"><a href="week-1-lab.html#pop_vs_sample_var"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab.html"><a href="week-1-lab.html#some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab.html"><a href="week-1-lab.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#week-2-readings"><i class="fa fa-check"></i><b>3.1</b> Week 2 Readings</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#todays-agenda-1"><i class="fa fa-check"></i><b>3.2</b> Today’s Agenda</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.4</b> Permutation tests</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.6</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.8" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.8</b> Jackknife</a></li>
<li class="chapter" data-level="3.9" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.9</b> Jackknife-after-bootstrap</a></li>
<li class="chapter" data-level="3.10" data-path="week-2-lecture.html"><a href="week-2-lecture.html#by-the-end-of-week-2-you-should-understand"><i class="fa fa-check"></i><b>3.10</b> By the end of Week 2, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a>
<ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a>
<ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#week-3-readings"><i class="fa fa-check"></i><b>5.1</b> Week 3 Readings</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.2</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.3</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.5</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.6</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.8</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.9</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.10</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.11" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.11</b> Some additional notes:</a></li>
<li class="chapter" data-level="5.12" data-path="week-3-lecture.html"><a href="week-3-lecture.html#by-the-end-of-week-3-you-should-understand"><i class="fa fa-check"></i><b>5.12</b> By the end of Week 3, you should understand…</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab</a>
<ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab.html"><a href="week-3-lab.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.1</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab.html"><a href="week-3-lab.html#standard-deviation-vs.-standard-error"><i class="fa fa-check"></i><b>6.2</b> Standard deviation vs. Standard error</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab.html"><a href="week-3-lab.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> The Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a>
<ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#week-4-readings"><i class="fa fa-check"></i><b>7.1</b> Week 4 Readings</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases"><i class="fa fa-check"></i><b>7.5</b> Estimating confidence intervals - 5 special cases</a></li>
<li class="chapter" data-level="7.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#to-recap"><i class="fa fa-check"></i><b>7.6</b> To recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a>
<ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#week-5-readings"><i class="fa fa-check"></i><b>9.1</b> Week 5 Readings</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#statistical-power"><i class="fa fa-check"></i><b>9.2</b> Statistical power</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The single sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#pooledvar"><i class="fa fa-check"></i><b>9.5</b> Pooling the variances</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.6</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.7</b> The F test</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.8</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.9" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.9</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.10" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.10</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.11" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.11</b> Side-note about the Wald test</a></li>
<li class="chapter" data-level="9.12" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>9.12</b> Chi-squared goodness-of-fit test</a></li>
<li class="chapter" data-level="9.13" data-path="week-5-lecture.html"><a href="week-5-lecture.html#chi-squared-test-of-independence"><i class="fa fa-check"></i><b>9.13</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a>
<ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#t-test"><i class="fa fa-check"></i><b>10.1</b> t-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.2</b> F-test</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.4" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.4</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a>
<ul>
<li class="chapter" data-level="11.1" data-path="week-6-lecture.html"><a href="week-6-lecture.html#week-6-readings"><i class="fa fa-check"></i><b>11.1</b> Week 6 Readings</a></li>
<li class="chapter" data-level="11.2" data-path="week-6-lecture.html"><a href="week-6-lecture.html#family-wise-error-rates"><i class="fa fa-check"></i><b>11.2</b> Family-wise error rates</a></li>
<li class="chapter" data-level="11.3" data-path="week-6-lecture.html"><a href="week-6-lecture.html#how-do-we-sort-the-signal-from-the-noise"><i class="fa fa-check"></i><b>11.3</b> How do we sort the signal from the noise?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a>
<ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#week-7-readings"><i class="fa fa-check"></i><b>13.1</b> Week 7 Readings</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#introduction-to-plotting-in-r"><i class="fa fa-check"></i><b>13.2</b> Introduction to plotting in R</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#box-plots"><i class="fa fa-check"></i><b>13.3</b> Box plots</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#two-dimensional-data"><i class="fa fa-check"></i><b>13.4</b> Two-dimensional data</a></li>
<li class="chapter" data-level="13.5" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#three-dimensional-data"><i class="fa fa-check"></i><b>13.5</b> Three-dimensional data</a></li>
<li class="chapter" data-level="13.6" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#multiple-plots"><i class="fa fa-check"></i><b>13.6</b> Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a>
<ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#week-8-readings"><i class="fa fa-check"></i><b>14.1</b> Week 8 Readings</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#warm-up"><i class="fa fa-check"></i><b>14.2</b> Warm-up</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.3</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.4</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.5</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.6</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.7</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.8</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.9</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.10</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.11</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.12" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.-non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.12</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.13" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.13</b> Error structure of linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a>
<ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#covariate-as-number-vs.-covariate-as-factor"><i class="fa fa-check"></i><b>15.1</b> Covariate as number vs. covariate as factor</a></li>
<li class="chapter" data-level="15.2" data-path="week-8-lab.html"><a href="week-8-lab.html#helmert-contrasts-in-r"><i class="fa fa-check"></i><b>15.2</b> Helmert contrasts in R</a></li>
<li class="chapter" data-level="15.3" data-path="week-8-lab.html"><a href="week-8-lab.html#polynomial-contrasts-in-r"><i class="fa fa-check"></i><b>15.3</b> Polynomial contrasts in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a>
<ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#week-9-readings"><i class="fa fa-check"></i><b>16.1</b> Week 9 Readings</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.2</b> Correlation</a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.3</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.4</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.5</b> Regression</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#estimating-the-slope-and-intercept-in-linear-regression"><i class="fa fa-check"></i><b>16.6</b> Estimating the slope and intercept in linear regression</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#ok-now-the-other-derivation-for-slope-and-intercept"><i class="fa fa-check"></i><b>16.7</b> OK, now the “other” derivation for slope and intercept</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.-prediction-intervals"><i class="fa fa-check"></i><b>16.9</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.10" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.10</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.11" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.11</b> Robust regression</a></li>
<li class="chapter" data-level="16.12" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.12</b> Type I and Type II Regression</a></li>
<li class="chapter" data-level="16.13" data-path="week-9-lecture.html"><a href="week-9-lecture.html#W9FAQ"><i class="fa fa-check"></i><b>16.13</b> Week 9 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a>
<ul>
<li class="chapter" data-level="17.1" data-path="week-9-lab.html"><a href="week-9-lab.html#correlation-1"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="week-9-lab.html"><a href="week-9-lab.html#linear-modelling"><i class="fa fa-check"></i><b>17.2</b> Linear modelling</a></li>
<li class="chapter" data-level="17.3" data-path="week-9-lab.html"><a href="week-9-lab.html#centering-the-covariates"><i class="fa fa-check"></i><b>17.3</b> Centering the covariates</a></li>
<li class="chapter" data-level="17.4" data-path="week-9-lab.html"><a href="week-9-lab.html#weighted-regression"><i class="fa fa-check"></i><b>17.4</b> Weighted regression</a></li>
<li class="chapter" data-level="17.5" data-path="week-9-lab.html"><a href="week-9-lab.html#robust-regression-1"><i class="fa fa-check"></i><b>17.5</b> Robust regression</a></li>
<li class="chapter" data-level="17.6" data-path="week-9-lab.html"><a href="week-9-lab.html#bootstrapping-standard-errors-for-robust-regression"><i class="fa fa-check"></i><b>17.6</b> Bootstrapping standard errors for robust regression</a></li>
<li class="chapter" data-level="17.7" data-path="week-9-lab.html"><a href="week-9-lab.html#type-i-vs.-type-ii-regression-the-smatr-package"><i class="fa fa-check"></i><b>17.7</b> Type I vs. Type II regression: The ‘smatr’ package</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a>
<ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-readings"><i class="fa fa-check"></i><b>18.1</b> Week 10 Readings</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#week-10-outline"><i class="fa fa-check"></i><b>18.2</b> Week 10 outline</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.3</b> An example</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.4</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.5</b> Logistic regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.6</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.7</b> Poisson regression</a></li>
<li class="chapter" data-level="18.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.8</b> Deviance</a></li>
<li class="chapter" data-level="18.9" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.9</b> Other methods – LOESS, splines, GAMs</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a>
<ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-practice"><i class="fa fa-check"></i><b>19.3</b> Logistic regression practice</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-practice"><i class="fa fa-check"></i><b>19.4</b> Poisson regression practice</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a>
<ul>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-readings"><i class="fa fa-check"></i><b>20.1</b> Week 11 Readings</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#week-11-outline"><i class="fa fa-check"></i><b>20.2</b> Week 11 outline</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.2.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.2.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.2.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.2.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.2.3</b> Comparing variance components</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.4</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.5</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.-random-effects"><i class="fa fa-check"></i><b>20.6</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.7" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.7</b> Post-hoc tests</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.7.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a>
<ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA in R</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a>
<ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-readings"><i class="fa fa-check"></i><b>22.1</b> Week 12 Readings</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#week-12-outline"><i class="fa fa-check"></i><b>22.2</b> Week 12 outline</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.3</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.4</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.5</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.6</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-model"><i class="fa fa-check"></i><b>22.7</b> Mixed model</a></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.8</b> Unbalanced designs</a>
<ul>
<li class="chapter" data-level="22.8.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.8.1</b> Unbalanced design – Different sample sizes</a></li>
<li class="chapter" data-level="22.8.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.8.2</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.8.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-ii-hierarchical-sums-of-squares"><i class="fa fa-check"></i><b>22.8.3</b> Type II (hierarchical) sums of squares</a></li>
<li class="chapter" data-level="22.8.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.8.4</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.8.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.8.5</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cell"><i class="fa fa-check"></i><b>22.9</b> Unbalanced design – Missing cell</a></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.10</b> Two factor nested ANOVA</a>
<ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.10.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.11" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.11</b> Experimental design</a>
<ul>
<li class="chapter" data-level="22.11.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.11.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.11.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.11.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.11.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.11.3</b> Latin square design</a></li>
<li class="chapter" data-level="22.11.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#split-plot-design"><i class="fa fa-check"></i><b>22.11.4</b> Split plot design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a>
<ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a>
<ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-readings"><i class="fa fa-check"></i><b>24.1</b> Week 13 Readings</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.2</b> Model criticism</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.3</b> Residuals</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.4</b> Leverage</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influence"><i class="fa fa-check"></i><b>24.5</b> Influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.6</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals-for-glms"><i class="fa fa-check"></i><b>24.7</b> Residuals for GLMs</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-selection-vs.-model-criticism"><i class="fa fa-check"></i><b>24.8</b> Model selection vs. model criticism</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.9</b> Comparing two models</a>
<ul>
<li class="chapter" data-level="24.9.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.9.1</b> Nested or not?</a></li>
<li class="chapter" data-level="24.9.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>24.9.2</b> Likelihood Ratio Test (LRT)</a></li>
<li class="chapter" data-level="24.9.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>24.9.3</b> Akaike’s Information Criterion (AIC)</a></li>
<li class="chapter" data-level="24.9.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>24.9.4</b> Bayesian Information Criterion (BIC)</a></li>
<li class="chapter" data-level="24.9.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.9.5</b> Comparing LRT and AIC/BIC</a></li>
</ul></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.10</b> Model weighting</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.11</b> Stepwise regression</a>
<ul>
<li class="chapter" data-level="24.11.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.11.1</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.11.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.11.2</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.11.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.11.3</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.12</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a>
<ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a>
<ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#week-14-readings"><i class="fa fa-check"></i><b>26.1</b> Week 14 Readings</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.2</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.3</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.4</b> Model criticism for multivariate analyses</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.4.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.5</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.6</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.7</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.8</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.9</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.11</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.12</b> PCA in R</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.13</b> Missing data</a></li>
<li class="chapter" data-level="26.14" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.14</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a>
<ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
<li class="chapter" data-level="27.2" data-path="week-14-lab.html"><a href="week-14-lab.html#finally-a-word-about-grades"><i class="fa fa-check"></i><b>27.2</b> Finally, a word about grades</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-4-lecture" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Week 4 Lecture<a href="week-4-lecture.html#week-4-lecture" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="week-4-readings" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Week 4 Readings<a href="week-4-lecture.html#week-4-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, I suggest reading Aho Sections 4.4.2, 5.1, 5.2, 5.3-5.3.4 and Logan Chapter 3. I also strongly recommend reading Sections 1.1, 2.1, 3.1, and 4.1 of <a href="https://github.com/hlynch/Biometry2023/tree/master/_data/Bolker_Chapter6.pdf">Bolker Chapter 6</a>. Note that most students dislike this chapter by Bolker but it covers some fundamental information that will be important throughout your statistics journey and many advanced students come around to Bolker’s way of thinking over time, so bear with it.</p>
</div>
<div id="t-distribution" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> t-distribution<a href="week-4-lecture.html#t-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(X_{1},X_{2},...,X_{n}\)</span> be independently and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and finite, non-zero variance <span class="math inline">\(\sigma^{2}\)</span>, and the average of these variable <span class="math inline">\(\bar{X}\)</span> be defined as</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n}(X_{1}+X_{2}+X_{3}+...+X_{n})
\]</span></p>
<p>Then the Central Limit Theorem states:</p>
<p><span class="math display">\[
lim_{n \rightarrow \infty} \bar{X} \rightarrow N\left(\mu,\frac{\sigma^{2}}{n}\right)
\]</span>
and therefore that</p>
<p><span class="math display">\[
lim_{n \rightarrow \infty} \frac{\bar{X}-\mu}{\sqrt{\sigma^{2}/n}} \rightarrow N(0,1)
\]</span></p>
<p>(Note that I could use <span class="math inline">\(s^{2}\)</span> synonymously with <span class="math inline">\(\sigma^{2}\)</span> here because <span class="math inline">\(s^{2}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span>, which means that <span class="math inline">\(lim_{n \rightarrow \infty} s^{2}\rightarrow \sigma^{2}\)</span>.)</p>
<p><strong>What is the distribution of <span class="math inline">\(\bar{X}\)</span> for finite n when we don’t know what the population variance <span class="math inline">\(\sigma^{2}\)</span> is and have to substitute <span class="math inline">\(s^{2}\)</span> instead? The t-distribution.</strong></p>
<p>The t-distribution has one parameter, n-1, where n=the number of degrees of freedom.</p>
<p>(Just like <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span> are the parameters of the Normal, the parameter of the t is called the “degrees of freedom”. Do not let the name confuse you. Also, keep in mind that there is no relationship between the <em>number</em> of draws from the t distribution and the parameter for the t distribution. You can have n=100 draws from a <span class="math inline">\(t_{n-1=10}\)</span> or n=10 draws from a <span class="math inline">\(t_{n-1=100}\)</span>. The two “n”s here are different and independent of one another.)</p>
<p>Notice that as n gets large, both the numerator and the denominator get small. The ratio of these two small numbers is the t-distribution, which is symmetric about 0 because the sample mean could end up either slightly too low or slightly too high.</p>
<p><strong>Where did we lose the degree of freedom?</strong></p>
<details>
<summary>
Click for Answer
</summary>
<span style="color: blueviolet;">
We lost one degree of freedom because we had to use the sample mean in the estimation of the sample variance.
</span>
</details>
<p>Sometimes <span class="math inline">\(s^{2}\)</span> is smaller than <span class="math inline">\(\sigma^{2}\)</span>, and sometimes it is larger, which explains why <strong>the t-distribution is flatter in the middle but fatter in the tails</strong>.</p>
<p><img src="Week-4-lecture_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>For large sample sizes, the t-distribution is indistinguishable from the normal distribution, but for small sample sizes, the t-distribution is flatter in the middle and fatter in the tails.</p>
<p>The p.d.f. looks like</p>
<p><span class="math display">\[
p.d.f. = f(x|p=n-1=d.o.f.) \sim \frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})}\frac{1}{(p\pi)^{1/2}}\frac{1}{(1+\frac{x^{2}}{p})^{(p+1)/2}}
\]</span></p>
<p>Side note: I mentioned that there were some distributions for which I was not requiring you to memorize the pdf. Note that for some of the remaining pdfs (like the t, F, and chi-squared) I may give you the pdf and ask you to identify it. Fair warning!!</p>
<p>The expected value is given by</p>
<p><span class="math display">\[
E[X]=0 \mbox{ if p}&gt;1
\]</span></p>
<p>The variance is given by</p>
<p><span class="math display">\[
Var[X]=\frac{p}{p-2} \mbox{ if p}&gt;2
\]</span></p>
</div>
<div id="chi-squared-distribution" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Chi-squared distribution<a href="week-4-lecture.html#chi-squared-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The chi-squared distribution is related to the standard normal distribution as follows:
Draw</p>
<p><span class="math display">\[
X_{1},X_{2},X_{3},...,X_{n}  \sim N(0,1)
\]</span></p>
<p>Create a new quantity</p>
<p><span class="math display">\[
Y= X_{1}^{2}+X_{2}^{2}+X_{3}^{2}+...+X_{n}^{2}
\]</span>
<span class="math display">\[
Y \sim χ_{n}^{2}
\]</span></p>
<p>In other words, the sum of squared standard deviates is distributed as a <span class="math inline">\(\chi^{2}\)</span> distribution with n degrees of freedom.</p>
<p>If we construct standard normal by transforming normal distributions with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>, we can re-write the sum above as</p>
<p><span class="math display">\[
Y = \frac{1}{\sigma^{2}}(X_{1}-\mu)^{2}+\frac{1}{\sigma^{2}}(X_{2}-\mu)^{2}+\frac{1}{\sigma^{2}}(X_{3}-\mu)^{2}+...+\frac{1}{\sigma^{2}}(X_{n}-\mu)^{2}
\]</span></p>
<p><span class="math display">\[
Y = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}{(X_{i}-\mu)^{2}} \sim \chi_{n}^{2}
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the population mean and <span class="math inline">\(\sigma^{2}\)</span> is the population variance.</p>
<p>To reiterate, it is important to remember the following properties of the chi-squared distribution:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim N(0,1)\)</span>, then <span class="math inline">\(X^{2} \sim \chi_{1}^{2}\)</span>. In other words, the square of a standard normal random variable is a chi-squared random variable.</li>
</ol>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="week-4-lecture.html#cb239-1" tabindex="-1"></a>chisqhist<span class="ot">&lt;-</span><span class="fu">hist</span>((<span class="fu">rnorm</span>(<span class="dv">1000</span>,<span class="dv">0</span>,<span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span>,<span class="at">freq=</span>F,<span class="at">breaks=</span><span class="dv">30</span>,<span class="at">main=</span><span class="st">&quot;Chi-squared with df=1&quot;</span>)</span>
<span id="cb239-2"><a href="week-4-lecture.html#cb239-2" tabindex="-1"></a><span class="fu">lines</span>(chisqhist<span class="sc">$</span>mids,<span class="fu">dchisq</span>(chisqhist<span class="sc">$</span>mids,<span class="dv">1</span>),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-4-lecture_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(X_{p1},X_{p2},X_{p3}... \sim \chi^{2}\)</span> distribution, then <span class="math inline">\(\sum_{i}{X_{pi}} \sim \chi^{2}_{\sum{p_{i}}}\)</span>. In other words, independent chi-squared random variables sum to a chi-squared random variable, and the degrees of freedom also add.</li>
</ol>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="week-4-lecture.html#cb240-1" tabindex="-1"></a>df1<span class="ot">&lt;-</span><span class="dv">5</span> </span>
<span id="cb240-2"><a href="week-4-lecture.html#cb240-2" tabindex="-1"></a>df2<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb240-3"><a href="week-4-lecture.html#cb240-3" tabindex="-1"></a>df3<span class="ot">&lt;-</span><span class="dv">3</span></span>
<span id="cb240-4"><a href="week-4-lecture.html#cb240-4" tabindex="-1"></a>chisqhist2<span class="ot">&lt;-</span><span class="fu">hist</span>(<span class="fu">rchisq</span>(<span class="dv">1000</span>,df1)<span class="sc">+</span><span class="fu">rchisq</span>(<span class="dv">1000</span>,df2)<span class="sc">+</span><span class="fu">rchisq</span>(<span class="dv">1000</span>,df3),<span class="at">freq=</span>F,<span class="at">breaks=</span><span class="dv">30</span>,<span class="at">main=</span><span class="st">&quot;Sum of chi-squared distributions&quot;</span>)</span>
<span id="cb240-5"><a href="week-4-lecture.html#cb240-5" tabindex="-1"></a><span class="fu">lines</span>(chisqhist2<span class="sc">$</span>mids,<span class="fu">dchisq</span>(chisqhist2<span class="sc">$</span>mids,df1<span class="sc">+</span>df2<span class="sc">+</span>df3),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="Week-4-lecture_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>While above I used the symbol “n” to designate the parameter of the <span class="math inline">\(\chi^{2}\)</span> distribution because it is intuitive how that parameter is equal to the number of (squared) random variables are being summed, here I will switch to the letter <span class="math inline">\(\nu\)</span>, which is slightly more traditional. The chi-squared distribution has an ugly pdf, which I include here for completeness.</p>
<p><span class="math display">\[
f(x \mid \nu=n) = \frac{1}{2^\frac{\nu}{2} \Gamma(\frac{\nu}{2})} x^{(\frac{\nu}{2}-1)} e^\frac{-x}{2}
\]</span>
Remembering that the Gamma distribution is given by</p>
<p><span class="math display">\[
f(x|\alpha,\beta) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}
\]</span>
With this we can see that the <span class="math inline">\(\chi^{2}\)</span> distribution is a special case of the gamma distribution with <span class="math inline">\(\alpha = \nu/2\)</span> and <span class="math inline">\(\beta=2\)</span>.</p>
<p>The shape of the <span class="math inline">\(\chi^{2}\)</span> distribution can be seen with a few examples:</p>
<p><img src="Week-4-lecture_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The expected value and variance are given by:</p>
<p><span class="math display">\[ E[X] = \nu \\ Var[X] =2\nu  \]</span></p>
<p>I don’t derive these, but they follow from the E[X] and Var[X] of the gamma distribution.</p>
</div>
<div id="f-distribution" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> F distribution<a href="week-4-lecture.html#f-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As you will see in detail throughout the semester, the F distribution is critical to modelling variances. The F distribution is related to the <span class="math inline">\(\chi^{2}\)</span> distribution, in fact the ratio of two chi-squared distributions is the F-distribution.</p>
<p><span class="math display">\[
\frac{\chi^{2}_{n-1}/(n-1)}{\chi^{2}_{m-1}/(m-1)} \sim F_{n-1,m-1}
\]</span></p>
<p>The F distribution has two parameters: the d.o.f. of both samples being compared (n-1 and m-1).</p>
<p>The pdf of the F distribution is given by</p>
<p><span class="math display">\[
f(x|r=n-1,s=m-1)=\frac{r\Gamma(\frac{1}{2}(r+s))}{s\Gamma(\frac{1}{2}r)\Gamma(\frac{1}{2}s)}\frac{\frac{rx}{s}^{\frac{r}{2}-1}}{(1+\frac{rx}{s})^{\frac{r+s}{2}}}
\]</span></p>
<p>(noting that I have used r=n-1 and s=m-1 to make the equation slightly more readable) and the shape of the F distribution looks like</p>
<p><img src="Week-4-lecture_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><span class="math display">\[ E[X] = \frac{m-1}{m-3} \]</span></p>
<p>and the variance is just ugly.</p>
<p>Why would we ever need to use the F-distribution? It turns out that the F-distribution is critical for assessing variances. Let’s say you have two datasets…</p>
<p>Data set A, with n data points:</p>
<p><span class="math display">\[
X_{1},X_{2},X_{3},...,X_{n} \sim N(\mu_{A},\sigma_{A}^{2})
\]</span></p>
<p>and data set B, with m data points:</p>
<p><span class="math display">\[
Y_{1},Y_{2},Y_{3},...,Y_{m} \sim N(\mu_{B},\sigma_{B}^{2})
\]</span></p>
<p>Now you have two sets of data, both normally distributed but with different means and variances. You want to know whether the variance of data set A is bigger or smaller than the variance for data set B.</p>
<p>We <em>want</em> to know the ratio <span class="math inline">\(\sigma_{A}^{2}/\sigma_{B}^{2}\)</span> but all we <em>have</em> at our disposal is the ratio of the sample variances <span class="math inline">\(s_{A}^{2}/s_{B}^{2}\)</span>. It turns out the F-distribution can help us.</p>
<p>We start with the fact that you know from our discussion a few minutes ago that</p>
<p><span class="math display">\[
\frac{1}{\sigma^2}\sum_{i=1}^n{(X_i-\mu)^2} \sim \chi^2_n
\]</span></p>
<p>If we have to estimate <span class="math inline">\(\mu\)</span> using <span class="math inline">\(\bar{X}\)</span> than we lose one degree of freedom</p>
<p><span class="math display">\[
\frac{1}{\sigma^2}\sum_{i=1}^n{(X_i-\bar{X})^2} \sim \chi^2_{n-1}
\]</span>
but we know that</p>
<p><span class="math display">\[
s^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n-1}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
s^2(n-1) = \sum_{i=1}^n(X_i-\bar{X})^2
\]</span></p>
<p>So we can write</p>
<p><span class="math display">\[
\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar{X})^2 =  \frac{s^2(n-1)}{\sigma^2} \sim \chi^2_{n-1}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim \frac{\chi^2_{n-1}/(n-1)}{\chi^2_{m-1}/(m-1)}
\]</span></p>
<p>is a ratio of scaled chi-squared distributions which is, as you now know, also an F-distribution:</p>
<p><span class="math display">\[
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim F_{n-1,m-1}
\]</span></p>
<p>So, if we know the population variances (<span class="math inline">\(\sigma^2\)</span>) and we calculate the sample variances (<span class="math inline">\(s^2\)</span>), than the ratio above has the F-distribution. While we don’t usually know the population variances, we are often testing a hypothesis that the two population variances are equal. If we <em>assume</em> that <span class="math inline">\(\sigma_{A}^{2}=\sigma_{B}^{2}\)</span>, than the expression above becomes</p>
<p><span class="math display">\[
\frac{s^2_A}{s^2_B} \sim F_{n-1,m-1}
\]</span>
This fact will be used in a few weeks when we discuss F-tests. (Stay tuned…)</p>
<p>The t-, F-, and chi-squared distributions are all closely related. I will not work through the proofs here, but I will highlight a couple important relationships that will come in handy later.</p>
<p><em>Relationship 1</em>: We’ve already shown that the F distribution is a ratio of scaled chi-squared distributions.</p>
<p><span class="math display">\[
\frac{\chi^2_{n-1}/(n-1)}{\chi^2_{m-1}/(m-1)} \sim F_{n-1,m-1}
\]</span>
<em>Relationship 2</em>: If <span class="math inline">\(X \sim t_{n-1}\)</span>, then <span class="math inline">\(X^2 \sim F_{1,n-1}\)</span>. In words, this means that if you square a t-distributed variable, you get an F distributed variable.</p>
<p>Once you decide on a distribution for your data, you need some way of estimating the best-fit parameters. This brings us to the next major topic of Biometry.</p>
</div>
<div id="estimating-confidence-intervals---5-special-cases" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Estimating confidence intervals - 5 special cases<a href="week-4-lecture.html#estimating-confidence-intervals---5-special-cases" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now learn how to fit a model to data, that is, to estimate the parameter values for the distribution being used to model the data. Since parameter estimates are useless without their corresponding confidence intervals, we will also learn out to estimate confidence intervals for these parameters.</p>
<p>Let’s review for a second what is meant by a confidence interval. <strong><span style="color: orangered;">A 95th percentile confidence interval is an interval estimated from the data that we are 95<span class="math inline">\(\%\)</span> certain contains the true but unknown value for the parameter.</span></strong> Because this interval is estimated from data, it has its own confidence interval. While we usually we don’t worry too much about this, keep in mind that <strong>a different dataset would also produce a different parameter estimate and a different confidence interval</strong>.</p>
<p>How do we calculate confidence intervals when we are using a parametric distribution to model some data? We are going to use what we know about these distributions to derive an analytical expression for the distribution of the parameter of interest, and then use the quantiles of that distribution to calculate the appropriate confidence intervals (typically we use the <span class="math inline">\(\alpha⁄2\)</span> and 1-<span class="math inline">\(\alpha⁄2\)</span> confidence intervals, where <span class="math inline">\(\alpha\)</span>=0.05).</p>
<p>The following five examples use the same methodology:</p>
<p><em>Step #1</em>: Start with an expression involving the parameter of interest that you know is true based on what we already know about the properties of the univariate distributions</p>
<p><em>Step #2</em>: Algebraically re-arrange that expression to isolate the parameter of interest on one side of the equation. You now have an expression for the statistical distribution describing the estimate of that parameter</p>
<p><em>Step #3</em>: Replace the distribution with the appropriate quantiles to generate the lower bound and upper bound of interest.</p>
<p>The first four examples involve the following model</p>
<p><span class="math display">\[
X \sim N(\mu,\sigma^{2})
\]</span>
If we want to find confidence intervals for <span class="math inline">\(\mu\)</span>, we can do so in the case where <span class="math inline">\(\sigma\)</span> is known (unrealistic) or where <span class="math inline">\(\sigma\)</span> is unknown. If we want to find confidence intervals for <span class="math inline">\(\sigma\)</span>, we can do so in cases where <span class="math inline">\(\mu\)</span> is known (unrealistic) or where <span class="math inline">\(\mu\)</span> is unknown. These form the first four of the five examples we will cover.</p>
<p><strong>Example 1: Confidence intervals for <span class="math inline">\(\mu\)</span> assuming <span class="math inline">\(\sigma\)</span> is known.</strong> Let’s say that we have some data <span class="math inline">\(X_{1},X_{2},X_{3},....,X_{n}\)</span>. To use a concrete example, let’s say that these data represent the population growth rate of some bird colonies that I am monitoring. I am going to model growth rate by a Normal distribution:</p>
<p><span class="math display">\[
X \sim N(\mu,\sigma^{2})
\]</span>
Let’s assume for the moment that I already know what the variance of growth rate is, so <strong><span class="math inline">\(\sigma\)</span> is already known and does not have to be estimated from the data</strong>, but I do need to use the data to estimate the mean of the distribution <span class="math inline">\(\mu\)</span>.</p>
<p>Let’s start with the model we have for the data</p>
<p><span class="math display">\[
X \sim N(\mu,\sigma^{2})
\]</span></p>
<p>From this, follows</p>
<p><span class="math display">\[
\bar{X} \sim N(\mu,\sigma^{2}/n)
\]</span>
Note that this is <em>exactly</em> true if the original data come from a Normal distribution but it is also approximately true as long as the Central Limit Theorem holds (and it does for almost all distributions that you are likely to encounter in your research). Subtracting off the parameter <span class="math inline">\(\mu\)</span> from both sides we get</p>
<p><span class="math display">\[
\bar{X}-\mu \sim N(0,\sigma^{2}/n)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\bar{X}-\mu \sim \sqrt{\frac{\sigma^{2}}{n}}N(0,1)
\]</span>
We can re-arrange this expression to get the distribution for <span class="math inline">\(\mu\)</span></p>
<p><span class="math display">\[
\mu-\bar{X} \sim \sqrt{\frac{\sigma^{2}}{n}}N(0,1)
\]</span>
(Why can I just reverse the signs on the left hand side?)</p>
<p><span class="math display">\[
\mu \sim \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}N(0,1)
\]</span></p>
<p>We now have the distribution for the parameter <span class="math inline">\(\mu\)</span> in terms of quantities we already know (<span class="math inline">\(\sigma\)</span>, which is assumed known, sample size n, and the average of the data <span class="math inline">\(\bar{X}\)</span>).</p>
<p>We can use this expression to get confidence intervals for <span class="math inline">\(\mu\)</span>, by plugging in the quantiles of the standard Normal distribution on the right hand side. The lower limit is defined by the [<span class="math inline">\(\alpha\)</span>/2] quantile of N(0,1), and the upper limit is defined by 1-[<span class="math inline">\(\alpha\)</span>/2] quantile of N(0,1). By tradition, we call the quantiles of the standard normal ‘’z’’.</p>
<p><span class="math display">\[
P(\bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{\alpha/2} \leq \mu \leq \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2}) = 0.95
\]</span></p>
<p>Because the N(0,1) is symmetric about zero,</p>
<p><span class="math display">\[
z_{\alpha/2}= -z_{1-\alpha/2}
\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="StandardNormalTails.png" alt="Standard normal diagram illustrating the symmetry of the distribution and the quantiles for the left and right tails." width="75%" />
<p class="caption">
Figure 7.1: Standard normal diagram illustrating the symmetry of the distribution and the quantiles for the left and right tails.
</p>
</div>
<p>this is the same as</p>
<p><span class="math display">\[
P(\bar{X}-\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2} \leq \mu \leq \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2}) = 0.95 = 1-\alpha
\]</span></p>
<p>This second version makes it easier to see that there is a quantity that is subtracted off <span class="math inline">\(\bar{X}\)</span> for the lower limit, and added to <span class="math inline">\(\bar{X}\)</span> for the upper limit. The confidence intervals in this case are symmetric about <span class="math inline">\(\bar{X}\)</span>.</p>
<p>Side note: This is called the “equal-tails” method, because we have constructed the confidence interval using equal amounts in each tail of the distribution. It is the most common way of constructing a confidence interval, but not the only way.</p>
<p>Notice that</p>
<p><span class="math display">\[
z_{1-\alpha/2}=qnorm(0.975)=1.96
\]</span></p>
<p>from which we arrive at a form for the confidence interval that might look familiar, that is, estimate <span class="math inline">\(\pm\)</span> 2 <span class="math inline">\(\times\)</span> SE (the standard error).</p>
<p><em>Note</em>: We often use this approach for the population mean even if we do not know the exact distribution because the CLT tells us that for large sample sizes, the mean of the distribution is normally distributed. But this approach is quite limited because it doesn’t tell us how to get estimates and CIs for other parameters, nor does it address the problem of estimates when the sample size is small and the CLT doesn’t apply.</p>
<p><strong>Example #2: Confidence intervals for <span class="math inline">\(\mu\)</span> assuming <span class="math inline">\(\sigma\)</span> is unknown.</strong> What about a (much more common) situation where you need to use the data to estimate both the mean and the variance? In this case we have to use the sample variance <span class="math inline">\(s^{2}\)</span> to estimate the parametric variance <span class="math inline">\(\sigma^{2}\)</span>. We start with the following fact</p>
<p><span class="math display">\[
\frac{\bar{X}-\mu}{\sqrt{\frac{s^{2}}{n}}} \sim t_{n-1}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
\bar{X}-\mu \sim \sqrt{\frac{s^{2}}{n}} t_{n-1}
\]</span></p>
<p><span class="math display">\[
P(\bar{X}-\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[n-1]} \leq \mu \leq \bar{X}+\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[n-1]}) = 1-\alpha
\]</span></p>
<p>It is almost never the case that you know <span class="math inline">\(\sigma^{2}\)</span>, so you should always use the quantiles of the t-distribution for building confidence intervals for <span class="math inline">\(\bar{X}\)</span>. People incorrectly use the normal approximation because back in the day of tables, the normal distribution was easier. <em>In the age of computers, no excuse – use the t-distribution.</em></p>
<p>One final note - next week we will learn about t-tests and depending on the assumptions being made, the appropriate t-distribution may have a degree-of-freedom different from n-1, so I’m including the most general statement about the confidence interval for the mean of a Normally distriuted population here:</p>
<p><span class="math display">\[
P(\bar{X}-\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]} \leq \mu \leq \bar{X}+\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]}) = 1-\alpha
\]</span></p>
<p>Where does the Central Limit Theorem come in here? In this case, if you are going to invoke the Central Limit Theorem (because your data are not actually Normally distributed) to get at confidence intervals for the mean of the distribution, you would need the sample size to be large anyways (because the CLT only says that the mean takes a Normal distribution in the limit of large sample sizes) and in this case <span class="math inline">\(s^{2} \rightarrow \sigma^{2}\)</span> so you can use Case 1 above.</p>
<p><strong>Example #3: Estimating the confidence intervals for the variance <span class="math inline">\(\sigma^{2}\)</span> assuming <span class="math inline">\(\mu\)</span> is known.</strong></p>
<p>Based on the definition of a <span class="math inline">\(\chi^{2}\)</span> distribution, we know that if we have data <span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>, then</p>
<p><span class="math display">\[
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2} \sim \chi^{2}_{n}
\]</span></p>
<p><em>Note that this is only true if the underlying data are actually Normally distributed.</em> (In other words, the parameter <span class="math inline">\(\sigma\)</span> is a parameter of the Normal distribution, so Examples 3-5 require that the original data are Normally distributed, otherwise the concept of putting CI on the parameter <span class="math inline">\(\sigma\)</span> doesn’t make sense.) Here we will use the fact that if <span class="math inline">\(\mu\)</span> is <em>known</em>, than the sample variance is calculated as</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2} = s^{2}
\]</span>
Notice that because <span class="math inline">\(\mu\)</span> is known and does not have to be estimated from the data, we retain all n degrees of freedom in the denominator.</p>
<p>We can re-arrange this equation to get</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(X_{i}-\mu)^{2} = ns^{2}
\]</span>
which we plug into our <span class="math inline">\(\chi^{2}\)</span> expression to yield</p>
<p><span class="math display">\[
\frac{1}{\sigma^{2}}ns^{2} \sim \chi^{2}_{n}
\]</span>
We then re-arrange a bit further to isolate <span class="math inline">\(\sigma^{2}\)</span></p>
<p><span class="math display">\[
\sigma^{2} \sim \frac{ns^{2}}{\chi^{2}_{n}}
\]</span></p>
<p>Now that we have the sampling distribution for <span class="math inline">\(\sigma^{2}\)</span> we can simply insert the appropriate quantiles to get the lower and upper limits of the confidence interval. Note that because the <span class="math inline">\(\chi^{2}\)</span> is in the denominator, the <em>larger</em> quantile is associated with the lower limit, and vice versa.</p>
<p><span class="math display">\[
P\left(\frac{ns^{2}}{\chi^{2}_{(1-\alpha/2)[n]}} \leq \sigma^{2} \leq \frac{ns^{2}}{\chi^{2}_{(\alpha/2)[n]}}\right) = 1-\alpha
\]</span></p>
<p><strong>Example #4: Estimating the confidence intervals for the variance <span class="math inline">\(\sigma^{2}\)</span> assuming <span class="math inline">\(\mu\)</span> is unknown.</strong></p>
<p>This example proceeds similarly to the one above, except we now need to estimate <span class="math inline">\(\mu\)</span> from the data, and this means we lose of degree of freedom both in the <span class="math inline">\(\chi^{2}\)</span> distribution and in the estimate of the sample variance.</p>
<p><span class="math display">\[
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2} \sim \chi^{2}_{n-1}
\]</span></p>
<p>Why n-1? Because we lost a degree of freedom when we had to estimate <span class="math inline">\(\bar{X}\)</span>.</p>
<p>As before (except with <span class="math inline">\(\mu\)</span> unknown, and estimated by <span class="math inline">\(\bar{X}\)</span>)</p>
<p><span class="math display">\[
\frac{1}{(n-1)}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2} = s^{2}
\]</span></p>
<p>so we can rearrange to show that</p>
<p><span class="math display">\[
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}=\frac{(n-1)s^{2}}{\sigma^{2}} \sim \chi^{2}_{n-1}
\]</span>
Using the above expression, we re-arrange to get the sampling distribution of <span class="math inline">\(\sigma^{2}\)</span></p>
<p><span class="math display">\[
\sigma^{2} \sim \frac{(n-1)s^{2}}{\chi^{2}_{n-1}}
\]</span></p>
<p>from which we get the confidence intervals</p>
<p><span class="math display">\[
P(\frac{(n-1)s^{2}}{\chi^{2}_{(1-\alpha/2)[n-1]}} \leq \sigma^{2} \leq \frac{(n-1)s^{2}}{\chi^{2}_{(\alpha/2)[n-1]}}) = 1-\alpha
\]</span></p>
<p><strong>Example #5: Estimating the ratio of two variances <span class="math inline">\(\sigma^{2}_{A}/\sigma^{2}_{B}\)</span></strong></p>
<p>We start with this expression, which we derived above:</p>
<p><span class="math display">\[
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim F_{n-1,m-1}
\]</span>
We can rewrite this equation as</p>
<p><span class="math display">\[
\frac{s^2_A/s^2_B}{\sigma_{A}^{2}/\sigma^2_B} \sim F_{n-1,m-1}
\]</span>
Now we are going to invert the fraction on the left hand side in order to get the <span class="math inline">\(\sigma\)</span>s in the numerator. The right hand side is still an F-distribution, but because we have flipped numerator and denominator, we need to switch the order of the two parameters</p>
<p><span class="math display">\[
\frac{\sigma_{A}^{2}/\sigma^2_B}{s^2_A/s^2_B} \sim F_{m-1,n-1}
\]</span></p>
<p>We isolate the ratio of <span class="math inline">\(\sigma\)</span>s in the numerator by multiplying the sample variances across, as follows</p>
<p><span class="math display">\[
\frac{\sigma_{A}^{2}}{\sigma^2_B} \sim \frac{s^2_A}{s^2_B} F_{m-1,n-1}
\]</span></p>
<p>Now, as before, we have the sampling distribution for the quantity we want, and we obtain the confidence intervals by substituting in the appropriate quantiles.</p>
<p><span class="math display">\[
\frac{s_{A}^{2}}{s_{B}^{2}}F_{(\alpha/2)[m-1,n-1]} \leq \frac{\sigma_{A}^{2}}{\sigma_{B}^{2}} \leq \frac{s_{A}^{2}}{s_{B}^{2}}F_{(1-\alpha/2)[m-1,n-1]}
\]</span></p>
</div>
<div id="to-recap" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> To recap<a href="week-4-lecture.html#to-recap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We used the Normal distribution to derive the confidence interval for the parametric mean <span class="math inline">\(\mu\)</span> when <span class="math inline">\(\sigma\)</span> is known.</p>
<p>We used the t-distribution to derive the confidence interval for the parametric mean <span class="math inline">\(\mu\)</span> when <span class="math inline">\(\sigma\)</span> is unknown (much more common).</p>
<p>We used the <span class="math inline">\(\chi^{2}\)</span> distribution to derive the confidence interval for the parametric variance <span class="math inline">\(\sigma^{2}\)</span> when <span class="math inline">\(\mu\)</span> is known.</p>
<p>We used the <span class="math inline">\(\chi^{2}\)</span> distribution to derive the confidence interval for the parametric variance <span class="math inline">\(\sigma^{2}\)</span> when <span class="math inline">\(\mu\)</span> is unknown (much more common).</p>
<p>We used the F distribution to derive the confidence interval for the ratio of two variances.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-3-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-4-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
