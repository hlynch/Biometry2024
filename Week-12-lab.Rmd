Week 12 Lab
=============

```{r include=FALSE, warning=FALSE}
text.size <- 16
```

Today we will discuss Hurlbert's [famous paper](https://github.com/hlynch/Biometry2020/tree/master/_data/Hurlbert.pdf) on pseudoreplication. You can also find an interview with Hurlbert [here](https://podcasts.apple.com/us/podcast/get-real-stuart-hurlbert-on-pseudoreplication-other/id426676729?i=1000368631395).

There are 5 parts to this week's lab:

1. Example #1: Two-way factorial ANOVA in R
2. Example #2: Nested deisgn
3. Example #3: Nested design
4. Example #4: Randomized block design
5. Example #5: Nested design 

Example #1: Two-way factorial ANOVA in R
---------------

Two-way ANOVA in R is a lot like one-way ANOVA in R, except now we have a second covariate in the model equation. We will go through a two-way example using the same data on income and education that we used last week. 

```{r}
salaries<-read.csv("_data/TwoWayANOVAdata.csv")
par(mfrow=c(2,1))
boxplot(salaries$Salary~salaries$Gender,ylab="Salary")
boxplot(salaries$Salary~salaries$Education,ylab="Salary")
```

NB: The data we are using this week are the original (unbalanced) version of the data we used last week. These data are unbalanced, which will have important consequences for the ANOVA as we will see.

Let's stop for a second and figure out how we can look at the data:

```{r}
summary(salaries)
str(salaries)
head(salaries)
```

I'm going to show you another nice way of looking at ANOVA data which defines a new function which calculates the mean and its standard error and prints it out in a nice format. Don't worry too much about the details, but I include the script here so you have it in case it is helpful for you later...

```{r}
meansd<-function(x)
  {
  tmp.mean<-format(mean(x),digits=3)
  tmp.sd<-format(apply(x,2,sd),digits=3)
  mean.sd<-paste(tmp.mean, " (",tmp.sd,")", sep="")
  mean.sd<-as.matrix(mean.sd)
  names(mean.sd)<- "Mean (SD)"
  return(mean.sd)
  }

library(rms)  #the function "summary" uses the rms package
summary(Salary~Education+Gender,data=salaries,method="cross",fun=meansd)
```

This is a quick and dirty way to look at the raw data to make sense of the model output to come. Note that you could stick this inside 'latex()' and it would make a latex ready formatted table for you.

Back to the modelling...

Note that

```
Salary~Education+Gender+Education:Gender
```

is the same as

```
Salary~Education*Gender
```

since the * says "consider Education and Gender and all interactions".

For reasons that will become clear in a second, we will use the first notation for now.

```{r}
lm.fit.2way<-lm(Salary~Education+Gender+Education:Gender,data=salaries)
summary(lm.fit.2way)
anova(lm.fit.2way)
```

(Question: How do we interpret the intercept?)

Note that these effect sizes should make sense if we keep track of the correct usage of the interaction terms. The intercept is the mean salary of a Female with a Masters degree (25k). We can re-create the other cells in the table above by adding or subtracting from the baseline the effects of swapping categories of gender or education.

For example, what is the mean salary of a man with a PhD. Going from the baseline condition of the intercept to a Man with a PhD we have to make two swaps: Female->Male and Masters->PhD. This involves both the main effects of Gender and Education but also the interaction between Gender and Education. Let's do this stepwise:

$$
\mbox{Female&Masters}=25 \\
\mbox{Change for a woman getting a PhD}=4.25 \\ 
 ------- \rightarrow\mbox{Women&PhD}=25+4.25=29.25 \\
\mbox{Change for a Woman with PhD becoming a Man} = 2-3.85 = -1.85\\  
------- \rightarrow \mbox{Man&PhD} = 29.25-1.85=27.4
$$

This is exactly what the table shows as well. Note that because of the interaction, a woman with a PhD is better off staying a woman because the benefit of getting a PhD is much bigger for a woman than for a man and this more than compensates for the negative effect being female has on average salary. Another way of thinking about it would be to do the gender swap first:

$$
\mbox{Female&Masters}=25 \\
\mbox{Change for a Woman with Masters becoming a Man} = 2 \\
------- \rightarrow \mbox{Man&Masters} = 25+2 = 27 \mbox{    [Magic, no additional skills required! :) ]}\\
\mbox{Change for a Man getting a PhD}=4.25-3.85 =0.4 \\
------- \rightarrow \mbox{Man&PhD}=27+0.4=27.4 \\
$$


Now try doing the "same" analysis but switching the order of inputs

```{r}
lm.fit.2way<-lm(Salary~Gender+Education+Gender:Education,data=salaries)
summary(lm.fit.2way)
anova(lm.fit.2way)
```

Did you get the same answer? (No. In fact, Gender is no longer significant the second way.)

Notice that while the ANOVA tables are different, the parameter estimates and their standard errors/p-values are the same in both cases. Why? Remember when we were discussing multiple regression, we said that the coefficients in a multiple regression represented the slope of the "partial regression line" which is to be interpreted as the effect of that covariate holding all other predictor variables at their mean value. In other words, the multiple regression implicitly uses a Type III approach because that is typically what is of interest. Likewise, we are usually interested in the Type III sum of squares ANOVA table. The only problem is that this is not R's default.

To get Type II and Type III sum of squares ANOVA tables, you have to use the 'Anova' function in the 'car' package.

Before moving on from this example, its worth thinking about a slightly simpler analysis in which you model income as a function of gender (men v. women) and education (Masters v. PhD). (Same as above but considering only two of the original three education levels.) Considering the 2-way ANOVA analysis for this, we may find ourselves asking the question "If you have a factorial design and you are going to include an interaction term, why bother doing a two-way ANOVA - why not just do two separate t-tests?". In other words, the interaction allows for the effect of Education to differ for men and for women, so why not just do a t-test for men and a separate t-test for women? One issue is the concern over multiple comparisons. The other difference involves how error is estimated.

Let's do the ANOVA first:

```{r}
salaries.v2<-salaries[-which(salaries$Education=="No degree"),]
summary(lm(Salary~Gender+Education+Gender:Education,data=salaries.v2))
```

Now lets do the analogous t-test, looking at the difference in salary between women with a Masters degree and women with a PhD:

```{r}
t.test(salaries.v2$Salary[salaries.v2$Gender=="Female"&salaries.v2$Education=="Masters"],salaries.v2$Salary[salaries.v2$Gender=="Female"&salaries.v2$Education=="PhD"])
```

Notice that the difference in means is exactly what you would expect from the summary table from the lm() function (4.25) but the t-statistic (and hence the p-value) are now different. Why? In the lm(), data on both males and females are used to estimate the within-group error (in other words, the data are pooled to estimate MSwithin) whereas the t-test uses data for women only and therefore ends up with a slightly different estimate for the test statistic and the p-value. 

Which is correct? It depends on whether you think that there is value in pooling the errors. It also depends on how concerned you are about multiple comparisons with the separate t-tests.

Example #2: Nested design
------------

We will work through one of the classic examples from Sokal & Rohlf in which we are looking at the effect of a treatment on rat livers. We have three treatments, two rats per treatment, three liver samples per rat, and two measurements of each liver sample. The experimental design is sketched out here:

```{r echo=FALSE, fig.align='center', fig.cap='Nested design of the rat experiment.', out.width='100%'}
knitr::include_graphics('RatDesign.png')
```

```{r}
rats<-read.csv("_data/rats.txt",header=T)
attach(rats)
Treatment<-factor(TREAT)
Rat<-factor(RAT)
Liver<-factor(PREP)
anova(lm(GLYCO~TREAT))
```

It looks like Treatment is highly significant! But...we made a big mistake here because we have assumed that all 36 liver measurements are independent samples when we know that they are not because they only come from 6 rats (2 for each treatment).

What happens if we simply average the Glycogen context for each rat and redo the ANOVA?

```{r}
tapply(GLYCO,list(TREAT,RAT),mean)
anova(lm(c(tapply(GLYCO,list(TREAT,RAT),mean))[!is.na(c(tapply(GLYCO,list(TREAT,RAT),mean)))==TRUE]~factor(c(1,2,3,1,2,3))))
```

Analyzed this way we see that Treatment is not in fact significant, but averaging over all the liver samples precludes the possibility of looking at variation at the other scales of study (within rat, between measurements). Nested designs allow us to do just that.

To analyze nested models, we use the '/' operator to show how things are nested, and we add a new error term to tell R how the errors are nested as well.

```{r}
model<-aov(GLYCO~TREAT+Error(RAT/PREP))
summary(model)
```

Note that if you leave the error term off, the sum of squares calculated are correct, but the F ratios are wrong and, as a result, the p-values are way too small.

Example #3: Nested design
----------------

Case study taken from Logan (see Figure 2): "In an unusually detailed preparation for an Environmental Effects Statement for a proposed discharge of dairy wastes in the Curdies River, in western Victoria, a team of stream ecologists wanted to describe the basic patterns of variation in a stream invertebrate thought to be sensitive to nutrient enrichment. As an indicator species, they focused on a small flatworm, Dugesia, and started by sampling populations of this worm at a range of scales. They sampled in two season, they sampled three randomly chosen (well, haphazardly, because sites are nearly always chosen to be close to road access) sites. A total of six sites in all were visited, 3 in each season. At each site, they sampled six stones, and counted the number of flatworms on each stone." 
Why is this a nested design and not a factorial (crossed) design? Because the sites chosen in each season are not the same sites, they are randomly selected in each season depending on logistical constraints.

```{r}
worms<-read.csv("_data/flatworms.csv",header=T)
worms$Site<-factor(worms$Site)
```

Question: What are the main hypotheses being tested?

$H_{0}$ for Effect #1: There is no effect of season on flatworm density

$H_{0}$ for Effect #2: There is no added variance due to the random variable Sites

First, let's check the assumption of normality by looking at boxplots of the data

```{r}
boxplot(Density~Season,data=worms)
```

We see that Winter has significantly more variance than Summer. What do we do? We try and transform Density so as to stabilize variance between two seasons. The original authors use a fourth-root transformation, so we will try that...

```{r}
boxplot(Density^(1/4)~Season,data=worms)
```

A log transformation would also have worked.There are strategies for finding the best transformation but we don't have time to get into that.

Question: What are the correct F-ratios and the appropriate degrees of freedom?

(See Table 1)

Let's fit the model in R:

```{r}
model2<-aov(Density^(1/4)~Season+Error(Site),data=worms)
summary(model2)
summary(lm(model2))
```

**Question: How do we interpret the model? The F-ratio for Season is significant - what does this mean biologically? If we were to go back and redesign the study to maximize power, what would we do?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
If a nested design was to be used, then since SITES are the replicates for the effect of SEASON, then power is maximized by having more sites. Whilst having more stones may increase the precision of the measure of Dugusia within a site, it will not improve the power of the test of SEASON.
</span>
</details> 

Let's fit the model using the untransformed data just to compare:

```{r}
model3<-aov(Density~Season+Error(Site),data=worms)
summary(model3)
```

Now we get a grossly inflated level of significance for Season driven by the larger variance of the Winter densities.

Example #4: Randomized Block Design
--------------

(From Quinn, Keough, and Carey; see Figure 3) A plant pathologist wanted to examine the effects of two different strengths of tobacco virus on the number of lesions on tobacco leaves. She knew from pilot studies that leaves were inherently very variable in response to the virus. In an attempt to account for this leaf to leaf variability, both treatments were applied to each leaf. Eight individual leaves were divided in half, with half of each leaf incoculated with weak strength virus and the other half inoculated with strong virus.

**Question: What kind of analysis is this?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
A randomized block design, with Leaf as the "blocking factor".
</span>
</details> 

So in this case, the leaves are considered the "blocks" and each treatment is represented once in each block. A completely randomized design would have had 16 leaves, with 8 whole leaves randomly allocated to each treatment.

```{r}
tobacco<-read.csv("_data/tobacco.csv")
head(tobacco)
```

**Question: What are the main hypotheses being tested?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
$H_{0}$ for Effect #1: There is no effect of treatment on lesion number within each leaf block.

$H_{0}$ for Effect #2: There is no added variance due to the random variable Leaf. (In other words, there is no effect of the blocking factor leaf.)

We can't actually test for an interaction between Block and Treatment in this design, but we can look at the data using an "Interaction plot".

```{r}
boxplot(Number~interaction(Leaf,Treatment),data=tobacco)
```

The interaction plot suggests that there is some evidence of an interaction. Although the number of lesions appear to be greater in strongly innoculated leaves than the weakly innoculated leaves in most of the leaf pairs (blocks), this trend is either absent or reversed in two of the eight (1/4) of the leaf pairs. As a result, the test of block may not be reliable, and the power of the main test of treatment is reduced.
</span>
</details> 

**Question: What is the right R command to fit this model?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
```{r}
model4<-aov(Number~Treatment+Error(Leaf/Treatment),data=tobacco)
summary(model4)
```

Note that Treatment is WITHIN Leaf, not the other way around.
</span>
</details> 

**Question: What are the relevant F-ratios here? **

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Going back to our table for Blocked design, we see that the F-ratio is MS/MS_resid.
</span>
</details> 

**Question: What is the biological interpretation here?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Strongly innoculated tobacco leaves were found to have significantly higher mean numbers of lesions than weakly innoculated leaves. Leaf pairs (blocks) explained substantial amounts of the variation and therefore probably contributed to the sensitivity of the main test of treatment - thereby justifying the blocking design over a completely randomized design.
</span>
</details>

How would we fit this with a mixed model?

```{r}
library(lme4)
model5<-lmer(Number~Treatment+(1|Leaf),tobacco)
summary(model5)
```


Example #5: Nested design
--------------

In an experiment on eye color, each male fly is mated with four different female flies. Two offspring are born from each mating, and the intensity of eye color among these offspring are measured. The question is, how much variation in eye color is due to differences between females and how much is due to differences between males?

```{r}
flies<-read.table("_data/flies.txt",header=T)
flies$male<-factor(flies$male)
flies$female<-factor(flies$female)
model6<-aov(eye~male/female+Error(male/female),data=flies)
summary(model6)
```

**Question: What is the interpretation of this result?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Females within male are much more important!
</span>
</details>
