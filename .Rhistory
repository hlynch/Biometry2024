{
aggregated_abundance <- matrix.with.index[selected==1,(19+i):(19+i+4)]%>%
summarise_all(sum)
trend.all[i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:5))$coef[2])
}
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
selected<-sample(binary,size=length(binary),replace=F)
matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- matrix.with.index[selected==1,(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) best.selected<-selected
}
prepared_ADPE[best.selected==1,]
matrix.with.index<- cbind(prepared_ADPE,selected=best.selected)
aggregated_abundance_all_selected <- matrix.with.index[best.selected==1,20:59]%>%
summarise_all(sum)
aggregated_abundance_all <- matrix.with.index[,20:59]%>%
summarise_all(sum)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e5,2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
?combn
combn(letters[1:4], 2)
combn(ADPE.time.series, 5)
combn(ADPE.time.series[1:5], 2)
temp<-combn(ADPE.time.series[1:5], 2)
dim(temp)
prepared_ADPE[prepared_ADPE$Site_ID%in%temp[,1]]
prepared_ADPE[prepared_ADPE$Site_ID%in%temp[,1],]
prepared_ADPE[prepared_ADPE$Site_ID%in%temp[,2],]
window.length<-5
portfolio.size<-3
trend.all<-vector(length=(39-window.length+2))
selected<-c(rep(1,times=nrow(prepared_ADPE)))
matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:(39-window.length+2))
{
aggregated_abundance <- matrix.with.index[selected==1,(19+i):(19+i+4)]%>%
summarise_all(sum)
trend.all[i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:5))$coef[2])
}
window.length<-5
portfolio.size<-3
trend.all<-vector(length=(39-window.length+2))
selected<-c(rep(1,times=nrow(prepared_ADPE)))
matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:(39-window.length+2))
{
aggregated_abundance <- matrix.with.index[selected==1,(19+i):(19+i+4)]%>%
summarise_all(sum)
trend.all[i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:5))$coef[2])
}
Combinations<-combn(ADPE.time.series, portfolio.size)
num.iterations<-dim(Combinations)[2]
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
num.iterations
for (k in 1:5)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%temp[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) best.selected<-selected
}
trend
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%temp[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) best.selected<-selected
}
k
dim(Combinations)
Combinations<-combn(ADPE.time.series, portfolio.size)
num.iterations<-dim(Combinations)[2]
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%ADPE.time.series[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) best.selected<-selected
}
Combinations<-combn(ADPE.time.series, portfolio.size)
num.iterations<-dim(Combinations)[2]
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) best.selected<-selected
}
hist(correlations.scores)
hist(correlations.scores,breaks=30)
prepared_ADPE[best.selected==1,]
head(combinations)
head(Combinations)
head(Combinations[,1:5])
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T)) k.best<-k
}
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
#    binary<-c(rep(1,times=portfolio.size),rep(0,times=nrow(prepared_ADPE)-portfolio.size))
#    selected<-sample(binary,size=length(binary),replace=F)
#    matrix.with.index<- cbind(prepared_ADPE,selected=selected)
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T))
{
k.best<-k
print(k)
}
}
k
dim(Combinations)
k.best
prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],]
aggregated_abundance_all_selected <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],20:59]%>%
summarise_all(sum)
aggregated_abundance_all <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],20:59]%>%
summarise_all(sum)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e5,2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e4,2e5))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
correlation.scores[k.best]
correlations.scores[k.best]
prepared_ADPE$Site_ID%in%Combinations[,k.best]
aggregated_abundance_all_selected/aggregated_abundance_all
aggregated_abundance_all_selected
prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],20:59]
aggregated_abundance_all_selected <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],20:59]%>%
summarise_all(sum)
aggregated_abundance_all <- prepared_ADPE[,20:59]%>%
summarise_all(sum)
aggregated_abundance_all_selected/aggregated_abundance_all
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e4,2e5))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e4,2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e4,1.2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
prepared_ADPE <- read_csv("Desktop/prepared_ADPE.csv")
prepared_ADPE_long<-prepared_ADPE[prepared_ADPE$Site_ID%in%ADPE.time.series,]
window.length<-5
portfolio.size<-3
trend.all<-vector(length=(39-window.length+2))
for (i in 1:(39-window.length+2))
{
aggregated_abundance <- prepared_ADPE[,(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend.all[i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:5))$coef[2])
}
plot(trend.all)
correlations.scores<-vector(length=num.iterations)
trend<-matrix(nrow=num.iterations,ncol=(39-window.length+2))
for (k in 1:num.iterations)
{
for (i in 1:39-window.length+2)
{
aggregated_abundance <- prepared_ADPE_long[prepared_ADPE_long$Site_ID%in%Combinations[,k],(19+i):(19+i+window.length-1)]%>%
summarise_all(sum)
trend[k,i]<-as.numeric(lm(as.numeric(aggregated_abundance)~c(1:window.length))$coef[2])
}
correlations.scores[k]<-cor(trend[k,],trend.all)
if (correlations.scores[k]==max(correlations.scores,na.rm=T))
{
k.best<-k
print(k)
}
}
prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],]
aggregated_abundance_all_selected <- prepared_ADPE[prepared_ADPE$Site_ID%in%Combinations[,k.best],20:59]%>%
summarise_all(sum)
aggregated_abundance_all <- prepared_ADPE[,20:59]%>%
summarise_all(sum)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(2e4,1.2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
aggregated_abundance_all_selected/aggregated_abundance_all
aggregated_abundance_all
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(0,2e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(0,3e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(0,5e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(0,6e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(1e6,7e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
plot(seq(1980:2019),aggregated_abundance_all,pch=16,ylim=c(1.5e6,7e6))
lines(seq(1980:2019),aggregated_abundance_all_selected*as.numeric((aggregated_abundance_all[1]/aggregated_abundance_all_selected[1])),col=2)
aggregated_abundance_all_selected/aggregated_abundance_all
hist(correlations.scores)
max(correlations.scores,na.rm=T)
=5.68/2.15
5.68/2.15
5.407/1.42
library(readr)
Vessel_data <- read_csv("Dropbox/Biometry/Week 9 Correlation and regression/Week 9 Problem set/Vessel_data.csv")
View(Vessel_data)
df<-data.frame(Length=Vessel_data$length_meters,MaxSpeed=Vessel_data$max_speed_knots)
nls.model.actual<-nls(MaxSpeed~beta0+beta1*Length,data=df,start=list(beta0=1,beta1=1),trace=T)
plot(df$Length,df$MaxSpeed)
x.vals<-seq(min(df$Length,na.rm=T),max(df$Length,na.rm=T),1)
coef.beta0<-c()
coef.beta1<-c()
for (i in 1:1000) #feel free to choose a different number here
{
indices<-sample(seq(1,nrow(df)),replace=T)
df.new<-df[indices,]
nls.model<-nls(MaxSpeed~beta0+beta1*Length,data=df.new,start=list(beta0=1,beta1=1),trace=T)
lines(x.vals,coef(nls.model)[1]+(coef(nls.model)[2])*x.vals,col=rgb(0.1,0.1,0.1,0.2)) #what is the rgb function doing for me here?
coef.beta0<-c(coef.beta0,coef(nls.model)[1])
coef.beta1<-c(coef.beta1,coef(nls.model)[2])
}
lines(x.vals,coef(nls.model.actual)[1]+(coef(nls.model.actual)[2])*x.vals,col="red",lwd=2)
plot(coef.beta0,coef.beta1)
hist(beta1)
hist(coef.beta1)
install.packages("circular")
library(circular)
?mean.circular
circular(runif(50, circular(0), pi))
plot(circular(runif(50, circular(0), pi)))
x <- circular(runif(50, circular(0), pi))
mean(x)
?var.circular
var(x)
plot(x)
angular.deviation(x)
3.14/4
x
angular.deviation(c(0.1,0.2,0.3,0.4))
angular.deviation(as.circular(c(0.1,0.2,0.3,0.4)))
?as.circular
x.circular<-circular(x,type="degrees",type="directionstemplate="geographics")
x.circular<-circular(x,type="degrees",type="directions",template="geographics")
x.circular<-circular(x,units="degrees",type="directions",template="geographics")
plot(x.circular)
angular.deviation(x)
angular.deviation(x.circular)
rose.diag(x)
library(maptools)
library(rgdal)
library(raster)
library(maps)
install.packages("maps")
library(maps)
library(mapdata)
install.packages("mapdata")
library(mapdata)
library(ggmap)
library(marmap)
library(shapefiles)
read.shapefile("/Users/hjlynch/Library/CloudStorage/Dropbox/Antarctic remote survey work/GIS baselayers/ADDcstpoly01.shp")
read.shapefile("\Users\hjlynch\Library\CloudStorage\Dropbox\Antarctic remote survey work\GIS baselayers\ADDcstpoly01.shp")
read.shapefile("/Users/hjlynch/Library/CloudStorage/Dropbox/Antarctic remote survey work/GIS baselayers/ADDcstpoly01")
ADDcstpoly01<-read.shapefile("/Users/hjlynch/Library/CloudStorage/Dropbox/Antarctic remote survey work/GIS baselayers/ADDcstpoly01")
plot(ADDcstpoly01)
library(rgdal)
my_spdf <- readOGR(
dsn= paste0(getwd(),"/Users/hjlynch/Library/CloudStorage/Dropbox/Antarctic remote survey work/GIS baselayers/ADDcstpoly01") ,
verbose=FALSE
)
?readODR
?readOGR
penmap(species_map = c("ADPE"))
library(mapppdr)
install.packages("sf")
library(mapppdr)
install.packages("units")
var(c(7,3,6))
var(c(9,3,6))
sqrt((47.9828^2) + (5.9191^2))
48.34651*(200/216)
sqrt((730.8735^2) + (45.9769^2))
732.3182*(100/216)
732.3182*(50/216)
732.3182*(20/216)
148/(64+148+62)
20/(16+20+19)
811*0.46
811*0.54
925*0.36
(811^2)*((811-373)/811)*(6342.806/373)+(295^2)*((295-209)/295)*(2156.25/209)
sqrt(6302168)
sqrt(6302168)*2
5020/116891
480.58+466.59+11395.00+3230.27
sqrt(0.8^2 + 0.63^2)
library("circular")
BRYS_06JAN2023_nesting_penguins_edge <- read.csv(file = "/Users/hjlynch/Downloads/UAV Images CSV Files'/BRYS_06JAN2023_nesting_penguins_edge_indices.csv", header = TRUE)
BRYS_06JAN2023_nesting_penguins_edge <- read.csv(file = "\Users\hjlynch\Downloads\UAV Images CSV Files'\BRYS_06JAN2023_nesting_penguins_edge_indices.csv", header = TRUE)
pwd()
getwd()
BRYS_06JAN2023_nesting_penguins_edge <- read.csv(file = "Downloads/UAV Images CSV Files'/BRYS_06JAN2023_nesting_penguins_edge_indices.csv", header = TRUE)
library(readr)
BRYS_06JAN2023_nesting_penguins_edge_indices <- read_csv("Desktop/UAV Images CSV Files/BRYS_06JAN2023_nesting_penguins_edge_indices.csv")
View(BRYS_06JAN2023_nesting_penguins_edge_indices)
x <- circular(BRYS_06JAN2023_nesting_penguins_edge$sitting_angles[BRYS_06JAN2023_nesting_penguins_edge$edge_indices == 0], units="degrees", type="directions", template="geographics")
ls()
View(BRYS_06JAN2023_nesting_penguins_edge_indices)
x <- circular(BRYS_06JAN2023_nesting_penguins_edge_indices$sitting_angles[BRYS_06JAN2023_nesting_penguins_edge$edge_indices == 0], units="degrees", type="directions", template="geographics")
x <- circular(BRYS_06JAN2023_nesting_penguins_edge_indices$sitting_angles[BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices == 0], units="degrees", type="directions", template="geographics")
x
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices,
units="degrees",
template="geographics")
BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices
BRYS_06JAN2023_nesting_penguins_edge_indices
x <- circular(BRYS_06JAN2023_nesting_penguins_edge_indices$sitting_angles, units="degrees", type="directions", template="geographics")
mean(x, trim = 0, na.rm = FALSE)
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices,
units="degrees",
template="geographics")
core<-circular(BRYS_06JAN2023_nesting_penguins_edge_indices$sitting_angles[BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices==0], units="degrees", type="directions", template="geographics")
edge<-circular(BRYS_06JAN2023_nesting_penguins_edge_indices$sitting_angles[BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices==1], units="degrees", type="directions", template="geographics")
rose.diag(core, pch = 16, cex = 1, axes = TRUE, shrink = .9, col = 5, prop = 3, bins = 34, upper = TRUE, ticks = TRUE, units= "degrees", type="directions", template="geographics")
rose.diag(edge, pch = 16, cex = 1, axes = TRUE, shrink = .9, col = 5, prop = 3, bins = 34, upper = TRUE, ticks = TRUE, units= "degrees", type="directions", template="geographics")
rose.diag(core, pch = 16, cex = 1, axes = TRUE, shrink = .9, col = 5, prop = 3, bins = 34, upper = TRUE, ticks = TRUE, units= "degrees", type="directions", template="geographics")
warnings()
rose.diag(core, pch = 16, cex = 1, axes = TRUE, shrink = .9, col = 5, prop = 3, bins = 34, upper = TRUE, ticks = TRUE, units= "degrees", template="geographics")
rose.diag(edge, pch = 16, cex = 1, axes = TRUE, shrink = .9, col = 5, prop = 3, bins = 34, upper = TRUE, ticks = TRUE, units= "degrees", template="geographics")
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices,
units="degrees",
template="geographics")
?wallraff.test
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices, ref = ref = circular(c(mean(core), mean(edge)), units="degrees",template="geographics"), units="degrees",template="geographics")
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices, ref = circular(c(mean(core), mean(edge)), units="degrees",template="geographics"), units="degrees",template="geographics")
wallraff.test(x, BRYS_06JAN2023_nesting_penguins_edge_indices$edge_indices, ref = circular(c(mean(edge), mean(core)), units="degrees",template="geographics"), units="degrees",template="geographics")
mean(edge)
mean(core)
var(edge)
var(core)
install.packages("openai")
library(openai)
Sys.setenv(
OPENAI_API_KEY = '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'
)
reate_image("An astronaut riding a horse in a photorealistic style")
create_image("An astronaut riding a horse in a photorealistic style")
create_image("A penguin scientist")
?openai
??openai
create_embedding(
model = "text-embedding-ada-002",
input = c(
"Ah, it is so boring to write documentation",
"But examples are really crucial"
)
)
answer<-create_embedding(
model = "text-embedding-ada-002",
input = c(
"Ah, it is so boring to write documentation",
"But examples are really crucial"
)
)
answer
names(answer)
answer$object
answer$data
names(answer)
dim(answer$data)
head*answer$data
head(answer$data)
answer$data[1,1]
answer$data[1,2]
answer$data[1,3]
plot(answer$data[1,3])
dim(answer$data[1,3])
length(answer$data[1,3])
answer$data[1,3]
plot(answer$data[1,3][[1]])
plot(answer$data[2,3][[1]])
plot(answer$data[1,3][[1]],answer$data[2,3][[1]])
dim(answer$data)
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("text")
install.packages("tune")
install.packages("textmineR")
1500+1129040+150000+137098+728339+55000+411573+134806+48200+2600+95696+5000+1815860+2992930+300000+41561+395475+108017+419804+8000+113120+782840+41317+1000+476608
1500+1129040+150000+137098+728339+55000+411573+134806+48200+2600+95696+5000+1815860+2992930+300000+41561+395475+108017+419804+8000+113120+782840+41317+1000+476608+10000
hist(dgamma(1000,1,1))
hist(rgamma(1000,1,1))
hist(rnorm(1000,0,100))
hist(exp(rnorm(1000,0,100)))
hist(exp(rnorm(1000,0,1)))
hist(exp(rnorm(1000,0,2)))
hist(exp(rnorm(1000,0,1)))
hist(exp(rnorm(1000,0,1)))
hist(exp(rnorm(1000,0,1)))
hist(exp(rnorm(1000,0,2)))
hist(exp(rnorm(1000,0,1.2)))
1/1.2
sd(c(rep(0,times=18),rep(1,times=2)))
sd(c(rep(0,times=19),rep(1,times=1)))
sd(c(rep(0,times=19),rep(1,times=1)))+sd(c(rep(0,times=17),rep(3,times=1)))
sd(c(rep(0,times=18),rep(1,times=2)))+sd(c(rep(0,times=18),rep(1,times=2)))
sd(c(rep(0,times=17),rep(1,times=3)))+sd(c(rep(0,times=19),rep(1,times=1)))
a<-c(1,2,3,4,5,6)
b<-c(2,6,3,4,5,1)
# plot a vs b
plot(a,b)
sd(c(rep(0,times=19),1))
sd(c(rep(0,times=19),1))*9
sd(c(rep(0,times=9),rep(1,times=11))
)
2.012461+0.5104178
(2.012461+0.5104178)^2
library(rjson)
fromJSON( json_str, "/Users/hjlynch/Desktop/conspiracy_subreddit/utterances.jsonl", method = "C", unexpected.escape = "error", simplify = TRUE )
json_str<-'{"id": "ckwzy", "conversation_id": "ckwzy", "text": "Wikileaks is one of the most awesome creations that the Internet has spawned. But we need 300 of these, not just one. I think there are some really interesting lessons to be learned from the Piratebay and filesharing witchhunts, and the technological and political response by the filesharing community.\r\n\r\nJust making some points up that I think are relevant here:\r\n\r\n**1) Remove points of control**\r\n\r\nMost important thing of all.\r\n\r\nThe creation of magnet links using Peer exchange and distributed hash tables (PEX - DHT) means that there is no need for a central hub tracking torrents. \r\n\r\n**2) distribute liability in very small pieces**\r\nBreak down the value chain / network in as atomic pieces as possible while still contributing.   \r\n\r\n**3) Enable deniability**\r\nthe rubberhose crypto software Assange programmed is one approach to establishing this. TAHOE FS is another http://allmydata.org/~zooko/lafs.pdf\r\n\r\n**4) Enable distributed mechanisms of Trust**\r\nWikileaks has said they by necessity must grow slowly, since Trust is such an important aspect of participating in a whistleblower / uncensored file sharing network. But if there is both a protocol and set of tools to copycat the beneficial functions of wikileaks without using the centralized site, then trust would be between many small groups leveraging an open platform, allowing the network as such to grow more quickly.\r\n\r\n**5) Enable mechanisms of distributed, anonymous and recognized work**\r\nSomething that impressed me in hearing about wikileaks experience is that you can't expect provided raw data to be analyzed, and my feeling is that that the division of work needs to be structured with online tools, peer review likewise. Make it a game, unlock achievements and have fun while doing valuable work.\r\n\r\nwikileaks has pointed the way to the future, let us widen the path that they have blazed!!\r\n\r\n", "speaker": "guchivoia", "meta": {"score": 15, "top_level_comment": null, "retrieved_on": 1522932108, "gilded": 0, "gildings": null, "subreddit": "conspiracy", "stickied": false, "permalink": "/r/conspiracy/comments/ckwzy/diy_wikileaks/", "author_flair_text": ""}, "reply-to": null, "timestamp": 1277989059, "vectors": []}'
json_str<-'"id": "ckwzy", "conversation_id": "ckwzy", "text": "Wikileaks is one of the most awesome creations that the Internet has spawned. But we need 300 of these, not just one. I think there are some really interesting lessons to be learned from the Piratebay and filesharing witchhunts, and the technological and political response by the filesharing community.\r\n\r\nJust making some points up that I think are relevant here:\r\n\r\n**1) Remove points of control**\r\n\r\nMost important thing of all.\r\n\r\nThe creation of magnet links using Peer exchange and distributed hash tables (PEX - DHT) means that there is no need for a central hub tracking torrents. \r\n\r\n**2) distribute liability in very small pieces**\r\nBreak down the value chain / network in as atomic pieces as possible while still contributing.   \r\n\r\n**3) Enable deniability**\r\nthe rubberhose crypto software Assange programmed is one approach to establishing this. TAHOE FS is another http://allmydata.org/~zooko/lafs.pdf\r\n\r\n**4) Enable distributed mechanisms of Trust**\r\nWikileaks has said they by necessity must grow slowly, since Trust is such an important aspect of participating in a whistleblower / uncensored file sharing network. But if there is both a protocol and set of tools to copycat the beneficial functions of wikileaks without using the centralized site, then trust would be between many small groups leveraging an open platform, allowing the network as such to grow more quickly.\r\n\r\n**5) Enable mechanisms of distributed, anonymous and recognized work**\r\nSomething that impressed me in hearing about wikileaks experience is that you can't expect provided raw data to be analyzed, and my feeling is that that the division of work needs to be structured with online tools, peer review likewise. Make it a game, unlock achievements and have fun while doing valuable work.\r\n\r\nwikileaks has pointed the way to the future, let us widen the path that they have blazed!!\r\n\r\n", "speaker": "guchivoia", "meta": {"score": 15, "top_level_comment": null, "retrieved_on": 1522932108, "gilded": 0, "gildings": null, "subreddit": "conspiracy", "stickied": false, "permalink": "/r/conspiracy/comments/ckwzy/diy_wikileaks/", "author_flair_text": ""}, "reply-to": null, "timestamp": 1277989059, "vectors": []'
install.packages('devtools')
install.packages("devtools")
devtools::install_github('CCheCastaldo/mapppdr', build_vignettes = TRUE)
hist(rgamma(1000,2,2))
hist(rgamma(1000,4,5))
mean(rgamma(1000,2,2))
mean(rgamma(1000,4,4))
hist(rgamma(1000,2,2))
hist(rgamma(1000,10,10))
hist(rgamma(1000,2,2))
hist(rgamma(1000,10,10))
setwd("~/Documents/Projects/Biometry2024")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
install.packages("bookdown")
setwd("~/Documents/Projects/Biometry2024")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
setwd("~/Documents/Projects/Biometry2024")
bookdown::render_book("index.Rmd")
